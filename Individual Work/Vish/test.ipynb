{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = r\"..\\..\\data\\Lectures\\\\\"  \n",
    "persist_dir = \"pdf_store\"  # Directory where the vector store will be saved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf',\n",
       " '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf',\n",
       " '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf',\n",
       " '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf',\n",
       " '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf',\n",
       " '..\\\\..\\\\data\\\\Lectures\\\\\\\\DS_3001_Day 2_Case_Study.pdf',\n",
       " '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf',\n",
       " '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf',\n",
       " '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf',\n",
       " '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf',\n",
       " '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf',\n",
       " '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf',\n",
       " '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf',\n",
       " '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf',\n",
       " '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf',\n",
       " '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf',\n",
       " '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf',\n",
       " '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidy_data_dplyr.pdf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdfs = [os.path.join(dirpath, f) for dirpath, _, files in os.walk(pdf_path) for f in files if f.lower().endswith('.pdf')]\n",
    "all_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF from ..\\..\\data\\Lectures\\\\3001_ETA.pdf\n",
      "33\n",
      "Loading PDF from ..\\..\\data\\Lectures\\\\Clustering_InClass_9.28.21-1.pdf\n",
      "57\n",
      "Loading PDF from ..\\..\\data\\Lectures\\\\Clustering_InClass_9.28.21-2.pdf\n",
      "57\n",
      "Loading PDF from ..\\..\\data\\Lectures\\\\Clustering_InClass_9.28.21.pdf\n",
      "60\n",
      "Loading PDF from ..\\..\\data\\Lectures\\\\Decision_Trees_4.28.21.pdf\n",
      "40\n",
      "Loading PDF from ..\\..\\data\\Lectures\\\\DS_3001_Day 2_Case_Study.pdf\n",
      "5\n",
      "Loading PDF from ..\\..\\data\\Lectures\\\\fun_with_functions_dplyr.pdf\n",
      "28\n",
      "Loading PDF from ..\\..\\data\\Lectures\\\\ggplot_overview.pdf\n",
      "137\n",
      "Loading PDF from ..\\..\\data\\Lectures\\\\knitr_reference.pdf\n",
      "35\n",
      "Loading PDF from ..\\..\\data\\Lectures\\\\Knn and Prob.pdf\n",
      "88\n",
      "Loading PDF from ..\\..\\data\\Lectures\\\\machine_learning_bootcamp_II copy.pdf\n",
      "55\n",
      "Loading PDF from ..\\..\\data\\Lectures\\\\machine_learning_bootcamp_II.pdf\n",
      "55\n",
      "Loading PDF from ..\\..\\data\\Lectures\\\\machine_learning_III.pdf\n",
      "55\n",
      "Loading PDF from ..\\..\\data\\Lectures\\\\machine_learning_overview.pdf\n",
      "46\n",
      "Loading PDF from ..\\..\\data\\Lectures\\\\Overview_ML_and_Clustering_InClass_3.21.pdf\n",
      "77\n",
      "Loading PDF from ..\\..\\data\\Lectures\\\\PerfMet.pdf\n",
      "35\n",
      "Loading PDF from ..\\..\\data\\Lectures\\\\tidydata_reference_Thursday_II.pdf\n",
      "118\n",
      "Loading PDF from ..\\..\\data\\Lectures\\\\tidy_data_dplyr.pdf\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pages = []\n",
    "for pdf in all_pdfs:\n",
    "    print(f\"Loading PDF from {pdf}\")\n",
    "    loader = PyPDFLoader(pdf)\n",
    "    pdf_pages = loader.load()\n",
    "    print(len(pdf_pages))\n",
    "    pages.extend(pdf_pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1009 Pages in the PDF\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 0}, page_content='Overview and Data Science\\nBrian Wright\\nbrianwright@virginia.edu\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 1}, page_content='2Course Administration \\n\\uf0d8Everybody Reads Even Computers: Text Mining'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 2}, page_content='Final Projects\\n\\uf0d8Work individually and use one of the areas below to answer \\na broad questions related to a given dataset.  I’ll provide several datasets for you to potential use, but you are also welcome to chose your own.  You can also use any dataset from the class if you choose.\\n\\uf0d8Topics we will/have covered that can be a focus of the final \\nproject:\\n\\uf076Data Visualization \\n\\uf076Fairness/Bias\\n\\uf076Text Mining\\n\\uf076KNN\\n\\uf076Tree based methods\\n\\uf0d8Ensemble –Random Forrest – time permitting '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 3}, page_content='Final Projects\\n\\uf0d8Generate a publishable Rmarkdown document with the following \\nsections:\\n1.Question and background information on the data and why you are \\nasking this question(s).  References to previous research/evidence \\ngenerally would be nice to include. \\n2.Exploratory Data Analysis –Initial summary statistics and graphs with \\nan emphasis on variables you believe to be important for your analysis. \\n3.Methods –Techniques you are using to address your question and the \\nresults of those methods.\\n4.Conclusions –What can you say about the results of the methods \\nsection as it relates to your question.\\n5.Future work –What additional analysis is needed or what limited your \\nanalysis on this project. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 4}, page_content='“Text Mining” '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 5}, page_content='Broader field: What is Exploratory Text \\nAnalytics?\\n(ETA)\\nMuch of the following content is from the Exploratory Text Analytics \\nClass as part of UVA’s MSDS taught by Rafael Alvarado\\n6Text Mining '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 6}, page_content='ETA refers to text analytics applied to long -form \\ntexts with the purpose of surfacing their latent \\ncognitive, cultural, and social content\\nTexts: Novels, essays, newspaper articles, letters, \\nblogs, journal articles, etc.\\nContent : concepts, categories, themes, emotions, \\nevents ...\\n7Text Mining'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 7}, page_content='Its called \"exploratory\" because it methods are \\nprimarily unsupervised and designed to support \\nhuman -in-the-loop interpretation\\n“interpretation support”\\n8Text Mining'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 8}, page_content=''),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 9}, page_content='From Kevin Murphy, 2012,  Machine Learning: A Probabilistic \\nPerspective, p. 9.\\nUnsupervised learning is about \\nknowledge discovery'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 10}, page_content='Some Unsupervised Methods:\\n11Clustering —K-means, hierarchical, etc.\\nTopic Modeling —PCA, LSI/A, NMF, LDA, etc.\\nWord Embedding —SGNS (word2vec), etc.\\nSentiment Analysis --dictionary- based methods, etc.Text Mining'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 11}, page_content='Extracting features from unstructured data to support machine \\nlearning\\nE.g. principal components are document features\\nInformation retrieval tasks such as document summarization, \\ngrouping, classification, and knowledge discovery\\nProvide data to support language modeling, including grammar, \\nsyntax, and pragmatics for NLP and computational linguistics\\nExtraction and representation of cultural and social patternsfrom text —\\nSee cultural analytics and culturomics\\n12Text Mining: Applications of ETA'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 12}, page_content='13Coined by Harvard researchers Jean -Baptiste Michel and \\nErez Lieberman Aiden , who helped create Google’s NGram \\nViewer\\nMichel and Aiden (2010):\\nInferences about culture made from trends in n- gram \\nusageAn n- gram is a sequence of n words\\nBased on Google Books\\nTransformed the field of text analytics\\nBased on application of genomic sequencing techniques to \\ntext ( See recent book, Uncharted)Text Mining: Culturomics'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 13}, page_content='14\\nTwo examples of inferences \\ndrawn from n- gram trends\\n(Hand 2011) \\nText Mining'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 14}, page_content='https://books.google.com/ngrams\\n15Text Mining\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 15}, page_content='ETA builds on the domain knowledge of textual \\ntheory and criticism from history, literary studies, \\nanthropology, sociolinguistics, religious studies, etc.\\nText is regarded as a first -class object of study,\\nnot an incidental container of language data\\n“We” study text as text\\nText is not necessarily language\\n16Text Mining'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 16}, page_content='Text as Text: Langue and Parole\\nLanguage (langue)\\nGrammar\\nCompetence \\nFinite rules (grammar)\\nSystem\\nCollective\\nUnconscious\\nStructure\\nLatentSpeech (parole)\\nDiscourse\\nPerformance\\nIndefinite patterns \\n(discourse)\\nUsage\\nIndividual\\nConscious\\nEvent\\nObserved\\n17'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 17}, page_content='\"Language\" is divided into \\ngrammar and discourse\\nDiscourse is expressed a \\nspeech and writing\\nWriting is \"fixed \\ndiscourse\"'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 18}, page_content='Writing is the direct \\nentextualization of \\ndiscourse in a document\\nDocuments have a \\nmaterial form (medium ) \\nand an \"immaterial\" \\ndimension --the text as \\nstructured sequence of symbols\\nText is not \\n\"unstructured\"!'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 19}, page_content='20Text Mining: Some Substantive Properties of Text\\nAbove all, texts contain cultural information\\nThey function as social genes that encode and express \\nbeliefs, opinions, ideas, symbolism, etc. --think of Homer, \\nthe Bible, etc.\\nAs discourse, distinctive of human beings\\nTexts may also represent events\\nSocial media and newspapers are like social sensors\\nAs more and more social life becomes entextualized through \\nsocial media and other conduits (e.g. Internet of Things)\\nTexts contain granular representations of human behavior\\nIt is the principal means by which behavioral surplus is \\ncaptured'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 20}, page_content='So, culture is a complex system of human thought and behavior\\nthat exhibits a consistent pattern in society \\nIt is expressed and communicated\\nby symbolic forms\\nA  primary vehicle in our society for the expression and \\ntransmission of symbolic forms is the written word — texts\\nA premise of ETA is that texts “contain” cultural patterns and \\nthese may be discovered  through unsupervised methods\\n21'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 21}, page_content=\"ETA Related Fields (Antecedents)\\n22Computational Linguistics (CL)\\nUse of computers to represent and study human language\\nInformation Retrieval (IR)\\nDocument summarization, retrieval, indexing, classification based on \\ncontents and metadata\\nNatural Language Processing (NLP)\\nGet computers to understand and produce human language\\nText Mining (TM)\\nConvert text -as-unstructured -data into features for data mining + ML\\nDigital Humanities (DH) / Humanities ComputingCreate digital collections of primary textual sources and new forms of \\nscholarship → 1949 Father Busa's Index Thomisticus\"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 22}, page_content='23\\nA genealogy of ETA'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 23}, page_content='Note that text mining (TM) \\nand natural language processing (NLP) \\nare not the same thing\\nAlthough often used as synonyms, they \\nhave different concerns, approaches, \\nand methods\\nThey are, however, closely related\\n24Text Mining'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 24}, page_content='25Areas of Focus\\nNLP\\nLanguage models\\nTokenization\\nPart of speech labeling\\nNamed entity recognition\\nDependency parsing \\nSpeech generationTM\\nText as structured data\\nDocument classification\\nContent summarization\\nNetwork analysis\\nKnowledge discovery\\nHypothesis discoveryText Mining'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 25}, page_content='The functional relationship between NLP and TM\\n26Text Mining'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 26}, page_content='27Text Mining: Implementation in R\\n\\uf0d8Couple of packages in R that specialize in using text data:\\n\\uf076tm –fairly popular (used often with the corpus package)\\n\\uf076quanteda –developed by political scientist to analyze \\npolitically oriented text data\\n\\uf076Tidytext –tidyverse of text analysis –this is what we will \\nfocus on this week. \\n\\uf076textmineR –developed mostly for topic modelling'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 27}, page_content='28Text Mining: Implementation in R\\n\\uf0d8The first step with conducting text analysis is getting the \\ndata loaded into R so we can tokenize the dataset \\n\\uf0d8Text data comes in a wide variety of forms and can be \\ndifficult to wrangle into a data frame. We are going to use dataset that are in CSV but note this is often not the case. \\n\\uf0d8Tokenization means that we take a block of text and separate it into separate observations for each\\n\\uf076word,\\n\\uf076combination of 2, 3, or 4 words,\\n\\uf076sentence,\\n\\uf076or paragraph.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 28}, page_content='29Text Mining: Implementation in R\\n\\uf0d8The first step with conducting text analysis is getting the \\ndata loaded into R so we can tokenize the dataset \\n\\uf0d8Text data comes in a wide variety of forms and can be \\ndifficult to wrangle into a data frame. We are going to use dataset that are in CSV but note this is often not the case. \\n\\uf0d8Tokenization means that we take a block of text and separate it into separate observations for each\\n\\uf076word,\\n\\uf076combination of 2, 3, or 4 words,\\n\\uf076sentence,\\n\\uf076or paragraph.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 29}, page_content='30Text Mining: Implementation in R\\nSwitch over to R'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 30}, page_content='31Text Mining: Sentiment Analysis\\nSource: Text Mining with R'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 31}, page_content='32Text Mining: Sentiment Analysis\\nSource: Text Mining with R\\uf0d8Sentiment analysis for our purposes considers text to \\ncomposed of individual words that can have positive or negative meaning.  \\n\\uf0d8The tidytext package provides access to several lexicons that \\ncan work to classify words in our documents in a variety of ways according to sentiment. Examples:\\n\\uf076AFINN provides a scale from -5 to 5 for included words\\n\\uf076NRC – classifies words in categories of positive, \\nnegative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.\\n\\uf076Bing –Straight poss or neg\\n\\uf0d8Let’s take a look….back to R  '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\3001_ETA.pdf', 'page': 32}, page_content='33Text Mining: Topic Modelling\\nSource: Laten Dirichlet Allocation in R: Martin Ponweiser\\uf0d8Next Step in the text journal is Topic Modelling\\n\\uf0d8Topic models are “[probabilistic] latent variable models of \\ndocuments that exploit the correlations among the words and latent \\nsemantic themes” ( Blei and Lafferty, 2007). \\n\\uf0d8The name “topics” signifies the hidden, to be estimated, variable \\nrelations (=distributions)that link words in a vocabulary and their \\noccurrence in documents. \\n\\uf0d8Essentially think of Topic Modelling as creating clusters of words \\nthat are associated with a set of similar documents in a corpus.  \\n\\uf076As an example if you were to gather newspaper articles from \\nacross the country from three sections: Politics, Sports and \\nEntertainment. If we ran LDA it would like classify the individual stories into these three topics.  '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 0}, page_content='Machine Learning Overview, EDA and Clustering\\nBrian Wright\\nbrianwright@virginia.edu\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 1}, page_content='2\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 2}, page_content='3\\uf06eGiven  D :{2,4,10,12,3,11,20,25,30},  and  k=2 clusters\\n\\uf06eRandomly assign the means:  m1=3, m2=4\\n\\uf06eK1={2,3}, K2={4,10,12,11,20,25,30}, m1=2.5, m2=16\\n\\uf06eK1={2,3,4}, K2={10,12,11,20,25,30}, m1=3, m2=18\\n\\uf06eK1={2,3,4,10}, K2={12,11,20,25,30}, m1=4.75, m2=19.6\\n\\uf06eK1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\\n\\uf06eK1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\\n\\uf06eStop, since the clusters and the means found in \\nall subsequent iterations will be the same .Example of K -Means'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 3}, page_content='1.What is Machine Learning?\\n2.What is exploratory data analysis?\\n3.k-means clustering\\n–Does Congress vote in patterns?\\n4.Multi -dimensional k -means clustering\\n–Are NBA players compensated according to performance?Outline: Intro to Unsupervised ML\\n4'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 4}, page_content='•Exploratory data analysis or “EDA” is an approach where the intent is to see \\nwhat the data can tell us beyond modeling or hypothesis testing\\n–Data visualization is one of the most common forms of EDAWhat is exploratory data analysis?\\n5'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 5}, page_content='When data is too big or complex to be analyzed just by \\nvisualizing it, these types of analysis can help:\\n1.Clustering: compare pieces of data by measuring \\nsimilarity among them\\n2.Network analysis: analyze how people, places and \\nentities are connected to evaluate the properties and \\nstructure of a network \\n3.Text mining: analyze what large bodies of \\nunstructured or structured text sayTypes of exploratory data analysis\\n6'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 6}, page_content='The data inputs have (x)no target outputs (y)Unsupervised machine learning\\nInput x:\\nVoterOutput y:\\nNot given\\n(To be discovered)?\\nWe want to impose structure on the inputs (x)to say something \\nmeaningful about the data\\n7\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 7}, page_content='1.Technique for finding similarity between groups\\n2.Type of unsupervised machine learning\\n•Not the only class of unsupervised learning        \\nalgorithms\\n3.Similarity needs to be defined\\n•Will depend on attributes of data\\n•Usually a distance metricWhat is clustering?\\n8\\nKey assumption: data points that are “closer” together are related or similar'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 8}, page_content='•Haimowitz and Schwarz 1997 paper on \\nclustering for credit line optimization\\n–http://www.aaai.org/Papers/Workshops/1997/\\nWS-97-07/WS97- 07-006.pdf\\n•Cluster existing GE Capital customers based on similarity in use of their credit cards to pay bills and customers’ profitability to GE Capital\\n•Resulted in five clusters of consumer credit behavior\\n•Created classification model to predict customer type and offer tailored productsGE Capital case study: grouping clients\\n9'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 9}, page_content='Example use case General question Concept\\nDoes Congress vote in patterns? Is there a pattern ?\\nIs there structure in \\nunstructured data?k-means clustering\\nAre basketball players \"priced\" efficiently (based on performance)?How to uncover trends with many variables that you    can\\'t easily visualize?k-means clustering in \\nmany dimensionsConcept summary\\n12'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 10}, page_content='1.Data set consists of 427 members \\n(observations) \\n2.Members served a full year in 2013\\n3.Three vote types:\\n•“Aye”\\n•“Nay”\\n•“Other”Goal: to understand how polarized the \\nUS Congress isPolitical clustering\\nThe joint session of Congress on Capitol Hill \\nin Washington\\n13'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 11}, page_content='•How do we identify \\nswing votes?\\n–Lobbying\\n–Bridging party lines\\n•Assumption:\\n–Democrats and Republicans \\nvote among partisan lines, \\nwhich generates clustersEach data point represents a member of CongressFinding voting patterns\\n14'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 12}, page_content='Intra vs. inter- cluster distance\\nIntra-Cluster \\nDistanceInter-Cluster \\nDistance\\nObjective: minimize intra -cluster dis tance, maximize inter -cluster distance\\n15'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 13}, page_content='•The centroid is the average location \\nof all points in the cluster\\n•Another definition: the centroid minimizes the distance between a central location and all the data points in the cluster\\nNote: Centroids are generally not existing data points, rather locations in spacek-means clustering is based on \\ncentroids\\n16'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 14}, page_content='1.Randomly choose k data points \\nto be centroids k-means in 4 steps\\n17\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 15}, page_content='1.Randomly choose k data points \\nto be centroids \\n2.Assign each point to closest centroidk-means in 4 steps\\n18\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 16}, page_content='1.Randomly choose k data points \\nto be centroids \\n2.Assign each point to closest centroid\\n3.Recalculate centroids based on current cluster membershipk-means in 4 steps\\n19'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 17}, page_content='1.Randomly choose k data \\npoints to be centroids \\n2.Assign each point to closest centroid\\n3.Recalculate centroids based on current cluster membershipk-means in 4 steps\\n204.Repeat steps 2 -3 with the new centroids until the centroids don’t \\nchange anymore'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 18}, page_content='Step 1: load packages and data\\n# Install packages\\ninstall.packages(\"e1071\") install.packages(\"ggplot2\" )\\n# Load librarieslibrary(e1071)library(ggplot2)\\nlibrary(help = e1071)Learn about all the functionality of the package, be \\nwell informed about what you\\'re doing!\\n21'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 19}, page_content='Step 1: load packages and data\\n# Loading house data\\nhouse_votes_Dem = read_csv (\"house_votes_Dem.csv\")\\n# What does the data look like?\\nView( house_votes_Dem )Script\\n22'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 20}, page_content='Step 2: run k -means\\n# Define the columns to be clustered by subsetting the data\\nclust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]\\n# Run an algorithm with 2 centersset.seed(1 )\\nkmeans_obj_Dem = kmeans(clust_data_Dem , centers = 2, \\nalgorithm = \"Lloyd\")\\n# What does the new variable kmeans_obj contain?kmeans_obj_Dem\\n# View the results of each output of the kmeans \\n# functionhead( kmeans_obj_Dem)Script\\n1.By placing the set of data we want     \\nafter the comma, we tell R we’re   looking for columns \\n2.kmeans uses a different starting data point each time it runs. To make the results reproducible make R start from the same point every time with set.seed()\\n3.We’re not specifying the number of iterations so R defaults to 10\\n4.We’ll see that kmeans produces a list    of vectors of different lengths. As a result, we cannot use the View() function\\n23'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 21}, page_content='Step 2: run k -means\\n1.Number of points each cluster contains\\n2.The “location” of each cluster center is specified by 3 \\ncoordinates, one for each column we’re clustering\\n3.The list assigning either cluster 1 or 2 to each data point\\n1.79.5% of the variance between the data points is explained by our clustering, we will discuss this in detail later\\n2.List of other types of data included in kmeans_obj\\n24'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 22}, page_content='•cluster: a vector indicating the cluster to which each point is allocated\\n•centers: a matrix of cluster centers\\n•totss: the total sum of squares (sum of distances between all points)\\n•withinss: vector of within -cluster sum of distances, one number per cluster\\n•tot.withinss: total within -cluster sum of distances, i.e. sum of withinss\\n•betweenss: the between -cluster sum of squares, i.e. totss -tot.withinss\\n•size: the number of points in each cluster\\nTo learn more about the kmeans function run ?kmeanskmeans outputs\\n26'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 23}, page_content='Intra vs. inter- cluster distance\\nIntra-Cluster \\nDistanceInter-Cluster \\nDistance\\nwithinssbetweensstotss = withinss +betweenss\\n27'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 24}, page_content='Step 3: visualize plot\\n# Tell R to read the cluster labels as factors so that ggplot2 (the \\n# graphing  package) can read them as category labels instead of # continuous variables (numeric variables).party_clusters_Dem = as.factor(kmeans_obj_Dem$cluster)\\n# What does party_clusters look like?View( party_clusters_Dem )\\nView(as.data.frame(party_clusters_Dem))\\n# Set up labels for our data so that we can compare Democrats and \\n# Republicans.party_labels_Dem = house_votes_Dem$partyScript\\n28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 25}, page_content='ggplot(house_votes_Dem, aes(x = aye, \\ny = nay,\\nshape = party_clusters_Dem)) + \\ngeom_point(size = 6) +\\nggtitle(\"Aye vs. Nay votes for Democrat -introduced bills\") +\\nxlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", \\nlabels = c(\"Cluster 1\", \"Cluster 2\" ),\\nvalues = c(\"1\" , \"2\")) +\\ntheme_light()Step 3: visualize plotCosmetics layerBase layer\\nGeom Layer\\nTitles and axis\\nShape\\nTheme\\n29Script'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 26}, page_content='Step 3: visualize plot\\n30'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 27}, page_content='•Two groups exist\\n•Algorithm identifies voting \\npatternsWhat can we infer about \\nthe different clusters?Step 4: analyze results\\n31\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 28}, page_content='ggplot(house_votes_Dem, aes(x = yea, \\ny = nay,\\ncolor= party_labels_Dem,\\nshape = party_clusters_Dem)) + \\ngeom_point(size = 6) +\\nggtitle(\"Aye vs. Nay votes for Democrat -introduced bills\") +\\nxlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", \\nlabels = c(\"Cluster 1\", \"Cluster 2\" ),\\nvalues = c(\"1\" , \"2\")) +\\nscale_color_manual(name = \"Party\", \\nlabels = c(\"Democratic\", \"Republican\"),\\nvalues = c(\"blue\" , \"red\"))+\\ntheme_light()Step 5: validate resultsCosmetics layerScript\\nBase layer\\nGeom Layer\\nTitles and axis\\nColor and \\nshape\\nTheme\\n32'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 29}, page_content='Step 5: validate results\\n33\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 30}, page_content='•Diffuse among Democrats\\n•Republicans more dense\\n•Can gauge “outliers”\\n•Can see the polarization \\nbetween the two political parties Step 6: interpret results\\n34\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 31}, page_content='•Clustering is more powerful than the \\nhuman eye in3D\\n•Clustering mathematically defines \\nwhich cluster the peripheral points \\nshould be in when it’s not obvious to \\nthe human eye\\n•Clustering is helpful when many \\ndimensions / variables exist that you \\ncan’t visualize at once\\n–Whiskey similarity example from \\nclassification lectureClustering vs. visualizing\\nAye, Nay and Other Votes\\nin House of Representatives\\n35'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 32}, page_content='•Goals of clustering:\\n–Maximize the separation between clusters \\n•i.e. Maximize inter -cluster distance \\n–Keep similar points in a \\ncluster close together \\n•i.e. Minimize intra -cluster distanceHow good is the clustering?\\n36'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 33}, page_content='•Look at the variance explained by \\nclusters\\n–In particular, the ratio of inter -cluster \\nvariance to total variance\\n•How much of the total variance is explained by the clustering?Assessing how well an algorithm performsHow good is the clustering?\\nVariation explained by clusters\\n= \\ninter-cluster variance / total variance\\n37'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 34}, page_content='•cluster: a vector indicating the cluster to which each point is allocated\\n•centers: a matrix of cluster centers\\n•totss: the total sum of squares (sum of distances between all points)\\n•withinss: vector of within -cluster sum of distances, one number per cluster\\n•tot.withinss: total within -cluster sum of distances, i.e. sum of withinss\\n•betweenss: the between -cluster sum of squares, i.e. totss -tot.withinss\\n•size: the number of points in each cluster\\nTo learn more about the kmeans function run ?kmeanskmeans outputs\\n38'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 35}, page_content='Intra vs. inter- cluster distance\\nIntra-Cluster \\nDistanceInter-Cluster \\nDistance\\nwithinssbetweensstotss = withinss +betweenss\\n39'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 36}, page_content='How good is the clustering?\\n# Inter-cluster variance,\\n# \"betweenss\" is the sum of the \\n# distances between points from # different clustersnum_Dem= kmeans_obj_Dem$betweenss\\n# Total variance# \"totss\" is the sum of the distances  # between all the points in # the data setdenom_Dem = kmeans_obj_Dem$totss\\n# Variance accounted for by \\n# clustersvar_exp_Dem = num_Dem/ denom_Dem\\nvar_exp_Dem[1] 0.7193405Script\\n40\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 37}, page_content=\"•It’s easier when the number of clusters is known ahead of time, but what if we don't \\nknow how many clusters we should have?\\n•Since different starting points may generate different clusters, we need a way to assess cluster quality as well.How do we choose the number of clusters (i.e. k)?How good is the clustering?\\n41\"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 38}, page_content='1.Elbow method\\n–Computes the percentage of variance explained by clusters for a range of cluster \\nnumbers\\n–Plots a graph so results are easier to see \\n–Not guaranteed to work! It depends on the data in question\\n2.NbClustHow to select k: two methods\\n–Runs 30 different tests and \\nprovides “majority vote” for \\nthe best number of clusters \\n(k’s) to use\\n42'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 39}, page_content='Elbow method: measure variance\\n# Run algorithm with 3 centers\\nset.seed(1 )\\nkmeans_obj_Dem = kmeans(clust_data_Dem,   \\ncenters = 3,\\nalgorithm = \"Lloyd\")\\n# Inter- cluster variance\\nnum_Dem = kmeans_obj_Dem$ betweenss\\n# Total variance\\ndenom_Dem = kmeans_obj_Dem $totss\\n# Variance accounted for by clustersvar_exp_Dem = num_Dem / denom_Dem\\nvar_exp_Dem\\n[1] 0.7949741Script\\n43'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 40}, page_content='•We want to repeat the variance calculation from the previous slide for several \\nnumbers of clusters automatically\\n•We can create a function that contains all the steps we want to automate Automating a step we want to repeat\\nfunction(data, item to iterate through)\\n44'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 41}, page_content='# The function explained_variance wraps our code from previous slides.\\nexplained_variance = function( data_in, k){\\n# Running k- means algorithm\\nset.seed(1 )  \\nkmeans_obj = kmeans(data_in, centers = k,\\nalgorithm = \"Lloyd\" )\\n# Variance accounted for by clusters\\nvar_exp = kmeans_obj $betweenss / \\nkmeans_obj$totss\\nvar_exp\\n}Automating a step we want to repeat\\nScript\\n1.A new variable is created and set equal \\nto our function()\\n2.The commands inside the function are wrapped in curly braces {}\\n3.Inside the parentheses, we specify the variables that the user will input and that will then be used inside the function where they appear\\n45'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 42}, page_content='# Recall the variable we are using for the \\n# data that we\\'re clustering.\\nclust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]\\nView( clust_data_Dem)\\n# The sapply() function plugs several values \\n# into explained_variance.\\nexplained_var_Dem = sapply(1 :10, explained_variance, \\ndata_in = clust_data_Dem)\\nView( explained_var_Dem)\\n# Data for ggplot2\\nelbow_data_Dem = data.frame(k = 1:10, \\nexplained_var_Dem)\\nView( elbow_data_Dem)Automating a step we want to repeat\\n1.sapply() applies a function to a \\nvector\\n2.We have to tell sapply() that \\nthe we want the \\nexplained_variance function \\nto use the clust_data data\\n3.Next, we create a data frame that contains both the new variance variable (explained_var_Dem) \\nand the different numbers of k that we used in the previous function (1 through 10)Function we created \\nScript\\n46'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 43}, page_content='# Plotting data\\nggplot(elbow_data_Dem, \\naes(x = k,  \\ny = explained_var_Dem)) + \\ngeom_point(size = 4) +\\ngeom_line(size = 1 ) +\\nxlab(\"k\" ) + \\nylab(\"Intercluster Variance/Total Variance\" ) + \\ntheme_light()Elbow method: plotting the graph\\nScript\\n1.geom_point() sets the size of the data points\\n2.geom_line() sets the thickness of the line\\n47'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 44}, page_content='Looking for the kink in graph of  inter- cluster variance / total \\nvarianceElbow method: measure variance\\nOriginal data Elbow methodk = 2\\n48\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 45}, page_content='•Library: \"NbClust\"\\nFunctions:  \"NbClust\"\\nInputs : \\n•data –data array or data frame\\n•min.nc / max.nc –minimum/maximum number of clusters\\n•method –\"kmeans\"\\n•There are other, more advanced arguments that can be customized but are outside of \\nthe scope of this course and are note necessary to for NbClust to workThere are a number of ways to choose the right k.\\nNbClust runs 30 tests and selects k based on majority voteNbClust: k by majority vote\\nNbClust(data, max.nc, method = \"kmeans\")\\n49'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 46}, page_content='# Install the package.\\ninstall.packages(\"NbClust\" )\\nlibrary(NbClust)\\n# Run NbClust.\\nnbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")\\n# View the output of NbClust.nbclust_obj_Dem\\n# View the output that shows the number of clusters each \\n# method recommends.View( nbclust_obj_Dem $Best.nc)NbClust : k by majority vote\\nScript\\n50'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 47}, page_content='NbClust: k by majority vote\\n> nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")\\n...\\n******************************************************************* * Among all indices:                                                * 14 proposed 2 as the best number of clusters * 3 proposed 3 as the best number of clusters * 1 proposed 4 as the best number of clusters * 3 proposed 6 as the best number of clusters * 1 proposed 9 as the best number of clusters * 1 proposed 10 as the best number of clusters * 1 proposed 15 as the best number of clusters \\n***** Conclusion *****                            \\n* According to the majority rule, the best number of clusters is  2\\nNote: additional information appears; the above information is most relevant to us for nowConsole\\n51'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 48}, page_content='NbClust: k by majority vote\\n> nbclust_obj_Dem Console•nbclust_obj_Dem shows the outputs of NbClust\\n–One of the outputs is Best.nc, which shows the number of clusters                               \\nrecommended by each test \\n52'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 49}, page_content='NbClust: k by majority vote\\n•We want to visualize a histogram to make it obvious how many votes there are \\nfor each number of clusters \\n53'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 50}, page_content='# Subset the 1st row from Best.nc and convert it  \\n# to a data frame, so ggplot2 can plot it.\\nfreq_k_Dem = nbclust_obj_Dem $Best.nc[1 ,]\\nfreq_k_Dem = data.frame( freq_k_Dem)\\nView(freq_k_Dem)\\n# Check the maximum number of clusters.\\nmax(freq_k_Dem )\\n# Plot as a histogram.\\nggplot(freq_k_Dem,\\naes(x = freq_k_Dem)) +\\ngeom_bar() +scale_x_continuous(breaks = seq(0 , 15, by = 1)) +\\nscale_y_continuous(breaks = seq(0 , 12, by = 1)) +\\nlabs(x = \"Number of Clusters\",\\ny = \"Number of Votes\" ,\\ntitle = \"Cluster Analysis\")NbClust: k by majority vote\\nScript\\n2 clusters is the \\nwinner with 12 votes\\n54'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 51}, page_content='•If you’re a lobbyist, which congressperson can you influence for swing votes?\\n•If you’re managing a campaign and your competitor is always voting along \\nparty lines, how can you use that information?\\n•If your congressperson is not an active voter, is she representing your interests?\\n•What do the voting patterns look like for Republican -introduced bills?Application of results\\n55'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 52}, page_content='•Could see differences between the \\npatterns of Reb lead bills and Democrat lead bills\\n•Could provide information on congressmen that might be see has swing votes. Implications of results\\n56'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 53}, page_content=\"•We are assuming that the patterns correspond with the same bills being voted \\non –perhaps some Congressmen have the same number of 'aye' and 'nay' \\nvotes, but voted on different bills\\n•Network analysis can help determine additional connections between Congressmen\\n•We haven't taken extenuating factors into account – political initiatives, \\ncurrent events, etc.\\nThis is a preliminary analysis that gives us initial \\ninsights and can help us direct further researchLimitations of results\\n57\"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 54}, page_content=\"•The good and bad\\n–+ cheap –NO LABELS , labels are expensive to create and maintain\\n–+/-clustering always works\\n–-Many methods to choose from and knowing the right one can be nontrivial and the \\ndifferences between many are almost zero, so you need to understand what you're \\ndoing\\n•The evil\\n–Curse of dimensionality\\n–Clusters may result from poor data quality\\n–Non-deterministic (e.g. k -means) subject to local minimum. Since it works with \\naverages, k-means does not get much better with Big Data (marginal improvements) \\n–Non spherical data may result in poor clustering (depending on method used)\\n–Unequal cluster sizes may result in poor clustering (depending on method used)The good, bad, and evil\\n58\"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 55}, page_content='59\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-1.pdf', 'page': 56}, page_content='•Analysts need to ask the following questions\\n–Do you want overlapping or non -overlapping clusters ?\\n–Does your data satisfy the assumptions of the clustering algorithm?\\n–How was the distance measure identified ? \\n–How many clusters and why ? Identifying the number of clusters is a difficult task if the \\nnumber of class labels is not known beforehand \\n–Does your method scale to the size of the data?\\n–Is the compute time congruent with the temporal budget of your business need (i.e. \\ndo you get answers back in time to make meaningful decisions)The good, bad, and evil\\n60'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 0}, page_content='Machine Learning Overview, EDA and Clustering\\nBrian Wright\\nbrianwright@virginia.edu\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 1}, page_content='2\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 2}, page_content='3\\uf06eGiven  D :{2,4,10,12,3,11,20,25,30},  and  k=2 clusters\\n\\uf06eRandomly assign the means:  m1=3, m2=4\\n\\uf06eK1={2,3}, K2={4,10,12,11,20,25,30}, m1=2.5, m2=16\\n\\uf06eK1={2,3,4}, K2={10,12,11,20,25,30}, m1=3, m2=18\\n\\uf06eK1={2,3,4,10}, K2={12,11,20,25,30}, m1=4.75, m2=19.6\\n\\uf06eK1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\\n\\uf06eK1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\\n\\uf06eStop, since the clusters and the means found in \\nall subsequent iterations will be the same .Example of K -Means'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 3}, page_content='1.What is Machine Learning?\\n2.What is exploratory data analysis?\\n3.k-means clustering\\n–Does Congress vote in patterns?\\n4.Multi -dimensional k -means clustering\\n–Are NBA players compensated according to performance?Outline: Intro to Unsupervised ML\\n4'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 4}, page_content='•Exploratory data analysis or “EDA” is an approach where the intent is to see \\nwhat the data can tell us beyond modeling or hypothesis testing\\n–Data visualization is one of the most common forms of EDAWhat is exploratory data analysis?\\n5'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 5}, page_content='When data is too big or complex to be analyzed just by \\nvisualizing it, these types of analysis can help:\\n1.Clustering: compare pieces of data by measuring \\nsimilarity among them\\n2.Network analysis: analyze how people, places and \\nentities are connected to evaluate the properties and \\nstructure of a network \\n3.Text mining: analyze what large bodies of \\nunstructured or structured text sayTypes of exploratory data analysis\\n6'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 6}, page_content='The data inputs have (x)no target outputs (y)Unsupervised machine learning\\nInput x:\\nVoterOutput y:\\nNot given\\n(To be discovered)?\\nWe want to impose structure on the inputs (x)to say something \\nmeaningful about the data\\n7\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 7}, page_content='1.Technique for finding similarity between groups\\n2.Type of unsupervised machine learning\\n•Not the only class of unsupervised learning        \\nalgorithms\\n3.Similarity needs to be defined\\n•Will depend on attributes of data\\n•Usually a distance metricWhat is clustering?\\n8\\nKey assumption: data points that are “closer” together are related or similar'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 8}, page_content='•Haimowitz and Schwarz 1997 paper on \\nclustering for credit line optimization\\n–http://www.aaai.org/Papers/Workshops/1997/\\nWS-97-07/WS97- 07-006.pdf\\n•Cluster existing GE Capital customers based on similarity in use of their credit cards to pay bills and customers’ profitability to GE Capital\\n•Resulted in five clusters of consumer credit behavior\\n•Created classification model to predict customer type and offer tailored productsGE Capital case study: grouping clients\\n9'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 9}, page_content='Example use case General question Concept\\nDoes Congress vote in patterns? Is there a pattern ?\\nIs there structure in \\nunstructured data?k-means clustering\\nAre basketball players \"priced\" efficiently (based on performance)?How to uncover trends with many variables that you    can\\'t easily visualize?k-means clustering in \\nmany dimensionsConcept summary\\n12'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 10}, page_content='1.Data set consists of 427 members \\n(observations) \\n2.Members served a full year in 2013\\n3.Three vote types:\\n•“Aye”\\n•“Nay”\\n•“Other”Goal: to understand how polarized the \\nUS Congress isPolitical clustering\\nThe joint session of Congress on Capitol Hill \\nin Washington\\n13'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 11}, page_content='•How do we identify \\nswing votes?\\n–Lobbying\\n–Bridging party lines\\n•Assumption:\\n–Democrats and Republicans \\nvote among partisan lines, \\nwhich generates clustersEach data point represents a member of CongressFinding voting patterns\\n14'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 12}, page_content='Intra vs. inter- cluster distance\\nIntra-Cluster \\nDistanceInter-Cluster \\nDistance\\nObjective: minimize intra -cluster dis tance, maximize inter -cluster distance\\n15'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 13}, page_content='•The centroid is the average location \\nof all points in the cluster\\n•Another definition: the centroid minimizes the distance between a central location and all the data points in the cluster\\nNote: Centroids are generally not existing data points, rather locations in spacek-means clustering is based on \\ncentroids\\n16'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 14}, page_content='1.Randomly choose k data points \\nto be centroids k-means in 4 steps\\n17\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 15}, page_content='1.Randomly choose k data points \\nto be centroids \\n2.Assign each point to closest centroidk-means in 4 steps\\n18\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 16}, page_content='1.Randomly choose k data points \\nto be centroids \\n2.Assign each point to closest centroid\\n3.Recalculate centroids based on current cluster membershipk-means in 4 steps\\n19'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 17}, page_content='1.Randomly choose k data \\npoints to be centroids \\n2.Assign each point to closest centroid\\n3.Recalculate centroids based on current cluster membershipk-means in 4 steps\\n204.Repeat steps 2 -3 with the new centroids until the centroids don’t \\nchange anymore'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 18}, page_content='Step 1: load packages and data\\n# Install packages\\ninstall.packages(\"e1071\") install.packages(\"ggplot2\" )\\n# Load librarieslibrary(e1071)library(ggplot2)\\nlibrary(help = e1071)Learn about all the functionality of the package, be \\nwell informed about what you\\'re doing!\\n21'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 19}, page_content='Step 1: load packages and data\\n# Loading house data\\nhouse_votes_Dem = read_csv (\"house_votes_Dem.csv\")\\n# What does the data look like?\\nView( house_votes_Dem )Script\\n22'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 20}, page_content='Step 2: run k -means\\n# Define the columns to be clustered by subsetting the data\\nclust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]\\n# Run an algorithm with 2 centersset.seed(1 )\\nkmeans_obj_Dem = kmeans(clust_data_Dem , centers = 2, \\nalgorithm = \"Lloyd\")\\n# What does the new variable kmeans_obj contain?kmeans_obj_Dem\\n# View the results of each output of the kmeans \\n# functionhead( kmeans_obj_Dem)Script\\n1.By placing the set of data we want     \\nafter the comma, we tell R we’re   looking for columns \\n2.kmeans uses a different starting data point each time it runs. To make the results reproducible make R start from the same point every time with set.seed()\\n3.We’re not specifying the number of iterations so R defaults to 10\\n4.We’ll see that kmeans produces a list    of vectors of different lengths. As a result, we cannot use the View() function\\n23'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 21}, page_content='Step 2: run k -means\\n1.Number of points each cluster contains\\n2.The “location” of each cluster center is specified by 3 \\ncoordinates, one for each column we’re clustering\\n3.The list assigning either cluster 1 or 2 to each data point\\n1.79.5% of the variance between the data points is explained by our clustering, we will discuss this in detail later\\n2.List of other types of data included in kmeans_obj\\n24'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 22}, page_content='•cluster: a vector indicating the cluster to which each point is allocated\\n•centers: a matrix of cluster centers\\n•totss: the total sum of squares (sum of distances between all points)\\n•withinss: vector of within -cluster sum of distances, one number per cluster\\n•tot.withinss: total within -cluster sum of distances, i.e. sum of withinss\\n•betweenss: the between -cluster sum of squares, i.e. totss -tot.withinss\\n•size: the number of points in each cluster\\nTo learn more about the kmeans function run ?kmeanskmeans outputs\\n26'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 23}, page_content='Intra vs. inter- cluster distance\\nIntra-Cluster \\nDistanceInter-Cluster \\nDistance\\nwithinssbetweensstotss = withinss +betweenss\\n27'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 24}, page_content='Step 3: visualize plot\\n# Tell R to read the cluster labels as factors so that ggplot2 (the \\n# graphing  package) can read them as category labels instead of # continuous variables (numeric variables).party_clusters_Dem = as.factor(kmeans_obj_Dem$cluster)\\n# What does party_clusters look like?View( party_clusters_Dem )\\nView(as.data.frame(party_clusters_Dem))\\n# Set up labels for our data so that we can compare Democrats and \\n# Republicans.party_labels_Dem = house_votes_Dem$partyScript\\n28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 25}, page_content='ggplot(house_votes_Dem, aes(x = aye, \\ny = nay,\\nshape = party_clusters_Dem)) + \\ngeom_point(size = 6) +\\nggtitle(\"Aye vs. Nay votes for Democrat -introduced bills\") +\\nxlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", \\nlabels = c(\"Cluster 1\", \"Cluster 2\" ),\\nvalues = c(\"1\" , \"2\")) +\\ntheme_light()Step 3: visualize plotCosmetics layerBase layer\\nGeom Layer\\nTitles and axis\\nShape\\nTheme\\n29Script'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 26}, page_content='Step 3: visualize plot\\n30'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 27}, page_content='•Two groups exist\\n•Algorithm identifies voting \\npatternsWhat can we infer about \\nthe different clusters?Step 4: analyze results\\n31\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 28}, page_content='ggplot(house_votes_Dem, aes(x = yea, \\ny = nay,\\ncolor= party_labels_Dem,\\nshape = party_clusters_Dem)) + \\ngeom_point(size = 6) +\\nggtitle(\"Aye vs. Nay votes for Democrat -introduced bills\") +\\nxlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", \\nlabels = c(\"Cluster 1\", \"Cluster 2\" ),\\nvalues = c(\"1\" , \"2\")) +\\nscale_color_manual(name = \"Party\", \\nlabels = c(\"Democratic\", \"Republican\"),\\nvalues = c(\"blue\" , \"red\"))+\\ntheme_light()Step 5: validate resultsCosmetics layerScript\\nBase layer\\nGeom Layer\\nTitles and axis\\nColor and \\nshape\\nTheme\\n32'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 29}, page_content='Step 5: validate results\\n33\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 30}, page_content='•Diffuse among Democrats\\n•Republicans more dense\\n•Can gauge “outliers”\\n•Can see the polarization \\nbetween the two political parties Step 6: interpret results\\n34\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 31}, page_content='•Clustering is more powerful than the \\nhuman eye in3D\\n•Clustering mathematically defines \\nwhich cluster the peripheral points \\nshould be in when it’s not obvious to \\nthe human eye\\n•Clustering is helpful when many \\ndimensions / variables exist that you \\ncan’t visualize at once\\n–Whiskey similarity example from \\nclassification lectureClustering vs. visualizing\\nAye, Nay and Other Votes\\nin House of Representatives\\n35'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 32}, page_content='•Goals of clustering:\\n–Maximize the separation between clusters \\n•i.e. Maximize inter -cluster distance \\n–Keep similar points in a \\ncluster close together \\n•i.e. Minimize intra -cluster distanceHow good is the clustering?\\n36'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 33}, page_content='•Look at the variance explained by \\nclusters\\n–In particular, the ratio of inter -cluster \\nvariance to total variance\\n•How much of the total variance is explained by the clustering?Assessing how well an algorithm performsHow good is the clustering?\\nVariation explained by clusters\\n= \\ninter-cluster variance / total variance\\n37'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 34}, page_content='•cluster: a vector indicating the cluster to which each point is allocated\\n•centers: a matrix of cluster centers\\n•totss: the total sum of squares (sum of distances between all points)\\n•withinss: vector of within -cluster sum of distances, one number per cluster\\n•tot.withinss: total within -cluster sum of distances, i.e. sum of withinss\\n•betweenss: the between -cluster sum of squares, i.e. totss -tot.withinss\\n•size: the number of points in each cluster\\nTo learn more about the kmeans function run ?kmeanskmeans outputs\\n38'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 35}, page_content='Intra vs. inter- cluster distance\\nIntra-Cluster \\nDistanceInter-Cluster \\nDistance\\nwithinssbetweensstotss = withinss +betweenss\\n39'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 36}, page_content='How good is the clustering?\\n# Inter-cluster variance,\\n# \"betweenss\" is the sum of the \\n# distances between points from # different clustersnum_Dem= kmeans_obj_Dem$betweenss\\n# Total variance# \"totss\" is the sum of the distances  # between all the points in # the data setdenom_Dem = kmeans_obj_Dem$totss\\n# Variance accounted for by \\n# clustersvar_exp_Dem = num_Dem/ denom_Dem\\nvar_exp_Dem[1] 0.7193405Script\\n40\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 37}, page_content=\"•It’s easier when the number of clusters is known ahead of time, but what if we don't \\nknow how many clusters we should have?\\n•Since different starting points may generate different clusters, we need a way to assess cluster quality as well.How do we choose the number of clusters (i.e. k)?How good is the clustering?\\n41\"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 38}, page_content='1.Elbow method\\n–Computes the percentage of variance explained by clusters for a range of cluster \\nnumbers\\n–Plots a graph so results are easier to see \\n–Not guaranteed to work! It depends on the data in question\\n2.NbClustHow to select k: two methods\\n–Runs 30 different tests and \\nprovides “majority vote” for \\nthe best number of clusters \\n(k’s) to use\\n42'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 39}, page_content='Elbow method: measure variance\\n# Run algorithm with 3 centers\\nset.seed(1 )\\nkmeans_obj_Dem = kmeans(clust_data_Dem,   \\ncenters = 3,\\nalgorithm = \"Lloyd\")\\n# Inter- cluster variance\\nnum_Dem = kmeans_obj_Dem$ betweenss\\n# Total variance\\ndenom_Dem = kmeans_obj_Dem $totss\\n# Variance accounted for by clustersvar_exp_Dem = num_Dem / denom_Dem\\nvar_exp_Dem\\n[1] 0.7949741Script\\n43'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 40}, page_content='•We want to repeat the variance calculation from the previous slide for several \\nnumbers of clusters automatically\\n•We can create a function that contains all the steps we want to automate Automating a step we want to repeat\\nfunction(data, item to iterate through)\\n44'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 41}, page_content='# The function explained_variance wraps our code from previous slides.\\nexplained_variance = function( data_in, k){\\n# Running k- means algorithm\\nset.seed(1 )  \\nkmeans_obj = kmeans(data_in, centers = k,\\nalgorithm = \"Lloyd\" )\\n# Variance accounted for by clusters\\nvar_exp = kmeans_obj $betweenss / \\nkmeans_obj$totss\\nvar_exp\\n}Automating a step we want to repeat\\nScript\\n1.A new variable is created and set equal \\nto our function()\\n2.The commands inside the function are wrapped in curly braces {}\\n3.Inside the parentheses, we specify the variables that the user will input and that will then be used inside the function where they appear\\n45'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 42}, page_content='# Recall the variable we are using for the \\n# data that we\\'re clustering.\\nclust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]\\nView( clust_data_Dem)\\n# The sapply() function plugs several values \\n# into explained_variance.\\nexplained_var_Dem = sapply(1 :10, explained_variance, \\ndata_in = clust_data_Dem)\\nView( explained_var_Dem)\\n# Data for ggplot2\\nelbow_data_Dem = data.frame(k = 1:10, \\nexplained_var_Dem)\\nView( elbow_data_Dem)Automating a step we want to repeat\\n1.sapply() applies a function to a \\nvector\\n2.We have to tell sapply() that \\nthe we want the \\nexplained_variance function \\nto use the clust_data data\\n3.Next, we create a data frame that contains both the new variance variable (explained_var_Dem) \\nand the different numbers of k that we used in the previous function (1 through 10)Function we created \\nScript\\n46'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 43}, page_content='# Plotting data\\nggplot(elbow_data_Dem, \\naes(x = k,  \\ny = explained_var_Dem)) + \\ngeom_point(size = 4) +\\ngeom_line(size = 1 ) +\\nxlab(\"k\" ) + \\nylab(\"Intercluster Variance/Total Variance\" ) + \\ntheme_light()Elbow method: plotting the graph\\nScript\\n1.geom_point() sets the size of the data points\\n2.geom_line() sets the thickness of the line\\n47'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 44}, page_content='Looking for the kink in graph of  inter- cluster variance / total \\nvarianceElbow method: measure variance\\nOriginal data Elbow methodk = 2\\n48\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 45}, page_content='•Library: \"NbClust\"\\nFunctions:  \"NbClust\"\\nInputs : \\n•data –data array or data frame\\n•min.nc / max.nc –minimum/maximum number of clusters\\n•method –\"kmeans\"\\n•There are other, more advanced arguments that can be customized but are outside of \\nthe scope of this course and are note necessary to for NbClust to workThere are a number of ways to choose the right k.\\nNbClust runs 30 tests and selects k based on majority voteNbClust: k by majority vote\\nNbClust(data, max.nc, method = \"kmeans\")\\n49'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 46}, page_content='# Install the package.\\ninstall.packages(\"NbClust\" )\\nlibrary(NbClust)\\n# Run NbClust.\\nnbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")\\n# View the output of NbClust.nbclust_obj_Dem\\n# View the output that shows the number of clusters each \\n# method recommends.View( nbclust_obj_Dem $Best.nc)NbClust : k by majority vote\\nScript\\n50'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 47}, page_content='NbClust: k by majority vote\\n> nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")\\n...\\n******************************************************************* * Among all indices:                                                * 14 proposed 2 as the best number of clusters * 3 proposed 3 as the best number of clusters * 1 proposed 4 as the best number of clusters * 3 proposed 6 as the best number of clusters * 1 proposed 9 as the best number of clusters * 1 proposed 10 as the best number of clusters * 1 proposed 15 as the best number of clusters \\n***** Conclusion *****                            \\n* According to the majority rule, the best number of clusters is  2\\nNote: additional information appears; the above information is most relevant to us for nowConsole\\n51'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 48}, page_content='NbClust: k by majority vote\\n> nbclust_obj_Dem Console•nbclust_obj_Dem shows the outputs of NbClust\\n–One of the outputs is Best.nc, which shows the number of clusters                               \\nrecommended by each test \\n52'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 49}, page_content='NbClust: k by majority vote\\n•We want to visualize a histogram to make it obvious how many votes there are \\nfor each number of clusters \\n53'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 50}, page_content='# Subset the 1st row from Best.nc and convert it  \\n# to a data frame, so ggplot2 can plot it.\\nfreq_k_Dem = nbclust_obj_Dem $Best.nc[1 ,]\\nfreq_k_Dem = data.frame( freq_k_Dem)\\nView(freq_k_Dem)\\n# Check the maximum number of clusters.\\nmax(freq_k_Dem )\\n# Plot as a histogram.\\nggplot(freq_k_Dem,\\naes(x = freq_k_Dem)) +\\ngeom_bar() +scale_x_continuous(breaks = seq(0 , 15, by = 1)) +\\nscale_y_continuous(breaks = seq(0 , 12, by = 1)) +\\nlabs(x = \"Number of Clusters\",\\ny = \"Number of Votes\" ,\\ntitle = \"Cluster Analysis\")NbClust: k by majority vote\\nScript\\n2 clusters is the \\nwinner with 12 votes\\n54'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 51}, page_content='•If you’re a lobbyist, which congressperson can you influence for swing votes?\\n•If you’re managing a campaign and your competitor is always voting along \\nparty lines, how can you use that information?\\n•If your congressperson is not an active voter, is she representing your interests?\\n•What do the voting patterns look like for Republican -introduced bills?Application of results\\n55'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 52}, page_content='•Could see differences between the \\npatterns of Reb lead bills and Democrat lead bills\\n•Could provide information on congressmen that might be see has swing votes. Implications of results\\n56'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 53}, page_content=\"•We are assuming that the patterns correspond with the same bills being voted \\non –perhaps some Congressmen have the same number of 'aye' and 'nay' \\nvotes, but voted on different bills\\n•Network analysis can help determine additional connections between Congressmen\\n•We haven't taken extenuating factors into account – political initiatives, \\ncurrent events, etc.\\nThis is a preliminary analysis that gives us initial \\ninsights and can help us direct further researchLimitations of results\\n57\"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 54}, page_content=\"•The good and bad\\n–+ cheap –NO LABELS , labels are expensive to create and maintain\\n–+/-clustering always works\\n–-Many methods to choose from and knowing the right one can be nontrivial and the \\ndifferences between many are almost zero, so you need to understand what you're \\ndoing\\n•The evil\\n–Curse of dimensionality\\n–Clusters may result from poor data quality\\n–Non-deterministic (e.g. k -means) subject to local minimum. Since it works with \\naverages, k-means does not get much better with Big Data (marginal improvements) \\n–Non spherical data may result in poor clustering (depending on method used)\\n–Unequal cluster sizes may result in poor clustering (depending on method used)The good, bad, and evil\\n58\"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 55}, page_content='59\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21-2.pdf', 'page': 56}, page_content='•Analysts need to ask the following questions\\n–Do you want overlapping or non -overlapping clusters ?\\n–Does your data satisfy the assumptions of the clustering algorithm?\\n–How was the distance measure identified ? \\n–How many clusters and why ? Identifying the number of clusters is a difficult task if the \\nnumber of class labels is not known beforehand \\n–Does your method scale to the size of the data?\\n–Is the compute time congruent with the temporal budget of your business need (i.e. \\ndo you get answers back in time to make meaningful decisions)The good, bad, and evil\\n60'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 0}, page_content='Machine Learning Overview, EDA and Clustering\\nBrian Wright\\nbrianwright@virginia.edu\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 1}, page_content='2\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 2}, page_content='3\\uf06eGiven  D :{2,4,10,12,3,11,20,25,30},  and  k=2 clusters\\n\\uf06eRandomly assign the means:  m1=3, m2=4\\n\\uf06eK1={2,3}, K2={4,10,12,11,20,25,30}, m1=2.5, m2=16\\n\\uf06eK1={2,3,4}, K2={10,12,11,20,25,30}, m1=3, m2=18\\n\\uf06eK1={2,3,4,10}, K2={12,11,20,25,30}, m1=4.75, m2=19.6\\n\\uf06eK1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\\n\\uf06eK1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\\n\\uf06eStop, since the clusters and the means found in \\nall subsequent iterations will be the same .Example of K -Means'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 3}, page_content='1.What is Machine Learning?\\n2.What is exploratory data analysis?\\n3.k-means clustering\\n–Does Congress vote in patterns?\\n4.Multi -dimensional k -means clustering\\n–Are NBA players compensated according to performance?Outline: Intro to Unsupervised ML\\n4'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 4}, page_content='•Exploratory data analysis or “EDA” is an approach where the intent is to see \\nwhat the data can tell us beyond modeling or hypothesis testing\\n–Data visualization is one of the most common forms of EDAWhat is exploratory data analysis?\\n5'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 5}, page_content='When data is too big or complex to be analyzed just by \\nvisualizing it, these types of analysis can help:\\n1.Clustering: compare pieces of data by measuring \\nsimilarity among them\\n2.Network analysis: analyze how people, places and \\nentities are connected to evaluate the properties and \\nstructure of a network \\n3.Text mining: analyze what large bodies of \\nunstructured or structured text sayTypes of exploratory data analysis\\n6'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 6}, page_content='The data inputs have (x)no target outputs (y)Unsupervised machine learning\\nInput x:\\nVoterOutput y:\\nNot given\\n(To be discovered)?\\nWe want to impose structure on the inputs (x)to say something \\nmeaningful about the data\\n7\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 7}, page_content='1.Technique for finding similarity between groups\\n2.Type of unsupervised machine learning\\n•Not the only class of unsupervised learning        \\nalgorithms\\n3.Similarity needs to be defined\\n•Will depend on attributes of data\\n•Usually a distance metricWhat is clustering?\\n8\\nKey assumption: data points that are “closer” together are related or similar'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 8}, page_content='•Haimowitz and Schwarz 1997 paper on \\nclustering for credit line optimization\\n–http://www.aaai.org/Papers/Workshops/1997/\\nWS-97-07/WS97- 07-006.pdf\\n•Cluster existing GE Capital customers based on similarity in use of their credit cards to pay bills and customers’ profitability to GE Capital\\n•Resulted in five clusters of consumer credit behavior\\n•Created classification model to predict customer type and offer tailored productsGE Capital case study: grouping clients\\n9'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 9}, page_content='•Between 2001 and 2004 most European countries passed legislation that allowed \\ncustomers to keep their cell phone number if they switched carriers\\n•Telenor, one of the largest telecommunications companies in Norway wanted to \\nensure it kept its customers\\n–Problem: the promotions the company sent to its clients reminded them that they could \\nleave and resulted in greater defections!\\n–Solution: predict which customers, if contacted, are more likely to stay with the company \\n•Results:\\n–Marketing campaign ROI increased 11x\\n–Customer churn decreased 36%\\n–Marketing campaign costs decreased 40%Telenor case study: predicting \\nbehavior\\n10'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 10}, page_content='1.What is Machine Learning?\\n2.What is exploratory data analysis?\\n3.k-means clustering\\n–Does Congress vote in patterns?\\n4.Multi -dimensional k -means clustering –Lab \\n–Are NBA players compensated according to performance?Outline: Intro to Unsupervised ML\\n11'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 11}, page_content='Example use case General question Concept\\nDoes Congress vote in patterns? Is there a pattern ?\\nIs there structure in \\nunstructured data?k-means clustering\\nAre basketball players \"priced\" efficiently (based on performance)?How to uncover trends with many variables that you    can\\'t easily visualize?k-means clustering in \\nmany dimensionsConcept summary\\n12'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 12}, page_content='1.Data set consists of 427 members \\n(observations) \\n2.Members served a full year in 2013\\n3.Three vote types:\\n•“Aye”\\n•“Nay”\\n•“Other”Goal: to understand how polarized the \\nUS Congress isPolitical clustering\\nThe joint session of Congress on Capitol Hill \\nin Washington\\n13'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 13}, page_content='•How do we identify \\nswing votes?\\n–Lobbying\\n–Bridging party lines\\n•Assumption:\\n–Democrats and Republicans \\nvote among partisan lines, \\nwhich generates clustersEach data point represents a member of CongressFinding voting patterns\\n14'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 14}, page_content='Intra vs. inter- cluster distance\\nIntra-Cluster \\nDistanceInter-Cluster \\nDistance\\nObjective: minimize intra -cluster dis tance, maximize inter -cluster distance\\n15'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 15}, page_content='•The centroid is the average location \\nof all points in the cluster\\n•Another definition: the centroid minimizes the distance between a central location and all the data points in the cluster\\nNote: Centroids are generally not existing data points, rather locations in spacek-means clustering is based on \\ncentroids\\n16'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 16}, page_content='1.Randomly choose k data points \\nto be centroids k-means in 4 steps\\n17\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 17}, page_content='1.Randomly choose k data points \\nto be centroids \\n2.Assign each point to closest centroidk-means in 4 steps\\n18\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 18}, page_content='1.Randomly choose k data points \\nto be centroids \\n2.Assign each point to closest centroid\\n3.Recalculate centroids based on current cluster membershipk-means in 4 steps\\n19'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 19}, page_content='1.Randomly choose k data \\npoints to be centroids \\n2.Assign each point to closest centroid\\n3.Recalculate centroids based on current cluster membershipk-means in 4 steps\\n204.Repeat steps 2 -3 with the new centroids until the centroids don’t \\nchange anymore'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 20}, page_content='Step 1: load packages and data\\n# Install packages\\ninstall.packages(\"e1071\") install.packages(\"ggplot2\" )\\n# Load librarieslibrary(e1071)library(ggplot2)\\nlibrary(help = e1071)Learn about all the functionality of the package, be \\nwell informed about what you\\'re doing!\\n21'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 21}, page_content='Step 1: load packages and data\\n# Loading house data\\nhouse_votes_Dem = read_csv (\"house_votes_Dem.csv\")\\n# What does the data look like?\\nView( house_votes_Dem )Script\\n22'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 22}, page_content='Step 2: run k -means\\n# Define the columns to be clustered by subsetting the data\\nclust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]\\n# Run an algorithm with 2 centersset.seed(1 )\\nkmeans_obj_Dem = kmeans(clust_data_Dem , centers = 2, \\nalgorithm = \"Lloyd\")\\n# What does the new variable kmeans_obj contain?kmeans_obj_Dem\\n# View the results of each output of the kmeans \\n# functionhead( kmeans_obj_Dem)Script\\n1.By placing the set of data we want     \\nafter the comma, we tell R we’re   looking for columns \\n2.kmeans uses a different starting data point each time it runs. To make the results reproducible make R start from the same point every time with set.seed()\\n3.We’re not specifying the number of iterations so R defaults to 10\\n4.We’ll see that kmeans produces a list    of vectors of different lengths. As a result, we cannot use the View() function\\n23'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 23}, page_content='Step 2: run k -means\\n1.Number of points each cluster contains\\n2.The “location” of each cluster center is specified by 3 \\ncoordinates, one for each column we’re clustering\\n3.The list assigning either cluster 1 or 2 to each data point\\n1.79.5% of the variance between the data points is explained by our clustering, we will discuss this in detail later\\n2.List of other types of data included in kmeans_obj\\n24'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 24}, page_content='Measuring distance\\n(3,3)\\n(1,2) 21Distance = √(22+12)\\nx\\n25'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 25}, page_content='•cluster: a vector indicating the cluster to which each point is allocated\\n•centers: a matrix of cluster centers\\n•totss: the total sum of squares (sum of distances between all points)\\n•withinss: vector of within -cluster sum of distances, one number per cluster\\n•tot.withinss: total within -cluster sum of distances, i.e. sum of withinss\\n•betweenss: the between -cluster sum of squares, i.e. totss -tot.withinss\\n•size: the number of points in each cluster\\nTo learn more about the kmeans function run ?kmeanskmeans outputs\\n26'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 26}, page_content='Intra vs. inter- cluster distance\\nIntra-Cluster \\nDistanceInter-Cluster \\nDistance\\nwithinssbetweensstotss = withinss +betweenss\\n27'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 27}, page_content='Step 3: visualize plot\\n# Tell R to read the cluster labels as factors so that ggplot2 (the \\n# graphing  package) can read them as category labels instead of # continuous variables (numeric variables).party_clusters_Dem = as.factor(kmeans_obj_Dem$cluster)\\n# What does party_clusters look like?View( party_clusters_Dem )\\nView(as.data.frame(party_clusters_Dem))\\n# Set up labels for our data so that we can compare Democrats and \\n# Republicans.party_labels_Dem = house_votes_Dem$partyScript\\n28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 28}, page_content='ggplot(house_votes_Dem, aes(x = aye, \\ny = nay,\\nshape = party_clusters_Dem)) + \\ngeom_point(size = 6) +\\nggtitle(\"Aye vs. Nay votes for Democrat -introduced bills\") +\\nxlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", \\nlabels = c(\"Cluster 1\", \"Cluster 2\" ),\\nvalues = c(\"1\" , \"2\")) +\\ntheme_light()Step 3: visualize plotCosmetics layerBase layer\\nGeom Layer\\nTitles and axis\\nShape\\nTheme\\n29Script'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 29}, page_content='Step 3: visualize plot\\n30'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 30}, page_content='•Two groups exist\\n•Algorithm identifies voting \\npatternsWhat can we infer about \\nthe different clusters?Step 4: analyze results\\n31\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 31}, page_content='ggplot(house_votes_Dem, aes(x = yea, \\ny = nay,\\ncolor= party_labels_Dem,\\nshape = party_clusters_Dem)) + \\ngeom_point(size = 6) +\\nggtitle(\"Aye vs. Nay votes for Democrat -introduced bills\") +\\nxlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", \\nlabels = c(\"Cluster 1\", \"Cluster 2\" ),\\nvalues = c(\"1\" , \"2\")) +\\nscale_color_manual(name = \"Party\", \\nlabels = c(\"Democratic\", \"Republican\"),\\nvalues = c(\"blue\" , \"red\"))+\\ntheme_light()Step 5: validate resultsCosmetics layerScript\\nBase layer\\nGeom Layer\\nTitles and axis\\nColor and \\nshape\\nTheme\\n32'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 32}, page_content='Step 5: validate results\\n33\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 33}, page_content='•Diffuse among Democrats\\n•Republicans more dense\\n•Can gauge “outliers”\\n•Can see the polarization \\nbetween the two political parties Step 6: interpret results\\n34\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 34}, page_content='•Clustering is more powerful than the \\nhuman eye in3D\\n•Clustering mathematically defines \\nwhich cluster the peripheral points \\nshould be in when it’s not obvious to \\nthe human eye\\n•Clustering is helpful when many \\ndimensions / variables exist that you \\ncan’t visualize at once\\n–Whiskey similarity example from \\nclassification lectureClustering vs. visualizing\\nAye, Nay and Other Votes\\nin House of Representatives\\n35'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 35}, page_content='•Goals of clustering:\\n–Maximize the separation between clusters \\n•i.e. Maximize inter -cluster distance \\n–Keep similar points in a \\ncluster close together \\n•i.e. Minimize intra -cluster distanceHow good is the clustering?\\n36'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 36}, page_content='•Look at the variance explained by \\nclusters\\n–In particular, the ratio of inter -cluster \\nvariance to total variance\\n•How much of the total variance is explained by the clustering?Assessing how well an algorithm performsHow good is the clustering?\\nVariation explained by clusters\\n= \\ninter-cluster variance / total variance\\n37'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 37}, page_content='•cluster: a vector indicating the cluster to which each point is allocated\\n•centers: a matrix of cluster centers\\n•totss: the total sum of squares (sum of distances between all points)\\n•withinss: vector of within -cluster sum of distances, one number per cluster\\n•tot.withinss: total within -cluster sum of distances, i.e. sum of withinss\\n•betweenss: the between -cluster sum of squares, i.e. totss -tot.withinss\\n•size: the number of points in each cluster\\nTo learn more about the kmeans function run ?kmeanskmeans outputs\\n38'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 38}, page_content='Intra vs. inter- cluster distance\\nIntra-Cluster \\nDistanceInter-Cluster \\nDistance\\nwithinssbetweensstotss = withinss +betweenss\\n39'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 39}, page_content='How good is the clustering?\\n# Inter-cluster variance,\\n# \"betweenss\" is the sum of the \\n# distances between points from # different clustersnum_Dem= kmeans_obj_Dem$betweenss\\n# Total variance# \"totss\" is the sum of the distances  # between all the points in # the data setdenom_Dem = kmeans_obj_Dem$totss\\n# Variance accounted for by \\n# clustersvar_exp_Dem = num_Dem/ denom_Dem\\nvar_exp_Dem[1] 0.7193405Script\\n40\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 40}, page_content=\"•It’s easier when the number of clusters is known ahead of time, but what if we don't \\nknow how many clusters we should have?\\n•Since different starting points may generate different clusters, we need a way to assess cluster quality as well.How do we choose the number of clusters (i.e. k)?How good is the clustering?\\n41\"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 41}, page_content='1.Elbow method\\n–Computes the percentage of variance explained by clusters for a range of cluster \\nnumbers\\n–Plots a graph so results are easier to see \\n–Not guaranteed to work! It depends on the data in question\\n2.NbClustHow to select k: two methods\\n–Runs 30 different tests and \\nprovides “majority vote” for \\nthe best number of clusters \\n(k’s) to use\\n42'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 42}, page_content='Elbow method: measure variance\\n# Run algorithm with 3 centers\\nset.seed(1 )\\nkmeans_obj_Dem = kmeans(clust_data_Dem,   \\ncenters = 3,\\nalgorithm = \"Lloyd\")\\n# Inter- cluster variance\\nnum_Dem = kmeans_obj_Dem$ betweenss\\n# Total variance\\ndenom_Dem = kmeans_obj_Dem $totss\\n# Variance accounted for by clustersvar_exp_Dem = num_Dem / denom_Dem\\nvar_exp_Dem\\n[1] 0.7949741Script\\n43'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 43}, page_content='•We want to repeat the variance calculation from the previous slide for several \\nnumbers of clusters automatically\\n•We can create a function that contains all the steps we want to automate Automating a step we want to repeat\\nfunction(data, item to iterate through)\\n44'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 44}, page_content='# The function explained_variance wraps our code from previous slides.\\nexplained_variance = function( data_in, k){\\n# Running k- means algorithm\\nset.seed(1 )  \\nkmeans_obj = kmeans(data_in, centers = k,\\nalgorithm = \"Lloyd\" )\\n# Variance accounted for by clusters\\nvar_exp = kmeans_obj $betweenss / \\nkmeans_obj$totss\\nvar_exp\\n}Automating a step we want to repeat\\nScript\\n1.A new variable is created and set equal \\nto our function()\\n2.The commands inside the function are wrapped in curly braces {}\\n3.Inside the parentheses, we specify the variables that the user will input and that will then be used inside the function where they appear\\n45'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 45}, page_content='# Recall the variable we are using for the \\n# data that we\\'re clustering.\\nclust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]\\nView( clust_data_Dem)\\n# The sapply() function plugs several values \\n# into explained_variance.\\nexplained_var_Dem = sapply(1 :10, explained_variance, \\ndata_in = clust_data_Dem)\\nView( explained_var_Dem)\\n# Data for ggplot2\\nelbow_data_Dem = data.frame(k = 1:10, \\nexplained_var_Dem)\\nView( elbow_data_Dem)Automating a step we want to repeat\\n1.sapply() applies a function to a \\nvector\\n2.We have to tell sapply() that \\nthe we want the \\nexplained_variance function \\nto use the clust_data data\\n3.Next, we create a data frame that contains both the new variance variable (explained_var_Dem) \\nand the different numbers of k that we used in the previous function (1 through 10)Function we created \\nScript\\n46'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 46}, page_content='# Plotting data\\nggplot(elbow_data_Dem, \\naes(x = k,  \\ny = explained_var_Dem)) + \\ngeom_point(size = 4) +\\ngeom_line(size = 1 ) +\\nxlab(\"k\" ) + \\nylab(\"Intercluster Variance/Total Variance\" ) + \\ntheme_light()Elbow method: plotting the graph\\nScript\\n1.geom_point() sets the size of the data points\\n2.geom_line() sets the thickness of the line\\n47'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 47}, page_content='Looking for the kink in graph of  inter- cluster variance / total \\nvarianceElbow method: measure variance\\nOriginal data Elbow methodk = 2\\n48\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 48}, page_content='•Library: \"NbClust\"\\nFunctions:  \"NbClust\"\\nInputs : \\n•data –data array or data frame\\n•min.nc / max.nc –minimum/maximum number of clusters\\n•method –\"kmeans\"\\n•There are other, more advanced arguments that can be customized but are outside of \\nthe scope of this course and are note necessary to for NbClust to workThere are a number of ways to choose the right k.\\nNbClust runs 30 tests and selects k based on majority voteNbClust: k by majority vote\\nNbClust(data, max.nc, method = \"kmeans\")\\n49'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 49}, page_content='# Install the package.\\ninstall.packages(\"NbClust\" )\\nlibrary(NbClust)\\n# Run NbClust.\\nnbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")\\n# View the output of NbClust.nbclust_obj_Dem\\n# View the output that shows the number of clusters each \\n# method recommends.View( nbclust_obj_Dem $Best.nc)NbClust : k by majority vote\\nScript\\n50'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 50}, page_content='NbClust: k by majority vote\\n> nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")\\n...\\n******************************************************************* * Among all indices:                                                * 14 proposed 2 as the best number of clusters * 3 proposed 3 as the best number of clusters * 1 proposed 4 as the best number of clusters * 3 proposed 6 as the best number of clusters * 1 proposed 9 as the best number of clusters * 1 proposed 10 as the best number of clusters * 1 proposed 15 as the best number of clusters \\n***** Conclusion *****                            \\n* According to the majority rule, the best number of clusters is  2\\nNote: additional information appears; the above information is most relevant to us for nowConsole\\n51'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 51}, page_content='NbClust: k by majority vote\\n> nbclust_obj_Dem Console•nbclust_obj_Dem shows the outputs of NbClust\\n–One of the outputs is Best.nc, which shows the number of clusters                               \\nrecommended by each test \\n52'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 52}, page_content='NbClust: k by majority vote\\n•We want to visualize a histogram to make it obvious how many votes there are \\nfor each number of clusters \\n53'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 53}, page_content='# Subset the 1st row from Best.nc and convert it  \\n# to a data frame, so ggplot2 can plot it.\\nfreq_k_Dem = nbclust_obj_Dem $Best.nc[1 ,]\\nfreq_k_Dem = data.frame( freq_k_Dem)\\nView(freq_k_Dem)\\n# Check the maximum number of clusters.\\nmax(freq_k_Dem )\\n# Plot as a histogram.\\nggplot(freq_k_Dem,\\naes(x = freq_k_Dem)) +\\ngeom_bar() +scale_x_continuous(breaks = seq(0 , 15, by = 1)) +\\nscale_y_continuous(breaks = seq(0 , 12, by = 1)) +\\nlabs(x = \"Number of Clusters\",\\ny = \"Number of Votes\" ,\\ntitle = \"Cluster Analysis\")NbClust: k by majority vote\\nScript\\n2 clusters is the \\nwinner with 12 votes\\n54'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 54}, page_content='•If you’re a lobbyist, which congressperson can you influence for swing votes?\\n•If you’re managing a campaign and your competitor is always voting along \\nparty lines, how can you use that information?\\n•If your congressperson is not an active voter, is she representing your interests?\\n•What do the voting patterns look like for Republican -introduced bills?Application of results\\n55'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 55}, page_content='•Could see differences between the \\npatterns of Reb lead bills and Democrat lead bills\\n•Could provide information on congressmen that might be see has swing votes. Implications of results\\n56'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 56}, page_content=\"•We are assuming that the patterns correspond with the same bills being voted \\non –perhaps some Congressmen have the same number of 'aye' and 'nay' \\nvotes, but voted on different bills\\n•Network analysis can help determine additional connections between Congressmen\\n•We haven't taken extenuating factors into account – political initiatives, \\ncurrent events, etc.\\nThis is a preliminary analysis that gives us initial \\ninsights and can help us direct further researchLimitations of results\\n57\"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 57}, page_content=\"•The good and bad\\n–+ cheap –NO LABELS , labels are expensive to create and maintain\\n–+/-clustering always works\\n–-Many methods to choose from and knowing the right one can be nontrivial and the \\ndifferences between many are almost zero, so you need to understand what you're \\ndoing\\n•The evil\\n–Curse of dimensionality\\n–Clusters may result from poor data quality\\n–Non-deterministic (e.g. k -means) subject to local minimum. Since it works with \\naverages, k-means does not get much better with Big Data (marginal improvements) \\n–Non spherical data may result in poor clustering (depending on method used)\\n–Unequal cluster sizes may result in poor clustering (depending on method used)The good, bad, and evil\\n58\"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 58}, page_content='59\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Clustering_InClass_9.28.21.pdf', 'page': 59}, page_content='•Analysts need to ask the following questions\\n–Do you want overlapping or non -overlapping clusters ?\\n–Does your data satisfy the assumptions of the clustering algorithm?\\n–How was the distance measure identified ? \\n–How many clusters and why ? Identifying the number of clusters is a difficult task if the \\nnumber of class labels is not known beforehand \\n–Does your method scale to the size of the data?\\n–Is the compute time congruent with the temporal budget of your business need (i.e. \\ndo you get answers back in time to make meaningful decisions)The good, bad, and evil\\n60'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 0}, page_content='Introduction to Decision Trees, \\nBackground and Application….Ensemble \\nOverview\\nBrian Wright'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 1}, page_content='Outline\\n\\uf0d8Decision Trees\\n\\uf076Basics\\n\\uf076Background \\n\\uf076Advantages and Limitations\\n\\uf076Mathematical Approaches and Example\\n\\uf076Example in R\\n\\uf076Evaluation\\n2'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 2}, page_content='Basics: Graph Elements\\n\\uf0d8Tree begins with a Root Node that has no incoming edges and two or \\nmore out going edges\\n\\uf0d8Internal Node –Has one incoming edge and two or more outgoing \\nand represent test conditions at every given level \\n\\uf0d8Leaf Node –One incoming edge and no outgoing edges\\n\\uf0d8Edges –Connections between nodes\\n3'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 3}, page_content='Basics: Graph Example\\n4\\nRoot Node\\nInternal \\nNodesEdges\\nLeaf Nodes'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 4}, page_content='1.What is the most important question to move on to a second date?\\nThe question with the most amount of relevant information.Basics: Intuition\\nAre you \\nmarried?What music \\ndo you like?>\\n5'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 5}, page_content='2.How do you combine questions?\\nConditional on the first answer -select the next most important \\nquestion for information gain.Basics: Intuition\\nBelief in a blue \\ncolored sky?Are you married?\\nYESNO\\nStop!Are you married?\\nYES NO\\n>Question 1\\nQuestion 2\\n What music do \\nyou like?\\n6Stop!'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 6}, page_content='3.When should you stop asking questions?\\nWhen the answer no longer provides additional relevant information.Basics: Intuition\\n50% WILL GO ON \\nA SECOND DATE\\n50% WILL GO ON \\nA SECOND DATE\\nWhat music do \\nyou like?\\n7'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 7}, page_content='\\uf0d8Step 1: Ask the question with the most amount of \\ninformation, where “most amount of information ”is \\nbased on some objective criteria.\\n\\uf0d8Step 2: Conditional on the first answer, select the next \\nmost important question.\\n\\uf0d8Step 3: When the answer no longer provides additional \\ninformation (no information gain), stop growing the \\nbranch.\\n\\uf0d8Step 4: Repeat steps 2 and 3 for each question branch.Basics: Building a tree in four steps\\n8'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 8}, page_content='Basics\\n\\uf0d8Decision trees are a hierarchical technique \\n\\uf076Meaning that a series of decisions are made until a predetermined metric is met\\n\\uf0d8Model is built such that a sequence of ordered decisions concerning values of data features \\nresults in assigning class labels\\n\\uf0d8Nonparametric\\n\\uf076Number of parameters is not pre- determined as is the case with linear models that \\nhave pre- determine parameters thus limiting their degrees of freedom\\n\\uf076No assumptions need to be met concerning parameters or distributions\\n\\uf0d8Best recognized through graphs produced\\n\\uf076Type of Acyclic graph -are used to model probabilities, connectivity, and causality. A \\n“graph” in this sense means a structure made from nodes and edges\\n\\uf076Trees consist of nodes and edges defined by decisions rules applied to the data \\nfeatures\\n9'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 9}, page_content='Background\\n10\\n Source: https://www.thehindu.com/features/friday -review/where -sanskrit -meets -computer -science/article7061379.ece'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 10}, page_content='Background \\n\\uf0d8Uses recursive binary splitting - Considering every possible partition of space is \\ncomputationally infeasible, a greedy approach is used to divide the space. \\n\\uf0d8Greedy algorithm because at each step of the tree building process, the best split is \\nmade at that particular step, rather than looking ahead and picking a split that will \\nlead to a better tree in the future.\\n\\uf0d8Trees can be regression or classification based, but in both instances will use \\nrecursive binary splitting\\n\\uf076The difference is that in regression based trees we are predicting the actual class \\nwhereas in classification we are generating the probability of class inclusion as the determinate of the splitting\\n\\uf076This probability measure that drives the splitting for classification comes in two \\nforms: Gini Index or Entropy \\n11'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 11}, page_content='Background: CART Algorithm and C4.5\\n12\\uf0d8Classification and Regression Tree (CART) –1984 Breiman , Friedman, Olshen and \\nStone –Binary Trees\\n\\uf076Can be used on numerical or categorical data\\n\\uf076First splits the training data in two subsets using a single feature k and a threshold tk\\n\\uf076Searches through all possible pairs (k, tk) to identify the split that produces the purest \\nsubsets, based on weighted average of information gain. \\n\\uf076Stops once it cannot find a split that reduces impurity or by a pre -determine node size \\n(hyperparameter ). \\n\\uf0d8C4.5 –Grew out of ID3 (early version) in the late 1980s early 90s both from J. \\nRoss Quinlan, uses gain ratio, accepts cont. and discrete, introduced pruning and \\nthe application of different weights to variables \\n\\uf0d8C5.0 –Next version of C4.5 –performance improvements, computationally more \\nefficient and allows for boosting'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 12}, page_content='Advantages and Limitations\\n13\\n Source: https://socialmediamanager.tweetinggoddess.com/2018/04/20/advantages -and- disadvantages -of-twitter/'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 13}, page_content='Advantages \\n\\uf0d8Simple to understand and to interpret through visualization.\\n\\uf0d8Requires little data preparation. Other techniques often require data normalization, \\ndummy variables need to be created and blank values to be removed. \\n\\uf0d8Able to handle both numerical and categorical data. \\n\\uf0d8Uses a white box model. If a given situation is observable in a model, the explanation \\nfor the condition is easily explained by boolean logic. By contrast, in a black box model \\n(some neural network approaches), results may be more difficult to interpret.\\n\\uf0d8Fairly straight forward to evaluate and understand reliability of the model. ROC/Hit \\nRate/Error Rate/\\n14'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 14}, page_content='Limitations\\n\\uf0d8Decision -tree learners can create over -complex trees that do not \\ngeneralize the data well. This is called overfitting . \\n\\uf076Compare terminal nodes to data points, use the depth of the tree to calculate \\nterminal nodes, for example 6 levels = 26 or 64 terminal nodes, if you have 100 data \\npoints that’s a lot of single data terminal nodes. Leaf nodes roughly double with \\nevery additional level of the tree. \\n\\uf076Mechanisms such setting the minimum number of samples required at a leaf node \\nor setting the maximum depth of the tree can be used to avoid this problem.\\n\\uf0d8Decision trees can be unstable as small variations in the data might result \\nin a completely different tree being generated. This problem is mitigated \\nby using decision trees within an ensemble like Random Forest.\\n15'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 15}, page_content='Limitations\\n\\uf0d8Practical decision -tree learning algorithms are based on heuristic \\nalgorithms such as the greedy algorithm where locally optimal \\ndecisions are made at each node. \\n\\uf076Such algorithms cannot guarantee to return the globally optimal decision \\ntree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\\n\\uf0d8Decision tree learners create biased trees if some classes dominate. \\nIt is therefore recommended to balance the dataset prior to fitting if \\nnecessary \\n16'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 16}, page_content='Mathematical Approaches and Examples\\n17'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 17}, page_content='Mathematical Approaches: Node split criterion \\n18\\uf0d8Decision Trees can use several different types of node split criteria depending \\non the data or data scientist’s preference \\n\\uf076Regression/MSE –Continuous data\\n\\uf076Entropy –Binary data splits \\n\\uf076Gini Coefficient –Most common approach\\n\\uf0d8Let’s take a look at each approachBoth Entropy and Gini \\nCoefficient use Information \\nGain to determine variable \\nsplit criteria '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 18}, page_content='Tree Based Methods\\n\\uf0d8Several approaches to tree building \\n\\uf076CART –Gini Index – Binary Trees\\n\\uf076ID3 –Information Gain  \\n\\uf076C4.5 –Gains Ratio –Introduced pruning \\n\\uf076C5.0 –Improvement on C4.5 –Boosting, computationally efficient \\n19'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 19}, page_content='\\uf0d8The formula for entropy is below, where Pi is the probability that a \\nrandom selection would have a state i\\n(6/6log2 6/6)  \\n= 0 Entropy = sum( -Pi* log2Pi)\\n= 3  \\n= 3  = 2  \\n= 4  = 0\\n= 6  \\n(2/6log2 2/6)  -(4/6log2 4/6)  \\n= 0.92 (3/6 log2 3/6) –(3/6log2 \\n3/6) = 1 Mathematical Approaches: Classification, Entropy (C4.5)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 20}, page_content='\\uf0d8Information gain helps us understand how important an attribute is \\nin the data\\n\\uf0d8We can use it to decide how to order the nodes of the decision tree\\nInformation gain = entropy (parent) –average entropy (children)\\n-(3/6 log2 3/6) –(3/6log2 3/6) = \\n1\\n-(2/2log2 2/2)\\n= 0-(3/4 log2 3/4) –(1/4log2 1/4)\\n= 0.81entropy (parent) average entropy (children)Mathematical Approaches: Classification, Entropy'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 21}, page_content='\\uf0d8In order to calculate the average entropy for the split, we need to weigh the \\nsplit by the number of data points in each node. So we create a weighted \\naverage of the entropy of the children nodes. \\nInformation gain (ratio) = entropy (parent) – average entropy (children)\\n= (2/6 * 0) + (4/6 * 0.81)\\n= 0.54entropy (parent) Weighted average entropy (children)\\n= 1Mathematical Approaches: Classification, Entropy'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 22}, page_content='\\uf0d8In order to construct the tree, we need to follow three steps:\\n1.Choose the attribute with the highest information gain\\n2.Construct the child nodes\\n3.Repeat steps 1 and 2 recursively until \\nno more information can be gainedMathematical Approaches: Information gain + entropy: example\\nOutlook Temp Humidity Play\\nSunny Hot High No\\nSunny Hot Low No\\nCloudy Cool High Yes\\nSunny Cool Low YesShould you play outside?\\n23'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 23}, page_content='1.Choose the attribute with the highest information gainMathematical Approaches: Information gain + entropy: example\\n2 yes 2 no\\nCloudy Sunny\\n1 yes, 2 no 1 yes\\n-(1/3 log2 1/3) –(2/3 log2 2/3)\\n= 0.92-(1/1 log2 1/1) = 0-(2/4 log2 2/4) –(2/4 log2 2/4) = 1\\nI.G. = 1 –((3/4 * 0.92) + (1/4 * 0 )) = 0.31weighted averageOutlook Temp Humidity Play\\nSunny Hot High No\\nSunny Hot Low No\\nCloudy Cool High Yes\\nSunny Cool Low YesShould you play outside?\\n24'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 24}, page_content='1.Choose the attribute with the highest information gainMathematical Approaches: Information gain + entropy: example\\n2 yes 2 no\\nCool Hot\\n2 no 2 yes\\n-(2/2 log2 2/2)= 0-(2/2 log2 2/2) = 0-(2/4 log2 2/4) –(2/4 log2 2/4) = 1\\nI.G. = 1 –((2/4 * 0) + (2/4 * 0)) = 1weighted averageOutlook Temp Humidity Play\\nSunny Hot High No\\nSunny Hot Low No\\nCloudy Cool High Yes\\nSunny Cool Low YesShould you play outside?\\n0.31\\n25I.G.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 25}, page_content='1.Choose the attribute with the highest information gainMathematical Approaches: Information gain + entropy: example\\n2 yes 2 no\\nLow High\\n1 yes 1 no 1 yes 1 no-(2/4 log2 2/4) –(2/4log2 2/4) = 1\\nI.G. = 1 –((2/4 * 1) + (2/4 * 1)) = 0weighted averageOutlook Temp Humidity Play\\nSunny Hot High No\\nSunny Hot Low No\\nCloudy Cool High Yes\\nSunny Cool Low YesShould you play outside?\\n0.31 1-(1/2 log2 1/2) –(1/2log2 1/2)\\n= 1-(1/2 log2 1/2) –(1/2log2 1/2) = 1\\n26I.G.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 26}, page_content='\\uf0d8Temp has the highest information gain resulting in an entropy of 1, \\nmeaning that this attribute perfectly matches class predictionMathematical Approaches: Information gain + entropy: example\\n2 yes 2 no\\nCool Hot\\n2 no 2 yesOutlook Temp Humidity Play\\nSunny Hot High No\\nSunny Hot Low No\\nCloudy Cool High Yes\\nSunny Cool Low YesShould you play outside?\\n0.31 1 0Play\\n27I.G.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 27}, page_content='C4.5/C5.0 and Gains Ratio \\nC4.5/C5.0 –Uses Gain Ratio that extends the previous example but \\ndividing the information gain by the overall split ratio for the feature\\nGr(S,A) = G(S,A)/ Split (S,A)\\n28Information \\nGain Split \\nInformation '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 28}, page_content='1.Choose the attribute with the highest info gain ratio Mathematical Approaches: Information gain + entropy + gains ratio (C4.5)\\n2 yes 2 no\\nCloudy Sunny\\n1 yes, 2 no 1 yes\\n-(1/3 log2 1/3) –(2/3 log2 2/3)\\n= 0.92-(1/1 log2 1/1) = 0-(2/4 log2 2/4) –(2/4 log2 2/4) = 1\\nI.G. = 1 –((3/4 * 0.92) + (1/4 * 0 )) = 0.31weighted averageOutlook Temp Humidity Play\\nSunny Hot High No\\nSunny Hot Low No\\nCloudy Cool High Yes\\nSunny Cool Low YesShould you play outside?\\n29Split = -3/4log23/4 –1/4log21/4 = .811 \\nG Ratio = .31/.811 = .38 '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 29}, page_content='Mathematical Approaches: Classification, Gini Coefficient (CART)\\n\\uf0d8Gini\\n\\uf076Gini Impurity = 1 – sum[(Pi)2]\\n\\uf076Pi –Represents the probability that a random selection would have state i(kinda like a \\ntarget)\\n\\uf076Same mathematical process as entropy \\n\\uf0d8Example:\\n302 yes 2 no\\n1 yes, 2 no 1 yes\\n1 –[ (1/3)2+ (2/3)2] = 0.44 1 –[ (1/1)2]= 01 –[ (2/4)2–(2/4)2] = 0.5\\nGini Impurity  = 0.5 –((3/4 * 0.44) + (1/4 * 0)) = 0.19weighted average'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 30}, page_content='Mathematical Approaches: Regression/MSE\\n31\\uf0d8Works to identify the split point in the data set that minimizes mean squared \\nerror (MSE) point\\n\\uf0d8The average of each of the groups is the term that minimizes the mean \\nsquared error\\n\\uf076MSE –is the average of the difference between the prediction and actual values\\n\\uf0d8The decision tree algorithm searches through all variables and all possible \\nsplit points to identify the point that minimizes error\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 31}, page_content='Practice…Poll  \\n32https://www.sli.do/\\nUse # 68881\\nOr \\nhttps://app.sli.do/event/tyl2nmrk'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 32}, page_content='Decision Trees: Overfitting and Hyper -parameters\\n\\uf0d8Decision trees are often prone to overfitting, one solution is to utilize \\nthe hyper -parameters to control how the tree grows\\n\\uf0d8Another option is to use an ensemble method via bagging or what’s \\nknown as Random Forest\\n33'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 33}, page_content='Decision Trees: Hyper -parameter tuning (Pruning)\\n\\uf0d8Minimum samples for a node split Minimum number of samples (or \\nobservations) which are required in a node to be considered for splitting. Higher \\nvalues prevent a model from learning relations which might be highly specific to the particular sample. It should be tuned using cross validation.\\n\\uf0d8Minimum samples for a terminal node (leaf) The minimum number of samples \\n(or observations) required in a terminal node or leaf. For imbalanced class problems, a lower value should be used since regions dominant with samples belonging to minority class will be much smaller in number.\\n\\uf0d8Maximum depth of tree (vertical depth) The maximum depth of trees, lower \\nvalues prevent a model from learning relations which might be highly specific to the particular sample. It should be tuned using cross validation.\\n34'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 34}, page_content='Decision Trees: Hyper -parameter tuning (Pruning)\\n\\uf0d8Maximum number of terminal nodes Also referred as number of leaves . Since \\nbinary trees are created, a depth of nwould produce a maximum of 2^n leaves.\\n\\uf0d8Maximum features to consider for split The number of features to consider \\n(selected randomly) while searching for a best split. A typical value is the square \\nroot of total number of available features. A higher number typically leads to over -fitting but is dependent on the problem as well.\\n35'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 35}, page_content='Cross -Validation\\n36\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 36}, page_content='Cross -Validation\\n37\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 37}, page_content='Decision Trees: Definitions\\n\\uf0d8Overfitting –model becomes overly complex and as a result is predicting \\nnoise or the space between features (random error) instead of the true \\nrelationship. It is in theory possible to create a leaf node for every data point. \\n\\uf0d8Ensemble methods – Process of running numerous models and codifying \\nthem using a decision rule to choose the optimal model result –example is \\nmajority vote on feature inclusion\\n\\uf0d8Heuristic algorithms –approaches designed for operational efficiency \\ngenerating an approximation to the ideal result but does not guarantee the best model\\n38'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 38}, page_content='Ensemble Methods\\nA standard error is by definition the standard deviation of the sampling \\ndistribution of a parameter estimate, generated by repeated sampling. \\nxerror reflects the mean of the sample means (of the errors) from the ten folds; \\nxstd reflects the standard deviation of the sample means (of the errors) from the \\nten folds. Thus, xstd is a standard deviation of sample means, which is also \\nknown as the standard error of the mean.\\n39'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Decision_Trees_4.28.21.pdf', 'page': 39}, page_content='Example in R\\n42'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\DS_3001_Day 2_Case_Study.pdf', 'page': 0}, page_content='1\\nCrash Trends in Commercial Vehicles \\nin Virginia \\n\\uf0d8Consultants: Students of Practice and Application 4001 \\n\\uf0d8Client: Commonwealth of Virginia- Department of Motor Vehicles \\n\\uf0d8Description: Crashes involving commercial motor vehicles have increased \\nsubstantially, especially on certain roadways, in the past five years. The \\nVirginia Department of Motor Vehicles is interested in learning about trends in \\nthose crashes and what they can tell us about potential driver training.  \\n\\uf0d8Objective: Determine factors involved in increasing commercial motor vehicle crashes. Identify trends that can alert us to additional training needs for \\nCommercial Drivers License holders that could then be provided to \\nCommercial Driver Training Schools.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\DS_3001_Day 2_Case_Study.pdf', 'page': 1}, page_content='2Group Activity 1\\nYou have 20ish minutes for this exercise. Read through the project \\nsummary again, assign a timekeeper, a note taker, and a presenter then consider and answer the following:\\n1.What is the current situation?  Would you like to know more?  What would you ask your client if you could?\\n2.Define a solid, measurable goal that you think would satisfy your client.  What is your metric of success?\\n3.What data would you like to have (think big)?  How would you get this data?  How would it have been gathered (sensors, cameras, etc.)?  Would retrospective data likely all have been in the same spot? \\n4.What deliverable would you like to hand over to your client at the end of the project (you can think big here –be creative)?'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\DS_3001_Day 2_Case_Study.pdf', 'page': 2}, page_content='3Stop Here for Now…'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\DS_3001_Day 2_Case_Study.pdf', 'page': 3}, page_content='Answer the Four Questions\\nAmerican Pharaoh won the Triple Crown.  He won the Belmont, a \\n12 furlong track, with a time of 2:26.65, the Kentucky Derby, a \\n10 furlong track, with a time of 2:03.02, and the Preakness Stakes, a 9.5 furlong track, with a time of 1.58.46.  What was his \\naverage speed in mph?\\n1.What do I know?\\n2.What am I looking for?\\n3.What else do I need to find it?\\n4.What do I expect?'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\DS_3001_Day 2_Case_Study.pdf', 'page': 4}, page_content=\"Now Solve the Problem\\n\\uf0d8Work in your Lab Teams to code the solution with an output \\nthat reads “[Horse’s Name] averaged [ x ] mph across the three \\ntriple crown races.” \\n\\uf0d8Psuedo Code the framework of the function using a google doc \\nthen code the answer together sharing one person's screen. \\nAmerican Pharaoh won the Triple Crown.  He won the Belmont, a \\n12 furlong track, with a time of 2:26.65, the Kentucky Derby, a 10 \\nfurlong track, with a time of 2:03.02, and the Preakness Stakes, a \\n9.5 furlong track, with a time of 1.58.46.  What was his average \\nspeed in mph?\"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 0}, page_content='Fun with functions and dplyr\\nBrian Wright\\n1/24/2020\\nBrian Wright Fun with functions and dplyr 1/24/2020 1 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 1}, page_content='Overview of Functions (Advanced R)\\nFunctions are at the core of R language, it’s really a function based\\nlanguage\\n“R, at its heart, is a functional language. This means that it has\\ncertain technical properties, but more importantly that it lends\\nitself to a style of problem solving centred on functions.” Hadley\\nWickham\\nBrian Wright Fun with functions and dplyr 1/24/2020 2 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 2}, page_content='What is a functional based language?\\nRecently functions have grown in popularity because they can produce\\neﬃcient and simple solutions to lots of problems. Many of the\\nproblems with performance have been solved.\\nFunctional programming compliments object oriented programming\\nBrian Wright Fun with functions and dplyr 1/24/2020 3 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 3}, page_content='What makes a programming approach “functional”?\\nFunctions can behave like any other data structure\\n▶Assign them to variables, store to lists, pass them as aurguments to\\nother functions, create them inside functions and even produce a\\nfunction as a result of a funcion\\nFunctions need to be “pure” meaning that if you call it again with the\\nsame inputs you get the same results. sys.time() not a “pure”\\nfunction\\nThe execution of the function shouldn’t change global variables, have\\nno side eﬀects.\\nBrian Wright Fun with functions and dplyr 1/24/2020 4 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 4}, page_content='Functions\\nFunction don’t have to be “pure” but it can help to ensure your code\\nis doing what you intend it to do.\\nFunctional programming helps to break a problem down into it’s\\npieces. When working to solve a problem it helps to divide the code\\ninto individually operating functions that solve parts of the problem.\\nBrian Wright Fun with functions and dplyr 1/24/2020 5 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 5}, page_content='Types of Functions\\nFigure 1: Function Types\\nBrian Wright Fun with functions and dplyr 1/24/2020 6 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 6}, page_content='Let’s Build a Function\\nBasically recipes composed of series of R statements\\nname <- funtion (variables){\\n#In here goes the series of R statements\\n}\\nBrian Wright Fun with functions and dplyr 1/24/2020 7 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 7}, page_content='Example, talk out the steps\\nmy_mean <- function (x){\\nSum <- sum(x)#Here we are using a function\\n#inside a function!\\nN <- length (x)\\nreturn (Sum /N)#return is optional but helps with\\n#clarity on some level.\\n}\\nCreate a little list and pass it to the function and see if it works.\\nAlso call the Sum and N variables...does this work?\\nBrian Wright Fun with functions and dplyr 1/24/2020 8 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 8}, page_content='Functional - Will show later, Function Factory\\n(Advanced R)\\npower1 <- function (exp) {\\nfunction (x) {\\nx^exp\\n}\\n}\\n#Assigning the exponentials\\nsquare <- power1 (2)\\ncube <- power1 (3)\\nBrian Wright Fun with functions and dplyr 1/24/2020 9 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 9}, page_content='Run the Created Functions\\nsquare (3)\\n> [1] 9\\ncube (3)\\n> [1] 27\\nBrian Wright Fun with functions and dplyr 1/24/2020 10 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 10}, page_content='Quick Exercise\\nCreate a function that computes the range of a variable and then\\nfornogoodreasonadds100anddividesby10. Writeoutthesteps\\nyou would need ﬁrst in Pseudocode, then develop the function.\\nBrian Wright Fun with functions and dplyr 1/24/2020 11 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 11}, page_content='dplyrverbs in the tidyverse\\nThe dplyrpackage gives us a few verbs for data manipulation\\nFunction Purpose\\nselect Select columns based on name or position\\nmutate Create or change a column\\nﬁlter Extract rows based on some criteria\\narrange Re-order rows based on values of variable(s)\\ngroup_by Split a dataset by unique values of a variable\\nsummarize Create summary statistics based on columns\\nBrian Wright Fun with functions and dplyr 1/24/2020 12 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 12}, page_content='select\\nYou can select columns by name or position, of course.\\nYou can also select columns based on some criteria, which are\\nencapsulated in functions.\\nstarts_with(“ \"), ends_with(\" ”), contains(“____”)\\none_of(“____”,“_____”,“______”)\\nThere are others; see help(starts_with) .\\nBrian Wright Fun with functions and dplyr 1/24/2020 13 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 13}, page_content='Example\\nLoad the weather.csv . This contains daily temperature data in 2010 for\\nsome location.\\nhead (weather, 2)\\n> # A tibble: 2 x 35\\n> id year month element d1 d2 d3 d4 d5 d6 d7 d8\\n> <chr> <int> <int> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\\n> 1 MX17~ 2010 1 tmax NA NA NA NA NA NA NA NA\\n> 2 MX17~ 2010 1 tmin NA NA NA NA NA NA NA NA\\n> # ... with 23 more variables: d9 <lgl>, d10 <dbl>, d11 <dbl>, d12 <lgl>,\\n> # d13 <dbl>, d14 <dbl>, d15 <dbl>, d16 <dbl>, d17 <dbl>, d18 <lgl>,\\n> # d19 <lgl>, d20 <lgl>, d21 <lgl>, d22 <lgl>, d23 <dbl>, d24 <lgl>,\\n> # d25 <dbl>, d26 <dbl>, d27 <dbl>, d28 <dbl>, d29 <dbl>, d30 <dbl>,\\n> # d31 <dbl>\\nBrian Wright Fun with functions and dplyr 1/24/2020 14 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 14}, page_content='How would you just select the columns with the daily\\ndata?\\nselect (weather, starts_with (\"d\"))\\nBrian Wright Fun with functions and dplyr 1/24/2020 15 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 15}, page_content='mutate\\nmutatecan either transform a column in place or create a new column in\\na dataset\\nWe’ll use the in-built mpgdataset for this example, We’ll select only the\\ncity and highway mileages. To use this selection later, we will need to\\nassign it to a new name\\nmpg1 <- select (mpg, cty, hwy)\\nBrian Wright Fun with functions and dplyr 1/24/2020 16 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 16}, page_content='mutate\\nWe’ll change the city and highway mileage to km/l from mpg. This will\\ninvolve multiplying it by 1.6 and dividing by 3.8\\nhead (mutate (mpg1, cty = cty *1.6 /3.8,\\nhwy = hwy *1.6/3.8), 5)\\n> # A tibble: 5 x 2\\n> cty hwy\\n> <dbl> <dbl>\\n> 1 7.58 12.2\\n> 2 8.84 12.2\\n> 3 8.42 13.1\\n> 4 8.84 12.6\\n> 5 6.74 10.9\\nThis is in-place replacement\\nBrian Wright Fun with functions and dplyr 1/24/2020 17 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 17}, page_content='New Variable Deﬁned\\nmutate (mpg1, cty1 = cty *1.6/3.8, hwy1 = hwy *1.6/3.8)\\n> # A tibble: 234 x 4\\n> cty hwy cty1 hwy1\\n> <int> <int> <dbl> <dbl>\\n> 1 18 29 7.58 12.2\\n> 2 21 29 8.84 12.2\\n> 3 20 31 8.42 13.1\\n> 4 21 30 8.84 12.6\\n> 5 16 26 6.74 10.9\\n> 6 18 26 7.58 10.9\\n> 7 18 27 7.58 11.4\\n> 8 18 26 7.58 10.9\\n> 9 16 25 6.74 10.5\\n> 10 20 28 8.42 11.8\\n> # ... with 224 more rows\\nThis creates new variables\\nBrian Wright Fun with functions and dplyr 1/24/2020 18 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 18}, page_content='ﬁlter\\nfilterextracts rows based on criteria\\nfilter (mpg, cyl ==4)\\n> # A tibble: 81 x 11\\n> manufacturer model displ year cyl trans drv cty hwy fl class\\n> <chr> <chr> <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\\n> 1 audi a4 1.8 1999 4 auto~ f 18 29 p comp~\\n> 2 audi a4 1.8 1999 4 manu~ f 21 29 p comp~\\n> 3 audi a4 2 2008 4 manu~ f 20 31 p comp~\\n> 4 audi a4 2 2008 4 auto~ f 21 30 p comp~\\n> 5 audi a4 q~ 1.8 1999 4 manu~ 4 18 26 p comp~\\n> 6 audi a4 q~ 1.8 1999 4 auto~ 4 16 25 p comp~\\n> 7 audi a4 q~ 2 2008 4 manu~ 4 20 28 p comp~\\n> 8 audi a4 q~ 2 2008 4 auto~ 4 19 27 p comp~\\n> 9 chevrolet mali~ 2.4 1999 4 auto~ f 19 27 r mids~\\n> 10 chevrolet mali~ 2.4 2008 4 auto~ f 22 30 r mids~\\n> # ... with 71 more rows\\nThis extracts only 4 cylinder vehicles\\nOther choices might be cyl != 4 ,cyl > 4,year == 1999 ,\\nmanufacturer==\"audi\"Brian Wright Fun with functions and dplyr 1/24/2020 19 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 19}, page_content='Practice Piping\\nadmit_df <- read_csv (\"LogReg.csv\")\\nstr(admit_df)\\n> Classes /quotesingle.ts1spec_tbl_df /quotesingle.ts1,/quotesingle.ts1tbl_df /quotesingle.ts1,/quotesingle.ts1tbl/quotesingle.ts1and /quotesingle.ts1data.frame /quotesingle.ts1: 400 obs. of 4 variables:\\n> $ admit: num 0 1 1 1 0 1 1 0 1 0 ...\\n> $ gre : num 380 660 800 640 520 760 560 400 540 700 ...\\n> $ gpa : num 3.61 3.67 4 3.19 2.93 3 2.98 3.08 3.39 3.92 ...\\n> $ rank : num 3 3 1 4 4 2 1 2 3 2 ...\\n> - attr(*, \"spec\")=\\n> .. cols(\\n> .. admit = col_double(),\\n> .. gre = col_double(),\\n> .. gpa = col_double(),\\n> .. rank = col_double()\\n> .. )\\n#Do we notice anything that seems a bit off.\\nBrian Wright Fun with functions and dplyr 1/24/2020 20 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 20}, page_content='Coercion num to factor\\nadmit_df $rank <- as.factor (admit_df $rank)\\n#changes rank to a factor\\nBrian Wright Fun with functions and dplyr 1/24/2020 21 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 21}, page_content='Five Basic Classes in R\\ncharacter\\nnumeric (double precision ﬂoating point numbers, default)\\ninteger (subset of numeric)\\ncomplex (j = 10 + 5i)\\nlogical (True/False)\\nBrian Wright Fun with functions and dplyr 1/24/2020 22 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 22}, page_content='All have coercion calls (example from: R Nuts and\\nBolts)\\nx <- 0 :6\\nclass (x)#why\\n> [1] \"integer\"\\nas.numeric (x)\\n> [1] 0 1 2 3 4 5 6\\nas.logical (x)\\n> [1] FALSE TRUE TRUE TRUE TRUE TRUE TRUE\\nas.character (x)\\n> [1] \"0\" \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"\\nBrian Wright Fun with functions and dplyr 1/24/2020 23 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 23}, page_content='Functional Example: Pass a function get a vector\\nWe can also convert multiple columns using lapply(), great example of\\nfunctional orientation of R.\\nnames <- c(\"admit\",\"rank\")\\n#using names as a index on admit_df,\\nadmit_df[,names] <- lapply (admit_df[,names], factor)\\n#Check class of those two variables\\n(as.character (meta_fun <- lapply (subset (admit_df,\\nselect = names),\\nclass)))\\n> [1] \"factor\" \"factor\"\\n#using a functional with two functions inside that creates a object\\ncoerced to a character list...what fun.\\nBrian Wright Fun with functions and dplyr 1/24/2020 24 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 24}, page_content='Using the code chunk below to “group_by” rank\\nBrian Wright Fun with functions and dplyr 1/24/2020 25 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 25}, page_content='Using the code chunk below to ﬁlter by 1 in the admit\\ncolumn\\nBrian Wright Fun with functions and dplyr 1/24/2020 26 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 26}, page_content='Ok now summarise by average GPA\\nBrian Wright Fun with functions and dplyr 1/24/2020 27 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\fun_with_functions_dplyr.pdf', 'page': 27}, page_content='Now Pipe everything together\\nBrian Wright Fun with functions and dplyr 1/24/2020 28 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 0}, page_content='R Data Vis: ggplot\\nBrian Wright, PhD\\nFeb, 2020\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 1 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 1}, page_content='Why visualize data? Anscombe’s data example\\ndataset3 dataset4dataset1 dataset2\\n5 10 15 5 10 155.07.510.012.5\\n5.07.510.012.5\\nxy\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 2 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 2}, page_content='All Have the Summary Stats\\nStatistic Value\\nmean(x) 9\\nmean(y) 7.5\\nvar(x) 11\\nvar(y) 4.13\\ncor(x,y) 0.82\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 3 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 3}, page_content='The DataSaurus dozen\\nx_shapestar v_lines wide_lineshigh_lines slant_down slant_updino dots h_linesaway bullseye circle\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 4 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 4}, page_content='Same Stats\\nStatistic Value\\nmean(x) 54.3\\nmean(y) 47.8\\nvar(x) 281\\nvar(y) 725\\ncor(x,y) -0.07\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 5 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 5}, page_content='Bottom line\\nSummary statistics cannot always distinguish datasets\\nTake advantage of humans’ ability to visually recognize and remember\\npatterns\\nFind discrepancies in the data more easily\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 6 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 6}, page_content='What is ggplot2?\\nA second (and ﬁnal) iteration of the ggplot\\nImplementation of Wilkerson’s Grammar of Graphics in R\\nConceptually, a way to layer diﬀerent elements onto a canvas to create\\na data visualization\\nStarted as Dr. Hadley Wickham’s PhD thesis (with Dr. Dianne Cook)\\nWon the John M. Chambers Statistical Software Award in 2006\\nMimicked in other software platforms\\n▶ggplotandseaborn in Python\\n▶Translated in plotly\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 7 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 7}, page_content='ggplot2 uses the grammar ofgraphics\\nA grammar . . .\\ncompose and re-use small parts\\nbuild complex structures from simpler units\\nof graphics\\nthink of yourself as a painter\\nbuild a visualization using layers on a canvas\\ndraw layers on top of each other\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 8 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 8}, page_content='A dataset\\nlibrary (tidyverse)\\nlibrary (rio)\\nbeaches <- import (/quotesingle.ts1beaches.csv /quotesingle.ts1)\\n#> # A tibble: 6 x 12\\n#> date year month day season rainfall temperature enterococci day_num\\n#> <date> <int> <int> <int> <int> <dbl> <dbl> <dbl> <int>\\n#> 1 2013-01-02 2013 1 2 1 0 23.4 6.7 2\\n#> 2 2013-01-06 2013 1 6 1 0 30.3 2 6\\n#> 3 2013-01-12 2013 1 12 1 0 31.4 69.1 12\\n#> 4 2013-01-18 2013 1 18 1 0 46.4 9 18\\n#> 5 2013-01-24 2013 1 24 1 0 27.5 33.9 24\\n#> 6 2013-01-30 2013 1 30 1 0.6 26.6 26.5 30\\n#> # ... with 3 more variables: month_num <int>, month_name <chr>,\\n#> # season_name <chr>\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 9 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 9}, page_content='Building a graph: Start with a blank canvas\\nggplot ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 10 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 10}, page_content='Add a data set\\nggplot (\\ndata = beaches #<< loaded earlier\\n)\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 11 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 11}, page_content='Add a mapping from data to elements\\nggplot (\\ndata = beaches,\\nmapping = aes(# everytime we add a \"data element\"\\n#we add a aesthetic\\nx = temperature,\\ny = rainfall\\n)\\n)\\nWhat goes in\\nthe x and y axes\\nthe color of markers\\nthe shape of markers\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 12 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 12}, page_content='0204060\\n20 30 40\\ntemperaturerainfallBrian Wright, PhD R Data Vis: ggplot Feb, 2020 13 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 13}, page_content='Add a geometry to draw\\nggplot (\\ndata = beaches,\\nmapping = aes(\\nx = temperature,\\ny = rainfall\\n)\\n)+\\ngeom_point ()#?\\nWhat to draw:\\nPoints, lines\\nHistogram, bars, pies, etc.\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 14 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 14}, page_content='0204060\\n20 30 40\\ntemperaturerainfallBrian Wright, PhD R Data Vis: ggplot Feb, 2020 15 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 15}, page_content='Add options for the geom\\nggplot (\\ndata = beaches,\\nmapping = aes(\\nx = temperature,\\ny = rainfall\\n)\\n)+\\ngeom_point (size = 4) #?\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 16 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 16}, page_content='0204060\\n20 30 40\\ntemperaturerainfallBrian Wright, PhD R Data Vis: ggplot Feb, 2020 17 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 17}, page_content='Add a mapping to modify the geom\\nggplot (\\ndata = beaches,\\nmapping = aes(\\nx = temperature,\\ny = rainfall\\n)\\n)+\\ngeom_point (\\nmapping = aes(color = season_name),\\nsize = 4 #Why do we need the mapping?\\n)\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 18 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 18}, page_content='0204060\\n20 30 40\\ntemperaturerainfallseason_name\\nAutumn\\nSpring\\nSummer\\nWinterBrian Wright, PhD R Data Vis: ggplot Feb, 2020 19 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 19}, page_content='Split into facets\\nggplot (\\ndata = beaches,\\nmapping = aes(\\nx = temperature,\\ny = rainfall\\n)\\n)+\\ngeom_point (\\nmapping = aes(color = season_name),\\nsize = 4\\n)+\\nfacet_wrap (~season_name) ###\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 20 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 20}, page_content='Summer WinterAutumn Spring\\n20 30 40 20 30 400204060\\n0204060\\ntemperaturerainfallseason_name\\nAutumn\\nSpring\\nSummer\\nWinterBrian Wright, PhD R Data Vis: ggplot Feb, 2020 21 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 21}, page_content='Remove the legend\\nggplot (\\ndata = beaches,\\nmapping = aes(\\nx = temperature,\\ny = rainfall\\n)\\n)+\\ngeom_point (\\nmapping = aes(color = season_name),\\nsize = 4,\\nshow.legend = FALSE ###\\n)+\\nfacet_wrap (~season_name)\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 22 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 22}, page_content='Summer WinterAutumn Spring\\n20 30 40 20 30 400204060\\n0204060\\ntemperaturerainfallBrian Wright, PhD R Data Vis: ggplot Feb, 2020 23 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 23}, page_content='Change the general theme\\nggplot (\\ndata = beaches,\\nmapping = aes(\\nx = temperature,\\ny = rainfall\\n)\\n)+\\ngeom_point (\\nmapping = aes(color = season_name),\\nsize = 4,\\nshow.legend = FALSE\\n)+\\nfacet_wrap (~season_name) +\\ntheme_bw ()###\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 24 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 24}, page_content='Summer WinterAutumn Spring\\n20 30 40 20 30 400204060\\n0204060\\ntemperaturerainfall##\\nUpdate the labels\\nggplot (\\ndata = beaches,\\nmapping = aes(\\nx = temperature,\\ny = rainfall\\n)\\n)+\\ngeom_point (\\nmapping = aes(color = season_name),\\nsize = 4,\\nshow.legend = FALSE\\n)+\\nfacet_wrap (~season_name) +\\ntheme_bw ()+\\nlabs (x = /quotesingle.ts1Temperature (C) /quotesingle.ts1, y = /quotesingle.ts1Rainfall (mm) /quotesingle.ts1)###Brian Wright, PhD R Data Vis: ggplot Feb, 2020 25 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 25}, page_content='Summer WinterAutumn Spring\\n20 30 40 20 30 400204060\\n0204060\\nTemperature (C)Rainfall (mm)Brian Wright, PhD R Data Vis: ggplot Feb, 2020 26 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 26}, page_content='Add titles\\nggplot (\\ndata = beaches,\\nmapping = aes(\\nx = temperature,\\ny = rainfall)\\n)+\\ngeom_point (\\nmapping = aes(color = season_name),\\nsize = 4,\\nshow.legend = FALSE\\n)+\\nfacet_wrap (~season_name) +\\ntheme_bw ()+\\nlabs (x = /quotesingle.ts1Temperature (C) /quotesingle.ts1,\\ny = /quotesingle.ts1Rainfall (mm) /quotesingle.ts1,\\ntitle = /quotesingle.ts1Sydney weather by season /quotesingle.ts1,###\\nsubtitle = \"Data from 2013 to 2018\") ###\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 27 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 27}, page_content='Summer WinterAutumn Spring\\n20 30 40 20 30 400204060\\n0204060\\nTemperature (C)Rainfall (mm)Data from 2013 to 2018Sydney weather by seasonBrian Wright, PhD R Data Vis: ggplot Feb, 2020 28 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 28}, page_content='Customize\\nggplot (\\ndata = beaches,\\nmapping = aes(\\nx = temperature,\\ny = rainfall\\n)\\n)+\\ngeom_point (\\nmapping = aes(color = season_name),\\nsize = 4,\\nshow.legend = FALSE\\n)+\\nfacet_wrap (~season_name) +\\ntheme_bw ()+\\nlabs (x = /quotesingle.ts1Temperature (C) /quotesingle.ts1,\\ny = /quotesingle.ts1Rainfall (mm) /quotesingle.ts1,\\ntitle = /quotesingle.ts1Sydney weather by season /quotesingle.ts1,\\nsubtitle = \"Data from 2013 to 2018\") +\\ntheme (axis.title = element_text (size = 14), ###\\naxis.text = element_text (size = 12), ###\\nstrip.text = element_text (size = 12)) ###\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 29 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 29}, page_content='Summer WinterAutumn Spring\\n20 30 40 20 30 400204060\\n0204060\\nTemperature (C)Rainfall (mm)Data from 2013 to 2018Sydney weather by seasonBrian Wright, PhD R Data Vis: ggplot Feb, 2020 30 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 30}, page_content='The grammar\\nData\\nAesthetics (or aesthetic mappings)\\nGeometries (as layers) or Statistics (as computed layers)\\nFacets\\nThemes\\n(Coordinates)\\n(Scales)\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 31 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 31}, page_content='Peeking under the hood ...\\nWe input the below items\\nggplot (\\ndata = beaches,\\naes(x = temperature,\\ny = rainfall)\\n)+\\ngeom_point ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 32 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 32}, page_content='What’s really run is ...\\nggplot (\\ndata = beaches,\\nmapping = aes(\\nx = temperature, y = rainfall)\\n)+\\nlayer (\\ngeom = \"point\",\\nstat = \"identity\",\\nposition = \"identity\") +\\nfacet_null ()+\\ntheme_grey ()+\\ncoord_cartesian ()+\\nscale_x_continuous ()+\\nscale_y_continuous ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 33 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 33}, page_content='Exploring aesthetics: Mapping color\\nggplot (\\ndata=beaches,\\naes(x = date,\\ny = log10 (enterococci),\\ncolor=season_name)\\n)+\\ngeom_line ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 34 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 34}, page_content='−10123\\n2014 2016 2018\\ndatelog10(enterococci)season_name\\nAutumn\\nSpring\\nSummer\\nWinterBrian Wright, PhD R Data Vis: ggplot Feb, 2020 35 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 35}, page_content='Adding groups to the mapping\\nggplot (\\ndata=beaches,\\naes(x = date,\\ny = log10 (enterococci),\\ncolor = season_name,\\ngroup = 1) ###\\n)+\\ngeom_line ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 36 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 36}, page_content='−10123\\n2014 2016 2018\\ndatelog10(enterococci)season_name\\nAutumn\\nSpring\\nSummer\\nWinterBrian Wright, PhD R Data Vis: ggplot Feb, 2020 37 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 37}, page_content='Fixing the legend ordering\\nggplot (\\ndata=beaches,\\naes(x = date,\\ny = log10 (enterococci),\\ncolor = fct_relevel (season_name,\\nc(/quotesingle.ts1Spring /quotesingle.ts1,/quotesingle.ts1Summer /quotesingle.ts1,/quotesingle.ts1Autumn /quotesingle.ts1,/quotesingle.ts1Winter /quotesingle.ts1)),\\ngroup = 1)\\n)+\\ngeom_line ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 38 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 38}, page_content='Yikes!\\n−10123\\n2014 2016 2018\\ndatelog10(enterococci)fct_relevel(season_name, c(\"Spring\", \"Summer\", \"Autumn\", \"Winter\"))\\nSpring\\nSummer\\nAutumn\\nWinter\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 39 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 39}, page_content='Fixing the legend ordering\\nggplot (\\ndata=beaches,\\naes(x = date,\\ny = log10 (enterococci),\\ncolor = fct_relevel (season_name,\\nc(/quotesingle.ts1Spring /quotesingle.ts1,/quotesingle.ts1Summer /quotesingle.ts1,/quotesingle.ts1Autumn /quotesingle.ts1,/quotesingle.ts1Winter /quotesingle.ts1)),\\ngroup = 1)\\n)+\\ngeom_line ()+\\nlabs (color = /quotesingle.ts1Seasons /quotesingle.ts1)###\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 40 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 40}, page_content='−10123\\n2014 2016 2018\\ndatelog10(enterococci)Seasons\\nSpring\\nSummer\\nAutumn\\nWinterBrian Wright, PhD R Data Vis: ggplot Feb, 2020 41 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 41}, page_content='You can also ﬁll based on data\\nggplot (\\ndata=beaches,\\naes(x = log10 (enterococci),\\nfill = season_name)\\n)+\\ngeom_histogram ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 42 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 42}, page_content='Works a little better\\n0102030\\n−1 0 1 2 3\\nlog10(enterococci)countseason_name\\nAutumn\\nSpring\\nSummer\\nWinter\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 43 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 43}, page_content='Exploring geometries: Univariate plots\\nlibrary (tidyverse)\\nlibrary (rio)\\ndat_spine <- import (/quotesingle.ts1Dataset_spine.csv /quotesingle.ts1,\\ncheck.names = T)\\nggplot (\\ndata=dat_spine,\\naes(x = Degree.spondylolisthesis)\\n)+\\ngeom_histogram ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 44 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 44}, page_content='#> /grave.ts1stat_bin() /grave.ts1using /grave.ts1bins = 30 /grave.ts1. Pick better value with /grave.ts1binwidth /grave.ts1.\\n050100\\n0 100 200 300 400\\nDegree.spondylolisthesiscount\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 45 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 45}, page_content='Histograms\\nggplot (\\ndata=dat_spine,\\naes(x = Degree.spondylolisthesis)\\n)+\\ngeom_histogram (bins = 100)\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 46 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 46}, page_content='Very diﬀerent view of the data\\n0204060\\n0 100 200 300 400\\nDegree.spondylolisthesiscount\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 47 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 47}, page_content='Density plots\\nggplot (\\ndata=dat_spine,\\naes(x = Degree.spondylolisthesis)\\n)+\\ngeom_density ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 48 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 48}, page_content='0.0000.0050.0100.0150.020\\n0 100 200 300 400\\nDegree.spondylolisthesisdensityBrian Wright, PhD R Data Vis: ggplot Feb, 2020 49 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 49}, page_content='Density plots\\nggplot (\\ndata=dat_spine,\\naes(x = Degree.spondylolisthesis)\\n)+\\ngeom_density (adjust = 1 /5)# Use 1/5 the bandwidth\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 50 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 50}, page_content='0.000.010.020.030.04\\n0 100 200 300 400\\nDegree.spondylolisthesisdensityBrian Wright, PhD R Data Vis: ggplot Feb, 2020 51 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 51}, page_content='Layering geometries\\nggplot (\\ndata=dat_spine,\\naes(x = Degree.spondylolisthesis,\\ny = stat (density))\\n)+# Re-scales histogram\\ngeom_histogram (bins = 100, fill= /quotesingle.ts1yellow /quotesingle.ts1)+\\ngeom_density (adjust = 1 /5, color = /quotesingle.ts1orange /quotesingle.ts1, size = 2)\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 52 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 52}, page_content='0.000.010.020.030.04\\n0 100 200 300 400\\nDegree.spondylolisthesisdensityBrian Wright, PhD R Data Vis: ggplot Feb, 2020 53 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 53}, page_content='Bar plots (categorical variable)\\ndat_brca <-\\nrio::import (/quotesingle.ts1clinical_data_breast_cancer_modified.csv /quotesingle.ts1,\\ncheck.names = T)\\nggplot (\\ndata=dat_brca,\\naes(x = Tumor)\\n)+\\ngeom_bar ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 54 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 54}, page_content='0204060\\nT1 T2 T3 T4\\nTumorcountBrian Wright, PhD R Data Vis: ggplot Feb, 2020 55 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 55}, page_content='Bar plots (categorical variable)\\nggplot (\\ndata=dat_brca,\\naes(x = Tumor,\\nfill = ER.Status)\\n)+#<<\\ngeom_bar ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 56 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 56}, page_content='Add additional information via mapping\\n0204060\\nT1 T2 T3 T4\\nTumorcountER.Status\\nIndeterminate\\nNegative\\nPositive\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 57 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 57}, page_content='Bar plots (categorical variable)\\nggplot (\\ndata=dat_brca,\\naes(x = Tumor,\\nfill = ER.Status)\\n)+\\ngeom_bar (position = /quotesingle.ts1dodge /quotesingle.ts1)\\n# Default is position = \"stack\"\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 58 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 58}, page_content='Change the nature of the geometry\\n010203040\\nT1 T2 T3 T4\\nTumorcountER.Status\\nIndeterminate\\nNegative\\nPositive\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 59 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 59}, page_content='Graphing tabulated data\\n(tabulated <- dat_brca %>% count (Tumor))\\n#> # A tibble: 4 x 2\\n#> Tumor n\\n#> <chr> <int>\\n#> 1 T1 15\\n#> 2 T2 65\\n#> 3 T3 19\\n#> 4 T4 6\\nggplot (\\ndata = tabulated,\\naes(x = Tumor, y = n)\\n)+\\ngeom_bar ()\\n#> Error: stat_count() can only have an x or y aesthetic.\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 60 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 60}, page_content='Graphing tabulated data\\ntabulated <- dat_brca %>% count (Tumor)\\ntabulated\\nggplot (\\ndata = tabulated,\\naes(x = Tumor, y = n)\\n)+\\ngeom_bar (stat = /quotesingle.ts1identity /quotesingle.ts1)###\\nHere we need to change the default computation\\nThe barplot usually computes the counts ( stat_count )\\nWe suppress that here since we have already\\ndone the computation\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 61 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 61}, page_content='0204060\\nT1 T2 T3 T4\\nTumornBrian Wright, PhD R Data Vis: ggplot Feb, 2020 62 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 62}, page_content='Peeking under the hood\\nplt <- ggplot (\\ndata = tabulated,\\naes(x = Tumor, y = n)\\n)+\\ngeom_bar ()\\nplt$layers\\n#> [[1]]\\n#> geom_bar: width = NULL, na.rm = FALSE, orientation = NA\\n#> stat_count: width = NULL, na.rm = FALSE, orientation = NA\\n#> position_stack\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 63 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 63}, page_content='Peeking under the hood\\nplt <- ggplot (\\ndata = tabulated,\\naes(x = Tumor, y = n)) +\\ngeom_bar (stat = /quotesingle.ts1identity /quotesingle.ts1)\\nplt$layers\\n#> [[1]]\\n#> geom_bar: width = NULL, na.rm = FALSE, orientation = NA\\n#> stat_identity: na.rm = FALSE\\n#> position_stack\\nEach layer has a geometry, statistic and position associated with it\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 64 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 64}, page_content='Bivariate plots: Scatter plots\\nggplot (\\ndata = beaches,\\naes(x = date, y = temperature)\\n)+\\ngeom_point ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 65 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 65}, page_content='203040\\n2014 2016 2018\\ndatetemperatureBrian Wright, PhD R Data Vis: ggplot Feb, 2020 66 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 66}, page_content='Scatter plots\\nggplot (\\ndata = beaches,\\naes(x = date, y = temperature, group=1) #Add the group argu.\\n)+\\ngeom_point (color= /quotesingle.ts1black /quotesingle.ts1, size = 3) +\\ngeom_line (color= /quotesingle.ts1red/quotesingle.ts1,size=2)\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 67 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 67}, page_content='Layer points and lines\\n203040\\n2014 2016 2018\\ndatetemperature\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 68 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 68}, page_content='Scatter plots\\nggplot (\\ndata = beaches,\\naes(x = date, y = temperature,group=1)\\n)+\\ngeom_line (color= /quotesingle.ts1red/quotesingle.ts1,size=2) +###\\ngeom_point (color= /quotesingle.ts1black /quotesingle.ts1, size = 3) ###\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 69 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 69}, page_content='Order of laying down geometries matters\\n203040\\n2014 2016 2018\\ndatetemperature\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 70 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 70}, page_content='Doing some computations\\nggplot (\\ndata = beaches,\\naes(x = date, y = temperature, group=1)\\n)+\\ngeom_point ()+\\ngeom_smooth (method= /quotesingle.ts1loess /quotesingle.ts1)\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 71 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 71}, page_content='Averages over 75% of the data\\n#> /grave.ts1geom_smooth() /grave.ts1using formula /quotesingle.ts1y ~ x /quotesingle.ts1\\n203040\\n2014 2016 2018\\ndatetemperature\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 72 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 72}, page_content='Doing some computations\\nggplot (\\ndata = beaches,\\naes(x = date, y = temperature, group=1)\\n)+\\ngeom_point ()+\\ngeom_smooth (method=\"loess\",span = 0.1) ###\\n#?geom_smooth, kinda funny...?\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 73 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 73}, page_content='Big O!\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 74 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 74}, page_content='Averages over 10% of the data\\n#> /grave.ts1geom_smooth() /grave.ts1using formula /quotesingle.ts1y ~ x /quotesingle.ts1\\n203040\\n2014 2016 2018\\ndatetemperature\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 75 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 75}, page_content='Computations over groups\\nggplot (\\ndata = dat_spine,\\naes(x = Sacral.slope,\\ny = Degree.spondylolisthesis)\\n)+\\ngeom_point ()+\\ngeom_smooth ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 76 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 76}, page_content='0100200300400\\n25 50 75 100 125\\nSacral.slopeDegree.spondylolisthesisBrian Wright, PhD R Data Vis: ggplot Feb, 2020 77 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 77}, page_content='Computations over groups\\nggplot (\\ndata = dat_spine,\\naes(x = Sacral.slope,\\ny = Degree.spondylolisthesis,\\ncolor = Class.attribute) ##\\n)+\\ngeom_point ()+\\ngeom_smooth ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 78 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 78}, page_content='Computation is done by groups\\n0100200300400\\n25 50 75 100 125\\nSacral.slopeDegree.spondylolisthesisClass.attribute\\nAbnormal\\nNormal\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 79 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 79}, page_content='Computations over groups\\nggplot (\\ndata = dat_spine,\\naes(x = Sacral.slope,\\ny = Degree.spondylolisthesis,\\ncolor = Class.attribute)\\n)+\\ngeom_point ()+\\ngeom_smooth ()+\\nxlim (0, 100) #Changing the demonsions of the graphic\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 80 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 80}, page_content='Ignore the outlier for now\\n0100200300400\\n0 25 50 75 100\\nSacral.slopeDegree.spondylolisthesisClass.attribute\\nAbnormal\\nNormal\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 81 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 81}, page_content='Computations over groups\\nggplot (\\ndata = dat_spine,\\naes(x = Sacral.slope,\\ny = Degree.spondylolisthesis)\\n)+\\ngeom_point ()+\\ngeom_smooth (aes(color = Class.attribute)) +#\\nxlim (0, 100)\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 82 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 82}, page_content='Only color-code the smoothers\\nYou can change the plot based on where you map the aesthetic\\n0100200300400\\n0 25 50 75 100\\nSacral.slopeDegree.spondylolisthesisClass.attribute\\nAbnormal\\nNormal\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 83 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 83}, page_content='Computations over groups\\nggplot (\\ndata = dat_spine,\\naes(x = Sacral.slope,\\ny = Degree.spondylolisthesis)\\n)+\\ngeom_point ()+\\ngeom_smooth (aes(color = Class.attribute),\\nse = F) +\\n#Turning off the confidence interval\\nxlim (0, 100)\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 84 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 84}, page_content='Looks a little cleaner\\n0100200300400\\n0 25 50 75 100\\nSacral.slopeDegree.spondylolisthesisClass.attribute\\nAbnormal\\nNormal\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 85 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 85}, page_content='Box Plots\\nggplot (\\ndata = beaches,\\naes(x = season_name,\\ny = temperature)\\n)+\\ngeom_boxplot ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 86 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 86}, page_content='203040\\nAutumn Spring Summer Winter\\nseason_nametemperature>What\\nare the components of a boxplot?\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 87 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 87}, page_content='Box Plots\\nggplot (\\ndata = beaches,\\naes(x = season_name,\\ny = temperature)\\n)+\\ngeom_boxplot ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 88 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 88}, page_content='MinimumMaximum (within fence)\\n3rd quartile\\n1st quartileMedian\\n\"Outliers\"\\n203040\\nAutumn Spring Summer Winter\\nseason_nametemperatureBrian Wright, PhD R Data Vis: ggplot Feb, 2020 89 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 89}, page_content='Layers, again\\nggplot (\\ndata = beaches,\\naes(x = season_name,\\ny = temperature)\\n)+\\ngeom_boxplot ()+\\ngeom_jitter (width = 0.2)\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 90 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 90}, page_content='Layers, again\\n203040\\nAutumn Spring Summer Winter\\nseason_nametemperature\\n##\\nLayers, again\\nggplot (\\ndata = beaches,\\naes(x = season_name,\\ny = temperature)\\n)+\\ngeom_violin ()+\\ngeom_jitter (width = 0.2)Brian Wright, PhD R Data Vis: ggplot Feb, 2020 91 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 91}, page_content='203040\\nAutumn Spring Summer Winter\\nseason_nametemperatureBrian Wright, PhD R Data Vis: ggplot Feb, 2020 92 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 92}, page_content='Exploring grouped data\\nggplot (\\ndata = beaches,\\naes(x = temperature,\\nfill = season_name)\\n)+\\ngeom_density ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 93 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 93}, page_content='Not very useful\\n0.000.050.100.15\\n20 30 40\\ntemperaturedensityseason_name\\nAutumn\\nSpring\\nSummer\\nWinter\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 94 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 94}, page_content='Overlaying graphs\\nggplot (\\ndata = beaches,\\naes(x = temperature,\\nfill = season_name)\\n)+\\ngeom_density (alpha = 0.4) # Changes the transparency\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 95 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 95}, page_content='Make graphs more transparent\\n0.000.050.100.15\\n20 30 40\\ntemperaturedensityseason_name\\nAutumn\\nSpring\\nSummer\\nWinter\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 96 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 96}, page_content='Exploding graphs\\nggplot (\\ndata = beaches,\\naes(x = temperature,\\nfill = season_name)\\n)+\\ngeom_density ()+\\nfacet_wrap (~season_name, ncol = 1) ###\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 97 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 97}, page_content='This is called “small multiples” (Tufte)\\nWinterSummerSpringAutumn\\n20 30 400.000.050.100.15\\n0.000.050.100.15\\n0.000.050.100.15\\n0.000.050.100.15\\ntemperaturedensityseason_name\\nAutumn\\nSpring\\nSummer\\nWinter\\n>\\nNotice that all the graphs have the same x-axis. This is a good thing\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 98 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 98}, page_content='Exploding graphs\\nggplot (\\ndata = beaches,\\naes(x = temperature,\\nfill = season_name)\\n)+\\ngeom_density ()+\\nfacet_wrap (~season_name, nrow = 1) ###\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 99 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 99}, page_content='We can arrange them the other way too\\nAutumn Spring Summer Winter\\n20 30 40 20 30 40 20 30 40 20 30 400.000.050.100.15\\ntemperaturedensityseason_name\\nAutumn\\nSpring\\nSummer\\nWinter\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 100 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 100}, page_content='Order and orientation: Arrests in the USA in 1973\\narrests <- import (/quotesingle.ts1USArrests.csv /quotesingle.ts1)\\nggplot (\\ndata = arrests,\\naes(x = State,\\ny = Murder)\\n)+\\ngeom_bar (stat = /quotesingle.ts1identity /quotesingle.ts1)\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 101 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 101}, page_content='051015\\nAlabamaAlaskaArizonaArkansasCaliforniaColoradoConnecticutDelawareFloridaGeorgiaHawaiiIdahoIllinoisIndianaIowaKansasKentuckyLouisianaMaineMarylandMassachusettsMichiganMinnesotaMississippiMissouriMontanaNebraskaNevadaNew HampshireNew JerseyNew MexicoNew YorkNorth CarolinaNorth DakotaOhioOklahomaOregonPennsylvaniaRhode IslandSouth CarolinaSouth DakotaTennesseeTexasUtahVermontVirginiaWashingtonWest VirginiaWisconsinWyoming\\nStateMurder>Hard\\nto read, there is no ordering, and x-labels can’t be seen\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 102 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 102}, page_content='Arrests in the USA in 1973\\nggplot (\\ndata = arrests,\\naes(x = fct_reorder (State, Murder), #Order by murder rate\\ny = Murder)\\n)+\\ngeom_bar (stat = /quotesingle.ts1identity /quotesingle.ts1)\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 103 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 103}, page_content='Arrest in the USA in 1973\\n051015\\nNorth DakotaMaineNew HampshireIowaVermontIdahoWisconsinMinnesotaUtahConnecticutRhode IslandSouth DakotaWashingtonNebraskaMassachusettsOregonHawaiiWest VirginiaDelawareKansasMontanaPennsylvaniaOklahomaWyomingIndianaOhioNew JerseyColoradoArizonaVirginiaArkansasCaliforniaMissouriKentuckyAlaskaIllinoisNew YorkMarylandNew MexicoMichiganNevadaTexasNorth CarolinaAlabamaTennesseeSouth CarolinaFloridaLouisianaMississippiGeorgia\\nfct_reorder(State, Murder)Murder\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 104 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 104}, page_content='Arrests in the USA in 1973\\nggplot (\\ndata = arrests,\\naes(x = fct_reorder (State, Murder), # Order by murder rate\\ny = Murder)\\n)+\\ngeom_bar (stat = /quotesingle.ts1identity /quotesingle.ts1)+\\ncoord_flip ()#Flipping the coordinates\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 105 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 105}, page_content='Flipping the axes makes the states readable\\nNorth DakotaMaineNew HampshireIowaVermontIdahoWisconsinMinnesotaUtahConnecticutRhode IslandSouth DakotaWashingtonNebraskaMassachusettsOregonHawaiiWest VirginiaDelawareKansasMontanaPennsylvaniaOklahomaWyomingIndianaOhioNew JerseyColoradoArizonaVirginiaArkansasCaliforniaMissouriKentuckyAlaskaIllinoisNew YorkMarylandNew MexicoMichiganNevadaTexasNorth CarolinaAlabamaTennesseeSouth CarolinaFloridaLouisianaMississippiGeorgia\\n0 5 10 15\\nMurderfct_reorder(State, Murder)\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 106 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 106}, page_content='Arrests in the USA in 1973\\nggplot (\\ndata = arrests,\\naes(x = fct_reorder (State, Murder), # Order by murder rate\\ny = Murder)\\n)+\\ngeom_bar (stat = /quotesingle.ts1identity /quotesingle.ts1, fill=\"red\") +\\nlabs (x = /quotesingle.ts1State /quotesingle.ts1, y = /quotesingle.ts1Murder rate /quotesingle.ts1)+# Adding labels\\ntheme_bw ()+# Theme\\ntheme (panel.grid.major.y = element_blank (),#\\npanel.grid.minor.x = element_blank ()) +\\ncoord_flip ()# Flip last\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 107 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 107}, page_content='Cleaning it up a little\\nNorth DakotaMaineNew HampshireIowaVermontIdahoWisconsinMinnesotaUtahConnecticutRhode IslandSouth DakotaWashingtonNebraskaMassachusettsOregonHawaiiWest VirginiaDelawareKansasMontanaPennsylvaniaOklahomaWyomingIndianaOhioNew JerseyColoradoArizonaVirginiaArkansasCaliforniaMissouriKentuckyAlaskaIllinoisNew YorkMarylandNew MexicoMichiganNevadaTexasNorth CarolinaAlabamaTennesseeSouth CarolinaFloridaLouisianaMississippiGeorgia\\n0 5 10 15\\nMurder rateState\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 108 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 108}, page_content='Themes\\nggplot comes with a default color scheme. There are several other schemes\\navailable\\nscale_*_brewer uses the ColorBrewer palettes\\nscale_*_gradient uses gradients\\nscale_*_distill uses the ColorBrewer palettes, for continuous\\noutcomes\\nHere * can be color orfill , depending on what you want to\\ncolor\\nNote color refers to the outline, and fill refers to the inside\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 109 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 109}, page_content='No Theme\\nggplot (\\ndata = beaches,\\naes(x = date, y = enterococci,\\ncolor = temperature)\\n)+\\ngeom_point ()+\\nscale_y_log10 (name = /quotesingle.ts1Enterococci /quotesingle.ts1,\\nlabel = scales ::number_format (digits=3))\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 110 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 110}, page_content='No Theme\\n0.101.0010.00100.001 000.00\\n2014 2016 2018\\ndateEnterococci\\n203040temperature\\n##\\nDark\\nggplot (\\ndata = beaches,\\naes(x = date, y = enterococci,\\ncolor = temperature)\\n)+\\ngeom_point ()+\\nscale_y_log10 (name = /quotesingle.ts1Enterococci /quotesingle.ts1,\\nlabel = scales ::number_format (digits=3)) +\\nscale_color_gradient (low = /quotesingle.ts1white /quotesingle.ts1, high= /quotesingle.ts1red/quotesingle.ts1)+\\ntheme_dark ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 111 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 111}, page_content='Dark\\n0.101.0010.00100.001 000.00\\n2014 2016 2018\\ndateEnterococci\\n203040temperature\\n##\\nBlack and White\\nggplot (\\ndata = beaches,\\naes(x = date, y = enterococci,\\ncolor = temperature)\\n)+\\ngeom_point ()+\\nscale_y_log10 (name = /quotesingle.ts1Enterococci /quotesingle.ts1,\\nlabel = scales ::number_format (digits=3)) +\\nscale_color_gradient (low = /quotesingle.ts1blue /quotesingle.ts1, high= /quotesingle.ts1red/quotesingle.ts1)+\\ntheme_bw ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 112 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 112}, page_content='Black and White\\n0.101.0010.00100.001 000.00\\n2014 2016 2018\\ndateEnterococci\\n203040temperature\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 113 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 113}, page_content='Themes\\nYou can create your own custom themes to keep a uniﬁed look to your\\ngraphs\\nggplot comes with\\ntheme_classic\\ntheme_bw\\ntheme_void\\ntheme_dark\\ntheme_gray\\ntheme_light\\ntheme_minimal\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 114 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 114}, page_content='Create your own\\nggplot (\\ndata = dat_spine,\\naes(x = Sacral.slope, y = Degree.spondylolisthesis,\\ncolor = Class.attribute)\\n)+\\ngeom_point ()+\\ngeom_smooth (se = F) +\\ncoord_cartesian (xlim = c(0, 100),\\nylim = c(0,200))\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 115 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 115}, page_content='050100150200\\n0 25 50 75 100\\nSacral.slopeDegree.spondylolisthesisClass.attribute\\nAbnormal\\nNormalBrian Wright, PhD R Data Vis: ggplot Feb, 2020 116 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 116}, page_content='Create your own\\nmy_theme <- function (){\\ntheme_bw ()\\n}\\nggplot (\\ndata = dat_spine,\\naes(x = Sacral.slope, y = Degree.spondylolisthesis,\\ncolor = Class.attribute)\\n)+\\ngeom_point ()+\\ngeom_smooth (se = F) +\\ncoord_cartesian (xlim = c(0, 100),\\nylim = c(0,200)) +\\nmy_theme ()# Just Black and White\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 117 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 117}, page_content='050100150200\\n0 25 50 75 100\\nSacral.slopeDegree.spondylolisthesisClass.attribute\\nAbnormal\\nNormalBrian Wright, PhD R Data Vis: ggplot Feb, 2020 118 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 118}, page_content='Create your own\\nmy_theme <- function (){\\ntheme_bw ()+\\ntheme (axis.text = element_text (size = 14),\\naxis.title = element_text (size = 16),\\npanel.grid.minor = element_blank (),\\nstrip.text = element_text (size=14),\\nstrip.background = element_blank ())\\n}\\nggplot (\\ndata = dat_brca,\\naes(x = Tumor)) +\\ngeom_bar ()+\\nfacet_grid (rows = vars (ER.Status),\\ncols = vars (PR.Status))\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 119 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 119}, page_content='Negative PositiveIndeterminate Negative Positive\\nT1 T2 T3 T4 T1 T2 T3 T40102030\\n0102030\\n0102030\\nTumorcountBrian Wright, PhD R Data Vis: ggplot Feb, 2020 120 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 120}, page_content='Create your own\\nggplot (\\ndata = dat_brca,\\naes(x = Tumor)\\n)+\\ngeom_bar ()+\\nfacet_grid (rows = vars (ER.Status),\\ncols = vars (PR.Status)) +\\nmy_theme ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 121 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 121}, page_content='Negative PositiveIndeterminate Negative Positive\\nT1 T2 T3 T4 T1 T2 T3 T40102030\\n0102030\\n0102030\\nTumorcountBrian Wright, PhD R Data Vis: ggplot Feb, 2020 122 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 122}, page_content='Annotations: Stand-alone stories\\nData visualization to stand on its own\\nRelevant information should be placed on the graph\\nHowever, you need to balance the information content with real estate\\n▶Don’t clutter the graph and make it not readable\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 123 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 123}, page_content='Adding derived statistics to a plot: Group means\\nggplot (iris,\\naes(x = Sepal.Length,\\ny = Sepal.Width,\\ncolor = Species)\\n)+\\ngeom_point ()+\\ntheme_bw ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 124 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 124}, page_content='Adding derived statistics to a plot: Group means\\n2.02.53.03.54.04.5\\n5 6 7 8\\nSepal.LengthSepal.WidthSpecies\\nsetosa\\nversicolor\\nvirginica\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 125 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 125}, page_content='Adding derived statistics to a plot: Group means\\nmeans <- iris %>% group_by (Species) %>%\\nsummarize_at (vars (starts_with (/quotesingle.ts1Sepal /quotesingle.ts1)),\\nmean)\\nmeans\\n#> # A tibble: 3 x 3\\n#> Species Sepal.Length Sepal.Width\\n#> <fct> <dbl> <dbl>\\n#> 1 setosa 5.01 3.43\\n#> 2 versicolor 5.94 2.77\\n#> 3 virginica 6.59 2.97\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 126 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 126}, page_content='Adding derived statistics to a plot: Group means\\nggplot (iris,\\naes(x = Sepal.Length,\\ny = Sepal.Width,\\ncolor = Species)\\n)+\\ngeom_point ()+\\ngeom_point (data = means,\\nsize=5) +\\ntheme_bw ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 127 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 127}, page_content='Adding derived statistics to a plot: Group means\\n2.02.53.03.54.04.5\\n5 6 7 8\\nSepal.LengthSepal.WidthSpecies\\nsetosa\\nversicolor\\nvirginica\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 128 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 128}, page_content='Adding regression metrics\\nRegress highway mileage on city mileage (data: mpg)\\nmod1 <- lm(hwy ~cty, data = mpg)\\nr2 <- broom ::glance (mod1) %>% pull (r.squared)\\nggplot (mpg,\\naes(x = cty, y = hwy)\\n)+\\ngeom_point ()+\\ngeom_smooth (method = /quotesingle.ts1lm/quotesingle.ts1, se=F) +\\ntheme_bw ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 129 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 129}, page_content='Adding regression metrics\\n#> /grave.ts1geom_smooth() /grave.ts1using formula /quotesingle.ts1y ~ x /quotesingle.ts1\\n203040\\n10 15 20 25 30 35\\nctyhwy\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 130 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 130}, page_content='Regress highway mileage on city mileage\\nmod1 <- lm(hwy ~cty, data = mpg)\\nr2 <- broom ::glance (mod1) %>% pull (r.squared) %>%#pull is part of dplyr\\nround (., 2) #part of base R, rounding behind the . by 2\\nggplot (mpg,\\naes(x = cty, y = hwy)) +\\ngeom_point ()+\\ngeom_smooth (method = /quotesingle.ts1lm/quotesingle.ts1, se=F) +\\nannotate (geom= /quotesingle.ts1text /quotesingle.ts1,\\nx = 15, y = 40,\\nlabel=glue ::glue (\"R^2 == {r}\",r=r2),\\nsize=6,\\nparse=T) +\\ntheme_bw ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 131 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 131}, page_content='Glance creates a quick summary of the model\\nbroom ::glance (mod1)\\n#> # A tibble: 1 x 12\\n#> r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC\\n#> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\\n#> 1 0.914 0.913 1.75 2459. 1.87e-125 1 -462. 931. 941.\\n#> # ... with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 132 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 132}, page_content='Nice Addition\\n#> /grave.ts1geom_smooth() /grave.ts1using formula /quotesingle.ts1y ~ x /quotesingle.ts1\\nR2=0.91\\n203040\\n10 15 20 25 30 35\\nctyhwy\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 133 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 133}, page_content='Highlighting regions\\nmpg %>%\\nmutate (cyl = as.factor (cyl)) %>%\\nggplot (aes(x = cyl, y = hwy)\\n)+\\ngeom_boxplot ()+\\ntheme_bw ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 134 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 134}, page_content='Highlight regions\\n203040\\n4 5 6 8\\ncylhwy\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 135 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 135}, page_content='Highlighting regions\\nmpg %>%\\nmutate (cyl = as.factor (cyl)) %>%\\nggplot (aes(x = cyl, y = hwy)\\n)+\\ngeom_boxplot ()+\\ntheme_bw ()+\\nannotate (geom = /quotesingle.ts1rect /quotesingle.ts1,\\nxmin=3.75,xmax=4.25,\\nymin = 22, ymax = 28,\\nfill = /quotesingle.ts1red/quotesingle.ts1,\\nalpha = 0.2) +\\nannotate (/quotesingle.ts1text /quotesingle.ts1,\\nx = 4.5, y = 25,\\nlabel = /quotesingle.ts1Outliers? /quotesingle.ts1,\\nhjust = 0) +\\ncoord_cartesian (xlim = c(0,5)) +\\ntheme_bw ()\\nBrian Wright, PhD R Data Vis: ggplot Feb, 2020 136 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\ggplot_overview.pdf', 'page': 136}, page_content='Highlighting regions\\nOutliers?\\n203040\\n4 5 6 8\\ncylhwy\\n################EXAMPLES for in\\nCLASS##################Brian Wright, PhD R Data Vis: ggplot Feb, 2020 137 / 137'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 0}, page_content='Markdown and the knitr Package in R\\nPractice of Data Science\\nCommunicating with Code'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 1}, page_content='Reproducible Research\\nBaggerly and Coombes’ Replication Work\\nA Strategy for Reproducible Research\\nThe Markdown Language\\nChoosing an Output and Compiling\\nThe Header\\nPublishing the HTML Output\\nSyntax\\nBold, Italics, etc.\\nSectioning\\nTabbing\\nEquations\\nLists\\nTables (by hand)\\nGraphics (from external ﬁles)\\nEmbedding Code\\nIn-line Code for Display Only\\nIn-line Code that gets Evaluated\\nCode Chunks\\nCode Chunk Options\\nCaching\\nTables and Figures (from code)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 2}, page_content='Baggerly and Coombes\\nThere’s a really infamous talk from 2010, called\\nThe Importance of Reproducible Research in High-Throughput\\nBiology: Case Studies in Forensic Bioinformatics :\\nhttps://www.youtube.com/watch?v=7gYIs7uYbMo\\nby Keith Baggerly and Kevin R. Coombes.\\nTheir talk illustrates what can go seriously wrong when people\\nworking with data are not transparent about how they get their\\nresults.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 3}, page_content='Baggerly and Coombes\\nBaggerly and Coombes are bioinformaticians who study cancer.\\nThey attempted to replicate this study:\\nThis paper claims to show how treatments for childhood leukemia\\nshould be tailored to patients based on the speciﬁc genomic\\ninformation in the patient’s DNA.\\nThis was an important ﬁnding and was published in a prominent\\njournal.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 4}, page_content='Baggerly and Coombes\\nIn order to replicate the study, Baggerly and Coombes started with\\nthe same raw dataset that the study’s authors used. Their goal\\nwas to use the data to reproduce the study’s results.\\nBut the problem was that the authors did not provide any\\nscripts or discussion of what they did to the data prior to running\\nthe tests.\\nAs a result, Baggerly and Coombes had to reproduce the results in\\na “forensic” way: ﬁguring out after-the-fact what the authors must\\nhave done to get these results.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 5}, page_content='Baggerly and Coombes\\nEventually, they were able to reproduce the results, but found the\\nauthors had made two errors:\\n1. The rows in the data refer to particular genes. Someone\\ncopy-and-pasted the cells in a way that left oﬀ the column headers.\\nThat created an oﬀ-by-one error where the data for a gene were\\nlisted one row above the gene name.\\n2. The treatment was coded as 1 or 2, but someone along the way\\nconfused what 1 and 2 meant. So the treated patients were\\nreported as control, and vice versa.\\nThat means that the reported positive eﬀect for the treatment\\ngroup is actually a positive eﬀect for the control group. In other\\nwords, the treatment harms people .'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 6}, page_content='Baggerly and Coombes\\nBy the time Baggerly and Coombes made this discovery, the\\nresearch had moved forward to the stage of clinical drug trials.\\nChildren with leukemia were being given harmful treatments\\nbased on mistaken research.\\nIn his talk, Baggerly describes all the ways that he and Coombes\\ntried to sound the alarm on this research. But the stakeholders\\nwere reluctant to retract and end the research because of the\\nimplications for reputation and grant money.\\nThe study continued for many months. It was only stopped when\\nit was revealed that the principal investigator on the original study\\nhad lied on his CV about being a Rhodes Scholar.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 7}, page_content='Baggerly and Coombes\\nSo, what exactly went so wrong here?\\n1. Dumb, typical mistakes that people make with spreadsheets all\\nthe time (the oﬀ-by-one error, confusing the 1s and 2s).\\n2. Laziness: no eﬀort to document the steps that were taken to\\nprepare the data for analysis.\\n3. Self-interest and ego: covering up mistakes instead of risking the\\npenalties of correcting them, thereby making the mistakes worse.\\n4. Magical thinking: because the work involves data, there’s a\\ntendency by most people to simply believe that the work is correct\\nwithout digging in to it (not Baggerly and Coombes though!)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 8}, page_content='Baggerly and Coombes\\nIf we are going to be working with data, how can we avoid the\\nmistakes that Baggerly and Coombes discovered?\\n1. Mistakes are inevitable. But, if we use code instead of\\npoint-and-clicking, it’s easier to see mistakes and to go back and\\ncorrect them.\\n2. If we document our steps as we go along, we’ll be transparent\\nand able to show anyone exactly what we did with the data.\\n3. We can feed our egos in a diﬀerent way: clear and professional\\ndocumentation looks impressive to others.\\n4. Working with code and explaining what each part of the code\\ndoes goes a long way towards dispelling the anxiety people have\\nabout data, and overcomes magical thinking.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 9}, page_content='Reproducible Research\\nOur goal : to give you the skills and practice you need to work with\\ndata in a way that\\n▶Is easy to document\\n▶Allows you to combine code, results, and text to better\\nconvey the context of what you are doing\\n▶Looks really good\\nThis morning we will discuss R markdown, one of the best tools we\\nhave for conducting transparent research.\\nThis afternoon we will walk through an entire research pipeline\\nusing R markdown, documenting everything we need to do to raw\\ndata to prepare it for analysis, and including the ﬁnal results in the\\ndocument.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 10}, page_content='Reproducible Research\\nReproducibility — the ability to get the same research results as an\\noriginal study by using the raw data and computer code provided\\nby researchers.\\nThere have been serious crises in many ﬁelds, including medicine,\\neconomics, and political science, because researchers failed to\\nmake their code and data available.\\nWe are going to learn how to make a document that shows ALL\\nthe steps involved in a project, going from raw data to ﬁnal\\nresults . We also want this document to be as easy to read and\\nunderstand as possible.\\nPractice on your own computer as we discuss the steps for\\ncreating a markdown document.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 11}, page_content='There are two ways to save R code\\nThe ﬁrst way is to save code in an R script .\\n▶Pros : Scripts are easy to work with, run, save, and share.\\n▶Cons : Scripts can be hard for even an advanced R user to\\nread and understand.\\nThe second way is with an R markdown document compiled with\\ntheknitr package. (If you’ve never used knitr before, install it\\nby typing install.packages(\"knitr\") into the console.)\\n▶Pros : Creates a beautiful, readable document by placing text,\\ncode, and the output of the code all in the same document\\n(this is also called weaving: hence the name knitr ). Able to\\ncreate HTML, PDF, or Word ﬁles.\\n▶Cons : More syntax to learn in addition to R code. Might take\\na while to compile documents.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 12}, page_content='The Markdown Language\\nYou need to become experienced using both methods to save\\ncode. They are each useful in diﬀerent situations.\\nMarkdown is a programming language for formatting text, using\\nminimal programming syntax. It’s basically a lightwight version of\\nHTML code. It’s not just for R — it’s for anything that involves\\ntext together with some other kind of code.\\nTo start a markdown ﬁle : open R Studio, click File, then New File,\\nthen R Markdown. Give the document a title and author, and\\nchoose whether you want this code to produce an HTML, PDF, or\\nWord ﬁle.\\nThis will call up an example page with some text and code already\\nin it. (You will end up deleting this example text and code and\\nwriting your own.)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 13}, page_content='Choosing an Output and Compiling\\nWhile R scripts are saved as “ .R” ﬁles, these markdown ﬁles are\\nsaved as “ .Rmd ” ﬁles. This ﬁle is separate from the HTML, PDF,\\nor Word document you will create with this code.\\nFirst notice the “ Knit ” button. This button compiles the\\ndocument – produces the desired output. To change the type of\\noutput, click the arrow to the right of Knit.\\nAt the top of the .Rmd ﬁle, there’s code separated on the top and\\nbottom by three dashes. This is the header. You can change the\\ntitle, author, etc. here. You can also change the output here:\\n▶For HTML output, output: html document\\n▶For PDF output, output: pdf document\\n▶For Word output, output: word document'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 14}, page_content='Using the Header to Set Options\\nTo create a table of contents that lists up to 5 levels of sectioning:\\noutput:\\nhtml_document:\\ntoc: true\\ntoc_depth: 5\\nBy default the table of contents appears at the top of the\\ndocument, just under the title. But, you can also use a ﬂoating\\nand collapsable table of contents window like this:\\noutput:\\nhtml_document:\\ntoc: true\\ntoc_depth: 5\\ntoc_float: true\\ntoc_collapsed: true'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 15}, page_content='Using the Header to Set Options\\nImportant : You will see three dashes --- at the beginning and\\nend of the header. Do not delete these! The document will not\\ncompile if you do. (It won’t even give you an intelligible error\\nmessage)\\nTo change the overall theme (colors, fonts, etc.) of the\\ndocument, add the theme argument, like this:\\noutput:\\nhtml_document:\\ntheme: cerulean\\nDiﬀerent themes are illustrated here:\\nhttps://www.datadreaming.org/post/r-markdown-theme-gallery/\\nTry a few right now.\\nOther options for the header are listed here:\\nhttp://rmarkdown.rstudio.com/html document format.html'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 16}, page_content='Publishing the HTML Output\\nIf you choose to compile your markdown code as an HTML ﬁle, it\\ndisplays in R Studio’s makeshift browser. That’s good enough for\\nchecking your work.\\nBut remember, HTML code creates webpages. So you can click\\nthe “ Open in Browser ” button to see your code displayed in a\\nweb browser.\\nIf you have your own webpage, you can post this HTML output to\\nyour webpage.\\nIf you need space on the web to host this page, click on\\n“Publish ”, then click “ RPubs ”. RPubs is a free service, run by R\\nStudio, that provides server space for your markdown documents.\\nIf you post online using RPubs, you can use a URL to share your\\nwork with your audience.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 17}, page_content='Markdown syntax\\nTo insert text into the output, just type into the .Rmd ﬁle and\\ncompile.\\nThe goal of markdown is to give you an easy way to create stylized\\ntext, sections, equations, lists, tables, etc. quickly by typing .\\nForitalicized text , place ONE star * before and after the text.\\nForbold text , place TWO stars ** before and after the text.\\nForstruck through text, place TWO tildes ∼∼ before and after the\\ntext.\\n*this will be italicized*\\n**this will be bold**\\n~~this will be struck out~~'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 18}, page_content=\"Markdown syntax\\nTo start a new paragraph, push Enter/Return TWICE, so that\\nthere is a blank line separating the paragraphs.\\nFor a hyperlink , either type the address itself (it will automatically\\nbecome a link), or use syntax like this to place the link on top of\\nother text:\\n[The best web comic](https://smbc-comics.com/)\\nFor block quotes, push Enter/Return TWICE then start every line\\nof the quote with >and a space.\\nHere's a profound quote:\\n> I'd rather have this bottle in front of me\\n> than a frontal lobotomy\"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 19}, page_content='Sectioning\\nOne of the most important ways to make a document readable is\\nto use sectioning to organize the document.\\nTwo hashtags (pound signs) ##followed by a name denote a\\nsection.\\nThree hashtags ### followed by a name denote a subsection.\\nFour hashtags #### followed by a name denote a sub-subsection,\\nand so on.\\nOne hashtag is used for titles, but generally, these titles appear too\\nlarge and look weird, so most people start at two hashtags.\\nIf thetoc: true option is speciﬁed, the section titles will\\nappear in the table of contents automatically.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 20}, page_content='Tabbing\\nSometimes it makes sense for only one of several sub-sections to\\nappear at a time. For example, suppose I wrote a paragraph in\\nEnglish and Spanish. I could have the two sections represented by\\ntabs, in which the user can switch between the two tabs.\\nTo create tabs, type {.tabset}immediately after the section\\ntitle. Then all sub-sections within this section will exist in tabs.\\n{.tabset .tabset-fade }does the same thing, but include a\\nnice fade-in animation when switching between tabs.\\n{.tabset .tabset-fade .tabset-pills }places the tabs into\\nsquares with rounded-edges.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 21}, page_content='Equations\\nIn statistics and data science, sometimes it makes sense to include\\nmathematical equations in the document. In Microsoft Word, you\\nhave to point-and-click every symbol, but in Markdown, you can\\nquickly type out equations .\\nTo include an equation in-line, place a dollar sign $before and\\nafter the equation. To place the equation on its own line, place\\nTWO dollar signs and a space before and after the equation.\\nSome special math characters:\\n▶^exponentiation\\n▶ subscripts▶\\\\sqrt{5}√\\n5\\n▶\\\\frac{1}{2}1\\n2\\nA list of the code for many other math symbols is here:\\nhttp://reu.dimacs.rutgers.edu/Symbols.pdf'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 22}, page_content='Equations\\nFor example, to include the quadratic formula in your document,\\nx=−b±√\\nb2−4ac\\n2a\\ntype\\n$$ x = \\\\frac{-b \\\\pm \\\\sqrt{b^2 - 4ac}}{2a} $$\\n(Here the\\\\pmrefers to the “plus or minus” symbol)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 23}, page_content='Lists\\nTo create an unordered list (just bullets), type:\\n* Item 1\\n* Item 2\\n* Item 2a\\n* Item 2b\\nTo create an ordered list (numbered), type:\\n1. Item 1\\n2. Item 2\\n3. Item 3\\na. Item 3a\\nb. Item 3b'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 24}, page_content='Lists\\nSome weird things about lists in R Markdown:\\n1. It won’t compile as a list unless there’s an entire blank line\\n(push enter twice) separating the list from the preceding text\\n2. To create a sublist, you need to tab twice . Once won’t register\\ndiﬀerently from the other items\\n3. For numbered lists, the top level must be regular (Arabic)\\nnumbers (1,2,3,...). The next levels can be lowercase letters or\\nlowercase Roman numerals\\n4. You can change the ﬁrst number to anything you want. But the\\nfollowing numbers will always count up by 1 from the ﬁrst\\nnumber, no matter what you type there'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 25}, page_content='Creating tables by hand\\nThere are two kinds of tables: ones you enter by hand and ones\\nyou can create with R. To create a nice-looking table by hand,\\nmarkdown has a peculiar notation.\\nIn general, I don’t like memorizing the speciﬁcs of this notation.\\nInstead I use https://tableconvert.com/ (or another website like\\nit). Then:\\n1. In the top toolbar, click the “Table” button to choose the\\ndesired dimensions\\n2. Fill in the table however you want\\n3. Select “Copy”\\n4. And paste the syntax into your R markdown document\\nThe table will appear in a neatly formatted way when you compile.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 26}, page_content='Graphics (from external ﬁles)\\nSometimes you might want to display a graphic in your document\\nthat you aren’t using R to create. For example, what kind of\\nmonster wouldn’t want to include this graphic?\\nYou can include graphics if you have the ﬁle on your local machine:\\n![A caption, if wanted, goes here](duck.jpg)\\nOr pull them directly oﬀ the web by entering the image’s URL:\\n![](http://1funny.com/wp-content/uploads/2010/08/Baby-Animals-24.jpg)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 27}, page_content='Embedding Code\\nRemember that the purpose of an R markdown ﬁle is to weave\\ntext, code, and the results of code together in one, readable\\ndocument. There are three ways to include code in a document:\\n1. in-line for display only,\\n2. in-line and evaluated,\\n3. and evaluated inside a “code chunk.”\\nIn-line code for display only is just for referring to speciﬁc R\\ncommands as you are writing. Markdown will place these pieces of\\ncode in a diﬀerent, computery font with a grey background. But\\nthis code WILL NOT be evaluated or run.\\nTo write in-line code, use single, forward-sloping quotes ‘(on the\\nsame key as the tilde). Then if you write about the lm()\\nfunction, or the ggplot2 package, it will appear in this diﬀerent\\nfont and have a grey background.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 28}, page_content='In-line Code that is Evaluated\\nIf you use the single, forward-sloping quotes, the code is displayed\\nbut not run. Alternatively, if you type rprior to any code within\\nthe quotes, markdown will evaluate the code and display the\\noutput in the text.\\nFor example, try typing the following into your markdown\\ndocument:\\nThe current date and time are `r Sys.time()`\\nSys.time() is R’s internal clock, so this syntax should display\\nthe current date and time as regular text, not as computer code.\\nThis feature is great for ﬁlling in details about the data into your\\ntext automatically.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 29}, page_content='Code Chunks\\nA code chuck contains several lines of code, on separate lines from\\nthe text. It gets run by R , and the output appears in the\\ndocument as well.\\nStart a code chunk like this:\\n```{r nameofthischunk}\\nFirst, type three forward single-quotes. Then within curly braces,\\nthe letter r— this tells markdown that this is R code, then a\\ndistinct name you give this code chunk. Naming the chunks is\\noptional, but will help you isolate errors if there are any.\\nEnd a code chunk by typing three more forward single-quotes\\non a new line:\\n```'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 30}, page_content='Code Chunks\\nYou can type as many lines of R code as you want inside one code\\nchunk. But, best practice is to only write a few lines at a time in\\none code chunk.\\nThe reason is that you are trying to bring a reader along and\\nexplain your code. It’s easier to explain a few lines at a time.\\nAlso, If there’s multiple outputs from one chunk, they will all be\\ndisplayed after the chunk. Keeping the chunk small helps us keep\\ntrack of which output goes with which code.\\nTake a moment to try out some code chunks in your markdown\\ndocument.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 31}, page_content='Code Chunk Options\\nYou can write options inside the curly braces, separated by\\ncommas, to change the behavior of a code chunk.\\nHere are some options you can use:\\necho=FALSE — don’t display the code\\neval=FALSE — don’t display the results\\nwarning=FALSE, message=FALSE, error=FALSE — don’t\\ndisplay warnings, messages, or errors (I always use these options for\\nthe code chunk that loads the packages I need)\\nOther options are listed here: https://yihui.name/knitr/options/'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 32}, page_content='Code Chunk Options\\nA new .Rmd ﬁle has this code chunk at the top of the document:\\n```{r setup, include=FALSE}\\nknitr::opts_chunk$set(echo = TRUE)\\n```\\nThis code chunk contains global options to be applied to all\\nsubsequent chunks.\\nBy default, it sets echo=TRUE , which tells all the chunks to\\ndisplay the code in the document in addition to the output. If this\\nwere instead echo=FALSE , the output would get displayed but the\\nR code that generates the output would not be displayed.\\nYou can set other global options here if you want them applied to\\nall code chunks. Just write knitr::opts chunk$set( option)in\\nthis chunk.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 33}, page_content='Caching\\nOne extremely useful code chunk option is caching the output of\\nthe code.\\nCaching means that R saves the output of the chunk so that\\nthe next time the document compiles, it can load the saved results\\ninstead of running the code again.\\nTo cache the output of a code chunk, use the option\\ncache=TRUE .\\nThis option saves A LOT OF TIME when using commands that\\ntake a while to run, such as loading a big dataset or running a\\ncomplicated model. But don’t use this option for every chunk, as it\\ncan cause problems with the keeping results accurate as code\\nchanges.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\knitr_reference.pdf', 'page': 34}, page_content='Tables and Figures (from code)\\nA big reason why markdown makes sense for R is the ability to\\ncleanly display tables and ﬁgures you produce with R code.\\nTo convert tables from R output to nice looking HTML, use the\\nkable() function. Just place the code that creates the table\\ninside of kable() .\\nFor ﬁgures, such as ggplot graphics, no extra function is needed to\\ndisplay. But use the fig.width andfig.height options on\\nthe code chunk to control the size of the ﬁgure in the output.\\nFor example:\\n```{r plot, fig.width=6, fig.height=8}\\nggplot(mtcars, aes(x=wgt, y=mpg)) + geom_point()\\n```'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 0}, page_content='KNN and Probability '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 1}, page_content='Classification\\n\\uf0d8Definition\\n\\uf076Classification can take two distinct meanings in Machine Learning\\n\\uf0d8Unsupervised Learning \\n\\uf076We may be given a set of observations with the aim of establishing \\nthe existence of classes or clusters in the data\\n\\uf0d8Supervised Learning \\n\\uf076We may know for certain that there are so many classes, and the aim \\nis to establish a rule that we can use to classify a new observation into \\none of the existing classes\\n\\uf076k-NN is a supervised method for classification \\n2'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 2}, page_content='Classification\\n\\uf0d8A few classification examples:\\n\\uf076A Person arrives at the emergency room with a set of symptoms that could \\npossibly be attributed to one of three medical conditions. Which of the three \\nconditions does the individual have?\\n\\uf076An online banking service must be able to determine whether or not a \\ntransaction being performed on the site is fraudulent, on the basis of the \\nuser’s IP address, past transaction history, and so forth.\\n\\uf076On the basis of DN sequence data for a number of patients with and without \\na given disease, a biologist would like to figure out which DNA mutations are disease -causing and which are not.\\n3'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 3}, page_content='Classification\\n\\uf0d8There are many possible techniques that a classifier might use to \\npredict a qualitative response. Today we will discuss k -NN\\n\\uf076k-Nearest Neighbors\\n\\uf076Naïve Bayes \\n\\uf076Logistic Regression\\n\\uf076Tree –based methods\\n4'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 4}, page_content='Classification\\nA few issues to keep in mind when building a classifier\\n\\uf0d8Accuracy. There is the reliability of the rule, usually represented by the \\nproportion of correct classifications, although it may be that some errors are \\nmore serious than others, and it may be important to control the error rate for some key class.\\n\\uf0d8Speed. In some circumstances, the speed of the classifier is a major issue. A \\nclassifier that is 85% accurate may be preferred over one that is 95% accurate if it \\nis 100 times faster in testing (and such differences in time -scales are not \\nuncommon in neural networks for example). \\n\\uf076Such considerations would be important for the automatic reading of postal \\ncodes, or automatic fault detection of items on a production line for example.\\n5'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 5}, page_content='Classification\\nA few issues to keep in mind when building a classifier\\n\\uf0d8Comprehensibility. If it is a human operator that must apply the classification \\nprocedure, the procedure must be easily understood else mistakes will be made \\nin applying the rule. It is important also, that human operators believe the system. \\n\\uf0d8Training Time. Especially in a rapidly changing environment, it may be necessary \\nto learn a classification rule quickly, or make adjustments to an existing rule in \\nreal time. “Quickly” might imply also that we need only a small number of observations to establish our rule.\\n6'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 6}, page_content='K-Nearest Neighbors\\n7'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 7}, page_content='Simple approach for k -NN\\nSimple goal:\\n\\uf0d8Predict the label of a data point by:\\n\\uf076Looking at the ‘k’ closest labeled data points (neighbors)\\n\\uf076Uses a majority vote\\n\\uf0d8One of the easiest algorithms to interpret, oftentimes used as a baseline for \\nmeasuring model performance\\n\\uf0d8Memory -Based Learning\\n\\uf076Also known as “case -based” or “example -based” learning\\n\\uf0d8Intuition behind memory -based learning\\n\\uf076Similar inputs map to similar outputs\\n\\uf0d8If true, we just have to define “similar”\\n\\uf0d8Not all similarities created equal…\\n8'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 8}, page_content='Memory -Based Learning\\n\\uf0d8How do we determine “similar”?\\n\\uf0d8For instance, if we wanted to:\\n\\uf0d8Predict Brian’s weight\\n\\uf076Who are the similar people?\\n\\uf076Similar age, diet, height, waistline, activity level …\\n\\uf0d8Predict Brian’s IQ\\n\\uf076Similar occupation, writing style, undergraduate degree, SAT \\nscore, …\\n\\uf0d8How do we calculate variously ranges in similarity?\\n\\uf076Need some metric…\\n\\uf0d8Distance\\n9'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 9}, page_content='k-NN Approach\\n\\uf0d8Define a distance 𝑑𝑑(𝑥𝑥1,𝑥𝑥2)between any 2 examples\\n\\uf076Examples are essentially rows\\n\\uf076Sowe could just use Euclidean distance … \\n\\uf0d8Training\\n\\uf076Index the training examples for fast lookup (build a “database”)\\n\\uf0d8Test\\n\\uf076Given a new 𝑥𝑥, find the closest neighbor (k=1) from training index\\n\\uf076Classify x the same as its closest neighbor\\n10'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 10}, page_content='k-NN Approach\\n\\uf0d8Euclidean Distance Equation:\\n\\uf0d8q is the current vector and p is new, think of q as the training and p as the test. \\nSmaller values mean lines or more similar in value or closer together on a graph. \\n11\\nx1 x2 x3 x4 x5 x6 x7 x8 Result\\nq 3 2 2 5 6 2 1 5\\np 4 3 3 6 4 4 5 6\\nEuclidean 1 1 1 1 4 4 16 1 5.39\\nz 3 2 2 5 6 2 1 5\\nEuclidean0 0 0 0 0 0 0 0 0Subtracting then \\nsquaring row p with q. Then summing and taking the sqrt. \\nDo the same with \\nz and compare'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 11}, page_content='kNN Decision Boundaries\\n\\uf0d8kNN can learn complex decision boundaries\\n12\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 12}, page_content='\\uf0d8Instead of picking the (1) nearest neighbor, what if we picked the \\nk-Nearest Neighbors and have them vote?\\n\\uf0d8Choosing k points is more reliable in the following cases:\\n\\uf076Noise in training vectors x\\n\\uf076Noise in training labels y\\n\\uf076Overlapping classeskNN\\n13x_1x_2\\n+++\\n++++++o\\noo\\no\\nooooooo\\n++\\n+ o\\nook=1'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 13}, page_content='kNN\\n\\uf0d8Instead of picking the (1) nearest neighbor, what if we picked the \\nk-Nearest Neighbors and have them vote?\\n\\uf0d8Choosing k points is more reliable in the following cases:\\n\\uf076Noise in training vectors x\\n\\uf076Noise in training labels y\\n\\uf076Overlapping classes\\n\\uf0d8Why?\\n14x_1x_2+++\\n++++++o\\noo\\no\\nooooooo\\n++\\n+ o\\nook=1X'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 14}, page_content='kNN Decision Boundaries\\n\\uf0d8Consider this example with R,G,B classes \\nwith significant overlap\\n15\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 15}, page_content='kNN Decision Boundaries\\n\\uf0d8Consider this example with R,G,B classes \\nwith significant overlap\\n\\uf0d8k=1 Decision Boundary\\n\\uf076Looks complex\\n\\uf076Overfitting?\\n16\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 16}, page_content='kNN Decision Boundaries\\n\\uf0d8k=1 Decision Boundary\\n\\uf076Looks complex\\n\\uf076Overfitting?\\n\\uf0d8What if we were to increase k?\\n17\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 17}, page_content='kNN Decision Boundaries\\n\\uf0d8k=1 Decision Boundary\\n\\uf076Looks complex\\n\\uf076Overfitting?\\n\\uf0d8What if we were to increase k?\\n\\uf076K=15 Decision boundary\\n18\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 18}, page_content='kNN Decision Boundaries\\n\\uf0d8k=1 Decision Boundary\\n\\uf076Looks complex\\n\\uf076Overfitting?\\n\\uf0d8What if we were to increase k?\\n\\uf076K=15 Decision boundary\\n\\uf076Smoother boundaries\\n19\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 19}, page_content='kNN Decision Boundaries\\n\\uf0d8k=1 Decision Boundary\\n\\uf076Looks complex\\n\\uf076Overfitting?\\n\\uf0d8What if we were to increase k?\\n\\uf076K=15 Decision boundary\\n\\uf076Smoother boundaries\\n\\uf076Generalizes better on unseen data\\n20\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 20}, page_content='kNN Decision Boundaries\\n\\uf0d8k=1 Decision Boundary\\n\\uf076Looks complex\\n\\uf076Overfitting?\\n\\uf0d8What if we were to increase k?\\n\\uf076K=15 Decision boundary\\n\\uf076Smoother boundaries\\n\\uf076Generalizes better on unseen data\\n\\uf0d8What makes the boundaries smoother?\\n21\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 21}, page_content='kNN Decision Boundaries\\n\\uf0d8k=1 Decision Boundary\\n\\uf076Looks complex\\n\\uf076Overfitting?\\n\\uf0d8What if we were to increase k?\\n\\uf076K=15 Decision boundary\\n\\uf076Smoother boundaries\\n\\uf076Generalizes better on unseen data\\n\\uf0d8What makes the boundaries smoother?\\n\\uf0d8Let’s look at a two -class (binary) example\\n22\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 22}, page_content='k-NN Graphical Example\\n23\\n\\uf0d8Consider this two -dimensional dataset with \\npoints classified as Red or Green '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 23}, page_content='k-NN Graphical Example\\n24\\n\\uf0d8Consider this two -dimensional dataset with \\npoints classified as Red or Green \\n\\uf0d8We want to Classify this point'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 24}, page_content='k-NN Graphical Example\\n25\\n\\uf0d8Consider this two -dimensional dataset with \\npoints classified as Red or Green \\n\\uf0d8We want to Classify this point\\n\\uf0d8If we consider k=3 neighbors '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 25}, page_content='k-NN Graphical Example\\n26\\n\\uf0d8Consider this two -dimensional dataset with \\npoints classified as Red or Green \\n\\uf0d8We want to Classify this point\\n\\uf0d8If we consider k=3 neighbors \\n\\uf076Measured by some distance\\nd'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 26}, page_content='k-NN Graphical Example\\n27\\n\\uf0d8Consider this two -dimensional dataset with \\npoints classified as Red or Green \\n\\uf0d8We want to Classify this point\\n\\uf0d8If we consider k=3 neighbors \\n\\uf076Measured by some distance\\n\\uf076The point is classified as Red'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 27}, page_content='k-NN Graphical Example\\n28\\n\\uf0d8Consider this two -dimensional dataset with \\npoints classified as Red or Green \\n\\uf0d8We want to Classify this point\\n\\uf0d8If we consider k=3 neighbors \\n\\uf076Measured by some distance\\n\\uf076The point is classified as Red\\n\\uf0d8If we consider k=5 neighbors'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 28}, page_content='k-NN Graphical Example\\n29\\n\\uf0d8Consider this two -dimensional dataset with \\npoints classified as Red or Green \\n\\uf0d8We want to Classify this point\\n\\uf0d8If we consider k=3 neighbors \\n\\uf076Measured by some distance\\n\\uf076The point is classified as Red\\n\\uf0d8If we consider k=5 neighbors\\n\\uf076Measured by some distanced'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 29}, page_content='k-NN Graphical Example\\n30\\n\\uf0d8Consider this two -dimensional dataset with \\npoints classified as Red or Green \\n\\uf0d8We want to Classify this point\\n\\uf0d8If we consider k=3 neighbors \\n\\uf076Measured by some distance\\n\\uf076The point is classified as Red\\n\\uf0d8If we consider k=5 neighbors\\n\\uf076Measured by some distance\\n\\uf076The point is classified as Green'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 30}, page_content='k-NN Graphical Example\\n31\\n\\uf0d8Consider this two -dimensional dataset with \\npoints classified as Red or Green \\n\\uf0d8We want to Classify this point\\n\\uf0d8If we consider k=3 neighbors \\n\\uf076Measured by some distance\\n\\uf076The point is classified as Red\\n\\uf0d8If we consider k=5 neighbors\\n\\uf076Measured by some distance\\n\\uf076The point is classified as Green\\n\\uf0d8So, how do we know what k to choose?'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 31}, page_content='How to choose “k”\\n\\uf0d8Odd k (often 1, 3, or 5):\\n\\uf076Avoids problem of breaking ties (in a binary classifier)\\n\\uf0d8Large k:\\n\\uf076Less sensitive to noise (particularly class noise)\\n\\uf076Better probability estimates for discrete classes\\n\\uf076Larger training sets allow larger values of k\\n\\uf0d8Small k:\\n\\uf076Captures fine structure of problem space better\\n\\uf076May be necessary with small training sets\\n\\uf0d8Balance between large and small k\\n32'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 32}, page_content='kNN distance problem\\n\\uf0d8Problem:\\n\\uf076What if the input represents weight in milligrams?\\n\\uf076Then small differences in physical weight dimension have a \\nhuge effect on distances, overwhelming other features\\n\\uf076Should really correct for these arbitrary “scaling” issues\\n\\uf0d8This leads to Standard Scaling\\n\\uf0d8Rescale weights so that standard deviation = 1\\n33+oo\\n+\\nweight (lb)attribute_2++\\n+++++o\\noooooo\\noo\\nweight (mg)attribute_2\\n++++\\n++ ++\\n+o\\nooo\\noooo\\no o\\nobad'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 33}, page_content='More kNN Details\\n\\uf0d8Nonparametric -makes no explicit assumptions about the underlying distribution \\nof the input\\n\\uf0d8Instance/memory -based learning means that this algorithm doesn’t explicitly \\nlearn a model. Instead, it chooses to memorize the training instances which are \\nsubsequently used as “knowledge” for the prediction phase\\n\\uf0d8Learns arbitrarily complicated decision boundaries\\n\\uf0d8Lazy learner -method that generalizes data in the testing (deployment) phase, \\nrather than during the training phase –designed to be continuously updating as \\nnew data comes in\\n\\uf076A benefit of lazy learning is that it can quickly adapt to changes, \\n\\uf0d8Think Netflix recommendations, new options are appearing constantly so have a static training set it’s really valuable .  \\n\\uf076Very fast training time, but very slow prediction ( has to search for the nearest \\nneighbors)\\n34'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 34}, page_content='Advantages of k -NN\\n\\uf0d8Simple and fast to deploy\\n\\uf076Little to no training time\\n\\uf0d8Easy to interpret/explain\\n\\uf0d8Naturally handles multiclass datasets\\n\\uf0d8Non-parametric \\n\\uf076Does not assume any probability distributions on the input data\\n35'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 35}, page_content='Disadvantages of k -NN\\n\\uf0d8Storage of model takes a lot of disk space (contains entire training \\ndataset)\\n\\uf0d8Curse of Dimensionality -often works best with 25 or fewer \\ndimensions\\n\\uf076There is little difference between the nearest and farthest \\nneighbor in high dimensional data (starts to normalize to 1)\\n\\uf0d8Computationally expensive predictions (large search problem to find \\nnearest neighbors)\\n\\uf076Might be impractical in industry settings\\n\\uf0d8Need to normalize -suffers from skewed class distributions\\n\\uf076If one type of category occurs much more than another, \\nclassifying an input will be more biased towards that one category \\n(dominates the majority vote since it is more likely to be neighbors \\nwith the input) 36'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 36}, page_content='kNN Exercise\\n37Switch to R'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 37}, page_content='Understanding Probability Classifiers\\n\\uf0d8To be able to unpack the probability classifiers, we need a good grasp on the \\nfollowing topics\\n\\uf076Random variables\\n\\uf076Distributions\\n\\uf0d8Continuous \\n\\uf0d8Discrete\\n\\uf076Statistical Independence\\n\\uf076Probability\\n\\uf0d8Conditional Probability\\n\\uf0d8Joint Probability\\n\\uf0d8Marginal Probability\\n38'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 38}, page_content=\"Random Variables\\n\\uf0d8A random variable is a random number determined by chance, or more formally, \\ndrawn according to a probability distribution which specifies the probability that \\nits value falls in any given interval.\\n\\uf0d8Discrete Random Variable\\n\\uf076Taking any of a specified finite or countable list of values, endowed with \\naprobability mass function characteristic of the random variable's probability \\ndistribution\\n\\uf0d8Continuous\\n\\uf076Taking any numerical value in an interval or collection of intervals, via \\naprobability density function that is characteristic of the random variable's \\nprobability distribution\\n39\"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 39}, page_content='Random Variables\\n\\uf0d8Why do we care about Random Variables?\\n\\uf0d8Our goal is to predict the target/class\\n\\uf0d8We are not given the true (presumably deterministic) function\\n\\uf0d8We are only given observations\\n\\uf0d8Uncertainty arises through:\\n\\uf076Noisy measurements\\n\\uf076Finite size of data sets\\n\\uf076Ambiguity: The word “bank” can mean (1) a financial institution, (2) the side of a \\nriver,or (3) tilting an airplane. Which meaning was intended, based on the words that \\nappear nearby?\\n\\uf0d8Probability theory provides a consistent framework for the quantification \\nand manipulation of uncertainty\\n\\uf0d8Allows us to make optimal predictions given all the information available to \\nus, even though that information may be incomplete or ambiguous\\n40'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 40}, page_content='Probability\\n\\uf0d8Probabilities assign numbers to possibilities\\n\\uf0d8A probability needs to satisfy three properties (Kolmogorov, 1956):\\n\\uf076A probability must be nonnegative\\n\\uf076The sum of the probabilities across all events in the entire sample space \\nmust be 1\\n\\uf076For any two mutually exclusive events, the probability that one or the other \\noccurs is the sum of their individual probabilities\\n\\uf0d8For example, the probability that a fair six -sided die comes up 3 OR 4 is 1/6 + 1/6 = 2/6.\\n41'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 41}, page_content='Probability Distributions\\n\\uf0d8A probability distribution is simply a list of all possible events and their \\ncorresponding probabilities\\n\\uf0d8There are two kinds of probability distributions\\n\\uf076Discrete Distribution:\\n\\uf0d8Probability of heads or tails\\n\\uf076Continuous Distribution:\\n\\uf0d8Probabilities of people’s heights\\n42'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 42}, page_content='Discrete Probability Distribution\\n\\uf0d8When the sample space consists of discrete outcomes (e.g., heads or tails), the \\nprobability distribution is a list of probabilities of the outcomes\\n\\uf0d8The probability of a discrete outcome is called a probability mass\\n\\uf0d8The sum of the probability masses across the sample space must be 1\\n43'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 43}, page_content='Discrete Probability Example\\n\\uf0d8Example\\n\\uf076Consider the simple experiment of tossing a coin three times. Let X = number \\nof times the coin comes up heads. The 8 possible elementary events and the \\ncorresponding values for X are:\\n44Elementary Event Count of Heads (X)\\nTTT 0\\nTTH 1\\nTHT 1\\nHTT 1\\nTHH 2\\nHTH 2\\nHHT 2\\nHHH 3'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 44}, page_content='Discrete Probability Example\\n\\uf0d8Example\\n\\uf076Therefore, the probability distribution for the number of heads occurring in \\nthree coin tosses is\\n45Count of Heads (X) p(x) F(x)\\n0 1/8 1/8\\n1 3/8 4/8\\n2 3/8 7/8\\n3 1/8 1'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 45}, page_content='Discrete Probability Example\\n46Count of Heads (X) p(x) F(x)\\n0 1/8 1/8\\n1 3/8 4/8\\n2 3/8 7/8\\n3 1/8 1\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 46}, page_content='Continuous Probability Distribution\\n\\uf0d8When the sample space consists of continuous outcomes (ex: \\npeople’s heights) we cannot use probability mass for a specific \\noutcome.\\n\\uf0d8Why not?\\n47'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 47}, page_content='Continuous Probability Distribution –Probability Density\\n\\uf0d8When the sample space consists of continuous outcomes (ex: people’s heights) \\nwe cannot use probability mass for a specific outcome.\\n\\uf0d8Why not?\\n\\uf076Because the probability mass for a specific outcome will be zero\\n\\uf076In other words, the probability of someone’s height being exactly \\n67.2141390842076153…\\n\\uf0d8Instead, we can:\\n\\uf076Discretize the space into a finite set of mutually exclusive and exhaustive \\nintervals\\n\\uf076Calculate the probability mass in each interval\\n\\uf076Use the ratio of probability mass to interval width\\n\\uf076This ratio is called the Probability Density\\n48'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 48}, page_content='Probability Density\\n\\uf0d8The top panel of this figure shows the \\ndiscretized intervals and probability mass in \\neach interval\\n\\uf0d8The second panel shows the probability density\\n\\uf0d8The third panel shows the narrower intervals \\nand probability mass in each interval\\n\\uf0d8The bottom panel shows the probability density \\ncorresponding to the more narrow intervals\\n\\uf0d8Generally, the skinnier the intervals are, the \\nmore accurate the probability density is\\n49\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 49}, page_content='Probability Density\\n\\uf0d8While probability mass cannot \\nexceed 1, probability densities \\ncan\\n\\uf0d8The upper panel of this figure \\nshows that most of the probability mass is concentrated around 84\\n\\uf0d8Consequently, the probability \\ndensity near 84 exceeds 1.0, as shown in the lower panel\\n\\uf0d8This simply means that there is a \\nhigh concentration of probability mass relative to the width of the interval\\n50\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 50}, page_content='Properties of Probability Density Functions\\n\\uf0d8We need to define some notations first\\n\\uf0d8Let:\\n\\uf076𝑥𝑥be the continuous variable\\n\\uf076Δ𝑥𝑥be the width of an interval on 𝑥𝑥\\n\\uf076𝑖𝑖be an index for the intervals\\n\\uf076[𝑥𝑥𝑖𝑖,𝑥𝑥𝑖𝑖+∆𝑥𝑥]be the interval between 𝑥𝑥𝑖𝑖and 𝑥𝑥𝑖𝑖+∆𝑥𝑥\\n\\uf076𝑃𝑃([𝑥𝑥𝑖𝑖,𝑥𝑥𝑖𝑖+∆𝑥𝑥])be the probability mass of the 𝑖𝑖th interval\\n\\uf0d8Then the sum of those probability masses must be 1:\\n�\\n𝑖𝑖𝑃𝑃𝑥𝑥𝑖𝑖,𝑥𝑥𝑖𝑖+∆𝑥𝑥=1\\n\\uf0d8We can rewrite the equation above in terms of the density of each interval, by  \\ndividing and multiplying by x:\\n�\\n𝑖𝑖∆𝑥𝑥∗𝑃𝑃𝑥𝑥𝑖𝑖,𝑥𝑥𝑖𝑖+∆𝑥𝑥\\n∆𝑥𝑥=1\\n51'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 51}, page_content='Properties of Probability Density Functions\\n\\uf0d8In the limit, as the interval width becomes infinitesimal, we denote:\\n\\uf076Summation as ∫ instead of ∑\\n\\uf0d8Then, the previous equation (in terms of density) can be rewritten as:\\n�\\n𝑖𝑖∆𝑥𝑥∗𝑃𝑃𝑥𝑥𝑖𝑖,𝑥𝑥𝑖𝑖+∆𝑥𝑥\\n∆𝑥𝑥=1⇒�𝑑𝑑𝑥𝑥𝑝𝑝𝑥𝑥=1\\n\\uf0d8We use𝑝𝑝(𝑥𝑥)to represent the probability mass when 𝑥𝑥 is discrete\\n\\uf0d8Thus, what 𝑝𝑝(𝑥𝑥)represents depends on the context\\n52'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 52}, page_content='The Normal Probability Density Functions\\n\\uf0d8Perhaps the most famous probability density \\nfunction is the normal distribution, also known as the Gaussian distribution\\n\\uf0d8The probability density function of normal distribution is\\np𝑥𝑥=1\\n𝜎𝜎√2𝜋𝜋𝑒𝑒−1\\n2𝑥𝑥−𝜇𝜇\\n𝜎𝜎2\\n\\uf0d8Recall, what are 𝜎𝜎and 𝜇𝜇? what do they control?\\n\\uf0d8An example of the probability density is shown in \\nthe figure where the x axis is divided into a dense comb of small intervals\\n\\uf0d8The figure also shows that the area under the curve is, in fact, 1\\n53\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 53}, page_content='Example -Continuous Normal Distribution\\n\\uf0d8Example of the continuous distribution of weights\\n\\uf076The continuous normal distribution can describe \\nthe distribution of weight of adult males. \\n\\uf076For example, you can calculate the probability \\nthat a man weighs between 160 and 170 pounds.\\n\\uf076The area of this range is 0.136; therefore, the \\nprobability that a randomly selected man weighs \\nbetween 160 and 170 pounds is 13.6%. \\n\\uf076The entire area under the curve equals 1.0\\n�\\n−∞+∞\\n𝑝𝑝𝑥𝑥𝑑𝑑𝑥𝑥=1\\n54\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 54}, page_content='Joint Probability\\n\\uf0d8Joint Probability\\n\\uf076Knowing that y occurred reduces the sample space to y\\n\\uf0d8The part of y where x also occurred, or the probability of x and y occurring, is:\\n\\uf0d8𝑃𝑃𝑥𝑥,𝑦𝑦=𝑃𝑃(𝑥𝑥∩𝑦𝑦)\\n\\uf076Order does not matter:\\n\\uf0d8𝑃𝑃(𝑥𝑥,𝑦𝑦)=𝑃𝑃(𝑦𝑦,𝑥𝑥)\\n55\\n\\uf0d8Disjoint Sets \\n\\uf0d8Mutually Exclusive Events\\n\\uf0d8𝑥𝑥∩𝑦𝑦=∅\\uf0d8Intersecting sets'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 55}, page_content='Joint Probability and Marginal Probability\\n56\\uf0d8This table shows the probabilities of various combinations \\nof people’s eye/hair color\\n\\uf0d8Each entry indicates the joint probability of particular \\ncombinations of eye color (𝑒𝑒)and hair color (ℎ), denoted \\nby 𝑝𝑝(𝑒𝑒,ℎ)\\n\\uf0d8The right margin of the table shows the probabilities of the eye colors overall, collapsed across hair colors\\n\\uf0d8Such probabilities are called marginal probability , \\ndenoted by 𝑝𝑝(𝑒𝑒):\\n𝑝𝑝𝑒𝑒=�\\nℎ𝑝𝑝(𝑒𝑒,ℎ)\\n\\uf0d8The marginal probabilities of the hair colors, 𝑝𝑝(ℎ), are \\nindicated on the lower margin of the table:\\n𝑝𝑝ℎ=�\\n𝑒𝑒𝑝𝑝(𝑒𝑒,ℎ)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 56}, page_content='Conditional Probability\\n\\uf0d8Conditional Probability\\n\\uf076P𝑥𝑥𝑦𝑦is the probability of the occurrence of event 𝑥𝑥, given that 𝑦𝑦occurred is given as:\\n\\uf0d8P𝑥𝑥𝑦𝑦=𝑃𝑃(𝑥𝑥∩𝑦𝑦)\\n𝑃𝑃(𝑦𝑦)=𝑃𝑃(𝑥𝑥,𝑦𝑦)\\n𝑃𝑃(𝑦𝑦)\\n\\uf076Answers the question: \\n\\uf0d8How does the probability of an event change if we have extra information?\\n57\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 57}, page_content='Conditional Probability Example\\n\\uf0d8Coin Toss Example:\\n\\uf076Toss a fair coin 3 times\\n\\uf076What is the probability of 3 heads?\\n\\uf0d8Answer: \\n\\uf076𝑆𝑆𝑆𝑆𝑆𝑆𝑝𝑝𝑆𝑆𝑒𝑒𝑆𝑆𝑝𝑝𝑆𝑆𝑆𝑆𝑒𝑒={𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 }\\n\\uf076All outcomes are equally likely (if the coin is fair)\\n\\uf076𝑃𝑃(𝐻𝐻𝐻𝐻𝐻𝐻 )=1\\n8\\n\\uf076Suppose we are told that the first toss was heads\\n\\uf076Given this information, how should we compute the probability of {HHH}?\\n\\uf0d8Answer: \\n\\uf076We have a new (reduced) 𝑆𝑆𝑆𝑆𝑆𝑆𝑝𝑝𝑆𝑆𝑒𝑒𝑆𝑆𝑝𝑝𝑆𝑆𝑆𝑆𝑒𝑒={𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 }\\n\\uf076All outcomes are still equally likely (the coin is still fair)\\n\\uf076𝑃𝑃(𝐻𝐻𝐻𝐻𝐻𝐻 )=14\\n58'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 58}, page_content='Conditional Probability Example\\n\\uf0d8We can visualize the conditional probability \\nas follows\\n\\uf076Think of 𝑃𝑃(𝐴𝐴)as the proportion of the \\narea of the whole sample space taken \\nup by A\\n\\uf076For 𝑃𝑃(𝐴𝐴|𝐵𝐵)we restrict our attention to \\nB\\n\\uf076𝑃𝑃(𝐴𝐴|𝐵𝐵)is the proportion of B taken up \\nby A\\n\\uf0d8𝑃𝑃𝐴𝐴𝐵𝐵=𝑃𝑃(𝐴𝐴∩𝐵𝐵)\\n𝑃𝑃(𝐵𝐵)\\n59\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 59}, page_content='Statistical Independence\\n\\uf0d8Independent Events\\n\\uf076If x and y are independent then they are unconnected and not related to each \\nother\\n\\uf076We have:\\nP𝑥𝑥𝑦𝑦=𝑃𝑃𝑥𝑥\\n\\uf076From there it follows that\\n𝑃𝑃𝑥𝑥,𝑦𝑦=𝑃𝑃𝑥𝑥∗𝑃𝑃(𝑦𝑦)\\n\\uf076In other words, knowing that y occurred does not change the probability that x \\noccurs (and vice versa)\\n\\uf076Examples of absolute independence include:\\n\\uf0d8Eye color and height\\n\\uf0d8Hair color and weight\\n60'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 60}, page_content='Statistical Independence Example\\n\\uf0d8Independent Events\\n\\uf076If we want to calculate the joint probability of two independent events, \\nwe can simply multiply each probability together to get the joint \\nprobability\\n\\uf076“Joint Distribution” = “Product Distribution”\\n\\uf076P𝑥𝑥,𝑦𝑦=𝑃𝑃𝑥𝑥∗𝑃𝑃(𝑦𝑦)\\n\\uf0d8For Example:\\n\\uf076Probability of tossing a coin and getting “Heads”:\\n𝑃𝑃𝐻𝐻𝑒𝑒𝑆𝑆𝑑𝑑𝐻𝐻=𝑃𝑃𝑥𝑥=1\\n2\\n\\uf076Probability of rolling a dice and getting “3”:\\n𝑃𝑃𝑅𝑅𝑅𝑅𝑆𝑆𝑆𝑆“3” =𝑃𝑃𝑦𝑦=16\\n𝑃𝑃𝐻𝐻𝑒𝑒𝑆𝑆𝑑𝑑𝐻𝐻∗𝑃𝑃𝑅𝑅𝑅𝑅𝑆𝑆𝑆𝑆“3” =P𝑥𝑥,𝑦𝑦=12∗16=1\\n12\\n61'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 61}, page_content='Linear regression finds the straight line, called the least squares \\nregression line or LSRL, that best represents observations in a data \\nset. Suppose Yis a dependent variable, and Xis an independent \\nvariable. Then, the equation for the regression line would be: ŷ = \\nb0+ b1xRegression Review\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 62}, page_content='63\\nGeorge Washington University, Intro to Data ScienceAssumptions\\n\\uf0d8Linear relationship between dependent and independent -Plot\\n\\uf0d8Multicollinearity –occurs when independent variables are not \\nindependent –checked with VIF \\n\\uf0d8Auto -correlation –Occurs when residuals are not independent \\nfrom each other – stock prices example -Durbin- Watson d tests\\n\\uf0d8Homoscedasticity –Error term along the regression line are equal \\n–Scatter Plot –can convert the dependent variable. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 63}, page_content='64\\nGeorge Washington University, Intro to Data ScienceRegression\\n\\uf0d8Works best with numeric/continuous data to increase the inferential \\npower\\n\\uf0d8We are trying to model or explain the relationship between a single \\nvariable Y (response, output, dependent) and one or more X1….. \\n(predictor, input, independent, explanatory, regessors..etc.)\\n\\uf0d8Y must be a continuous variable but X categorical, continuous, \\n\\uf0d8Centering (scale/ Zscores )variables so that the predictors have \\nmean 0, is often recommended. The intercept term is then \\ninterpreted as the expected value of Yiwhen the predictor values \\nare set to their means . Otherwise, the intercept is interpreted as the \\nexpected value of Yi when the predictors are set to 0, which may not \\nbe a realistic or interpretable situation (e.g. what if the predictors were height and weight?).'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 64}, page_content='65\\nGeorge Washington University, Intro to Data ScienceWhat Does r2(Coefficient of Determination) Mean?\\nThis statistic quantifies the proportion of the variance of one \\nvariable “explained” (in a statistical sense, not a causal sense) by \\nthe other.\\nAs an example at data set in R (.2697) = 27% of the variability in trunk Height \\nis explained by Girth\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 65}, page_content='66\\nGeorge Washington University, Intro to Data ScienceR Regression Output: Coefficients\\n\\uf0d8Estimate? Coefficients for the regression equation\\n\\uf0d8Standard Error?\\nMeasures the average amount that the coefficient estimates vary from the \\nactual average value of our response variable, can be used to generate \\nconfidence intervals.\\n\\uf0d8P value?\\nA small p -value indicates that it is unlikely we will observe a relationship \\nbetween the predictor (Girth) and response (Height) variables due to chance, a p-value of .05 or less is considered statistical significant \\uf0d8Formula Call? Regression equation'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 66}, page_content='67\\nGeorge Washington University, Intro to Data ScienceR Regression Output\\n\\uf0d8Residual Standard Error\\nThe average amount that the response will deviate from the true regression line. \\nUsed to evaluate our model. \\nIn our example we have a residual error of 5.538 in the predication of tree height. \\nThe average tree height is 76 meaning we could be off by roughly 7%. \\n\\uf0d8Multiple R -squared?\\nProvides a measure of how well the model is fitting the actual data, percentage of variance explained by the predictors, ours example is 27%'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 67}, page_content='68\\nGeorge Washington University, Intro to Data ScienceVisual \\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 68}, page_content='69\\nGeorge Washington University, Intro to Data Science\\nBike Share Output'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 69}, page_content='Another Definition: Functional Approximation\\n\\uf0d8What is a functional approximation problem?\\n\\uf076Target variable: Dependent: What we are trying to predict\\n\\uf076Other Variables: Independent: Using to Predict\\n\\uf0d8Functional approximation is a approach that uses the other variables \\nwe have access to approximate the dependent and does so through \\nthe function development\\n\\uf0d8We will use regression and which assumes that we have a numeric \\ntarget variable, for classification it’s often a bi- variate or class level \\nvariable \\n70'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 70}, page_content='Assessment Measures \\n\\uf0d8Assessing Regression Models: MSE, RMSE and MAE\\n\\uf076MSE –The difference between the predicted values and the actual values \\nsquared\\n\\uf076RSME –Same as above only the square root is taken to put the error back in \\nterms of the dependent variable\\n\\uf0d8Can also normalize the RSME to the range of the data in order to be able to compare \\nRSME outputs that include different data ranges \\n\\uf076MAE –The same approach only taking the absolute value instead of squaring \\n71'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 71}, page_content='Equations \\n72nX X\\nRMSEn\\niidelmo iobs∑=−\\n=12\\n, , ) (\\nmin, max, obs obs X XRMSENRMSE\\n−=\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 72}, page_content='Linear Regression\\n\\uf0d8Best used in situations where the data being deploy lacks a high level of \\ncomplexity \\n\\uf0d8Trains very fast but isn’t able to create complex decision boundaries when \\ndata is heterogeneous. \\n\\uf0d8Also as compared to most ML approaches OLS does not have a built in \\nfunction that can control for overfitting \\n\\uf076However not all hope is lost, we can use forward stepwise regression –which we \\ndiscussed in Intro to DS or\\n\\uf076Ridge/Penalized Regression \\n73'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 73}, page_content='Regression \\n\\uf0d8Said another way basic linear regression has a Prediction Accuracy Problem:\\n\\uf076Has a low bias (overfitting) but a high variance\\n\\uf076This can be improved by injecting some level of bias into the equation by reducing the \\nimpact of certain coefficients \\n\\uf076This can improve overall accuracy by reducing variance \\n\\uf0d8Draw Dart Board –\\n\\uf0d8Another issue is interpretation –with a large number of predictor variables \\nand large data sets it often hard to identify variable importance and explain \\nmodel outcomes\\n74'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 74}, page_content='Regression and Sparsity\\n\\uf0d8Often we have more features than observations in the world of big \\ndata\\n\\uf076What type of problem is this?\\n\\uf0d8So we strive to have Sparse models\\n\\uf076What do we mean by Sparse?\\n75'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 75}, page_content='Bias Versus Variance \\n76\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 76}, page_content='Ridge Regression \\n\\uf0d8Injecting bias can be done through a regulator or utilization of a penalizing \\nattribute. two common examples are Ridge and Lasso, let’s start with Ridge\\n77\\nBasic Regression Equation'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 77}, page_content='78\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 78}, page_content='Ridge Penalty \\n\\uf0d8In this example if 𝛼𝛼 is 0 we simply have normal regression equations\\n\\uf0d8If 𝛼𝛼is large or ∞ then the 𝛽𝛽approaches zero and only the constant term is \\navailable to predict y\\n\\uf0d8Essentially provides a weight on the squared residuals during the normal OLS \\nprocess \\n\\uf0d8We want to train the regulator to fall between 0 and 1 using cross -validation \\nin such a way that it minimize mean square error\\n79\\nEssentially the square of the Euclidean norm (magnitude) \\nof 𝛽𝛽which is the vector of coefficientsor\\n𝛼𝛼'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 79}, page_content='Gradient Decent \\n\\uf0d8Penalized regression methods use a very common algorithm called \\ngradient decent\\n\\uf0d8It is essentially a step function that works to minimize some cost \\nfunction through a iterative process\\n80\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 80}, page_content='Gradient Decent \\n81\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 81}, page_content='82\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 82}, page_content='L2 Norm or Euclidean Norm or Euclidean Length\\n\\uf0d8Square of all the elements in matrix \\n\\uf0d8Sum the values together\\n\\uf0d8Take the square root \\n\\uf0d8Ridge also squares the final result of this, so we are taking the square \\nof the Euclidean norm and multiply this number by the penalty to \\nweight the coefficients \\n83In words, the L2 norm is defined as, 1) square all the elements in the vector together; 2) sum these squared values; and, 3) take the square root o f this sum.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 83}, page_content='Ridge Visualization\\n84\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 84}, page_content='Measuring the Magnitude of Vectors, L2 and L1\\n\\uf0d8𝛽𝛽=𝛽𝛽0\\n𝛽𝛽1\\n\\uf0d8L2 –Norm : II𝛽𝛽II2 = 𝛽𝛽02+ 𝛽𝛽12 : What we used for ridge, essentially \\ncalculates the magnitude of the coefficient vector of the regression \\nequation, gives us a bias minimum \\n\\uf0d8L1 \\n85'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 85}, page_content='Measuring the Magnitude of Vectors, L2 and L1\\n\\uf0d8𝛽𝛽=𝛽𝛽0\\n𝛽𝛽1\\n\\uf0d8L1 –Norm : II𝛽𝛽II1 = I𝛽𝛽0I + I𝛽𝛽1I : What we used for ridge, essentially \\ncalculates the magnitude of the coefficient vector of the regression \\nequation, gives us a bias minimum. Cartesian Distance or Euclidean \\nDistance of our vector\\n86'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 86}, page_content='Measuring the Magnitude of Vectors, L2 and L1\\n87'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Knn and Prob.pdf', 'page': 87}, page_content='Lasso Equation\\n88\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 0}, page_content='Machine Learning Overview\\nBrian Wright, PhD'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 1}, page_content='2Themes\\nMachine Learning Lifecycle\\nAre you ready for Machine Learning?\\nTerms and Phases '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 2}, page_content='Engineering of \\nMachine Learning \\nAlgos versus \\nSoftware \\nDevelopment \\nSource: https://towardsdatascience.com/stoend -to-\\nend-data -science -life-cycle -6387523b5afc'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 3}, page_content='Brian’s Version of Data Science Lifecycle\\n4Question IDBusiness \\nUnderstandingData \\nAcquisition -\\nETLInitial Model\\nEvaluationData \\nUnderstanding -\\nEDAInitial Model(s) \\nBuilding\\nEvaluation \\nCriteria Value \\nMetric Model Creation \\n& Training \\nFeature \\nEngineering \\nand Evaluation\\nOptimization –\\nHyperpara and \\nEvaluationModel \\nDeployment\\nData Drift \\nAnalysisModel Performance \\n–Evaluation Value \\nMetric\\nModel Drift \\nAnalysis –Model \\nEvaluation\\nReports –Dashboards -Products \\nG1 G2\\nG3\\nGate \\nReviews'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 4}, page_content='5Machine Learning \\nTime'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 5}, page_content='6“A field of Computer Science that gives computers the ability to learn\\nwithout being explicitly programmed.”\\n-Arthur Samuel (Coined the term in 1959 at IBM)\\n“The ability [for systems] to acquire their own knowledge, by\\nextracting patterns from raw data.”\\n-Deep Learning, Goodfellow et al\\n“A computer program is said to learn from experience E with respect\\nto some set of tasks T and performance measure P if its performance\\ntasks in T, as measured by P, improves with experience E.”\\n-Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 6}, page_content='Machine vs. human\\nMachine Human\\nUnderstanding context ✔\\nThinking through the problem ✔\\nAsking the right questions ✔\\nSelecting the right tools ✔\\nPerforming calculations quickly ✔\\nPerforming repetitive tasks ✔\\nFollowing pre-defined rules ✔\\nInterpreting results ✔'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 7}, page_content='8\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 8}, page_content='Pattern discovery when inputs (x) and outputs (y) are knownSupervised machine learning\\nInput x:\\nVoterOutput y:\\nPolitical \\naffiliation\\nExamples: Classification and regression are supervised machine learning \\n9'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 9}, page_content='The data inputs (x)have no target outputs (y)Unsupervised machine learning\\nInput x:\\nVoterOutput y:\\nNot given\\n(to be discovered)?\\nWe want to impose structure on the inputs (x)to say something meaningful about the data\\n10\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 10}, page_content='11'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 11}, page_content='12Machine Learning is a general use technology \\nwhat does that mean?'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 12}, page_content='Machine Learning Overview \\n\\uf0d8Ageneral -purpose technology orGPT is a term coined to describe \\na new method of producing and inventing that is important enough to \\nhave a protracted aggregate impact.\\n\\uf0d8Similar to electricity or the internet, in that it can be applied across \\ndomains and work to improve market outcomes. \\n13'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 13}, page_content='Machine Learning Overview \\n\\uf0d8Twitter Data Usage\\n\\uf0d8Error rates on ImageNet (10,000 labelled images) have been driven \\ndown from 30% in 2010 to less than 3% today. \\n\\uf076Below 5% is important why? \\n\\uf0d8Chess: Deep Blue (IBM AI) searched some 200 million positions per \\nsecond, Kasparov was searching not more than 5– 10 positions \\nprobably, per second. Yet he played almost at the same level….why?\\n14'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 14}, page_content='Machine Learning Overview \\n\\uf0d8However, before we all turn into robots consider two important facts: \\n1.We remain remarkably far away from what would be consider a similar \\ngeneral intelligence that can be compared to humans\\n2.Machines cannot do the full range of tasks that humans can do\\nWe can then refer to jobs or activities that might be good cases for Machine \\nLearning as SML or Suitable for Machine Learning\\n15What are examples of tasks that might be SML \\nand how do we know if our organizations are \\nready?'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 15}, page_content='Machine Learning Overview \\n\\uf0d8Successful implementation of ML requires very detailed \\nspecifications on what is to be learned and data to support that \\nlearning activity. \\n\\uf0d8Including the development of engineering features through a series \\nof trial-and- error and.. \\n\\uf0d8Then most importantly embedding these products into normal \\nbusiness operations in such a way that efficiencies can be realized.\\n16'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 16}, page_content='Machine Learning Overview \\n\\uf0d8What tasks are most suitable for ML to take over: \\n\\uf076Most recent successes are predicated on supervised learning \\n\\uf076Competency is narrow as compared to the complexity of human decision making  \\n1.Learning a function that maps well -defined inputs to well- defined outputs\\noIf can predict Y given any value of X –still might not produce the actual causal effect\\n2.Large Data is present or can be created containing input -output pairs\\noThe more training data available the more arcuate the model\\n3.Task provides clear feedback with well definable goals and metrics \\noIf we know what to achieve –(optimize flight patterns not a single flight)\\n4.Where reasoning and diverse background knowledge is not necessary\\noGood at empirical associations but terrible at decision making that requires common \\nsense of historical knowledge\\n5.No need for why the decision was made to be clear\\noNN could use millions numerical weights\\n17'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 17}, page_content='Machine Learning Overview \\n6.A tolerance for error or sub- optimal solutions\\noML use probabilistic outputs which means some error is always assumed\\n7.Function of item being learned should not change rapidly over time\\noWork best when the distribution of future test examples is the same roughly as the \\ntraining set over time\\noIf not the case systems need to be in place to refresh algorithms \\n18'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 18}, page_content='How do machines learn?\\n\\uf0d8The basic machine learning process can be divided into \\nthree parts.\\n\\uf076Data Input: Past data or information is utilized as a basis \\nfor future decision-making\\n\\uf076Abstraction: The input data is represented in a broader \\nway through the underlying algorithm\\n\\uf076Generalization: The abstracted representation is \\ngeneralized to form a framework for making decisions\\n19(reference Introduction to ML by Subramanian Chandramouli , Saikat Dutt , Amit Kumar Das\\n(https://learning.oreilly.com/library/view/machine -learning/9789389588132/xhtml/chapter001.xhtml#ch1_1)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 19}, page_content='20\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 20}, page_content='Brian’s Version of Data Science Lifecycle\\n21Question ID\\nBusiness \\nUnderstandingData \\nAcquisition/ \\nRepresentationInitial Model\\nEvaluationData \\nUnderstanding -\\nEDAInitial Model(s) \\nBuilding\\nEvaluation \\nCriteria/Value \\nMetric Model Creation \\n& Training \\nFeature \\nEngineering \\nand Evaluation\\nFinal Model \\nDevelopmentModel \\nDeployment\\nData Drift \\nAnalysisModel Performance \\n–Evaluation Value \\nMetric\\nModel Drift \\nAnalysis –Model \\nEvaluation\\nReports – Dashboards -Products \\nG1 G2\\nG3\\nGate \\nReviews'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 21}, page_content='Overview of SOME Key ML Methods/Terms \\n\\uf0d8Phase –1 Idea Development \\n\\uf076Prediction versus Inference \\n\\uf076Independent Metric for Business Value\\n\\uf076Target Variable and features \\n\\uf076Classification versus Regression\\n\\uf076Probabilistic Interpretation\\n\\uf076Data acquisition/gathering   \\n\\uf0d8Phase –2 Data Prep and Problem Exploration\\n\\uf076Variable types and data types\\n\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\n\\uf076Missing Data \\n\\uf076Baseline –prevalence\\n\\uf076Data Partitioning/Sampling \\n\\uf076EDA and (Summary Stats and Visuals)\\n\\uf076Cross Validation \\n\\uf0d8Phase –3 –Solution Model Development\\n\\uf076Parameters versus Hyperparameters\\n\\uf076Thresholding \\n\\uf076Feature Engineering\\n\\uf076Bias versus Variance Tradeoff \\n\\uf076Model Evaluation \\n\\uf076Non -parametric modelling (random state)\\n22'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 22}, page_content='23Phase I'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 23}, page_content='\\uf0d8# Prediction versus Inference \\n24'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 24}, page_content='\\uf0d8# Prediction versus Inference\\n\\uf076Goals of prediction are not centered on how the features are interacting or \\nresulting in an event but are instead focused on the ability of the model to \\npredicted an event. \\n\\uf076Almost all ML methods are focused on predication not causation or \\ninference. \\n\\uf076This is why model performance is based largely on how well a model predicts \\nnot necessarily how much individual variables are contributing to error reduction. \\n25'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 25}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Independent Metric for Business Value \\n\\uf076A key part of building a solution using Machine Learning Techniques is \\nhaving a metric that is independent of the model that can be used to \\ndetermine if the model is providing value.  \\n\\uf076Examples\\n\\uf0d8Recommender Engine for Netflix: Number of user clicks\\n\\uf0d8Spam Block Predictor: Number of viruses in the network \\n\\uf0d8Market Clustering: Did sales increase\\n\\uf0d8Others? \\n26'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 26}, page_content='Overview of Key ML Methods/Terms: Target Variable versus Features \\n\\uf076Target variable –Is the variable that includes the patterns the machine \\nlearning algorithm is trying to learn.  It is the variable of interest and key to \\nevaluating the model output.  \\n\\uf0d8More simply it is the variable we are trying to predict.  \\n\\uf076Feature variables – Are the variables the model will use to learn the patterns \\nof the target variable. The process of feature engineering can result in additional features. \\n\\uf0d8More simply these are the variables used for predicting the target\\n27'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 27}, page_content='Overview of Key ML Methods/Terms: Classification versus Regression \\n\\uf0d8Classification is the process of developing a model to predict \\nwhether a target variable is in defined categories.  This is driven by \\nhaving either a binary or multi- level categorical variable as the target \\nvariable. \\n\\uf0d8Examples: \\n\\uf076Predicting whether someone is male, or female based on 1,000s of pictures.\\n\\uf076Predicting whether a team will have a winning season or not based on player \\nperformance \\n\\uf076Predicting whether a person will default on a loan or not\\n\\uf0d8Key point: The predications of the model are not binary (1s or 0s) but are \\ngiven as percentages indicating the likelihood that any one row of data \\nbelongs to any one category.  In the case of target variables with multiple \\ncategories each row will get the same number of percent predictions as categories.\\n28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 28}, page_content='Overview of Key ML Methods/Terms: Classification versus Regression \\n\\uf0d8Regression is the process of developing a model to predict a \\nspecific number or range of numbers. This is driven by having \\na continuous variable as the target variable for the model\\n\\uf0d8Examples: \\n\\uf076Predicting the score given the players playing a game. \\n\\uf076Predicting an amount of rain given weather conditions \\n\\uf076Predicting a persons weight based on various personal statistics\\n29'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 29}, page_content='Overview of Key ML Methods/Terms: Probabilistic Interpretation \\n\\uf0d8A significant portion of this class will focus on building models for \\nclassification. Classification is a much more common machine \\nlearning goal versus regression. \\n\\uf0d8We all know the range of values for probabilities, 0 to 100, the key to \\nunderstanding these outputs is to think of them as risk measures, \\nwith 100 being no risk and 0 being all the risk! \\n\\uf0d8How the outputs are used will depend on your question. \\n\\uf076Example: How certain do you want to be that a drug is effective as compared \\nto whether a customer will open a marketing email? The results could both \\nyield 75% probabilities but is that high enough? \\n\\uf0d8Could also think of the outputs as a quantification of uncertainty, the \\nquestion becomes given your problem how much uncertainty are you \\nwilling to accept? \\n30'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 30}, page_content='Overview of Key ML Methods/Terms: Data Brainstorming \\n\\uf076Data to Concept –Does the data available support the algo target \\nand goal\\n\\uf0d8How difficult is the data to gather?\\n\\uf0d8Is the data large enough? \\n\\uf0d8What is the rate of change of the data? \\n\\uf0d8Do we believe this is the correct source and data content to address the \\nproblem?\\n\\uf076Learning Difficulty –How complex or vague is the target variable? \\n\\uf0d8Are there imbalances in the classes?\\n\\uf0d8Does the data clearly link to the problem? \\n\\uf0d8Has this data been used in the past, to what success?\\n\\uf0d8Is the target difficult to measure or break into smaller components?  \\n\\uf0d8What risk level are you willing to accept given the question?31'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 31}, page_content='32Phase II'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 32}, page_content='Brian’s Version of Data Science Lifecycle\\n33Question ID\\nBusiness \\nUnderstandingData \\nAcquisition/ \\nRepresentationInitial Model\\nEvaluationData \\nUnderstanding -\\nEDAInitial Model(s) \\nBuilding\\nEvaluation \\nCriteria/Value \\nMetric Model Creation \\n& Training \\nFeature \\nEngineering \\nand Evaluation\\nFinal Model \\nDevelopmentModel \\nDeployment\\nData Drift \\nAnalysisModel Performance \\n–Evaluation Value \\nMetric\\nModel Drift \\nAnalysis –Model \\nEvaluation\\nReports – Dashboards -Products \\nG1 G2\\nG3\\nGate \\nReviews'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 33}, page_content='Overview of SOME Key ML Methods/Terms \\n\\uf0d8Phase –1 Idea Development \\n\\uf076Prediction versus Inference \\n\\uf076Independent Metric for Business Value\\n\\uf076Target Variable and features \\n\\uf076Classification versus Regression\\n\\uf076Probabilistic Interpretation \\n\\uf076Data acquisition/gathering     \\n\\uf0d8Phase –2 Data Prep and Problem Exploration\\n\\uf076Variable types and data types\\n\\uf076Baseline –prevalence\\n\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\n\\uf076Missing Data \\n\\uf076Data Partitioning/Sampling \\n\\uf076EDA (Summary Stats and Visuals)\\n\\uf076Cross Validation \\n\\uf0d8Phase –3 –Solution Model Development\\n\\uf076Parameters versus Hyperparameters\\n\\uf076Thresholding \\n\\uf076Feature Engineering\\n\\uf076Bias versus Variance Tradeoff \\n\\uf076Model Evaluation\\n\\uf076Non -parametric modelling (random state)\\n34'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 34}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Variable Types and Data Types\\n\\uf076Five Atomic Variable Types in R\\n\\uf0d8Numeric –number unlimited size\\n\\uf0d8Integer –number with constraints on size  \\n\\uf0d8Complex –numbers and characters\\n\\uf0d8Character –words\\n\\uf0d8Factor –unique character class that is limited in the number of categories\\n\\uf0d8Logical –True or False \\n\\uf0d8Data Types \\n\\uf076List -A list is an R -object containing different types of elements inside it like vectors, \\nfunctions, and even another list inside it.\\n\\uf076Vector -Avector in R is a series of data items of the same basic type (from above)\\n\\uf076Array -is alistorvector with two or more dimensions\\n\\uf076Matrix -A matrix is a two -dimensional rectangular data structure, created through the \\nuse of matrix function. Usually numeric, can’t have different data types, think of it as \\nmany vectors \\n\\uf076Dataframe –A two dimensional object that can contain multiple variable types 35'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 35}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Some useful variable and data type  \\n\\uf076str()\\n\\uf076class()\\n\\uf076names()\\n\\uf076length()\\n\\uf076dim()\\nOpen up Rstudio and try these functions out on the mtcars dataset. See if you agree with \\nthe output. \\n36'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 36}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Baseline –prevalence \\n\\uf076The proportion of a particular population found to be in the positive class at \\na specific time. “Positive class” in this example is the class to which we are \\ntrying to learn. Percentage split across classes of our target variable. \\n\\uf076Using mtcars again, what is the prevalence of vs variable? \\n37'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 37}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Scaling and/or Normalizing Data/One -Hot Encoding\\n\\uf076Many DS approaches require the data to be normalized or placed into a \\nstandard format so comparison between variables is possible. \\n\\uf076For factor variables this measure creating individual columns for each level \\nthat are logical or boolien 1s and 0s. \\n\\uf076We will mostly use a min max scaler that will maintain the variance of the \\nvalues but re -calculate them to be between 1 and 0. \\n\\uf0d8Use the minmax scaling function in the gradDescent package and \\nscale the mtcars dataset setting the results to a new object. What \\nhappens? What class is the object? Can you view the data.frame?  \\n38'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 38}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Missing Data \\n\\uf076Large area of study concerning missing data. Here we just need to be aware \\nof how to check for missing data and quick solutions \\n\\uf076R comes with several functions/packages that handle missing data we are \\ngoing to focus on the MICE package. \\n\\uf0d8First you need to try to detect if there are patterns of missing data, is it random or not. If \\nyou detect patterns than you have to develop a strategy to deal with that issue. \\n\\uf076MCAR –missing completely at random\\n\\uf076MNAR  -missing not at random\\n\\uf0d8Start with the summary() function on a data frame \\n\\uf076Load in the beaches dataframe from the data file and find the columns that have missing \\ndata using the summary function\\n\\uf076Generally variables with more the about 5% missing values should be deleted or imputation \\nneeds to occur \\n\\uf0d8Dig a little deeper and use the md.pattern () function in the Mice package. \\n\\uf0d8Since there doesn’t appear to be a pattern we will use complete cases to remove the \\nNAs. \\n39'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 39}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Missing Data \\n\\uf076Complete.cases () function creates aindex to remove missing values \\n\\uf0d8remove missing values from a vector \\nx <-x[complete.cases(x)]\\n\\uf0d8remove from a data.frame\\ndf <-df[complete.cases(df), ]\\n\\uf0d8remove from individual rows \\ndf <-df[complete.cases(df[ , c(row1, row2, ….)]), ]\\nTry the dataframe version on the beaches dataset, then use summary() \\nto see if the missing datapoints are gone. \\nMICE package can also do imputation (NA replacement) very easily, lots \\nof examples online on how to do these in very robust ways. \\n40'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 40}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Partitioning and Sampling\\n\\uf076We need to split our data into three sections (in most cases) to build \\nmachine learning models \\n\\uf076Training –What we use to build the original model \\n\\uf076Tune –Data used to evaluate initial outputs of a model after it’s been \\nmodified (example: changing the k in kNN ) (Feature Engineering)\\n\\uf076Test –Very last step to evaluate the quality of the model after training and \\ntune\\n\\uf0d8The function we will be using throughout the course will be the \\ncreateDataPartition() function in the caret package. \\n\\uf076The problem is that it’s not great at creating multiple partition, so we \\nessentially use it twice to create a sample, then a sample of a sample. \\n\\uf076Need to make sure to use the target variable to do stratified sampling, \\notherwise we could create imbalances in our samples. 41'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 41}, page_content='Cross -Validation\\n42\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 42}, page_content='Cross -Validation\\n43\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 43}, page_content='44Phase III'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 44}, page_content='Overview of SOME Key ML Methods/Terms \\n45\\uf0d8Phase –1 Idea Development \\n\\uf076Prediction versus Inference \\n\\uf076Independent Metric for Business Value\\n\\uf076Target Variable and features \\n\\uf076Classification versus Regression\\n\\uf076Probabilistic Interpretation \\n\\uf076Data acquisition/gathering     \\n\\uf0d8Phase –2 Data Prep and Problem Exploration\\n\\uf076Variable types and data types\\n\\uf076Baseline –prevalence\\n\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\n\\uf076Missing Data \\n\\uf076Data Partitioning/Sampling \\n\\uf076EDA (Summary Stats and Visuals)\\n\\uf076Cross Validation \\n\\uf0d8Phase –3 –Solution Model Development\\n\\uf076Parameters versus Hyperparameters\\n\\uf076Thresholding \\n\\uf076Feature Engineering\\n\\uf076Bias versus Variance Tradeoff \\n\\uf076Model Evaluation\\n\\uf076Non- parametric modelling (random state)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 45}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8# Feature Engineering – Combining or exploring different levels of \\nvariable that best work in your model. Likely going to dedicate a week \\nto just this topic.  \\n46'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 46}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Thresholding –The percentage point where our models will predict \\nthe result to be either a 0 or 1, in the typical binary case. \\n\\uf0d8Adjust the threshold associated with indication of a positive class.  \\nThe default is 50%, could be that we want to be extra careful and \\ninstead adjust that measure up to 75% or 90%. \\n47'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 47}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Evaluation –The metrics you use to assess model quality. There are a \\nton of this measures, and we are dedicating an entire week to the \\nexploring these further.  I’ll show some examples in the code for this \\nweek. \\n48'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 48}, page_content='Bias Versus Variance \\n49\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 49}, page_content='50Extra Material '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 50}, page_content='51Bookings.com  '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 51}, page_content='52Lesson Learned: Booking.com'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 52}, page_content='Bookings.com\\n\\uf0d8Swiss Army Knife –Their approach to ML is highly adoptable , \\nmeaning it can be used in a variety of settings –generate specific results \\nor more generalizable depending on the inputs (data)\\n\\uf0d8Offline Health Check– Use Randomized Control Trails (RCT) to test \\nmodel outputs aligned with normative business metrics to assess quality \\n(customer conversion)\\n\\uf076Increase model performance doesn’t necessary translate to better gain in value\\n53'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 53}, page_content='Bookings.com\\n\\uf0d8Make a Target Before you Shoot –Develop a clear understanding of \\nthe business case and target variable (what is date flexibility) \\n\\uf076Learning Difficulty –How complex or vague is the target\\n\\uf076Data to Concept –Does the data available support the algo target and goal\\n\\uf076Selection Bias –Does the model perform better for a subset of the target \\n\\uf0d8Speed Kills –ML algos, even simple ones, take a lot of computing \\npower –to reduce user weight time (latency) measures should be taken\\n\\uf076See page 1748 (sparsity, model redundancy, caching…etc.)\\n54'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II copy.pdf', 'page': 54}, page_content='Machine Learning Overview \\n\\uf0d8Keep a watchful eye –Used specialized monitoring tools to \\nunderstand how the models are performing in practice (even when the \\nresult was unclear)\\n\\uf0d8Traditional Research Methods (Experimental Design) is a Best \\nPractice Approach to ML –\\n\\uf076“Experimentation through Randomized Controlled Trials is ingrained into \\nBooking.com culture”\\n55'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 0}, page_content='Machine Learning Overview\\nBrian Wright, PhD'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 1}, page_content='2Themes\\nMachine Learning Lifecycle\\nAre you ready for Machine Learning?\\nTerms and Phases '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 2}, page_content='Engineering of \\nMachine Learning \\nAlgos versus \\nSoftware \\nDevelopment \\nSource: https://towardsdatascience.com/stoend -to-\\nend-data -science -life-cycle -6387523b5afc'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 3}, page_content='Brian’s Version of Data Science Lifecycle\\n4Question IDBusiness \\nUnderstandingData \\nAcquisition -\\nETLInitial Model\\nEvaluationData \\nUnderstanding -\\nEDAInitial Model(s) \\nBuilding\\nEvaluation \\nCriteria Value \\nMetric Model Creation \\n& Training \\nFeature \\nEngineering \\nand Evaluation\\nOptimization –\\nHyperpara and \\nEvaluationModel \\nDeployment\\nData Drift \\nAnalysisModel Performance \\n–Evaluation Value \\nMetric\\nModel Drift \\nAnalysis –Model \\nEvaluation\\nReports –Dashboards -Products \\nG1 G2\\nG3\\nGate \\nReviews'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 4}, page_content='5Machine Learning \\nTime'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 5}, page_content='6“A field of Computer Science that gives computers the ability to learn\\nwithout being explicitly programmed.”\\n-Arthur Samuel (Coined the term in 1959 at IBM)\\n“The ability [for systems] to acquire their own knowledge, by\\nextracting patterns from raw data.”\\n-Deep Learning, Goodfellow et al\\n“A computer program is said to learn from experience E with respect\\nto some set of tasks T and performance measure P if its performance\\ntasks in T, as measured by P, improves with experience E.”\\n-Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 6}, page_content='Machine vs. human\\nMachine Human\\nUnderstanding context ✔\\nThinking through the problem ✔\\nAsking the right questions ✔\\nSelecting the right tools ✔\\nPerforming calculations quickly ✔\\nPerforming repetitive tasks ✔\\nFollowing pre-defined rules ✔\\nInterpreting results ✔'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 7}, page_content='8\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 8}, page_content='Pattern discovery when inputs (x) and outputs (y) are knownSupervised machine learning\\nInput x:\\nVoterOutput y:\\nPolitical \\naffiliation\\nExamples: Classification and regression are supervised machine learning \\n9'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 9}, page_content='The data inputs (x)have no target outputs (y)Unsupervised machine learning\\nInput x:\\nVoterOutput y:\\nNot given\\n(to be discovered)?\\nWe want to impose structure on the inputs (x)to say something meaningful about the data\\n10\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 10}, page_content='11'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 11}, page_content='12Machine Learning is a general use technology \\nwhat does that mean?'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 12}, page_content='Machine Learning Overview \\n\\uf0d8Ageneral -purpose technology orGPT is a term coined to describe \\na new method of producing and inventing that is important enough to \\nhave a protracted aggregate impact.\\n\\uf0d8Similar to electricity or the internet, in that it can be applied across \\ndomains and work to improve market outcomes. \\n13'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 13}, page_content='Machine Learning Overview \\n\\uf0d8Twitter Data Usage\\n\\uf0d8Error rates on ImageNet (10,000 labelled images) have been driven \\ndown from 30% in 2010 to less than 3% today. \\n\\uf076Below 5% is important why? \\n\\uf0d8Chess: Deep Blue (IBM AI) searched some 200 million positions per \\nsecond, Kasparov was searching not more than 5– 10 positions \\nprobably, per second. Yet he played almost at the same level….why?\\n14'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 14}, page_content='Machine Learning Overview \\n\\uf0d8However, before we all turn into robots consider two important facts: \\n1.We remain remarkably far away from what would be consider a similar \\ngeneral intelligence that can be compared to humans\\n2.Machines cannot do the full range of tasks that humans can do\\nWe can then refer to jobs or activities that might be good cases for Machine \\nLearning as SML or Suitable for Machine Learning\\n15What are examples of tasks that might be SML \\nand how do we know if our organizations are \\nready?'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 15}, page_content='Machine Learning Overview \\n\\uf0d8Successful implementation of ML requires very detailed \\nspecifications on what is to be learned and data to support that \\nlearning activity. \\n\\uf0d8Including the development of engineering features through a series \\nof trial-and- error and.. \\n\\uf0d8Then most importantly embedding these products into normal \\nbusiness operations in such a way that efficiencies can be realized.\\n16'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 16}, page_content='Machine Learning Overview \\n\\uf0d8What tasks are most suitable for ML to take over: \\n\\uf076Most recent successes are predicated on supervised learning \\n\\uf076Competency is narrow as compared to the complexity of human decision making  \\n1.Learning a function that maps well -defined inputs to well- defined outputs\\noIf can predict Y given any value of X –still might not produce the actual causal effect\\n2.Large Data is present or can be created containing input -output pairs\\noThe more training data available the more arcuate the model\\n3.Task provides clear feedback with well definable goals and metrics \\noIf we know what to achieve –(optimize flight patterns not a single flight)\\n4.Where reasoning and diverse background knowledge is not necessary\\noGood at empirical associations but terrible at decision making that requires common \\nsense of historical knowledge\\n5.No need for why the decision was made to be clear\\noNN could use millions numerical weights\\n17'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 17}, page_content='Machine Learning Overview \\n6.A tolerance for error or sub- optimal solutions\\noML use probabilistic outputs which means some error is always assumed\\n7.Function of item being learned should not change rapidly over time\\noWork best when the distribution of future test examples is the same roughly as the \\ntraining set over time\\noIf not the case systems need to be in place to refresh algorithms \\n18'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 18}, page_content='How do machines learn?\\n\\uf0d8The basic machine learning process can be divided into \\nthree parts.\\n\\uf076Data Input: Past data or information is utilized as a basis \\nfor future decision-making\\n\\uf076Abstraction: The input data is represented in a broader \\nway through the underlying algorithm\\n\\uf076Generalization: The abstracted representation is \\ngeneralized to form a framework for making decisions\\n19(reference Introduction to ML by Subramanian Chandramouli , Saikat Dutt , Amit Kumar Das\\n(https://learning.oreilly.com/library/view/machine -learning/9789389588132/xhtml/chapter001.xhtml#ch1_1)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 19}, page_content='20\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 20}, page_content='Brian’s Version of Data Science Lifecycle\\n21Question ID\\nBusiness \\nUnderstandingData \\nAcquisition/ \\nRepresentationInitial Model\\nEvaluationData \\nUnderstanding -\\nEDAInitial Model(s) \\nBuilding\\nEvaluation \\nCriteria/Value \\nMetric Model Creation \\n& Training \\nFeature \\nEngineering \\nand Evaluation\\nFinal Model \\nDevelopmentModel \\nDeployment\\nData Drift \\nAnalysisModel Performance \\n–Evaluation Value \\nMetric\\nModel Drift \\nAnalysis –Model \\nEvaluation\\nReports – Dashboards -Products \\nG1 G2\\nG3\\nGate \\nReviews'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 21}, page_content='Overview of SOME Key ML Methods/Terms \\n\\uf0d8Phase –1 Idea Development \\n\\uf076Prediction versus Inference \\n\\uf076Independent Metric for Business Value\\n\\uf076Target Variable and features \\n\\uf076Classification versus Regression\\n\\uf076Probabilistic Interpretation\\n\\uf076Data acquisition/gathering   \\n\\uf0d8Phase –2 Data Prep and Problem Exploration\\n\\uf076Variable types and data types\\n\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\n\\uf076Missing Data \\n\\uf076Baseline –prevalence\\n\\uf076Data Partitioning/Sampling \\n\\uf076EDA and (Summary Stats and Visuals)\\n\\uf076Cross Validation \\n\\uf0d8Phase –3 –Solution Model Development\\n\\uf076Parameters versus Hyperparameters\\n\\uf076Thresholding \\n\\uf076Feature Engineering\\n\\uf076Bias versus Variance Tradeoff \\n\\uf076Model Evaluation \\n\\uf076Non -parametric modelling (random state)\\n22'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 22}, page_content='23Phase I'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 23}, page_content='\\uf0d8# Prediction versus Inference \\n24'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 24}, page_content='\\uf0d8# Prediction versus Inference\\n\\uf076Goals of prediction are not centered on how the features are interacting or \\nresulting in an event but are instead focused on the ability of the model to \\npredicted an event. \\n\\uf076Almost all ML methods are focused on predication not causation or \\ninference. \\n\\uf076This is why model performance is based largely on how well a model predicts \\nnot necessarily how much individual variables are contributing to error reduction. \\n25'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 25}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Independent Metric for Business Value \\n\\uf076A key part of building a solution using Machine Learning Techniques is \\nhaving a metric that is independent of the model that can be used to \\ndetermine if the model is providing value.  \\n\\uf076Examples\\n\\uf0d8Recommender Engine for Netflix: Number of user clicks\\n\\uf0d8Spam Block Predictor: Number of viruses in the network \\n\\uf0d8Market Clustering: Did sales increase\\n\\uf0d8Others? \\n26'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 26}, page_content='Overview of Key ML Methods/Terms: Target Variable versus Features \\n\\uf076Target variable –Is the variable that includes the patterns the machine \\nlearning algorithm is trying to learn.  It is the variable of interest and key to \\nevaluating the model output.  \\n\\uf0d8More simply it is the variable we are trying to predict.  \\n\\uf076Feature variables – Are the variables the model will use to learn the patterns \\nof the target variable. The process of feature engineering can result in additional features. \\n\\uf0d8More simply these are the variables used for predicting the target\\n27'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 27}, page_content='Overview of Key ML Methods/Terms: Classification versus Regression \\n\\uf0d8Classification is the process of developing a model to predict \\nwhether a target variable is in defined categories.  This is driven by \\nhaving either a binary or multi- level categorical variable as the target \\nvariable. \\n\\uf0d8Examples: \\n\\uf076Predicting whether someone is male, or female based on 1,000s of pictures.\\n\\uf076Predicting whether a team will have a winning season or not based on player \\nperformance \\n\\uf076Predicting whether a person will default on a loan or not\\n\\uf0d8Key point: The predications of the model are not binary (1s or 0s) but are \\ngiven as percentages indicating the likelihood that any one row of data \\nbelongs to any one category.  In the case of target variables with multiple \\ncategories each row will get the same number of percent predictions as categories.\\n28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 28}, page_content='Overview of Key ML Methods/Terms: Classification versus Regression \\n\\uf0d8Regression is the process of developing a model to predict a \\nspecific number or range of numbers. This is driven by having \\na continuous variable as the target variable for the model\\n\\uf0d8Examples: \\n\\uf076Predicting the score given the players playing a game. \\n\\uf076Predicting an amount of rain given weather conditions \\n\\uf076Predicting a persons weight based on various personal statistics\\n29'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 29}, page_content='Overview of Key ML Methods/Terms: Probabilistic Interpretation \\n\\uf0d8A significant portion of this class will focus on building models for \\nclassification. Classification is a much more common machine \\nlearning goal versus regression. \\n\\uf0d8We all know the range of values for probabilities, 0 to 100, the key to \\nunderstanding these outputs is to think of them as risk measures, \\nwith 100 being no risk and 0 being all the risk! \\n\\uf0d8How the outputs are used will depend on your question. \\n\\uf076Example: How certain do you want to be that a drug is effective as compared \\nto whether a customer will open a marketing email? The results could both \\nyield 75% probabilities but is that high enough? \\n\\uf0d8Could also think of the outputs as a quantification of uncertainty, the \\nquestion becomes given your problem how much uncertainty are you \\nwilling to accept? \\n30'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 30}, page_content='Overview of Key ML Methods/Terms: Data Brainstorming \\n\\uf076Data to Concept –Does the data available support the algo target \\nand goal\\n\\uf0d8How difficult is the data to gather?\\n\\uf0d8Is the data large enough? \\n\\uf0d8What is the rate of change of the data? \\n\\uf0d8Do we believe this is the correct source and data content to address the \\nproblem?\\n\\uf076Learning Difficulty –How complex or vague is the target variable? \\n\\uf0d8Are there imbalances in the classes?\\n\\uf0d8Does the data clearly link to the problem? \\n\\uf0d8Has this data been used in the past, to what success?\\n\\uf0d8Is the target difficult to measure or break into smaller components?  \\n\\uf0d8What risk level are you willing to accept given the question?31'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 31}, page_content='32Phase II'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 32}, page_content='Brian’s Version of Data Science Lifecycle\\n33Question ID\\nBusiness \\nUnderstandingData \\nAcquisition/ \\nRepresentationInitial Model\\nEvaluationData \\nUnderstanding -\\nEDAInitial Model(s) \\nBuilding\\nEvaluation \\nCriteria/Value \\nMetric Model Creation \\n& Training \\nFeature \\nEngineering \\nand Evaluation\\nFinal Model \\nDevelopmentModel \\nDeployment\\nData Drift \\nAnalysisModel Performance \\n–Evaluation Value \\nMetric\\nModel Drift \\nAnalysis –Model \\nEvaluation\\nReports – Dashboards -Products \\nG1 G2\\nG3\\nGate \\nReviews'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 33}, page_content='Overview of SOME Key ML Methods/Terms \\n\\uf0d8Phase –1 Idea Development \\n\\uf076Prediction versus Inference \\n\\uf076Independent Metric for Business Value\\n\\uf076Target Variable and features \\n\\uf076Classification versus Regression\\n\\uf076Probabilistic Interpretation \\n\\uf076Data acquisition/gathering     \\n\\uf0d8Phase –2 Data Prep and Problem Exploration\\n\\uf076Variable types and data types\\n\\uf076Baseline –prevalence\\n\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\n\\uf076Missing Data \\n\\uf076Data Partitioning/Sampling \\n\\uf076EDA (Summary Stats and Visuals)\\n\\uf076Cross Validation \\n\\uf0d8Phase –3 –Solution Model Development\\n\\uf076Parameters versus Hyperparameters\\n\\uf076Thresholding \\n\\uf076Feature Engineering\\n\\uf076Bias versus Variance Tradeoff \\n\\uf076Model Evaluation\\n\\uf076Non -parametric modelling (random state)\\n34'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 34}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Variable Types and Data Types\\n\\uf076Five Atomic Variable Types in R\\n\\uf0d8Numeric –number unlimited size\\n\\uf0d8Integer –number with constraints on size  \\n\\uf0d8Complex –numbers and characters\\n\\uf0d8Character –words\\n\\uf0d8Factor –unique character class that is limited in the number of categories\\n\\uf0d8Logical –True or False \\n\\uf0d8Data Types \\n\\uf076List -A list is an R -object containing different types of elements inside it like vectors, \\nfunctions, and even another list inside it.\\n\\uf076Vector -Avector in R is a series of data items of the same basic type (from above)\\n\\uf076Array -is alistorvector with two or more dimensions\\n\\uf076Matrix -A matrix is a two -dimensional rectangular data structure, created through the \\nuse of matrix function. Usually numeric, can’t have different data types, think of it as \\nmany vectors \\n\\uf076Dataframe –A two dimensional object that can contain multiple variable types 35'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 35}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Some useful variable and data type  \\n\\uf076str()\\n\\uf076class()\\n\\uf076names()\\n\\uf076length()\\n\\uf076dim()\\nOpen up Rstudio and try these functions out on the mtcars dataset. See if you agree with \\nthe output. \\n36'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 36}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Baseline –prevalence \\n\\uf076The proportion of a particular population found to be in the positive class at \\na specific time. “Positive class” in this example is the class to which we are \\ntrying to learn. Percentage split across classes of our target variable. \\n\\uf076Using mtcars again, what is the prevalence of vs variable? \\n37'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 37}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Scaling and/or Normalizing Data/One -Hot Encoding\\n\\uf076Many DS approaches require the data to be normalized or placed into a \\nstandard format so comparison between variables is possible. \\n\\uf076For factor variables this measure creating individual columns for each level \\nthat are logical or boolien 1s and 0s. \\n\\uf076We will mostly use a min max scaler that will maintain the variance of the \\nvalues but re -calculate them to be between 1 and 0. \\n\\uf0d8Use the minmax scaling function in the gradDescent package and \\nscale the mtcars dataset setting the results to a new object. What \\nhappens? What class is the object? Can you view the data.frame?  \\n38'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 38}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Missing Data \\n\\uf076Large area of study concerning missing data. Here we just need to be aware \\nof how to check for missing data and quick solutions \\n\\uf076R comes with several functions/packages that handle missing data we are \\ngoing to focus on the MICE package. \\n\\uf0d8First you need to try to detect if there are patterns of missing data, is it random or not. If \\nyou detect patterns than you have to develop a strategy to deal with that issue. \\n\\uf076MCAR –missing completely at random\\n\\uf076MNAR  -missing not at random\\n\\uf0d8Start with the summary() function on a data frame \\n\\uf076Load in the beaches dataframe from the data file and find the columns that have missing \\ndata using the summary function\\n\\uf076Generally variables with more the about 5% missing values should be deleted or imputation \\nneeds to occur \\n\\uf0d8Dig a little deeper and use the md.pattern () function in the Mice package. \\n\\uf0d8Since there doesn’t appear to be a pattern we will use complete cases to remove the \\nNAs. \\n39'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 39}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Missing Data \\n\\uf076Complete.cases () function creates aindex to remove missing values \\n\\uf0d8remove missing values from a vector \\nx <-x[complete.cases(x)]\\n\\uf0d8remove from a data.frame\\ndf <-df[complete.cases(df), ]\\n\\uf0d8remove from individual rows \\ndf <-df[complete.cases(df[ , c(row1, row2, ….)]), ]\\nTry the dataframe version on the beaches dataset, then use summary() \\nto see if the missing datapoints are gone. \\nMICE package can also do imputation (NA replacement) very easily, lots \\nof examples online on how to do these in very robust ways. \\n40'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 40}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Partitioning and Sampling\\n\\uf076We need to split our data into three sections (in most cases) to build \\nmachine learning models \\n\\uf076Training –What we use to build the original model \\n\\uf076Tune –Data used to evaluate initial outputs of a model after it’s been \\nmodified (example: changing the k in kNN ) (Feature Engineering)\\n\\uf076Test –Very last step to evaluate the quality of the model after training and \\ntune\\n\\uf0d8The function we will be using throughout the course will be the \\ncreateDataPartition() function in the caret package. \\n\\uf076The problem is that it’s not great at creating multiple partition, so we \\nessentially use it twice to create a sample, then a sample of a sample. \\n\\uf076Need to make sure to use the target variable to do stratified sampling, \\notherwise we could create imbalances in our samples. 41'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 41}, page_content='Cross -Validation\\n42\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 42}, page_content='Cross -Validation\\n43\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 43}, page_content='44Phase III'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 44}, page_content='Overview of SOME Key ML Methods/Terms \\n45\\uf0d8Phase –1 Idea Development \\n\\uf076Prediction versus Inference \\n\\uf076Independent Metric for Business Value\\n\\uf076Target Variable and features \\n\\uf076Classification versus Regression\\n\\uf076Probabilistic Interpretation \\n\\uf076Data acquisition/gathering     \\n\\uf0d8Phase –2 Data Prep and Problem Exploration\\n\\uf076Variable types and data types\\n\\uf076Baseline –prevalence\\n\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\n\\uf076Missing Data \\n\\uf076Data Partitioning/Sampling \\n\\uf076EDA (Summary Stats and Visuals)\\n\\uf076Cross Validation \\n\\uf0d8Phase –3 –Solution Model Development\\n\\uf076Parameters versus Hyperparameters\\n\\uf076Thresholding \\n\\uf076Feature Engineering\\n\\uf076Bias versus Variance Tradeoff \\n\\uf076Model Evaluation\\n\\uf076Non- parametric modelling (random state)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 45}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8# Feature Engineering – Combining or exploring different levels of \\nvariable that best work in your model. Likely going to dedicate a week \\nto just this topic.  \\n46'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 46}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Thresholding –The percentage point where our models will predict \\nthe result to be either a 0 or 1, in the typical binary case. \\n\\uf0d8Adjust the threshold associated with indication of a positive class.  \\nThe default is 50%, could be that we want to be extra careful and \\ninstead adjust that measure up to 75% or 90%. \\n47'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 47}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Evaluation –The metrics you use to assess model quality. There are a \\nton of this measures, and we are dedicating an entire week to the \\nexploring these further.  I’ll show some examples in the code for this \\nweek. \\n48'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 48}, page_content='Bias Versus Variance \\n49\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 49}, page_content='50Extra Material '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 50}, page_content='51Bookings.com  '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 51}, page_content='52Lesson Learned: Booking.com'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 52}, page_content='Bookings.com\\n\\uf0d8Swiss Army Knife –Their approach to ML is highly adoptable , \\nmeaning it can be used in a variety of settings –generate specific results \\nor more generalizable depending on the inputs (data)\\n\\uf0d8Offline Health Check– Use Randomized Control Trails (RCT) to test \\nmodel outputs aligned with normative business metrics to assess quality \\n(customer conversion)\\n\\uf076Increase model performance doesn’t necessary translate to better gain in value\\n53'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 53}, page_content='Bookings.com\\n\\uf0d8Make a Target Before you Shoot –Develop a clear understanding of \\nthe business case and target variable (what is date flexibility) \\n\\uf076Learning Difficulty –How complex or vague is the target\\n\\uf076Data to Concept –Does the data available support the algo target and goal\\n\\uf076Selection Bias –Does the model perform better for a subset of the target \\n\\uf0d8Speed Kills –ML algos, even simple ones, take a lot of computing \\npower –to reduce user weight time (latency) measures should be taken\\n\\uf076See page 1748 (sparsity, model redundancy, caching…etc.)\\n54'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_bootcamp_II.pdf', 'page': 54}, page_content='Machine Learning Overview \\n\\uf0d8Keep a watchful eye –Used specialized monitoring tools to \\nunderstand how the models are performing in practice (even when the \\nresult was unclear)\\n\\uf0d8Traditional Research Methods (Experimental Design) is a Best \\nPractice Approach to ML –\\n\\uf076“Experimentation through Randomized Controlled Trials is ingrained into \\nBooking.com culture”\\n55'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 0}, page_content='Machine Learning Overview\\nBrian Wright, PhD'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 1}, page_content='2Themes\\nMachine Learning Lifecycle\\nAre you ready for Machine Learning?\\nTerms and Phases '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 2}, page_content='Engineering of \\nMachine Learning \\nAlgos versus \\nSoftware \\nDevelopment \\nSource: https://towardsdatascience.com/stoend -to-\\nend-data -science -life-cycle -6387523b5afc'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 3}, page_content='Brian’s Version of Data Science Lifecycle\\n4Question IDBusiness \\nUnderstandingData \\nAcquisition -\\nETLInitial Model\\nEvaluationData \\nUnderstanding -\\nEDAInitial Model(s) \\nBuilding\\nEvaluation \\nCriteria Value \\nMetric Model Creation \\n& Training \\nFeature \\nEngineering \\nand Evaluation\\nOptimization –\\nHyperpara and \\nEvaluationModel \\nDeployment\\nData Drift \\nAnalysisModel Performance \\n–Evaluation Value \\nMetric\\nModel Drift \\nAnalysis –Model \\nEvaluation\\nReports –Dashboards -Products \\nG1 G2\\nG3\\nGate \\nReviews'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 4}, page_content='5Machine Learning \\nTime'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 5}, page_content='6“A field of Computer Science that gives computers the ability to learn\\nwithout being explicitly programmed.”\\n-Arthur Samuel (Coined the term in 1959 at IBM)\\n“The ability [for systems] to acquire their own knowledge, by\\nextracting patterns from raw data.”\\n-Deep Learning, Goodfellow et al\\n“A computer program is said to learn from experience E with respect\\nto some set of tasks T and performance measure P if its performance\\ntasks in T, as measured by P, improves with experience E.”\\n-Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 6}, page_content='Machine vs. human\\nMachine Human\\nUnderstanding context ✔\\nThinking through the problem ✔\\nAsking the right questions ✔\\nSelecting the right tools ✔\\nPerforming calculations quickly ✔\\nPerforming repetitive tasks ✔\\nFollowing pre-defined rules ✔\\nInterpreting results ✔'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 7}, page_content='8\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 8}, page_content='Pattern discovery when inputs (x) and outputs (y) are knownSupervised machine learning\\nInput x:\\nVoterOutput y:\\nPolitical \\naffiliation\\nExamples: Classification and regression are supervised machine learning \\n9'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 9}, page_content='The data inputs (x)have no target outputs (y)Unsupervised machine learning\\nInput x:\\nVoterOutput y:\\nNot given\\n(to be discovered)?\\nWe want to impose structure on the inputs (x)to say something meaningful about the data\\n10\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 10}, page_content='11'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 11}, page_content='12Machine Learning is a general use technology \\nwhat does that mean?'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 12}, page_content='Machine Learning Overview \\n\\uf0d8Ageneral -purpose technology orGPT is a term coined to describe \\na new method of producing and inventing that is important enough to \\nhave a protracted aggregate impact.\\n\\uf0d8Similar to electricity or the internet, in that it can be applied across \\ndomains and work to improve market outcomes. \\n13'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 13}, page_content='Machine Learning Overview \\n\\uf0d8Twitter Data Usage\\n\\uf0d8Error rates on ImageNet (10,000 labelled images) have been driven \\ndown from 30% in 2010 to less than 3% today. \\n\\uf076Below 5% is important why? \\n\\uf0d8Chess: Deep Blue (IBM AI) searched some 200 million positions per \\nsecond, Kasparov was searching not more than 5– 10 positions \\nprobably, per second. Yet he played almost at the same level….why?\\n14'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 14}, page_content='Machine Learning Overview \\n\\uf0d8However, before we all turn into robots consider two important facts: \\n1.We remain remarkably far away from what would be consider a similar \\ngeneral intelligence that can be compared to humans\\n2.Machines cannot do the full range of tasks that humans can do\\nWe can then refer to jobs or activities that might be good cases for Machine \\nLearning as SML or Suitable for Machine Learning\\n15What are examples of tasks that might be SML \\nand how do we know if our organizations are \\nready?'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 15}, page_content='Machine Learning Overview \\n\\uf0d8Successful implementation of ML requires very detailed \\nspecifications on what is to be learned and data to support that \\nlearning activity. \\n\\uf0d8Including the development of engineering features through a series \\nof trial-and- error and.. \\n\\uf0d8Then most importantly embedding these products into normal \\nbusiness operations in such a way that efficiencies can be realized.\\n16'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 16}, page_content='Machine Learning Overview \\n\\uf0d8What tasks are most suitable for ML to take over: \\n\\uf076Most recent successes are predicated on supervised learning \\n\\uf076Competency is narrow as compared to the complexity of human decision making  \\n1.Learning a function that maps well -defined inputs to well- defined outputs\\noIf can predict Y given any value of X –still might not produce the actual causal effect\\n2.Large Data is present or can be created containing input -output pairs\\noThe more training data available the more arcuate the model\\n3.Task provides clear feedback with well definable goals and metrics \\noIf we know what to achieve –(optimize flight patterns not a single flight)\\n4.Where reasoning and diverse background knowledge is not necessary\\noGood at empirical associations but terrible at decision making that requires common \\nsense of historical knowledge\\n5.No need for why the decision was made to be clear\\noNN could use millions numerical weights\\n17'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 17}, page_content='Machine Learning Overview \\n6.A tolerance for error or sub- optimal solutions\\noML use probabilistic outputs which means some error is always assumed\\n7.Function of item being learned should not change rapidly over time\\noWork best when the distribution of future test examples is the same roughly as the \\ntraining set over time\\noIf not the case systems need to be in place to refresh algorithms \\n18'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 18}, page_content='How do machines learn?\\n\\uf0d8The basic machine learning process can be divided into \\nthree parts.\\n\\uf076Data Input: Past data or information is utilized as a basis \\nfor future decision-making\\n\\uf076Abstraction: The input data is represented in a broader \\nway through the underlying algorithm\\n\\uf076Generalization: The abstracted representation is \\ngeneralized to form a framework for making decisions\\n19(reference Introduction to ML by Subramanian Chandramouli , Saikat Dutt , Amit Kumar Das\\n(https://learning.oreilly.com/library/view/machine -learning/9789389588132/xhtml/chapter001.xhtml#ch1_1)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 19}, page_content='20\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 20}, page_content='Brian’s Version of Data Science Lifecycle\\n21Question ID\\nBusiness \\nUnderstandingData \\nAcquisition/ \\nRepresentationInitial Model\\nEvaluationData \\nUnderstanding -\\nEDAInitial Model(s) \\nBuilding\\nEvaluation \\nCriteria/Value \\nMetric Model Creation \\n& Training \\nFeature \\nEngineering \\nand Evaluation\\nFinal Model \\nDevelopmentModel \\nDeployment\\nData Drift \\nAnalysisModel Performance \\n–Evaluation Value \\nMetric\\nModel Drift \\nAnalysis –Model \\nEvaluation\\nReports – Dashboards -Products \\nG1 G2\\nG3\\nGate \\nReviews'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 21}, page_content='Overview of SOME Key ML Methods/Terms \\n\\uf0d8Phase –1 Idea Development \\n\\uf076Prediction versus Inference \\n\\uf076Independent Metric for Business Value\\n\\uf076Target Variable and features \\n\\uf076Classification versus Regression\\n\\uf076Probabilistic Interpretation\\n\\uf076Data acquisition/gathering   \\n\\uf0d8Phase –2 Data Prep and Problem Exploration\\n\\uf076Variable types and data types\\n\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\n\\uf076Missing Data \\n\\uf076Baseline –prevalence\\n\\uf076Data Partitioning/Sampling \\n\\uf076EDA and (Summary Stats and Visuals)\\n\\uf076Cross Validation \\n\\uf0d8Phase –3 –Solution Model Development\\n\\uf076Parameters versus Hyperparameters\\n\\uf076Thresholding \\n\\uf076Feature Engineering\\n\\uf076Bias versus Variance Tradeoff \\n\\uf076Model Evaluation \\n\\uf076Non -parametric modelling (random state)\\n22'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 22}, page_content='23Phase I'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 23}, page_content='\\uf0d8# Prediction versus Inference \\n24'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 24}, page_content='\\uf0d8# Prediction versus Inference\\n\\uf076Goals of prediction are not centered on how the features are interacting or \\nresulting in an event but are instead focused on the ability of the model to \\npredicted an event. \\n\\uf076Almost all ML methods are focused on predication not causation or \\ninference. \\n\\uf076This is why model performance is based largely on how well a model predicts \\nnot necessarily how much individual variables are contributing to error reduction. \\n25'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 25}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Independent Metric for Business Value \\n\\uf076A key part of building a solution using Machine Learning Techniques is \\nhaving a metric that is independent of the model that can be used to \\ndetermine if the model is providing value.  \\n\\uf076Examples\\n\\uf0d8Recommender Engine for Netflix: Number of user clicks\\n\\uf0d8Spam Block Predictor: Number of viruses in the network \\n\\uf0d8Market Clustering: Did sales increase\\n\\uf0d8Others? \\n26'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 26}, page_content='Overview of Key ML Methods/Terms: Target Variable versus Features \\n\\uf076Target variable –Is the variable that includes the patterns the machine \\nlearning algorithm is trying to learn.  It is the variable of interest and key to \\nevaluating the model output.  \\n\\uf0d8More simply it is the variable we are trying to predict.  \\n\\uf076Feature variables – Are the variables the model will use to learn the patterns \\nof the target variable. The process of feature engineering can result in additional features. \\n\\uf0d8More simply these are the variables used for predicting the target\\n27'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 27}, page_content='Overview of Key ML Methods/Terms: Classification versus Regression \\n\\uf0d8Classification is the process of developing a model to predict \\nwhether a target variable is in defined categories.  This is driven by \\nhaving either a binary or multi- level categorical variable as the target \\nvariable. \\n\\uf0d8Examples: \\n\\uf076Predicting whether someone is male, or female based on 1,000s of pictures.\\n\\uf076Predicting whether a team will have a winning season or not based on player \\nperformance \\n\\uf076Predicting whether a person will default on a loan or not\\n\\uf0d8Key point: The predications of the model are not binary (1s or 0s) but are \\ngiven as percentages indicating the likelihood that any one row of data \\nbelongs to any one category.  In the case of target variables with multiple \\ncategories each row will get the same number of percent predictions as categories.\\n28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 28}, page_content='Overview of Key ML Methods/Terms: Classification versus Regression \\n\\uf0d8Regression is the process of developing a model to predict a \\nspecific number or range of numbers. This is driven by having \\na continuous variable as the target variable for the model\\n\\uf0d8Examples: \\n\\uf076Predicting the score given the players playing a game. \\n\\uf076Predicting an amount of rain given weather conditions \\n\\uf076Predicting a persons weight based on various personal statistics\\n29'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 29}, page_content='Overview of Key ML Methods/Terms: Probabilistic Interpretation \\n\\uf0d8A significant portion of this class will focus on building models for \\nclassification. Classification is a much more common machine \\nlearning goal versus regression. \\n\\uf0d8We all know the range of values for probabilities, 0 to 100, the key to \\nunderstanding these outputs is to think of them as risk measures, \\nwith 100 being no risk and 0 being all the risk! \\n\\uf0d8How the outputs are used will depend on your question. \\n\\uf076Example: How certain do you want to be that a drug is effective as compared \\nto whether a customer will open a marketing email? The results could both \\nyield 75% probabilities but is that high enough? \\n\\uf0d8Could also think of the outputs as a quantification of uncertainty, the \\nquestion becomes given your problem how much uncertainty are you \\nwilling to accept? \\n30'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 30}, page_content='Overview of Key ML Methods/Terms: Data Brainstorming \\n\\uf076Data to Concept –Does the data available support the algo target \\nand goal\\n\\uf0d8How difficult is the data to gather?\\n\\uf0d8Is the data large enough? \\n\\uf0d8What is the rate of change of the data? \\n\\uf0d8Do we believe this is the correct source and data content to address the \\nproblem?\\n\\uf076Learning Difficulty –How complex or vague is the target variable? \\n\\uf0d8Are there imbalances in the classes?\\n\\uf0d8Does the data clearly link to the problem? \\n\\uf0d8Has this data been used in the past, to what success?\\n\\uf0d8Is the target difficult to measure or break into smaller components?  \\n\\uf0d8What risk level are you willing to accept given the question?31'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 31}, page_content='32Phase II'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 32}, page_content='Brian’s Version of Data Science Lifecycle\\n33Question ID\\nBusiness \\nUnderstandingData \\nAcquisition/ \\nRepresentationInitial Model\\nEvaluationData \\nUnderstanding -\\nEDAInitial Model(s) \\nBuilding\\nEvaluation \\nCriteria/Value \\nMetric Model Creation \\n& Training \\nFeature \\nEngineering \\nand Evaluation\\nFinal Model \\nDevelopmentModel \\nDeployment\\nData Drift \\nAnalysisModel Performance \\n–Evaluation Value \\nMetric\\nModel Drift \\nAnalysis –Model \\nEvaluation\\nReports – Dashboards -Products \\nG1 G2\\nG3\\nGate \\nReviews'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 33}, page_content='Overview of SOME Key ML Methods/Terms \\n\\uf0d8Phase –1 Idea Development \\n\\uf076Prediction versus Inference \\n\\uf076Independent Metric for Business Value\\n\\uf076Target Variable and features \\n\\uf076Classification versus Regression\\n\\uf076Probabilistic Interpretation \\n\\uf076Data acquisition/gathering     \\n\\uf0d8Phase –2 Data Prep and Problem Exploration\\n\\uf076Variable types and data types\\n\\uf076Baseline –prevalence\\n\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\n\\uf076Missing Data \\n\\uf076Data Partitioning/Sampling \\n\\uf076EDA (Summary Stats and Visuals)\\n\\uf076Cross Validation \\n\\uf0d8Phase –3 –Solution Model Development\\n\\uf076Parameters versus Hyperparameters\\n\\uf076Thresholding \\n\\uf076Feature Engineering\\n\\uf076Bias versus Variance Tradeoff \\n\\uf076Model Evaluation\\n\\uf076Non -parametric modelling (random state)\\n34'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 34}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Variable Types and Data Types\\n\\uf076Five Atomic Variable Types in R\\n\\uf0d8Numeric –number unlimited size\\n\\uf0d8Integer –number with constraints on size  \\n\\uf0d8Complex –numbers and characters\\n\\uf0d8Character –words\\n\\uf0d8Factor –unique character class that is limited in the number of categories\\n\\uf0d8Logical –True or False \\n\\uf0d8Data Types \\n\\uf076List -A list is an R -object containing different types of elements inside it like vectors, \\nfunctions, and even another list inside it.\\n\\uf076Vector -Avector in R is a series of data items of the same basic type (from above)\\n\\uf076Array -is alistorvector with two or more dimensions\\n\\uf076Matrix -A matrix is a two -dimensional rectangular data structure, created through the \\nuse of matrix function. Usually numeric, can’t have different data types, think of it as \\nmany vectors \\n\\uf076Dataframe –A two dimensional object that can contain multiple variable types 35'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 35}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Some useful variable and data type  \\n\\uf076str()\\n\\uf076class()\\n\\uf076names()\\n\\uf076length()\\n\\uf076dim()\\nOpen up Rstudio and try these functions out on the mtcars dataset. See if you agree with \\nthe output. \\n36'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 36}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Baseline –prevalence \\n\\uf076The proportion of a particular population found to be in the positive class at \\na specific time. “Positive class” in this example is the class to which we are \\ntrying to learn. Percentage split across classes of our target variable. \\n\\uf076Using mtcars again, what is the prevalence of vs variable? \\n37'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 37}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Scaling and/or Normalizing Data/One -Hot Encoding\\n\\uf076Many DS approaches require the data to be normalized or placed into a \\nstandard format so comparison between variables is possible. \\n\\uf076For factor variables this measure creating individual columns for each level \\nthat are logical or boolien 1s and 0s. \\n\\uf076We will mostly use a min max scaler that will maintain the variance of the \\nvalues but re -calculate them to be between 1 and 0. \\n\\uf0d8Use the minmax scaling function in the gradDescent package and \\nscale the mtcars dataset setting the results to a new object. What \\nhappens? What class is the object? Can you view the data.frame?  \\n38'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 38}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Missing Data \\n\\uf076Large area of study concerning missing data. Here we just need to be aware \\nof how to check for missing data and quick solutions \\n\\uf076R comes with several functions/packages that handle missing data we are \\ngoing to focus on the MICE package. \\n\\uf0d8First you need to try to detect if there are patterns of missing data, is it random or not. If \\nyou detect patterns than you have to develop a strategy to deal with that issue. \\n\\uf076MCAR –missing completely at random\\n\\uf076MNAR  -missing not at random\\n\\uf0d8Start with the summary() function on a data frame \\n\\uf076Load in the beaches dataframe from the data file and find the columns that have missing \\ndata using the summary function\\n\\uf076Generally variables with more the about 5% missing values should be deleted or imputation \\nneeds to occur \\n\\uf0d8Dig a little deeper and use the md.pattern () function in the Mice package. \\n\\uf0d8Since there doesn’t appear to be a pattern we will use complete cases to remove the \\nNAs. \\n39'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 39}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Missing Data \\n\\uf076Complete.cases () function creates aindex to remove missing values \\n\\uf0d8remove missing values from a vector \\nx <-x[complete.cases(x)]\\n\\uf0d8remove from a data.frame\\ndf <-df[complete.cases(df), ]\\n\\uf0d8remove from individual rows \\ndf <-df[complete.cases(df[ , c(row1, row2, ….)]), ]\\nTry the dataframe version on the beaches dataset, then use summary() \\nto see if the missing datapoints are gone. \\nMICE package can also do imputation (NA replacement) very easily, lots \\nof examples online on how to do these in very robust ways. \\n40'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 40}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Partitioning and Sampling\\n\\uf076We need to split our data into three sections (in most cases) to build \\nmachine learning models \\n\\uf076Training –What we use to build the original model \\n\\uf076Tune –Data used to evaluate initial outputs of a model after it’s been \\nmodified (example: changing the k in kNN ) (Feature Engineering)\\n\\uf076Test –Very last step to evaluate the quality of the model after training and \\ntune\\n\\uf0d8The function we will be using throughout the course will be the \\ncreateDataPartition() function in the caret package. \\n\\uf076The problem is that it’s not great at creating multiple partition, so we \\nessentially use it twice to create a sample, then a sample of a sample. \\n\\uf076Need to make sure to use the target variable to do stratified sampling, \\notherwise we could create imbalances in our samples. 41'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 41}, page_content='Cross -Validation\\n42\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 42}, page_content='Cross -Validation\\n43\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 43}, page_content='44Phase III'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 44}, page_content='Overview of SOME Key ML Methods/Terms \\n45\\uf0d8Phase –1 Idea Development \\n\\uf076Prediction versus Inference \\n\\uf076Independent Metric for Business Value\\n\\uf076Target Variable and features \\n\\uf076Classification versus Regression\\n\\uf076Probabilistic Interpretation \\n\\uf076Data acquisition/gathering     \\n\\uf0d8Phase –2 Data Prep and Problem Exploration\\n\\uf076Variable types and data types\\n\\uf076Baseline –prevalence\\n\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\n\\uf076Missing Data \\n\\uf076Data Partitioning/Sampling \\n\\uf076EDA (Summary Stats and Visuals)\\n\\uf076Cross Validation \\n\\uf0d8Phase –3 –Solution Model Development\\n\\uf076Parameters versus Hyperparameters\\n\\uf076Thresholding \\n\\uf076Feature Engineering\\n\\uf076Bias versus Variance Tradeoff \\n\\uf076Model Evaluation\\n\\uf076Non- parametric modelling (random state)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 45}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8# Feature Engineering – Combining or exploring different levels of \\nvariable that best work in your model. Likely going to dedicate a week \\nto just this topic.  \\n46'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 46}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Thresholding –The percentage point where our models will predict \\nthe result to be either a 0 or 1, in the typical binary case. \\n\\uf0d8Adjust the threshold associated with indication of a positive class.  \\nThe default is 50%, could be that we want to be extra careful and \\ninstead adjust that measure up to 75% or 90%. \\n47'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 47}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Evaluation –The metrics you use to assess model quality. There are a \\nton of this measures, and we are dedicating an entire week to the \\nexploring these further.  I’ll show some examples in the code for this \\nweek. \\n48'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 48}, page_content='Bias Versus Variance \\n49\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 49}, page_content='50Extra Material '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 50}, page_content='51Bookings.com  '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 51}, page_content='52Lesson Learned: Booking.com'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 52}, page_content='Bookings.com\\n\\uf0d8Swiss Army Knife –Their approach to ML is highly adoptable , \\nmeaning it can be used in a variety of settings –generate specific results \\nor more generalizable depending on the inputs (data)\\n\\uf0d8Offline Health Check– Use Randomized Control Trails (RCT) to test \\nmodel outputs aligned with normative business metrics to assess quality \\n(customer conversion)\\n\\uf076Increase model performance doesn’t necessary translate to better gain in value\\n53'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 53}, page_content='Bookings.com\\n\\uf0d8Make a Target Before you Shoot –Develop a clear understanding of \\nthe business case and target variable (what is date flexibility) \\n\\uf076Learning Difficulty –How complex or vague is the target\\n\\uf076Data to Concept –Does the data available support the algo target and goal\\n\\uf076Selection Bias –Does the model perform better for a subset of the target \\n\\uf0d8Speed Kills –ML algos, even simple ones, take a lot of computing \\npower –to reduce user weight time (latency) measures should be taken\\n\\uf076See page 1748 (sparsity, model redundancy, caching…etc.)\\n54'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_III.pdf', 'page': 54}, page_content='Machine Learning Overview \\n\\uf0d8Keep a watchful eye –Used specialized monitoring tools to \\nunderstand how the models are performing in practice (even when the \\nresult was unclear)\\n\\uf0d8Traditional Research Methods (Experimental Design) is a Best \\nPractice Approach to ML –\\n\\uf076“Experimentation through Randomized Controlled Trials is ingrained into \\nBooking.com culture”\\n55'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 0}, page_content='Machine Learning Overview\\nBrian Wright, PhD'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 1}, page_content='2Themes\\nMachine Learning Lifecycle\\nAre you ready for Machine Learning?\\nTerms and Phases '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 2}, page_content='Engineering of \\nMachine Learning \\nAlgos versus \\nSoftware \\nDevelopment \\nSource: https://towardsdatascience.com/stoend -to-\\nend-data -science -life-cycle -6387523b5afc'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 3}, page_content='Brian’s Version of Data Science Lifecycle\\n4Question IDBusiness \\nUnderstandingData \\nAcquisition -\\nETLInitial Model\\nEvaluationData \\nUnderstanding -\\nEDAInitial Model(s) \\nBuilding\\nEvaluation \\nCriteria Value \\nMetric Model Creation \\n& Training \\nFeature \\nEngineering \\nand Evaluation\\nOptimization –\\nHyperpara and \\nEvaluationModel \\nDeployment\\nData Drift \\nAnalysisModel Performance \\n–Evaluation Value \\nMetric\\nModel Drift \\nAnalysis –Model \\nEvaluation\\nReports –Dashboards -Products \\nG1 G2\\nG3\\nGate \\nReviews'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 4}, page_content='5Machine Learning \\nTime'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 5}, page_content='6“A field of Computer Science that gives computers the ability to learn\\nwithout being explicitly programmed.”\\n-Arthur Samuel (Coined the term in 1959 at IBM)\\n“The ability [for systems] to acquire their own knowledge, by\\nextracting patterns from raw data.”\\n-Deep Learning, Goodfellow et al\\n“A computer program is said to learn from experience E with respect\\nto some set of tasks T and performance measure P if its performance\\ntasks in T, as measured by P, improves with experience E.”\\n-Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 6}, page_content='Machine vs. human\\nMachine Human\\nUnderstanding context ✔\\nThinking through the problem ✔\\nAsking the right questions ✔\\nSelecting the right tools ✔\\nPerforming calculations quickly ✔\\nPerforming repetitive tasks ✔\\nFollowing pre-defined rules ✔\\nInterpreting results ✔'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 7}, page_content='8\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 8}, page_content='Pattern discovery when inputs (x) and outputs (y) are knownSupervised machine learning\\nInput x:\\nVoterOutput y:\\nPolitical \\naffiliation\\nExamples: Classification and regression are supervised machine learning \\n9'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 9}, page_content='The data inputs (x)have no target outputs (y)Unsupervised machine learning\\nInput x:\\nVoterOutput y:\\nNot given\\n(to be discovered)?\\nWe want to impose structure on the inputs (x)to say something meaningful about the data\\n10\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 10}, page_content='11'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 11}, page_content='12Machine Learning is a general use technology \\nwhat does that mean?'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 12}, page_content='Machine Learning Overview \\n\\uf0d8Ageneral -purpose technology orGPT is a term coined to describe \\na new method of producing and inventing that is important enough to \\nhave a protracted aggregate impact.\\n\\uf0d8Similar to electricity or the internet, in that it can be applied across \\ndomains and work to improve market outcomes. \\n13'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 13}, page_content='Machine Learning Overview \\n\\uf0d8Twitter Data Usage\\n\\uf0d8Error rates on ImageNet (10,000 labelled images) have been driven \\ndown from 30% in 2010 to less than 3% today. \\n\\uf076Below 5% is important why? \\n\\uf0d8Chess: Deep Blue (IBM AI) searched some 200 million positions per \\nsecond, Kasparov was searching not more than 5– 10 positions \\nprobably, per second. Yet he played almost at the same level….why?\\n14'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 14}, page_content='Machine Learning Overview \\n\\uf0d8However, before we all turn into robots consider two important facts: \\n1.We remain remarkably far away from what would be consider a similar \\ngeneral intelligence that can be compared to humans\\n2.Machines cannot do the full range of tasks that humans can do\\nWe can then refer to jobs or activities that might be good cases for Machine \\nLearning as SML or Suitable for Machine Learning\\n15What are examples of tasks that might be SML \\nand how do we know if our organizations are \\nready?'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 15}, page_content='Machine Learning Overview \\n\\uf0d8Successful implementation of ML requires very detailed \\nspecifications on what is to be learned and data to support that \\nlearning activity. \\n\\uf0d8Including the development of engineering features through a series \\nof trial-and- error and.. \\n\\uf0d8Then most importantly embedding these products into normal \\nbusiness operations in such a way that efficiencies can be realized.\\n16'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 16}, page_content='Machine Learning Overview \\n\\uf0d8What tasks are most suitable for ML to take over: \\n\\uf076Most recent successes are predicated on supervised learning \\n\\uf076Competency is narrow as compared to the complexity of human decision making  \\n1.Learning a function that maps well -defined inputs to well- defined outputs\\noIf can predict Y given any value of X –still might not produce the actual causal effect\\n2.Large Data is present or can be created containing input -output pairs\\noThe more training data available the more arcuate the model\\n3.Task provides clear feedback with well definable goals and metrics \\noIf we know what to achieve –(optimize flight patterns not a single flight)\\n4.Where reasoning and diverse background knowledge is not necessary\\noGood at empirical associations but terrible at decision making that requires common \\nsense of historical knowledge\\n5.No need for why the decision was made to be clear\\noNN could use millions numerical weights\\n17'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 17}, page_content='Machine Learning Overview \\n6.A tolerance for error or sub- optimal solutions\\noML use probabilistic outputs which means some error is always assumed\\n7.Function of item being learned should not change rapidly over time\\noWork best when the distribution of future test examples is the same roughly as the \\ntraining set over time\\noIf not the case systems need to be in place to refresh algorithms \\n18'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 18}, page_content='How do machines learn?\\n\\uf0d8The basic machine learning process can be divided into \\nthree parts .\\n\\uf076Data Input: Past data or information is utilized as a basis \\nfor future decision-making\\n\\uf076Abstraction : The input data is represented in a broader \\nway through the underlying algorithm\\n\\uf076Generalization : The abstracted representation is \\ngeneralized to form a framework for making decisions\\n19(reference Introduction to ML by Subramanian Chandramouli , Saikat Dutt , Amit Kumar Das\\n(https://learning.oreilly.com/library/view/machine -learning/9789389588132/xhtml/chapter001.xhtml#ch1_1)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 19}, page_content='20\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 20}, page_content='Brian’s Version of Data Science Lifecycle\\n21Question ID\\nBusiness \\nUnderstandingData \\nAcquisition/ \\nRepresentationInitial Model\\nEvaluationData \\nUnderstanding -\\nEDAInitial Model(s) \\nBuilding\\nEvaluation \\nCriteria/Value \\nMetric Model Creation \\n& Training \\nFeature \\nEngineering \\nand Evaluation\\nFinal Model \\nDevelopmentModel \\nDeployment\\nData Drift \\nAnalysisModel Performance \\n–Evaluation Value \\nMetric\\nModel Drift \\nAnalysis –Model \\nEvaluation\\nReports – Dashboards -Products \\nG1 G2\\nG3\\nGate \\nReviews'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 21}, page_content='Overview of SOME Key ML Methods/Terms \\n\\uf0d8Phase –1 Idea Development \\n\\uf076Prediction versus Inference \\n\\uf076Independent Metric for Business Value\\n\\uf076Target Variable and features \\n\\uf076Classification versus Regression\\n\\uf076Probabilistic Interpretation\\n\\uf076Data acquisition/gathering   \\n\\uf0d8Phase –2 Data Prep and Problem Exploration\\n\\uf076Variable classes/types\\n\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\n\\uf076Missing Data \\n\\uf076Baseline –prevalence\\n\\uf076Data Partitioning/Sampling \\n\\uf076EDA and (Summary Stats and Visuals)\\n\\uf0d8Phase –3 –Solution Model Development\\n\\uf076Parameters versus Hyperparameters\\n\\uf076Thresholding \\n\\uf076Feature Engineering\\n\\uf076Bias versus Variance Tradeoff \\n\\uf076Model Evaluation \\n\\uf076Non -parametric modelling (random state) 22'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 22}, page_content='23Phase I'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 23}, page_content='\\uf0d8# Prediction versus Inference \\n24'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 24}, page_content='\\uf0d8# Prediction versus Inference\\n\\uf076Goals of prediction are not centered on how the features are interacting or \\nresulting in an event but are instead focused on the ability of the model to \\npredicted an event. \\n\\uf076Almost all ML methods are focused on predication not causation or \\ninference. \\n\\uf076This is why model performance is based largely on how well a model predicts \\nnot necessarily how much individual variables are contributing to error reduction. \\n25'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 25}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Independent Metric for Business Value \\n\\uf076A key part of building a solution using Machine Learning Techniques is \\nhaving a metric that is independent of the model that can be used to \\ndetermine if the model is providing value.  \\n\\uf076Examples\\n\\uf0d8Recommender Engine for Netflix: Number of user clicks\\n\\uf0d8Spam Block Predictor: Number of viruses in the network \\n\\uf0d8Market Clustering: Did sales increase\\n\\uf0d8Others? \\n26'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 26}, page_content='Overview of Key ML Methods/Terms: Target Variable versus Features \\n\\uf076Target variable –Is the variable that includes the patterns the machine \\nlearning algorithm is trying to learn.  It is the variable of interest and key to \\nevaluating the model output.  \\n\\uf0d8More simply it is the variable we are trying to predict.  \\n\\uf076Feature variables – Are the variables the model will use to learn the patterns \\nof the target variable. The process of feature engineering can result in additional features. \\n\\uf0d8More simply these are the variables used for predicting the target\\n27'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 27}, page_content='Overview of Key ML Methods/Terms: Classification versus Regression \\n\\uf0d8Classification is the process of developing a model to predict \\nwhether a target variable is in defined categories.  This is driven by \\nhaving either a binary or multi- level categorical variable as the target \\nvariable. \\n\\uf0d8Examples: \\n\\uf076Predicting whether someone is male, or female based on 1,000s of pictures.\\n\\uf076Predicting whether a team will have a winning season or not based on player \\nperformance \\n\\uf076Predicting whether a person will default on a loan or not\\n\\uf0d8Key point: The predications of the model are not binary (1s or 0s) but are \\ngiven as percentages indicating the likelihood that any one row of data \\nbelongs to any one category.  In the case of target variables with multiple \\ncategories each row will get the same number of percent predictions as categories.\\n28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 28}, page_content='Overview of Key ML Methods/Terms: Classification versus Regression \\n\\uf0d8Regression is the process of developing a model to predict a \\nspecific number or range of numbers. This is driven by having \\na continuous variable as the target variable for the model\\n\\uf0d8Examples: \\n\\uf076Predicting the score given the players playing a game. \\n\\uf076Predicting an amount of rain given weather conditions \\n\\uf076Predicting a persons weight based on various personal statistics\\n29'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 29}, page_content='Overview of Key ML Methods/Terms: Probabilistic Interpretation \\n\\uf0d8A significant portion of this class will focus on building models for \\nclassification. Classification is a much more common machine \\nlearning goal versus regression. \\n\\uf0d8We all know the range of values for probabilities, 0 to 100, the key to \\nunderstanding these outputs is to think of them as risk measures, \\nwith 100 being no risk and 0 being all the risk! \\n\\uf0d8How the outputs are used will depend on your question. \\n\\uf076Example: How certain do you want to be that a drug is effective as compared \\nto whether a customer will open a marketing email? The results could both \\nyield 75% probabilities but is that high enough? \\n\\uf0d8Could also think of the outputs as a quantification of uncertainty, the \\nquestion becomes given your problem how much uncertainty are you \\nwilling to accept? \\n30'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 30}, page_content='Overview of Key ML Methods/Terms: Data Brainstorming \\n\\uf076Data to Concept –Does the data available support the algo target \\nand goal\\n\\uf0d8How difficult is the data to gather?\\n\\uf0d8Is the data large enough? \\n\\uf0d8What is the rate of change of the data? \\n\\uf0d8Do we believe this is the correct source and data content to address the \\nproblem?\\n\\uf076Learning Difficulty –How complex or vague is the target variable? \\n\\uf0d8Are there imbalances in the classes?\\n\\uf0d8Does the data clearly link to the problem? \\n\\uf0d8Has this data been used in the past, to what success?\\n\\uf0d8Is the target difficult to measure or break into smaller components?  \\n\\uf0d8What risklevel areyou willing toaccept given thequestion?31'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 31}, page_content='32Phase II'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 32}, page_content='Overview of SOME Key ML Methods/Terms \\n\\uf0d8Phase –1 Idea Development \\n\\uf076Prediction versus Inference \\n\\uf076Independent Metric for Business Value\\n\\uf076Target Variable and features \\n\\uf076Classification versus Regression\\n\\uf076Probabilistic Interpretation \\n\\uf076Data acquisition/gathering     \\n\\uf0d8Phase –2 Data Prep and Problem Exploration\\n\\uf076Variable classes/types\\n\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\n\\uf076Missing Data \\n\\uf076Baseline –prevalence\\n\\uf076Data Partitioning/Sampling \\n\\uf076EDA (Summary Stats and Visuals)\\n\\uf0d8Phase –3 –Solution Model Development\\n\\uf076Parameters versus Hyperparameters\\n\\uf076Thresholding \\n\\uf076Feature Engineering\\n\\uf076Bias versus Variance Tradeoff \\n\\uf076Model Evaluation\\n\\uf076Non -parametric modelling (random state) 33'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 33}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8# Baseline – prevalence \\n34'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 34}, page_content='35Phase III'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 35}, page_content='Overview of SOME Key ML Methods/Terms \\n\\uf0d8Phase –1 Idea Development \\n\\uf076Prediction versus Inference \\n\\uf076Independent Metric for Business Value\\n\\uf076Target Variable and features \\n\\uf076Classification versus Regression\\n\\uf076Probabilistic Interpretation\\n\\uf076Data acquisition/gathering      \\n\\uf0d8Phase –2 Data Prep and Problem Exploration\\n\\uf076Variable classes/types\\n\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\n\\uf076Missing Data \\n\\uf076Baseline –prevalence\\n\\uf076Data Partitioning/Sampling \\n\\uf076EDA (Summary Stats and Visuals)\\n\\uf0d8Phase –3 –Solution Model Development\\n\\uf076Parameters versus Hyperparameters\\n\\uf076Thresholding \\n\\uf076Feature Engineering\\n\\uf076Bias versus Variance Tradeoff \\n\\uf076Model Evaluation \\n\\uf076Non -parametric modelling (random state) 36'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 36}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8# Feature Engineering – Combining or exploring different levels of \\nvariable that best work in your model. Likely going to dedicate a week \\nto just this topic.  \\n37'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 37}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Thresholding –The percentage point where our models will predict \\nthe result to be either a 0 or 1, in the typical binary case. \\n\\uf0d8Adjust the threshold associated with indication of a positive class.  \\nThe default is 50%, could be that we want to be extra careful and \\ninstead adjust that measure up to 75% or 90%. \\n38'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 38}, page_content='Overview of Key ML Methods/Terms \\n\\uf0d8Evaluation –The metrics you use to assess model quality. There are a \\nton of this measures, and we are dedicating an entire week to the \\nexploring these further.  I’ll show some examples in the code for this \\nweek. \\n39'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 39}, page_content='Bias Versus Variance \\n40\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 40}, page_content='41Extra Material '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 41}, page_content='42Bookings.com  '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 42}, page_content='43Lesson Learned: Booking.com'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 43}, page_content='Bookings.com\\n\\uf0d8Swiss Army Knife –Their approach to ML is highly adoptable , \\nmeaning it can be used in a variety of settings –generate specific results \\nor more generalizable depending on the inputs (data)\\n\\uf0d8Offline Health Check– Use Randomized Control Trails (RCT) to test \\nmodel outputs aligned with normative business metrics to assess quality \\n(customer conversion)\\n\\uf076Increase model performance doesn’t necessary translate to better gain in value\\n44'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 44}, page_content='Bookings.com\\n\\uf0d8Make a Target Before you Shoot –Develop a clear understanding of \\nthe business case and target variable (what is date flexibility) \\n\\uf076Learning Difficulty –How complex or vague is the target\\n\\uf076Data to Concept –Does the data available support the algo target and goal\\n\\uf076Selection Bias –Does the model perform better for a subset of the target \\n\\uf0d8Speed Kills –ML algos, even simple ones, take a lot of computing \\npower –to reduce user weight time (latency) measures should be taken\\n\\uf076See page 1748 (sparsity, model redundancy, caching…etc.)\\n45'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\machine_learning_overview.pdf', 'page': 45}, page_content='Machine Learning Overview \\n\\uf0d8Keep a watchful eye –Used specialized monitoring tools to \\nunderstand how the models are performing in practice (even when the \\nresult was unclear)\\n\\uf0d8Traditional Research Methods (Experimental Design) is a Best \\nPractice Approach to ML –\\n\\uf076“Experimentation through Randomized Controlled Trials is ingrained into \\nBooking.com culture”\\n46'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 0}, page_content='Machine Learning Overview, EDA and Clustering\\nBrian Wright\\nbrianwright@virginia.edu\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 1}, page_content='1.What is Machine Learning ?\\n2.What is exploratory data analysis?\\n3.k-means clustering\\n–Does Congress vote in patterns?\\n4.Multi -dimensional k -means clustering\\n–Are NBA players compensated according to performance?Outline: Intro to Unsupervised ML\\n2'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 2}, page_content='“A field of Computer Science that gives computers the ability to learn\\nwithout being explicitly programmed.”\\n-Arthur Samuel (Coined the term in 1959 at IBM)\\n“The ability [for systems] to acquire their own knowledge, by\\nextracting patterns from raw data.”\\n-Deep Learning , Goodfellow et al'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 3}, page_content='Machine vs. human\\nMachine Human\\nUnderstanding context ✔\\nThinking through the problem ✔\\nAsking the right questions ✔\\nSelecting the right tools ✔\\nPerforming calculations quickly ✔\\nPerforming repetitive tasks ✔\\nFollowing pre-defined rules ✔\\nInterpreting results ✔'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 4}, page_content='5\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 5}, page_content='Pattern discovery when inputs (x) and outputs (y) are knownSupervised machine learning\\nInput x:\\nVoterOutput y:\\nPolitical \\naffiliation\\nExamples: Classification and regression are supervised machine learning \\n6'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 6}, page_content='The data inputs (x)have no target outputs (y)Unsupervised machine learning\\nInput x:\\nVoterOutput y:\\nNot given\\n(to be discovered)?\\nWe want to impose structure on the inputs (x) to say something meaningful about the data\\n7\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 7}, page_content='8'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 8}, page_content='9\\uf06eGiven  D :{2,4,10,12,3,11,20,25,30},  and  k=2 clusters\\n\\uf06eRandomly assign the means:  m1=3, m2=4\\n\\uf06eK1={2,3}, K2={4,10,12,11,20,25,30}, m1=2.5, m2=16\\n\\uf06eK1={2,3,4}, K2={10,12,11,20,25,30}, m1=3, m2=18\\n\\uf06eK1={2,3,4,10}, K2={12,11,20,25,30}, m1=4.75, m2=19.6\\n\\uf06eK1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\\n\\uf06eK1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\\n\\uf06eStop, since the clusters and the means found in \\nall subsequent iterations will be the same .Example of K -Means'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 9}, page_content='•Value of more accurate information = incremental improvement of results above \\nstatus quo\\n•What is the value of being 5% more accurate ?\\n–For Wal -Mart: on ~$470B of revenues, $23B of potential revenue increase by more \\naccurately predicting demand or recommending the right products\\n–For the IRS: on ~$21B of annual tax fraud, $1B of potential incremental tax collections\\n•~30% of Amazon.com sales come from product recommendationsMaking $ with Machine Learnin g\\nSource: Predictive Analytics by Eric Siegel; NBC News\\n10'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 10}, page_content='•The value of data mining \\nand predictive analytics is in how you use these tools\\n•Insights on their own can have little value without intelligent applicationHow does data mining make $?\\n11'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 11}, page_content='Engineering of \\nMachine Learning Algos versus Software Development \\nSource: https://towardsdatascience.com/stoend -to-\\nend-data -science -life-cycle -6387523b5afc'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 12}, page_content='Data Science Life Cycle \\n(Everything includes Evaluation\\n13\\nTrain\\nFeature \\nEngineer\\nTest\\n Deploy\\nEvaluate\\n Evaluate\\n Evaluate\\n Evaluate\\nMonitor'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 13}, page_content='1.What is Machine Learning?\\n2.What is exploratory data analysis?\\n3.k-means clustering\\n–Does Congress vote in patterns?\\n4.Multi -dimensional k -means clustering\\n–Are NBA players compensated according to performance?Outline: Intro to Unsupervised ML\\n14'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 14}, page_content='•Exploratory data analysis or “EDA” is an approach where the intent is to see \\nwhat the data can tell us beyond modeling or hypothesis testing\\n–Data visualization is one of the most common forms of EDAWhat is exploratory data analysis?\\n15'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 15}, page_content='When data is too big or complex to be analyzed just \\nby visualizing it, these types of analysis can help:\\n1.Clustering: compare pieces of data by measuring \\nsimilarity among them\\n2.Network analysis: analyze how people, places and \\nentities are connected to evaluate the properties \\nand structure of a network \\n3.Text mining: analyze what large bodies of \\nunstructured or structured text sayTypes of exploratory data analysis\\n16'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 16}, page_content='The data inputs have (x)no target outputs (y)Unsupervised machine learning\\nInput x:\\nVoterOutput y:\\nNot given\\n(To be discovered)?\\nWe want to impose structure on the inputs (x)to say something \\nmeaningful about the data\\n17\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 17}, page_content='1.Technique for finding similarity between groups\\n2.Type of unsupervised machine learning\\n•Not the only class of unsupervised learning        \\nalgorithms\\n3.Similarity needs to be defined\\n•Will depend on attributes of data\\n•Usually a distance metricWhat is clustering?\\n18\\nKey assumption: data points that are “closer” together are related or similar'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 18}, page_content='•Haimowitz and Schwarz 1997 paper on \\nclustering for credit line optimization\\n–http://www.aaai.org/Papers/Workshops/1997/W\\nS-97-07/WS97- 07-006.pdf\\n•Cluster existing GE Capital customers based on similarity in use of their credit cards to pay bills and customers’ profitability to GE Capital\\n•Resulted in five clusters of consumer credit behavior\\n•Created classification model to predict customer type and offer tailored productsGE Capital case study: grouping clients\\n19'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 19}, page_content='•Between 2001 and 2004 most European countries passed legislation that allowed \\ncustomers to keep their cell phone number if they switched carriers\\n•Telenor, one of the largest telecommunications companies in Norway wanted to \\nensure it kept its customers\\n–Problem: the promotions the company sent to its clients reminded them that they could \\nleave and resulted in greater defections !\\n–Solution: predict which customers, if contacted, are more likely to stay with the company \\n•Results:\\n–Marketing campaign ROI increased 11x\\n–Customer churn decreased 36%\\n–Marketing campaign costs decreased 40%Telenor case study: predicting \\nbehavior\\n20'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 20}, page_content='1.What is Machine Learning?\\n2.What is exploratory data analysis?\\n3.k-means clustering\\n–Does Congress vote in patterns?\\n4.Multi -dimensional k -means clustering –Lab \\n–Are NBA players compensated according to performance?Outline: Intro to Unsupervised ML\\n21'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 21}, page_content='Example use case General question Concept\\nDoes Congress vote in patterns? Is there a pattern ?\\nIs there structure in \\nunstructured data?k-means clustering\\nAre basketball players \"priced\" efficiently (based on performance)?How to uncover trends with many variables that you    can\\'t easily visualize?k-means clustering in \\nmany dimensionsConcept summary\\n22'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 22}, page_content='1.Data set consists of 427 members \\n(observations) \\n2.Members served a full year in 2013\\n3.Three vote types:\\n•“Aye”\\n•“Nay”\\n•“Other”Goal: to understand how polarized the \\nUS Congress isPolitical clustering\\nThe joint session of Congress on Capitol \\nHill in Washington\\n23'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 23}, page_content='•How do we identify \\nswing votes?\\n–Lobbying\\n–Bridging party lines\\n•Assumption:\\n–Democrats and Republicans \\nvote among partisan lines, \\nwhich generates clustersEach data point represents a member of CongressFinding voting patterns\\n24'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 24}, page_content='Intra vs. inter- cluster distance\\nIntra-Cluster \\nDistanceInter-Cluster \\nDistance\\nObjective: minimize intra -cluster distan ce, maximize inter -cluster distance\\n25'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 25}, page_content='•The centroid is the average \\nlocation of all points in the cluster\\n•Another definition: the centroid minimizes the distance between a central location and all the data points in the cluster\\nNote: Centroids are generally not existing data points, rather locations in spacek-means clustering is based on \\ncentroids\\n26'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 26}, page_content='1.Randomly choose k data \\npoints to be centroids k-means in 4 steps\\n27\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 27}, page_content='1.Randomly choose k data \\npoints to be centroids \\n2.Assign each point to closest centroidk-means in 4 steps\\n28\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 28}, page_content='1.Randomly choose k data \\npoints to be centroids \\n2.Assign each point to closest centroid\\n3.Recalculate centroids based on current cluster membershipk-means in 4 steps\\n29'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 29}, page_content='1.Randomly choose k data \\npoints to be centroids \\n2.Assign each point to closest centroid\\n3.Recalculate centroids based on current cluster membershipk-means in 4 steps\\n304.Repeat steps 2 -3 with the new centroids until the centroids don’t \\nchange anymore'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 30}, page_content='Step 1: load packages and data\\n# Install packages\\ninstall.packages(\"e1071\") install.packages(\"ggplot2\" )\\n# Load librarieslibrary(e1071)library(ggplot2)\\nlibrary(help = e1071)Learn about all the functionality of the package, be \\nwell informed about what you\\'re doing!\\n31'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 31}, page_content='Step 1: load packages and data\\n# Loading house data\\nhouse_votes_Dem = read_csv (\"house_votes_Dem.csv\")\\n# What does the data look like?\\nView( house_votes_Dem )Script\\n32'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 32}, page_content='Step 2: run k -means\\n# Define the columns to be clustered by subsetting the data\\nclust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]\\n# Run an algorithm with 2 centersset.seed(1 )\\nkmeans_obj_Dem = kmeans(clust_data_Dem , centers = 2, \\nalgorithm = \"Lloyd\")\\n# What does the new variable kmeans_obj contain?kmeans_obj_Dem\\n# View the results of each output of the kmeans \\n# functionhead( kmeans_obj_Dem)Script\\n1.By placing the set of data we want     \\nafter the comma, we tell R we’re   looking for columns \\n2.kmeans uses a different starting data point each time it runs. To make the results reproducible make R start from the same point every time with set.seed()\\n3.We’re not specifying the number of iterations so R defaults to 10\\n4.We’ll see that kmeans produces a list    of vectors of different lengths. As a result, we cannot use the View() function\\n33'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 33}, page_content='Step 2: run k -means\\n1.Number of points each cluster contains\\n2.The “location” of each cluster center is specified by 3 \\ncoordinates, one for each column we’re clustering\\n3.The list assigning either cluster 1 or 2 to each data point\\n1.79.5% of the variance between the data points is explained by our clustering, we will discuss this in detail later\\n2.List of other types of data included in kmeans_obj\\n34'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 34}, page_content='Measuring distance\\n(3,3)\\n(1,2) 21Distance = √(22+12)\\nx\\n35'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 35}, page_content='•cluster: a vector indicating the cluster to which each point is allocated\\n•centers: a matrix of cluster centers\\n•totss: the total sum of squares (sum of distances between all points)\\n•withinss: vector of within -cluster sum of distances, one number per cluster\\n•tot.withinss: total within -cluster sum of distances, i.e.sum of withinss\\n•betweenss: the between -cluster sum of squares, i.e. totss -tot.withinss\\n•size: the number of points in each cluster\\nTo learn more about the kmeans function run ?kmeanskmeans outputs\\n36'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 36}, page_content='Intra vs. inter- cluster distance\\nIntra-Cluster \\nDistanceInter-Cluster \\nDistance\\nwithinssbetweensstotss = withinss +betweenss\\n37'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 37}, page_content='Step 3: visualize plot\\n# Tell R to read the cluster labels as factors so that ggplot2 (the \\n# graphing  package) can read them as category labels instead of # continuous variables (numeric variables).party_clusters_Dem = as.factor(kmeans_obj_Dem$cluster)\\n# What does party_clusters look like?View( party_clusters_Dem )\\nView(as.data.frame(party_clusters_Dem))\\n# Set up labels for our data so that we can compare Democrats and \\n# Republicans.party_labels_Dem = house_votes_Dem$partyScript\\n38'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 38}, page_content='ggplot(house_votes_Dem, aes(x = aye, \\ny = nay,\\nshape = party_clusters_Dem)) + \\ngeom_point(size = 6) +\\nggtitle(\"Aye vs. Nay votes for Democrat -introduced bills\") +\\nxlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", \\nlabels = c(\"Cluster 1\", \"Cluster 2\" ),\\nvalues = c(\"1\" , \"2\")) +\\ntheme_light()Step 3: visualize plotCosmetics layerBase layer\\nGeom Layer\\nTitles and axis\\nShape\\nTheme\\n39Script'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 39}, page_content='Step 3: visualize plot\\n40'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 40}, page_content='•Two groups exist\\n•Algorithm identifies voting \\npatternsWhat can we infer about \\nthe different clusters?Step 4: analyze results\\n41'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 41}, page_content='ggplot(house_votes_Dem, aes(x = yea, \\ny = nay,\\ncolor= party_labels_Dem,\\nshape = party_clusters_Dem)) + \\ngeom_point(size = 6) +\\nggtitle(\"Aye vs. Nay votes for Democrat -introduced bills\") +\\nxlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", \\nlabels = c(\"Cluster 1\", \"Cluster 2\" ),\\nvalues = c(\"1\" , \"2\")) +\\nscale_color_manual(name = \"Party\", \\nlabels = c(\"Democratic\", \"Republican\"),\\nvalues = c(\"blue\" , \"red\"))+\\ntheme_light()Step 5: validate resultsCosmetics layerScript\\nBase layer\\nGeom Layer\\nTitles and axis\\nColor and \\nshape\\nTheme\\n42'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 42}, page_content='Step 5: validate results\\n43'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 43}, page_content='•Diffuse among Democrats\\n•Republicans more dense\\n•Can gauge “outliers”\\n•Can see the polarization \\nbetween the two political parties Step 6: interpret results\\noutlieroutlier\\n44'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 44}, page_content='•Clustering is more powerful than \\nthe human eye in3D\\n•Clustering mathematically defines \\nwhich cluster the peripheral points should be in when it’s not \\nobvious to the human eye\\n•Clustering is helpful when many \\ndimensions / variables exist that \\nyou can’t visualize at once\\n–Whiskey similarity example from \\nclassification lectureClustering vs. visualizing\\nAye, Nay and Other Votes\\nin House of Representatives\\n45'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 45}, page_content='•Goals of clustering:\\n–Maximize the separation between clusters \\n•i.e.Maximize inter -cluster distance \\n–Keep similar points in a \\ncluster close together \\n•i.e.Minimize intra-cluster distanceHow good is the clustering?\\n46'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 46}, page_content='•Look at the variance explained by \\nclusters\\n–In particular, the ratio of inter -cluster \\nvariance to total variance\\n•How much of the total variance is explained by the clustering?Assessing how well an algorithm performsHow good is the clustering?\\nVariation explained by clusters\\n= \\ninter-cluster variance / total variance\\n47'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 47}, page_content='•cluster: a vector indicating the cluster to which each point is allocated\\n•centers: a matrix of cluster centers\\n•totss: the total sum of squares (sum of distances between all points)\\n•withinss: vector of within -cluster sum of distances, one number per cluster\\n•tot.withinss: total within -cluster sum of distances, i.e.sum of withinss\\n•betweenss: the between -cluster sum of squares, i.e. totss -tot.withinss\\n•size: the number of points in each cluster\\nTo learn more about the kmeans function run ?kmeanskmeans outputs\\n48'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 48}, page_content='Intra vs. inter- cluster distance\\nIntra-Cluster \\nDistanceInter-Cluster \\nDistance\\nwithinssbetweensstotss = withinss +betweenss\\n49'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 49}, page_content='How good is the clustering?\\n# Inter-cluster variance,\\n# \"betweenss\" is the sum of the \\n# distances between points from # different clustersnum_Dem= kmeans_obj_Dem$betweenss\\n# Total variance# \"totss\" is the sum of the distances  # between all the points in # the data setdenom_Dem = kmeans_obj_Dem$totss\\n# Variance accounted for by \\n# clustersvar_exp_Dem = num_Dem/ denom_Dem\\nvar_exp_Dem[1] 0.7952692Script\\n50'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 50}, page_content=\"•It’s easier when the number of clusters is known ahead of time, but what if we don't \\nknow how many clusters we should have?\\n•Since different starting points may generate different clusters, we need a way to assess cluster quality as well.How do we choose the number of clusters ( i.e.k)?How good is the clustering?\\n51\"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 51}, page_content='1.Elbow method\\n–Computes the percentage of variance explained by clusters for a range of cluster \\nnumbers\\n–Plots a graph so results are easier to see \\n–Not guaranteed to work! It depends on the data in question\\n2.NbClustHow to select k: two methods\\n–Runs 30 different tests and \\nprovides “majority vote” for \\nthe best number of clusters \\n(k’s) to use\\n52'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 52}, page_content='Elbow method: measure variance\\n# Run algorithm with 3 centers\\nset.seed(1 )\\nkmeans_obj_Dem = kmeans(clust_data_Dem,   \\ncenters = 3,\\nalgorithm = \"Lloyd\")\\n# Inter- cluster variance\\nnum_Dem = kmeans_obj_Dem$ betweenss\\n# Total variance\\ndenom_Dem = kmeans_obj_Dem $totss\\n# Variance accounted for by clustersvar_exp_Dem = num_Dem / denom_Dem\\nvar_exp_Dem\\n[1] 0.8463623Script\\n53'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 53}, page_content='•We want to repeat the variance calculation from the previous slide for several \\nnumbers of clusters automatically\\n•We can create a function that contains all the steps we want to automate Automating a step we want to repeat\\nfunction(data, item to iterate through)\\n54'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 54}, page_content='# The function explained_variance wraps our code from previous slides.\\nexplained_variance = function( data_in, k){\\n# Running k- means algorithm\\nset.seed(1 )  \\nkmeans_obj = kmeans(data_in, centers = k,\\nalgorithm = \"Lloyd\" )\\n# Variance accounted for by clusters\\nvar_exp = kmeans_obj $betweenss / \\nkmeans_obj$totss\\nvar_exp\\n}Automating a step we want to repeat\\nScript\\n1.A new variable is created and set \\nequal to our function()\\n2.The commands inside the function are wrapped in curly braces {}\\n3.Inside the parentheses, we specify the variables that the user will input and that will then be used inside the function where they appear\\n55'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 55}, page_content='# Recall the variable we are using for the \\n# data that we\\'re clustering.\\nclust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]\\nView( clust_data_Dem)\\n# The sapply() function plugs several values \\n# into explained_variance.\\nexplained_var_Dem = sapply(1 :10, explained_variance, \\ndata_in = clust_data_Dem)\\nView( explained_var_Dem)\\n# Data for ggplot2\\nelbow_data_Dem = data.frame(k = 1:10, \\nexplained_var_Dem)\\nView( elbow_data_Dem)Automating a step we want to repeat\\n1.sapply() applies a function to \\na vector\\n2.We have to tell sapply() that \\nthe we want the \\nexplained_variance function \\nto use the clust_data data\\n3.Next, we create a data frame that contains both the new variance variable (explained_var_Dem) \\nand the different numbers of k that we used in the previous function (1 through 10)Function we created \\nScript\\n56'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 56}, page_content='# Plotting data\\nggplot(elbow_data_Dem, \\naes(x = k,  \\ny = explained_var_Dem)) + \\ngeom_point(size = 4) +\\ngeom_line(size = 1 ) +\\nxlab(\"k\" ) + \\nylab(\"Intercluster Variance/Total Variance\" ) + \\ntheme_light()Elbow method: plotting the graph\\nScript\\n1.geom_point() sets the size of the data points\\n2.geom_line() sets the thickness of the line\\n57'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 57}, page_content='Looking for the kink in graph of  inter- cluster variance / total \\nvarianceElbow method: measure variance\\nOriginal data Elbow methodk = 2\\n58'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 58}, page_content='Elbow method: measure variance\\nk 1 2 3\\nInter-cluster \\nvariance/ total \\nvariance~0% 79.5% 86.4%\\nk =1 k =2 k =3\\n59'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 59}, page_content='•Library: \"NbClust\"\\nFunctions:  \"NbClust\"\\nInputs : \\n•data –data array or data frame\\n•min.nc / max.nc –minimum/maximum number of clusters\\n•method –\"kmeans\"\\n•There are other, more advanced arguments that can be customized but are outside \\nof the scope of this course and are note necessary to for NbClust to workThere are a number of ways to choose the right k.\\nNbClust runs 30 tests and selects k based on majority voteNbClust: k by majority vote\\nNbClust(data, max.nc, method = \"kmeans\")\\n60'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 60}, page_content='# Install the package.\\ninstall.packages(\"NbClust\" )\\nlibrary(NbClust)\\n# Run NbClust.\\nnbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")\\n# View the output of NbClust.nbclust_obj_Dem\\n# View the output that shows the number of clusters each \\n# method recommends.View( nbclust_obj_Dem $Best.nc)NbClust : k by majority vote\\nScript\\n61'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 61}, page_content='NbClust: k by majority vote\\n> nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")\\n...\\n******************************************************************* \\n* Among all indices:                                                * 12 proposed 2 as the best number of clusters * 4 proposed 3 as the best number of clusters ...\\n***** Conclusion *****                            \\n* According to the majority rule, the best number of clusters is  2 \\n*******************************************************************\\nNote: additional information appears; the above information is most relevant to us for nowConsole\\n62'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 62}, page_content='NbClust: k by majority vote\\n> nbclust_obj_Dem Console•nbclust_obj_Dem shows the outputs of NbClust\\n–One of the outputs is Best.nc, which shows the number of clusters                               \\nrecommended by each test \\n63'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 63}, page_content='NbClust: k by majority vote\\n•We want to visualize a histogram to make it obvious how many votes there \\nare for each number of clusters \\n64'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 64}, page_content='# Subset the 1st row from Best.nc and convert it  \\n# to a data frame, so ggplot2 can plot it.\\nfreq_k_Dem = nbclust_obj_Dem $Best.nc[1 ,]\\nfreq_k_Dem = data.frame( freq_k_Dem)\\nView(freq_k_Dem)\\n# Check the maximum number of clusters.\\nmax(freq_k_Dem )\\n# Plot as a histogram.\\nggplot(freq_k_Dem,\\naes(x = freq_k_Dem)) +\\ngeom_bar() +scale_x_continuous(breaks = seq(0 , 15, by = 1)) +\\nscale_y_continuous(breaks = seq(0 , 12, by = 1)) +\\nlabs(x = \"Number of Clusters\",\\ny = \"Number of Votes\" ,\\ntitle = \"Cluster Analysis\")NbClust: k by majority vote\\nScript\\n2 clusters is the \\nwinner with 12 votes\\n65'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 65}, page_content='•If you’re a lobbyist, which congressperson can you influence for swing votes?\\n•If you’re managing a campaign and your competitor is always voting along \\nparty lines, how can you use that information?\\n•If your congressperson is not an active voter, is she representing your interests?\\n•What do the voting patterns look like for Republican-introduced bills?Application of results\\n66'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 66}, page_content='•Could see differences between the \\npatterns of Reb lead bills and Democrat lead bills\\n•Could provide information on congressmen that might be see has swing votes. Implications of results\\n67'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 67}, page_content=\"•We are assuming that the patterns correspond with the same bills being \\nvoted on –perhaps some Congressmen have the same number of 'aye' and \\n'nay' votes, but voted on different bills\\n•Network analysis can help determine additional connections between Congressmen\\n•We haven't taken extenuating factors into account –political initiatives, \\ncurrent events, etc.\\nThis is a preliminary analysis that gives us initial \\ninsights and can help us direct further researchLimitations of results\\n68\"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 68}, page_content='Problem 1: clusters overlap when data points are unequally distributedCommon pitfalls with clustering\\nOriginal data k-means clustering\\n69'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 69}, page_content='Problem 2: clusters should have roughly the same density or else they may split!Common pitfalls with clustering\\nOriginal data k-means clustering\\n70'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 70}, page_content='Problem 3: clusters should be circular / elliptical. We can’t use this method for \\nparticular shapesCommon pitfalls with clustering\\nOriginal data k-means clustering\\n71'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 71}, page_content='Problem 4: a bad starting point can lead to bad clustering!Common problems with clustering\\nOriginal data k-means clustering\\n72'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 72}, page_content='Lab, Part 1 \\n73•Go through the same process only using bills introduced \\nby republican congressmen. \\n–What is the ideal number of clusters’ (You can probably guess this answer)\\n–Are the patterns the same between the parties or do they vary?\\n–Evaluate the model, is it better or worse than the model using the \\ndemocratic introduced bills.  '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 73}, page_content='Which clustering algorithm to use?\\n74\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 74}, page_content='Which clustering algorithm to you use?\\nMethod Parameters Scalability Use case Geometry (metric used)\\nK-means Number of clusters Very large n_samples, \\nmedium n_clusters with \\nMiniBatch codeGeneral purpose, even cluster \\nsize, flat geometry, not too  many clustersDistances between points\\nAffinity propagation Damping, sample \\npreferenceNot scalable with n_samplesMany clusters, uneven cluster size, non- flat geometryGraph distance ( e.g.\\nnearest -neighbor graph)\\nMean -shift Bandwidth Not scalable with n_samplesMany clusters, uneven cluster \\nsize, non- flat geometryDistances between points\\nSpectral clustering Number of clusters Medium n_samples, \\nsmall n_clustersFew clusters, even cluster size, \\nnon- flat geometryGraph distances ( e.g.\\nnearest -neighbor graph)\\nWard hierarchical\\nclusteringNumber of clusters Large n_samples \\nand n_clustersMany clusters, possibly connectivity constraintsDistances between points\\nAgglomerative clusteringNumber of clusters, linkage type, distanceLarge n_samples \\nand n_clustersMany clusters, possiblyconnectivity constraints, non -\\nEuclidean distancesAny pairwise distance\\nDBSCAN Neighborhood size Very large n_samples,   \\nmedium n_clustersNon-flatgeometry, uneven \\ncluster sizesDistances between nearest \\npoints\\nGaussian mixtures Many Not scalable Flatgeometry, good for density \\nestimationMahalanobis distances to \\ncenters\\nBirch Branching factor, threshold, optional global clusterLarge n_samples \\nand n_clustersLarge dataset, outlier removal, \\ndata reductionEuclidean distance between points\\n75'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 75}, page_content=\"•The good and bad\\n–+ cheap –NO LABELS , labels are expensive to create and maintain\\n–+/-clustering always works\\n–-Many methods to choose from and knowing the right one can be nontrivial and the differences \\nbetween many are almost zero, so you need to understand what you're doing\\n•The evil\\n–Curse of dimensionality\\n–Clusters may result from poor data quality\\n–Non-deterministic ( e.g.k-means) subject to local minimum. Since it works with averages, k -means \\ndoes not get much better with Big Data (marginal improvements) but luckily is naïve to parallelize \\n–Non spherical data may result in poor clustering (depending on method used)\\n–Unequal cluster sizes may result in poor clustering (depending on method used)The good, bad, and evil\\n76\"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\Overview_ML_and_Clustering_InClass_3.21.pdf', 'page': 76}, page_content='•Analysts need to ask the following questions\\n–Do you want overlapping or non- overlapping clusters ?\\n–Does your data satisfy the assumptions of the clustering algorithm?\\n–How was the distance measure identified ? \\n–How many clusters and why ? Identifying the number of clusters is a difficult task if \\nthe number of class labels is not known beforehand \\n–Does your method scale to the size of the data?\\n–Is the compute time congruent with the temporal budget of your business need ( i.e.\\ndo you get answers back in time to make meaningful decisions)The good, bad, and evil\\n77'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 0}, page_content=''),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 1}, page_content='Supervised Machine \\nLearning\\nComputer learns through examples how to \\nclassify events/objects/instances.\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 2}, page_content='Supervised Machine \\nLearning\\nComputer learns through examples how to \\nclassify events/objects/instances.\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 3}, page_content='Supervised Machine \\nLearning\\nComputer learns through examples how to \\nclassify events/objects/instances.\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 4}, page_content='Performance \\nMetrics'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 5}, page_content='Supervised Machine \\nLearning\\nComputer learns through examples how to \\nclassify events/objects/instances.\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 6}, page_content='•What metrics should we use?\\n•Accuracy may not be enough\\n•How reliable are the predicted values \\nfrom your model?\\n•Are errors on the training data a good indication of errors on future data?\\n•optimisticEvaluation of \\nPerformance\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 7}, page_content='8\\nSource: https://developers.google.com/machine -learning/crash- course/classification/true -false -positive -negative'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 8}, page_content='Machine Learning Bias and Social Science \\nMethods\\uf0d8Confusion Matrix, ROC (receiver operating curve) and AUC (area under the \\ncurve) are very common approaches for measuring the performance of classification models\\n\\uf0d8Classification models output percentages that an individual input will belong to a specific class, usually a 1 or 0, with one being a positive attribute. \\n\\uf076Likelihood of email spam/fraud is an example. The higher the model percentage \\nprediction on any one email the higher chance it is fraud. \\n\\uf0d8Essentially both measure the misclassification error rate associated with your \\nmodel\\n\\uf0d8A Confusion Matrix is a good tool for understanding how accurate you model is \\nclassifying and is used to build ROC'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 9}, page_content=\"Machine Learning Bias and Social Science \\nMethods\\uf0d8Let's use intruder/fraud detection as an example\\n\\uf0d8Say we have 135 emails entering our system and we are trying to detect whether \\nthey are fraudulent or not \\n\\uf076We use lots of criteria –source, subject, if they came from a prince…\\n\\uf0d8Generate probability measures as a result for a tree -based classifier to determine \\nthe likelihood that any one of these emails is fraudulent \\n\\uf0d8The cutoff point that is predetermined in the tree (and is a universal standard) is \\n50% but can be modified as an input if needed\"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 10}, page_content='Machine Learning Bias and Social Science \\nMethods\\uf0d8Below is are the results of our model in a Confusion Matrix. They center on \\nthe positive and negative classifications in sub-categories of true and false \\npositive.  \\n\\uf0d8Keep in mind we know because of the labels, what is fraud and not, so we can measure how good the model is classifying. \\n\\uf0d8Both true negative and true positive are good, false negative and false positive are errors. \\n1 = Fraud/Spam\\n0 = Not Fraud/SpamPredicted Class\\nPositive Fraud Pred (1)Negative\\nNot Fraud Pred (0)\\nActual ClassPositive \\nFraud Actual (1)True Positive10False Negatives\\n22\\nNegative Not Fraud Actual (0)False Positives7True Negative\\n96'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 11}, page_content='Machine Learning Bias and Social Science \\nMethods\\uf0d8Let’s consider the extremes: what if we set the threshold to 0? \\n\\uf076Means that everything is captured as Fraud and no ever gets an email again!\\n1 = Fraud/Spam\\n0 = Not Fraud/SpamPredicted Class\\nPositive Fraud Pred (1)Negative\\nNot Fraud Pred (0)\\nActual ClassPositive \\nFraud Actual (1)True Positive32False Negatives\\n0\\nNegative Not Fraud Actual (0)False Positives103True Negative0'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 12}, page_content='Machine Learning Bias and Social Science \\nMethods \\uf0d8Let’s consider the other extreme: what if we set the threshold to 100?\\n\\uf076Means nothing is fraud and now everyone is getting rich off of Arabian princes\\n1 = Fraud/Spam\\n0 = Not Fraud/SpamPredicted Class\\nPositive Fraud Pred (1)NegativeNot Fraud Pred (0)\\nActual ClassPositive Fraud Actual (1)True Positive0False Negatives\\n32\\nNegative Not Fraud Actual (0)False Positives0True Negative103'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 13}, page_content='Machine Learning Bias and Social Science \\nMethods\\uf0d8We can further assess our model by generating classification rates:\\n\\uf076True Positive Rate (TPR) (Sensitivity) = TP/(TP+FN) = 10/(10+22) = .31\\n\\uf0d8% of fraud correctly labeled as fraud\\n\\uf076False Positive Rate (FPR) (1- Specificity) = FP/(FP+TN) = 7/(7+96) = .06\\n\\uf0d8% of emails labelled not fraud that were false positives\\n1 = Fraud/Spam\\n0 = Not Fraud/SpamPredicted Class\\nPositive Fraud Pred (1)Negative\\nNot Fraud Pred (0)\\nActual ClassPositive \\nFraud Actual (1)True Positive10False Negatives\\n22\\nNegative Not Fraud Actual (0)False Positives7True Negative96'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 14}, page_content='Machine Learning Bias and Social Science \\nMethods\\uf0d8These two data points can used to begin to develop a Receiver Operating \\nCharacteristic Curve or ROC curve\\n\\uf076True Positive Rate (TPR) = 10/(10+22) = .31 = y-axis\\n\\uf076False Positive Rate (FPR) = 7/(7+96) = .06 = x-axis\\n1 = Fraud/Spam\\n0 = Not Fraud/SpamPredicted Class\\nPositive Fraud Pred (1)Negative\\nNot Fraud Pred (0)\\nActual ClassPositive \\nFraud Actual (1)True Positive10False Negatives\\n22\\nNegative Not Fraud Actual (0)False Positives7True Negative\\n96'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 15}, page_content='Machine Learning Bias and Social Science \\nMethods\\uf0d8ROC curve is essentially a graphical representation of the adjusted threshold \\nvalues of the confusion matrix, below are two examples\\nAUC'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 16}, page_content='Machine Learning Bias and Social Science \\nMethods\\uf0d8ROC curve generates the area under the curve as a percentage of the total graph \\nunder the curve. \\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 17}, page_content='The Area Under the Curve (AUC) is indicative \\nof performance.\\nAUC:\\n0.9 –1.0 = Excellent\\n0.8 –0.9 = Good\\n0.7 –0.8 = Fair\\n0.6 –0.7 = Poor\\n0.5 –0.6 = Fail\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 18}, page_content='19\\nSource: https://developers.google.com/machine -learning/crash- course/classification/true -false -positive -negative'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 19}, page_content='Machine Learning Bias and Social Science \\nMethodsAdditional Performance Measures\\nAccuracy –(TP+TN )/(TP+FP+FN+TN)\\nPrevalance –The percentage of the positive class in the\\ntest data set \\nDetection Rate -The rate of true events also predicted to be events\\nBalanced Accuracy -(sensitivity+specificity )/2\\nPrecision -TP/TP+FP –When predicting True Positives , what percentage is correct? (no FP, precision = 1)\\nRecall –TP/TP+FN (same as sensitivity) –What proportion of Actual Positives where identified correctly?\\nF1 Score –Harmonic mean of Precision and Recall, where accuracy is used when True Positives and True \\nNegatives are important, F1 is used when False Negatives and False Positives are more of a concern.  Also really \\nbestused on unbalanced datasets \\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 20}, page_content='Machine Learning Bias and Social Science \\nMethodsAdditional Performance Measures\\nLog Loss - log loss measures the UNCERTAINTY of the probabilities of your model by \\ncomparing them to the true labels –CLASSIFICATION. It heavily penalizes classifications \\nthat are highly confident in the wrong direction.  \\nSo, seeing a log loss of 1 \\ncan be expected in the case \\nwhen our model only gives less than a ~ 40 % \\nprobability estimate for \\nselecting the actual class. Knowing the baseline rate (prevalence) here is important!\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 21}, page_content='Machine Learning Bias and Social Science \\nMethodsAdditional Performance Measures\\nKappa - Landis and Koch (1977) provide a way to characterize values. According to their \\nscheme a value < 0 is indicating no agreement , 0–0.20 as slight, 0.21–0.40 as fair, 0.41–0.60 \\nas moderate, 0.61–0.80 as substantial, and 0.81–1 as almost perfect agreement. \\nIndicates how much better our classifier is performing over the performance of a classifier that \\nwould just guess at random according to the frequency of each class.  \\nIts especially useful for multi -class models as many of the metrics we have reviewed are better \\nsuited to binary examples. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 22}, page_content='Machine Learning Bias and Fairness \\uf0d8These metrics can be used to assess the fairness of the machine learning models\\n\\uf0d8We will review these topics again later in the semester, as there’s much more to \\nbe learning but having a basic understand will help when we walk through the \\nfairness formulas. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 23}, page_content='Another Definition: Functional Approximation\\n•What is a functional approximation problem?\\n•Target variable: Dependent: What we are trying to predict\\n•Other Variables: Independent: Using to Predict\\n•Functional approximation is an approach that uses the other variables \\nwe have access to approximate the dependent and does so through the \\nfunction development\\n•We will use regression, which assumes that we have a numeric target \\nvariable, for classification it’s often a bi- variate or class level variable \\n24'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 24}, page_content='Assessment Measures \\n•Assessing Regression Models: MSE, RMSE and MAE and Log Loss \\n•MSE –The difference between the predicted values and the actual values \\nsquared\\n•RSME –Same as above only the square root is taken to put the error back in \\nterms of the dependent variable\\n•Can also normalize the RSME to the range of the data in order to be able to compare \\nRSME outputs that include different data ranges \\n•MAE –The same approach only taking the absolute value instead of squaring\\n25'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 25}, page_content='Equations \\n26nX X\\nRMSEn\\niidelmo iobs∑=−\\n=12\\n, , ) (\\nmin, max, obs obs X XRMSENRMSE\\n−=\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 26}, page_content='Regression \\n•Said another way basic linear regression has a Prediction Accuracy Problem:\\n•Has a low bias (overfitting) but a high variance\\n•This can be improved by injecting some level of bias into the equation by reducing the \\nimpact of certain coefficients \\n•This can improve overall accuracy by reducing variance \\n•Draw Dart Board –\\n•Another issue is interpretation –with a large number of predictor variables \\nand large data sets it often hard to identify variable importance and explain \\nmodel outcomes\\n27'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 27}, page_content='Regression and Sparsity\\n•Often we have more features than observations in the world of big \\ndata\\n•What type of problem is this?\\n•So we strive to have Sparse models\\n•What do we mean by Sparse?\\n28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 28}, page_content='Bias Versus Variance \\n29\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 29}, page_content='The Confusion Matrix and \\nAccuracy\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 30}, page_content='Most widely used metricAccuracy•Accuracy is better measured using test data that was not used \\nto build the classifier.\\n•Referred to as the overall recognition rate of the classifier\\n•Error rate or misclassification rate:\\n1-Accuracy\\n•When training data are used to compute the accuracy, the error rate is called resubstitution error .\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 31}, page_content='Accuracy may not be enough.Consider this two- class \\ncase.\\nConsider the accuracy if a model predicts that the \\noutcome is always class a.\\n28/30 = 93% accuracy\\nThis is misleading.\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 32}, page_content='Sensitivity and Specificity\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 33}, page_content='Receiver Operating Characteristic (ROC) Curve\\n1-Specificity\\nFalse Negative RateSensitivity\\nTrue Positive Rate\\n0 101'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\PerfMet.pdf', 'page': 34}, page_content='The ROC Curve\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 0}, page_content='Cleaning the Data and Documenting the\\nProcess Using R Markdown\\nIntroduction to R Programming\\nDistrict Data Labs'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 1}, page_content='The Data “Pipeline”\\nDocument Your Data Cleaning Process\\nTidy Data\\nData cleaning methods\\nOpening data ﬁles in R and viewing them\\nWorking with data frames\\nDropping variables\\nRenaming variables\\nLogical operators\\nDropping observations\\nReshaping\\nReshaping from wide to long\\nReshaping from long to wide\\nCreating new variables'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 2}, page_content='Replacing existing variables\\nManaging string/character variables\\nMerging\\nBinding columns or rows\\nMatching on ID variables\\nChecking the merge\\nSorting rows\\nAdditional data management techniques\\nManaging the workspace\\nDeleting objects\\nSaving/loading the workspace\\nOpening ASCII data ﬁles'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 3}, page_content='Saving ASCII data ﬁles\\nArranging columns\\nMissing data\\nManaging categorical variables\\nRecoding/labeling values\\nCombining categories\\nReordering categories\\nCombining categories across multiple variables\\nMore methods for managing string/character variables\\nManaging date/time variables\\nPiping\\nCollapsing and within-group calculations\\nUsing group by()\\nUsing count()'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 4}, page_content='Document Your Data Cleaning Process\\nRemember the work by Baggerly and Coombes. They had to\\nspend months trying to replicate a study in a “forensic” way\\nbecause the authors did not document their data cleaning steps.\\nThey had to deal with this:\\nInstead, we will learn how to do this:\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 5}, page_content='Document Your Data Cleaning Process\\nReal-world data are always messy: that is, original data are never\\nimmediately ready for analysis. Data must be cleaned in order to\\nbe useful.\\nBut small mistakes in the data cleaning stage can have profound\\neﬀects for the analysis. If your data are corrupted, no amount of\\nstatistical-machine-learning-artiﬁcial-intelligence-magic will save\\nthe analysis.\\nThis afternoon, we will review the tidyverse packages, and all the\\nmethods we have for cleaning data. We will also introduce a\\ncouple new methods.\\nWe will work through how to do this work using R markdown, so\\nthat we document our steps as we go along . We can catch errors\\nthis way, and make them easier to ﬁx.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 6}, page_content='What is Tidy Data?\\n“Happy families are all alike; every unhappy family is unhappy in\\nits own way. ” — Leo Tolstoy\\n“Like families, tidy datasets are all alike but every messy dataset is\\nmessy in its own way. ” — Hadley Wickham\\nData can come in many shapes and formats. The data can be\\nassembled in one place or might be in many diﬀerent places.\\nTidy data is a philosophy about the best way to code a dataset to\\nmake data easy to analyze in a reproducible way.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 7}, page_content='What is Tidy Data?\\nAtidy dataset meets the following minimum standards :\\n1. The dataset exists as one table.\\n2. It’s square: every row has an entry for every column (even if it\\nis missing).\\n3. The rows represent observations.\\n4. The columns represent variables.\\n5. The observations are comparable units (for example: people,\\ncountries, but not a mix of the two)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 8}, page_content='What is Tidy Data?\\nEven better tidy data also:\\n6. Sorts the rows and columns into a logical order\\n7. Has descriptive variable names\\n8. Creates new variables to convey the important information\\n9. Deletes irrelevant variables (and sometime observations)\\n10. Reads date variables as dates\\n11. Uses consistent codes for missing values\\nThese are all things to do with the edited, working data. Never\\noverwrite the original data ﬁle. We may need to return to the\\noriginal data many times.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 9}, page_content='Opening data ﬁles in R and viewing them\\nThe most common type of data ﬁle is CSV (comma separated\\nvalues). CSVs are space eﬃcient, portable, and can be transferred\\nto any data/statistics software.\\nTo load a CSV ﬁle called data.csv , load the tidyverse package\\nby typing\\nlibrary(tidyverse)\\nThen type\\ndata <- read_csv(\"data.csv\")\\nThen to look at the data spreadsheet type View(data) in the\\nconsole . (Please note the capital V.)\\nLooking at the data is the most useful way to understand the\\nchallenges ahead of you for preparing the data for analysis.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 10}, page_content='Working with data frames\\nAll of the following commands start by assigning the data\\nmanagement step to a data frame.\\nIf you assign the step to a new name, you create a new object .\\nThe step is applied to the NEW data frame, but the step is NOT\\napplied to the original data frame object.\\nExample : If the data frame is called data ,\\n▶data <- command overwrites the data object,\\n▶data2 <- command creates another object, data2 , in\\nwhich the command was applied. It leaves the data object\\nalone.\\nIn general : assign to the same object over and over, UNLESS you\\ndon’t want the step to be permanent.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 11}, page_content='Dropping variables\\nSometimes datasets have hundreds or thousands of variables.\\nOnly a small portion of them might be relevant to your work. You\\nmay have to curate the data by deleting variables.\\nUse the select() command to keep only the variables you\\nspecify. To keep only the country name andyear variables, type\\nvdem <- select(vdem, country_name, year)\\nYou can keep country name ,year , and every variable that\\nstarts with “v2x” by typing\\nvdem <- select(vdem, country_name, year,\\nstarts_with(\"v2x\"))'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 12}, page_content='Dropping variables\\nYou can keep country name ,year , and every variable that ends\\nwith “ocracy” by typing\\nvdem <- select(vdem, country_name, year,\\nends_with(\"ocracy\"))\\nYou can also specify which variables to drop instead of keep by\\nusing a minus sign:\\nvdem <- select(vdem, -ends_with(\"error\"))'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 13}, page_content='Renaming variables\\nTidy data should have descriptive variable names so you actually\\nknow what each variable is. It’s crazy how often the data collectors\\nuse dumb names like XAB14G7B!\\nTo rename the variable v2xpolyarchy todemocracy , type:\\nvdem <- rename(vdem, democracy=v2x_polyarchy)\\nNote that the new name comes ﬁrst, and the old name comes\\nsecond.\\nYou can rename many variables with one command. Just separate\\nthese statements with commas:\\nvdem <- rename(vdem, democracy=v2x_polyarchy,\\ncorrupt=v2x_corr,\\ncivil_lib=v2x_civlib)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 14}, page_content='Logical operators\\nA logical statement is one that can only take on two values: true\\nor false. Logical operators are used to build logical statements.\\nThese operators are\\n▶== is equal to?\\n▶>is greater than?\\n▶>=is greater than or\\nequal to?\\n▶<is less than?▶<=is less than or equal\\nto?\\n▶%in% is an element of the\\nset?\\nA!sign in front of >,>=,<, or<= means “not greater than”,\\n“not greater than or equal to”, etc.\\nAlso, != means “not equal to”, and !(3 %in% c(4,5,6))\\nmeans “3 is not an element of the set 4, 5, 6”'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 15}, page_content='Logical operators\\nTwo symbols are used to combine logical operators:\\n▶&“and”\\n▶|“or” (shift + the button just above enter)\\nParentheses work here too to designate an order of evaluation.\\nExamples:\\n1==1 true 2==3 false\\n(1==1)|(2==3) true (1==1)&(2==3) false\\n(1!=1)|(2!=3) true !((1==1)&(2==3)) true\\n(4*3-2) %in% c(10, 11, 12) true !(4*3-2) %in% c(10, 11, 12) false\\nIf variables are used in a logical statement, R puts the value TRUE\\non the rows for which the statement is true, and FALSE on the\\nrows for which the statement is false.\\nIf you change the class of a logical variable to numeric, the variable\\nwill be binary (0,1).'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 16}, page_content='Dropping observations\\nThefilter() command works just like select() , except it keeps\\nobservations instead of variables, and you use logical statements\\nto identify rows instead of variable names.\\nTo keep only the rows corresponding to the year 2014, type\\nvdem <- filter(vdem, year==2014)\\nTo keep only the rows in 2014 when the democracy variable is\\ngreater than .5, type\\nvdem <- filter(vdem, year==2014 & democracy > .5)\\nTo keep only the rows for 1997, 2003, and 2011, type\\nvdem <- filter(vdem, year %in% c(1997, 2003, 2011))'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 17}, page_content='Reshaping\\nReshaping is a way to transform a dataset by turning columns into\\nrows, or rows into columns.\\nThere are two kinds of reshapes :\\n1.gather() : turning columns into rows. (In Stata, this is called\\nareshape long .)\\n2.spread() : turning rows into columns. (In Stata, this is called\\nareshape wide .)\\nRemember: tidy data requires that rows are observations and\\ncolumns are variables . Years are observations, not variables.\\nReshaping is tricky. But any command can be broken down to its\\nsimple logical elements. We just need to examine the rules on\\nwhich the gather() andspread() commands operate.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 18}, page_content='Reshaping\\nHere’s an untidy dataset because observations are in the columns:\\ncountry 1999 2000\\n1 Afghanistan 745 2666\\n2 Brazil 37737 80488\\n3 China 212258 213766\\nThe gather() command brings the observations to the rows. This\\ncommand has the following syntax:\\ngather(data, ... , key, value)\\nLet’s clearly deﬁne what each argument does.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 19}, page_content='Reshaping\\ngather(data, ... , key, value)\\ndata is the name of the data frame you want to reshape.\\n. . .represents the names of the variables that you will move to the\\nrows.\\n▶You can type all the raw variable names here, separated by\\ncommas\\n▶If the variable names are numeric (as with years), place\\nforward-slanted single quotes around the names\\n▶You can type something like var1:var10 to refer to all\\nvariables in between between var1 andvar10 in the data\\nframe.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 20}, page_content='Reshaping\\ngather(data, ... , key, value)\\nkey is the name of the variable that will contain the names of the\\nold variables you are moving. This variable doesn’t yet exist. If the\\ncolumn names are numeric years, you should type key=\"year\" .\\nvalue is the name of the variable that will contain the data inside\\nthe old variables you are moving. This variable also doesn’t yet\\nexist.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 21}, page_content='Reshaping\\nThis data is an object named table4a in R:\\ncountry 1999 2000\\n1 Afghanistan 745 2666\\n2 Brazil 37737 80488\\n3 China 212258 213766\\nThe data inside the columns represent a variable named cases.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 22}, page_content='Reshaping\\nTo reshape this data, we type:\\ngather(table4a, ‘1999‘, ‘2000‘, key=\"year\", value=\"cases\")\\nThe data now looks like\\ncountry year cases\\nAfghanistan 1999 745\\nBrazil 1999 37737\\nChina 1999 212258\\nAfghanistan 2000 2666\\nBrazil 2000 80488\\nChina 2000 213766'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 23}, page_content='Reshaping\\nHere’s an untidy dataset because variables are in the rows:\\ncountry year type count\\nAfghanistan 1999 cases 745\\nAfghanistan 1999 population 19987071\\nAfghanistan 2000 cases 2666\\nAfghanistan 2000 population 20595360\\nBrazil 1999 cases 37737\\nBrazil 1999 population 172006362\\nBrazil 2000 cases 80488\\nBrazil 2000 population 174504898\\nChina 1999 cases 212258\\nChina 1999 population 1272915272\\nChina 2000 cases 213766\\nChina 2000 population 1280428583'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 24}, page_content='Reshaping\\nTo clean this dataset, we use the spread() command, which\\nworks similarly (but not exactly like) the gather() command:\\nspread(data, key, value)\\nThe notable diﬀerence here is the lack of an . . .argument. We\\ndon’t have to specify columns, because the variables are already in\\nthe rows, and R knows what to place in the columns\\nautomatically by looking at the levels of the factor (named type in\\nthis data).\\ndata is the name of the data frame you want to reshape.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 25}, page_content='Reshaping\\nspread(data, key, value)\\nkey is the name of the existing variable that contains the names of\\nthe variables you will move to the columns.\\nvalue is the name of the existing variable that contains the data\\ninside the variables you will move to the columns.\\nTo reshape the data (named table2 in R), we type:\\nspread(table2, key=\"type\", value=\"count\")'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 26}, page_content='Reshaping\\nThe data now looks like this:\\ncountry year cases population\\n1 Afghanistan 1999 745 19987071\\n2 Afghanistan 2000 2666 20595360\\n3 Brazil 1999 37737 172006362\\n4 Brazil 2000 80488 174504898\\n5 China 1999 212258 1272915272\\n6 China 2000 213766 1280428583'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 27}, page_content='Creating new variables\\nUse the mutate() command to create new variables.\\nWrite the new variable name, one equal sign, then what the new\\nvariable equals. You can use math and logic operators and the\\nnames of existing variables. For example:\\nvdem <- mutate(vdem,\\nafter2001 = (year>=2001),\\nadjusted_dem = democracy - corruption,\\navg = (democracy + civil_rights)/2)\\nIf you use logical operators , the new variable will be BINARY to\\nexpress whether the logical statement is TRUE or FALSE.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 28}, page_content='Replacing existing variables\\nYou can also use the mutate() command to replace old variables.\\nThe diﬀerence is that now you write old variable names instead of\\nnew variables.\\nIn this example, the variables democracy andcorruption\\nalready exist :\\nvdem <- mutate(vdem,\\ndemocracy = 100*democracy,\\nadjusted_dem = democracy - corruption)\\nThe ﬁrst transformation says “replace each value of democracy\\nwith 100 times its old value.”\\nThe second transformation says “create adjusted dem to be the\\ndiﬀerence of democracy andcorruption .”\\nBut there’s a problem here. See it?'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 29}, page_content='Replacing existing variables\\nvdem <- mutate(vdem,\\ndemocracy = 100*democracy,\\nadjusted_dem = democracy - corruption)\\nThese transformations are done one at a time and in order.\\nIn the second transformation, our intention was to use the old\\nversion ofdemocracy . Instead it uses the new version (×100).\\nThis command instead does what we want:\\nvdem <- mutate(vdem,\\nadjusted_dem = democracy - corruption\\ndemocracy = 100*democracy)\\nSo pay attention to the order in which you create/replace variables.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 30}, page_content='Managing string/character variables\\nSome variables are coded as character (words), but in reality, they\\nare categorical variables. We’ve covered how to deal with variables\\nlike that.\\nBut other variables are character because they really are\\nopen-ended responses .\\nOpen-ended questions contain a LOT of data, some of it novel and\\nunexpected, since these responses are not constrained to follow any\\nmultiple choices.\\nBut how to extract this data?'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 31}, page_content='Managing string/character variables\\nThe following commands are part of the stringr library, which is\\npart of the tidyverse packages.\\nHere’s an example of a string:\\nresponse <- \"I have the faintest idea.\\nI really don’t watch the news\"\\nString is just another word for character, text, words, etc. It has\\nthe character class in R:\\n> class(response)\\n[1] \"character\"'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 32}, page_content='Managing string/character variables\\nThe length of a string is the number of characters in the string,\\nincluding spaces:\\n> str_length(response)\\n[1] 55\\nTo pull out a subsection of this string, use the strsub()\\ncommand. Specify the string variable to work with, the NUMBER\\nof the character that starts the substring, and the NUMBER of the\\nlast character of the substring:\\n> str_sub(response, 1, 10)\\n[1] \"I have the\"\\n> str_sub(response, 25, 37)\\n[1] \". I really do\"'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 33}, page_content='Managing string/character variables\\nNegative numbers with the strsub() command tell R to count\\nbackwards . To get a substring starting 15 characters from the\\nend, until the end, type:\\n> str_sub(response, -15, -1)\\n[1] \" watch the news\"\\nIt might be useful to convert all letters to UPPERCASE, so we\\ndon’t have to worry about case sensitivity when looking for speciﬁc\\npatterns and words:\\n> str_to_upper(response)\\n[1] \"I HAVE THE FAINTEST IDEA. I REALLY DON’T WATCH\\nTHE NEWS\"'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 34}, page_content='Merging\\nMerging is the technique of combining two data frames. There are\\na few diﬀerent ways to perform a merge.\\n(1) Adding columns, without trying to match observations .\\nThebind cols() (or the older cbind() ) function pastes two data\\nframes together, side by side, exactly as they are . So if you sort\\none or both data frames, it changes the result of the bind cols()\\ncommand.\\nFor example, consider data1 anddata2 :\\ncountry x y\\nUSA -0.10 -0.90\\nCanada 0.18 -0.88\\nUK 0.50 0.92country x y\\nChina -0.61 -1.23\\nJapan -0.24 -1.05\\nS. Korea -0.11 -1.10'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 35}, page_content='Merging\\nUsing bind cols() to combine these data frames has the\\nfollowing result:\\ncbind(data1,data2)\\ncountry x y country1 x1 y1\\nUSA -0.10 -0.90 China -0.61 -1.23\\nCanada 0.18 -0.88 Japan -0.24 -1.05\\nUK 0.50 0.92 S. Korea -0.11 -1.10\\nTo avoid duplicate column names, R placed an arbitrary “1” after\\nthe variable names for data2 .\\nR pasted the two data frames together without any regard for the\\ndata’s structure or meaning. In general, this is NOT what we want\\nto do.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 36}, page_content='Merging\\n(2) Adding rows (in Stata this is called “appending”)\\nThe bind rows() (or the older rbind() ) function is similar to\\nbind cols() , but with two important diﬀerences:\\n1.bind rows() pastes data frames together by pasting one on\\ntop of the other, resulting in more rows but not more columns\\n2.bind rows() rearranges the columns of each data frame to\\ntry to match corresponding variables.\\nThis is a good option to group observations that belong together,\\nbut are stored in separate places for whatever reason.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 37}, page_content='Merging\\nUsing bind rows() to combine these data frames has the\\nfollowing result:\\nrbind(data1,data2)\\ncountry x y\\n1 USA -0.10 -0.90\\n2 Canada 0.18 -0.88\\n3 UK 0.50 0.92\\n4 China -0.61 -1.23\\n5 Japan -0.24 -1.05\\n6 S. Korea -0.11 -1.10'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 38}, page_content='Merging\\n(3) Adding columns, matching observations that share the\\nsame ID variable(s) .\\nThis is the most important kind of merge. It’s also the trickiest to\\ndo correctly.\\nSuppose you have two data frames that look like this:\\ncountry x y\\nUSA 0.97 0.76\\nChina -0.96 -2.12\\nRussia 1.10 0.24country w z\\nChina 0.99 0.33\\nUSA 0.37 -0.64\\nRussia 0.10 -0.49\\nHow do you combine the two data frames?'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 39}, page_content='Merging\\nYou can sort both data frames alphabetically by country, then use\\ncbind() . But while that works here, that approach won’t work in\\ngeneral.\\nWe need a way for R to recognize that country is the ID\\nvariable , and that the two data frames share IDs. Then R needs to\\nmatch rows that share the same ID .\\nThe most important merge command is full join() . Applying\\nthis command gives the following result:\\nfull_join(data1, data2)\\ncountry x y w z\\nUSA 0.97 0.76 0.37 -0.64\\nChina -0.96 -2.12 0.99 0.33\\nRussia 1.10 0.24 0.10 -0.49'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 40}, page_content='Merging\\nHere’s what full join() does :\\n1. It looks at the variable names for each dataset, and\\ndetermines whether there is any overlap in these names.\\n2. It assumes that the shared variable names are the ID variables\\nyou want to use to match the observations in each data frame.\\n3. It combines observations with the same IDs.\\nfull join() is very useful and easy to use compared to merge\\ncommands in other packages and software.\\nBut it tends to mess up without displaying any warnings or\\nerrors ! You have to understand how full join() works, and you\\nhave to visualize what you want the data to look like after using\\nfull join() .'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 41}, page_content='Merging\\nIf an observation has no match in the other data frame, it still\\nappears in the merged data with NAvalues for the variables from\\nthe other data frame.\\nThere are related commands that treat unmatched observations\\ndiﬀerently:\\n▶inner join() : drops all unmatched observations\\n▶left join(data1, data2) : keeps all observations from\\ndata1 , but drops all unmatched observations in data2\\n▶right join(data1, data2) : keeps all observations from\\ndata2 , but drops all unmatched observations in data1\\nIn general, it is safer to stick with full join() . These other\\ncommands might delete observations you don’t want to delete.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 42}, page_content='Merging\\nSuppose for example we want to merge these two data frames:\\ncountry x y\\nUSA 0.43 -0.57\\nChina -0.70 -1.38\\nRussia 1.64 0.60\\nFrance -0.38 -0.30country w z\\nUSA 0.99 -1.27\\nChina 1.06 0.13\\nRussia 1.65 0.72\\nJapan 0.28 -0.09\\nNote that some, but not all, of the ID values have a match in the\\nother data frame.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 43}, page_content='Merging\\nfull join() keeps all observations that appear in either data\\nframe, but creates missing values for unmatched observations:\\ndata3 <- full_join(data1, data2)\\ncountry x y w z\\nUSA 0.43 -0.57 0.99 -1.27\\nChina -0.70 -1.38 1.06 0.13\\nRussia 1.64 0.60 1.65 0.72\\nFrance -0.38 -0.30 NA NA\\nJapan NA NA 0.28 -0.09'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 44}, page_content='Merging\\ninner join() keeps the observations that have a match in both\\nobservations, and deletes all other observations:\\ndata3 <- inner_join(data1, data2)\\ncountry x y w z\\nUSA 0.43 -0.57 0.99 -1.27\\nChina -0.70 -1.38 1.06 0.13\\nRussia 1.64 0.60 1.65 0.72'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 45}, page_content='Merging\\nleft join() keeps all observations from data1 (the data frame\\ntyped FIRST), keeps all observations from data2 (the data frame\\ntyped SECOND) that have a match in data1 , and deletes\\nobservations from data2 without a match in data1 :\\ndata3 <- left_join(data1, data2)\\ncountry x y w z\\nUSA 0.43 -0.57 0.99 -1.27\\nChina -0.70 -1.38 1.06 0.13\\nRussia 1.64 0.60 1.65 0.72\\nFrance -0.38 -0.30 NA NA'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 46}, page_content='Merging\\nright join() keeps all observations from data2 (the data frame\\ntyped SECOND), keeps all observations from data1 (the data\\nframe typed FIRST) that have a match in data2 , and deletes\\nobservations from data1 without a match in data2 :\\ndata3 <- left_join(data1, data2)\\ncountry x y w z\\nUSA 0.43 -0.57 0.99 -1.27\\nChina -0.70 -1.38 1.06 0.13\\nRussia 1.64 0.60 1.65 0.72\\nJapan NA NA 0.28 -0.09'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 47}, page_content='Merging\\nSometimes there is more than one ID variable that identiﬁes an\\nobservation. For example, we might have data on\\n▶U.S States within years\\n▶Political parties within countries within years\\n▶Students within classes within schools within school districts\\nwithin states\\nA set of ID variables are called unique identiﬁers of the rows if no\\ntwo rows share the same values of every ID variable. Two rows\\nmight represent China at diﬀerent years. So country alone is not a\\nunique identiﬁer, but country and year TOGETHER are.\\nThe full join() command can match on multiple ID variables,\\nas long as corresponding ID variables have the same name in\\neach data frame .'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 48}, page_content='Merging\\nSuppose we want to merge the following two data frames:\\ncountry year x y\\nUSA 2014 0.53 0.26\\nUSA 2015 -1.12 -1.68\\nUSA 2016 0.75 -1.24\\nChina 2014 -0.01 0.17\\nChina 2015 -0.09 0.14\\nChina 2016 -0.32 0.07\\nRussia 2014 1.49 0.83\\nRussia 2015 -0.96 1.04\\nRussia 2016 2.50 -0.43country year w z\\nUSA 2014 0.27 -1.95\\nUSA 2015 0.88 0.14\\nUSA 2016 -0.06 0.49\\nChina 2014 -0.25 -1.02\\nChina 2015 0.72 0.22\\nChina 2016 0.89 1.20\\nRussia 2014 0.59 0.45\\nRussia 2015 0.06 -1.57\\nRussia 2016 -0.93 -0.01\\nThere are two ID variables, country and year. Because the have\\nthe same name in each data frame, we can use full join() like\\nwe did before.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 49}, page_content='Merging\\nWe use the following command:\\ndata3 <- full_join(data1, data2)\\ncountry year x y w z\\nUSA 2014 0.53 0.26 0.27 -1.95\\nUSA 2015 -1.12 -1.68 0.88 0.14\\nUSA 2016 0.75 -1.24 -0.06 0.49\\nChina 2014 -0.01 0.17 -0.25 -1.02\\nChina 2015 -0.09 0.14 0.72 0.22\\nChina 2016 -0.32 0.07 0.89 1.20\\nRussia 2014 1.49 0.83 0.59 0.45\\nRussia 2015 -0.96 1.04 0.06 -1.57\\nRussia 2016 2.50 -0.43 -0.93 -0.01'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 50}, page_content='Merging\\nSometimes one data frame has more unique identifying variables\\nthan the other. For example:\\ncountry year x y\\nUSA 2014 0.53 0.26\\nUSA 2015 -1.12 -1.68\\nUSA 2016 0.75 -1.24\\nChina 2014 -0.01 0.17\\nChina 2015 -0.09 0.14\\nChina 2016 -0.32 0.07\\nRussia 2014 1.49 0.83\\nRussia 2015 -0.96 1.04\\nRussia 2016 2.50 -0.43country w z\\nUSA 0.99 -1.27\\nChina 1.06 0.13\\nRussia 1.65 0.72'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 51}, page_content='Merging\\nfull join() handles this situation too. It recognizes the shared\\nID of country, and places the data for a country in data2 on every\\nrow for that country:\\ndata3 <- full_join(data1, data2)\\ncountry year x y w z\\nUSA 2014 0.53 0.26 0.99 -1.27\\nUSA 2015 -1.12 -1.68 0.99 -1.27\\nUSA 2016 0.75 -1.24 0.99 -1.27\\nChina 2014 -0.01 0.17 1.06 0.13\\nChina 2015 -0.09 0.14 1.06 0.13\\nChina 2016 -0.32 0.07 1.06 0.13\\nRussia 2014 1.49 0.83 1.65 0.72\\nRussia 2015 -0.96 1.04 1.65 0.72\\nRussia 2016 2.50 -0.43 1.65 0.72'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 52}, page_content='What can go wrong while merging\\nfull join() doesn’t always know if something went wrong. So\\nthere’s no automated procedure to check errors. But there are\\ncommon problems you can check for before merging the data.\\nPrior to merging, we will perform three checks :\\n1.ID name check : do the shared ID variables have the same\\nname? Are these variables the ONLY ones that share the\\nsame name in both data frames?\\n2.Unique ID check : do we expect the ID variables to be\\nunique identiﬁers in one or both data frames? If so, are they?\\n3.ID value check : are there discrepancies in the values of the\\nID variable?\\nIf all three checks pass, then the merge will work without problems.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 53}, page_content='What can go wrong while merging\\nID name check : First, look at each data frame with the View() ,\\nsummary() , and head() commands. Decide on what the unique\\nID variables are in each data frame, and which you will use to\\nmatch in the merge.\\nUse the names() command to display the names for each data\\nframe, and use the intersect() command to see which variable\\nnames are shared by both data frames:\\n> names(data1)\\n[1] \"country\" \"year\" \"x\" \"y\"\\n> names(data2)\\n[1] \"country\" \"year\" \"w\" \"z\"\\n> intersect(names(data1), names(data2))\\n[1] \"country\" \"year\"\\nThere are two problems this check can reveal.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 54}, page_content='What can go wrong while merging\\nProblem 1 : the ID variables don’t have the same name in each data\\nframe. In this case R won’t know which variable to match on.\\nThat can happen if one data frame has “state” while the other has\\n“State” (case sensitive!). Or “year” vs. “yr”, “countrycode” vs.\\n“ccode”, etc.\\nSolution :prior to merging , rename the ID variable in one of the\\ntwo data frames, so that they have the same name.\\nProblem 2 : non-ID variables in each data frame unexpectedly have\\nthe same name. In this case, R will mistakenly think this variable is\\nan ID.\\nSolution :prior to merging , rename the non-ID variable in one of\\nthe two data frames, so that they have they DON’T same name\\nanymore.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 55}, page_content='What can go wrong while merging\\nUnique ID check : If the ID variables are not unique identiﬁers in\\neither data frame, then R places ALL combinations of matching\\nobservations in the merged data.\\nHere’s a simple example:\\nname x\\nA 2\\nA 7\\nA 1name y\\nA 6\\nA 5\\nA 8\\nThese two data frames share an ID variable named “name”. But\\nname is NOT a unique ID in either data frame since multiple rows\\nhave the same value of “A”.\\nThere’s no second ID variable like year that we can use to identify\\nthe rows.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 56}, page_content='What can go wrong while merging\\nIf we just go ahead and merge, the result is\\ndata3 <- full_join(data1, data2)\\nname x y\\nA 2 6\\nA 2 5\\nA 2 8\\nA 7 6\\nA 7 5\\nA 7 8\\nA 1 6\\nA 1 5\\nA 1 8'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 57}, page_content='What can go wrong while merging\\nR didn’t have enough information to match each row to a single\\nrow in the other data frame, so it matched each row to every\\npossible row. That’s not what we want.\\nTo check whether this is a problem :\\n1. Create a second data frame with just the ID variables\\n2. Use the unique() command to keep only the non-repeated\\nrows\\n3. Use nrow() to compare the number of rows of the unique ID\\ndata frame to the number of rows in the original data. If\\nthese numbers are the same, then the ID variables are unique.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 58}, page_content='What can go wrong while merging\\nFor example, to check whether country andyear are unique IDs\\nin this data frame:\\ncountry year x y\\nUSA 2014 0.53 0.26\\nUSA 2015 -1.12 -1.68\\nUSA 2016 0.75 -1.24\\nChina 2014 -0.01 0.17\\nChina 2015 -0.09 0.14\\nChina 2016 -0.32 0.07\\nRussia 2014 1.49 0.83\\nRussia 2015 -0.96 1.04\\nRussia 2016 2.50 -0.43'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 59}, page_content='What can go wrong while merging\\n(1)Create a second data frame with just the ID variables\\ndata.temp <- select(data1, country, year)\\n(2)Use the unique() command to keep only the non-repeated\\nrows\\ndata.temp <- unique(data.temp)\\n(3)Usenrow() to compare the number of rows of the unique ID\\ndata frame to the number of rows in the original data. If these\\nnumbers are the same, then the ID variables are unique.\\n> nrow(data.temp)\\n[1] 9\\n> nrow(data1)\\n[1] 9\\nBecause the numbers of rows are equal, country and year are\\nunique IDs.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 60}, page_content='What can go wrong while merging\\nIDs don’t always have to be unique. In this example:\\ncountry year x y\\nUSA 2014 0.53 0.26\\nUSA 2015 -1.12 -1.68\\nUSA 2016 0.75 -1.24\\nChina 2014 -0.01 0.17\\nChina 2015 -0.09 0.14\\nChina 2016 -0.32 0.07\\nRussia 2014 1.49 0.83\\nRussia 2015 -0.96 1.04\\nRussia 2016 2.50 -0.43country w z\\nUSA 0.99 -1.27\\nChina 1.06 0.13\\nRussia 1.65 0.72\\nWe merge on country . We didn’t expect country to be unique in\\nthe ﬁrst data frame, just the second. That’s okay! Make sure the\\nresults of this test match your expectations.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 61}, page_content='What can go wrong while merging\\nID value check : It’s common for two diﬀerent datasets to code\\nthe same observations with slightly diﬀerent IDs. Some examples:\\n▶“District of Columbia” vs. “DC”\\n▶“South Korea” vs. “S. Korea” vs. “ROK”\\n▶1998 vs 98\\nIf you don’t catch these discrepancies, the data won’t get\\nmatched for this observation .\\nAlso, two datasets might not cover the exact same cases. Or they\\nmight use diﬀerent time frames. You have to decide whether or\\nnot that’s okay.\\nSometimes data contain thousands of unique IDs. That’s too\\nmany to read through manually. We need a reliable way to identify\\nthe unmatched IDs.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 62}, page_content='What can go wrong while merging\\nIf we merge two data frames, data1 anddata2 , there are two\\nways for an ID value to be unmatched:\\n▶An ID appears in data1 but not data2 , or\\n▶An ID appears in data2 but not data1\\nWe can look at each type of unmatched ID separately.\\nThe anti join() command is a kind of “reverse merging”. It\\ndrops the matched observations and leaves the unmatched ones.\\nSpeciﬁcally:\\n▶anti join(data1, data2) keeps only observations in\\ndata1 that have no match in data2\\n▶anti join(data2, data1) keeps only observations in\\ndata2 that have no match in data1'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 63}, page_content='What can go wrong while merging\\nTo identify the unmatched observations, type:\\ncheck1 <- anti_join(data1, data2, by=c(\"id1\", \"id2\"))\\ncheck2 <- anti_join(data2, data1, by=c(\"id1\", \"id2\"))\\nUnlike full join() , these commands require the byargument,\\nwhich takes a character vector with the names of the ID variables\\nto match on. (If there’s only one ID, type its name in quotes\\nwithout the c() command.)\\nNow check1 has all the observations in data1 that have no match\\nindata2 , and check2 has all the observations in data2 that have\\nno match in data1 .\\nYou can use View() to see these observations, and nrow() to\\ncount the number of observations in check1 andcheck2 . If both\\nhave 0 observations, then every observation was matched .'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 64}, page_content='Sorting rows\\nSorting means rearranging the rows/columns in a way that does\\nnot break any row or column apart.\\nYou can sort rows based on the values of one variable numerically\\n(from smallest to largest, or from largest to smallest) or\\nalphabetically .\\nYou can move the columns to appear in any order you like.\\nThese steps are mostly cosmetic (they won’t change statistical\\nresults) but they make the data much easier to look at .'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 65}, page_content='Sorting rows\\nTo sort observations, use the arrange() function.\\nExample : The state legislature data lists estimates of left/right\\nideologies for U.S. state legislatures from 1993-2014. To sort the\\nrows from the most liberal (smallest value) to the most\\nconservative (largest value) state house, type\\nstateleg <- arrange(stateleg, hou_chamber)\\nTo sort from most conservative (largest value) to most liberal\\n(smallest value), use a minus sign in front of the sorting variable:\\nstateleg <- arrange(stateleg, -hou_chamber)\\nSometimes I get an error when I use the minus sign. If that\\nhappens, this should work instead:\\nstateleg <- arrange(stateleg, desc(hou_chamber))'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 66}, page_content='Sorting rows\\nYou can specify more than one sorting variable:\\n1. The data are sorted by the ﬁrst variable.\\n2. If there are ties , they are broken by sorting the second\\nvariable within values of the ﬁrst.\\n3. The third variable breaks ties with the ﬁrst two variables, and\\nso on.\\nTo sort alphabetically by state name, then sort the observations\\nfrom the same state by year, type\\nstateleg <- arrange(stateleg, st, year)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 67}, page_content='Deleting Objects\\nThe memory that R sets aside for all the objects you load and\\ncreate is called the workspace.\\nTo see all of the objects that currently exist in the workspace, type\\nls() .\\nTo delete an object, use rm() . To delete an object named data ,\\ntype\\nrm(data)\\nTo delete three objects named data ,polity , and cow, type\\nrm(list=c(\"data\", \"polity\", \"cow\"))\\nNote : you need to put object names in quotes here.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 68}, page_content='Deleting Objects\\nWhen you start a new R session (by closing and restarting R),\\nthere are no objects in the workspace.\\nSometimes you switch from working on one project to another.\\nWhen you do, it might be a good idea to delete ALL objects in\\nthe workspace . That way, there’s less chance of confusing new\\nobjects with old ones. To delete all objects, type\\nrm(list=ls())\\nSometimes this command is compared to clear in Stata. It’s\\nsimilar, but two big diﬀerences:\\n1.clear closes datasets only, rm(list=ls()) removes any\\nobject.\\n2.clear closes a ﬁle without saving it. rm(list=ls())\\nDELETES objects.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 69}, page_content='Saving and Loading the Workspace\\nThe workspace (the set of all of the objects you’ve made) is\\ntemporary.\\nBut, when you close R and R Studio, they always ask you if you\\nwant to save the “workspace image”, even when there’s nothing\\nin it .\\nYou can also save the whole workspace without being prompted by\\ntyping\\nsave(list=ls(), file=\"filename.Rdata\")\\nIf you save the workspace, it gets saved as an .Rdata ﬁle in your\\nworking directory.\\nThis is NOT the same as a data ﬁle. It can only be read by R\\nand R Studio. It contains many objects, potentially, not just data\\nframes.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 70}, page_content='Managing the workspace\\nYou can save just a few of the objects by typing something like\\nsave(list=c(\"data\", \"polity\", \"cow\"),\\nfile=\"filename.Rdata\")\\nCaution : if “ﬁlename.Rdata” already exists in the working\\ndirectory, this command OVERWRITES it. Be very careful if the\\nraw data is stored in an .Rdata ﬁle.\\nYou can load a saved workspace when you start a new R session by\\ntyping\\nload(\"filename.Rdata\")\\nNow all the objects you had saved are in the workspace again.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 71}, page_content='Managing the workspace\\nI don’t recommend relying on the save() andload() commands\\nor.Rdata ﬁles. Here’s why :\\n▶.Rdata ﬁles are speciﬁc to R and do not transfer to other\\nprograms.\\n▶.Rdata ﬁles are too easy to overwrite .\\n▶.Rdata ﬁles encourage saving data in disparate objects\\ninstead of in a clean dataset.\\nAvoiding .Rdata ﬁles is counter-intuitive since it’s natural to\\nequate .Rdata ﬁles to .dta ﬁles for Stata or .xls ﬁles for Excel.\\nI use .Rdata ﬁles only when there is a very speciﬁc reason for\\nsaving objects instead of a data frame.\\nMuch more often, I use commands to load and save data in ASCII\\nformat ﬁles .'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 72}, page_content='Opening and Saving ASCII Data Files\\nBy default, read csv() assumes row 1 contains the variable\\nnames. But if the data does not have names in row 1, type\\ndata <- read_csv(\"data.csv\", col_names=FALSE)\\nTo replace the given variable names with names you choose , type\\nsomething like\\ndata <- read_csv(\"data.csv\",\\ncol_names=c(\"id\", \"vote\", \"age\"))\\nSometimes data ﬁles have a few lines of text at the top with\\ninformation like authors, title, grant number, etc. To skip 3 lines\\nat the top, type\\ndata <- read_csv(\"data.csv\", skip=3)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 73}, page_content='Opening and Saving ASCII Data Files\\nSometimes certain rows in the ASCII ﬁle are commented out with\\nsome symbol the data’s authors chose. To avoid reading rows that\\nare marked with % at the beginning as data, type\\ndata <- read_csv(\"data.csv\", comment=\"%\")\\nYou might need to convert missing codes. For example, Stata\\nmarks missing values as .and R marks them as NA. To read the .\\nmarkers as missing, type\\ndata <- read_csv(\"data.csv\", na=\".\")'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 74}, page_content='Opening and Saving ASCII Data Files\\nread csv2() is for semi-colon separated ﬁles.\\nread tsv() is for tab separated ﬁles.\\nread table() is for ﬁles where datapoints are separated by white\\nspace, not necessary tab.\\nread fwf() is for ﬁxed width ﬁles. You will have to also specify\\nwhich characters (counting left to right) correspond to which\\nvariables.\\nTo read data that is delimited in any other way (by &, for\\nexample), type\\ndata <- read_delim(\"data.txt\", delim=\"&\")'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 75}, page_content='Saving CSV data ﬁles\\nRemember the workﬂow : First, load the raw data. Then do stuﬀ.\\nEnd by saving the cleaned/edited data under a diﬀerent\\nname , and NEVER overwrite the original data.\\nTo save a data frame object data as a new CSV ﬁle, type\\nwrite_csv(data, \"different_name.csv\")\\nTo save a data frame object data as a new tab separated ﬁle, type\\nwrite_tsv(data, \"different_name.txt\")\\nThese two commands will be enough the vast majority of the time.\\nSeeR for Data Science for some commands for very special\\ncircumstances.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 76}, page_content='Arranging columns\\nYou can arrange columns from left to right. Mostly, this task just\\nmakes the data nicer to look at when you use View() .\\nNote, however, that it will change the column numbers . So\\nstateleg[,15] no longer refers to the same variable as before.\\nTo arrange the stateleg data so that state name comes ﬁrst,\\nthen year, then everything else, type:\\nstateleg <- select(stateleg, st, year, everything())\\nCareful : The select() command is also used to delete variables.\\nIf you forget to include everything() in the above command:\\nstateleg <- select(stateleg, st, year)\\nthen every variable EXCEPT for standyear gets deleted.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 77}, page_content='Missing data\\nA data point is missing if for some reason we can’t know what it\\nis. R denotes missing values as NA.\\n“Missing values are contagious. ” Since we don’t know what an\\nNAis, we can’t know what operations with an NAare either:\\n1 + NA=NA\\nIf you want to see the mean of a variable, this might happen:\\n> mean(stateleg$hou_dem)\\n[1] NA\\nThat happened because the variable houdem has at least one\\nmissing value. To ignore missing values when using functions like\\nthe mean, use the na.rm=TRUE argument:\\n> mean(stateleg$hou_dem, na.rm=TRUE)\\n[1] -0.7418481'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 78}, page_content='Missing data\\nThe summary() command for a data frame will show you the\\nnumber of missing values for each variable.\\nTo see a giant matrix of TRUE/FALSE values, indicating whether\\neach data point is missing, type\\nis.na(stateleg)\\nToforcibly delete any row with at least one missing value, type\\nstateleg <- na.omit(stateleg)\\nIn general, I don’t recommend doing that. It’s heavy handed and\\nthere are better approaches to handling missing data.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 79}, page_content='Some notes on missing data\\nData may be missing for a lot of reasons. On a survey, respondents\\nmight say “don’t know,” may refuse to answer, or the question\\nmight not be applicable.\\nSometimes surveys will place a marker value to indicate a missing\\nvalue.(For example, using 998 for “don’t know”). If you do not\\nrecode these values, then it messes up mathematical functions like\\naverages.\\n“Missing values are contagious.”\\nAny mathematical function that includes even one missing value\\nwill be missing.\\n1 + NA = NA , 1 + NA̸= 1\\nIn other words, don’t confuse missing with 0! 0 is information,\\nand missing is a lack of information.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 80}, page_content='Missing values for continuous variables\\nWe can use logical statements to deal with situations in which\\nmissing values for continuous variables are given numeric codes.\\nFor example, in the data data, thermometer scores are continuous,\\ncoded 0 to 100 scales . But if a respondent says “don’t know”,\\nthe score is marked as 998. If we don’t replace these values, all\\nresults with these variables are thrown oﬀ.\\nThe logical statement data$ft obama == 998 returns a vector\\nof logical values: TRUE if the ftobama variable is 998, FALSE if\\nnot.\\nThis command replaces these values with NAs:\\ndata$ft_obama[data$ft_obama == 998] <- NA'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 81}, page_content='Recoding values\\nRecoding values means replacing many values of a categorical\\nvariable with new values, simultaneously.\\nThere are a few reasons why you may want to recode :\\n1. Change the labels of values. Make it easier to see and\\nremember what each value means. Better to code as \"Male\",\\n\"Female\" than 1, 2.\\n2. Replace missing codes with NA.\\n3. Change the order of categories for display in graphs, or in case\\nyou want to treat the variable as ordinal and categories are\\nout of order.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 82}, page_content='Labeling categorical values\\nSuppose you have a categorical variable with values 1, 2, 3, 4, 8\\nand 9 .\\nYou look in the data’s codebook (hopefully it has one) and see that\\nCategory Meaning\\n1 I speak Spanish primarily\\n2 I speak both Spanish and English equally\\n3 I speak English primarily but can speak Spanish\\n4 I can not speak Spanish\\n8 refused\\n9 skipped\\nWe can replace the numeric values with their written meanings,\\nwithout changing the way the categorical data is treated in R.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 83}, page_content='Using fctrecode()\\nThere are many ways to replace numeric categories with their\\nwritten meanings. The easiest method uses the fctrecode()\\nfunction from the forcats package (one of the tidyverse ).\\nHere’s an example of how to use fctrecode() :\\nLet’s break down the elements of this code:'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 84}, page_content='Using fctrecode()\\nThis function must be only ever used inside the mutate()\\nfunction.\\nType mutate() , then the data frame, then the name of the\\ncategorical variable you are editing, then an equal sign.\\nParentheses will appear automatically and will indent correctly\\nwhen you push enter. Leave the closing parentheses alone.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 85}, page_content='Using fctrecode()\\nThen type fctrecode()\\nThe ﬁrst argument of fctrecode() is the categorical variable,\\nwhich must be of the factor class. If it is not (here it is numeric),\\nuseas.factor() to coerce the variable:\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 86}, page_content='Using fctrecode()\\nThe press enter, and on each new line write the new categorical\\ntext label, in quotes, equal to the old categorical label, also in\\nquotes.\\nRemember, as with the rename() function: new ﬁrst, then old .\\nThis code works whether the old labels are numbers or text.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 87}, page_content='Using fctrecode()\\nFinally, for the categories you want to set to be missing , write the\\nnew category labels as NULL , with no quotes:\\nThis code is more space-consuming than other approaches. But\\nthe advantage is that we can more easily keep track of the new\\nand old categories, minimizing the risk of confusing which label\\ngoes with which number.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 88}, page_content='Using fctrecode()\\nOne more nice thing that fctrecode() can do: combine\\ncategories.\\nTo combine two old categories into the same new category, just\\nuse the same new category label for multiple old categories.\\nFor example, suppose we want to group every category in which a\\nperson knows at least some Spanish as Yes, and the category in\\nwhich a person knows no Spanish as No. We can write:\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 89}, page_content='Reordering categories\\nThe categories of a factor variable have a built-in order. The\\norder controls a few things:\\n1. The order of the categories appear in any table\\n2. The order the categories appear left-to-right in any graph\\n3. The meaning of the variable when used in a regression model\\nSometimes the categories have a natural ordering: for example, we\\ncan arrange categories in order of how much Spanish a person\\nspeaks.\\nSometimes the categories don’t have a natural ordering, but it\\nmakes sense to choose a particular order because it makes a table\\nor graph looks better, or to change the base category in a\\nregression.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 90}, page_content='Reordering categories\\nTo change the order, use the fctrelevel() function. It works a\\nlot like fctrecode() , only instead of writing old categories,\\nsimply write the existing categories in the order you want.\\nFor example:\\nWe’ll talk more about why it matters to change the order in more\\ndetail over the next few weeks.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 91}, page_content='Reordering categories\\nAlso, note that both the fctrecode() and fctrelevel()\\nfunctions can be called within the same call to mutate() :\\nIf you have multiple categorical variables to edit in this way, you\\ncan place all the calls to fctrecode() and fctrelevel() in\\nthe same mutate() command.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 92}, page_content='Combining categories across multiple variables\\nSometimes a survey will store categorical data in multiple\\nvariables.\\nFor example:\\n▶Variable 1 : are you a Democrat, Republican, or neither?\\n▶Variable 2 : (if Democrat/Republican) are you a strong\\nDemocrat/Republican?\\n▶Variable 3 : (if neither) do you lean towards one party or the\\nother?\\nTo combine categorical variables, use the unite() command.\\ndata <- unite(data, pid, pid1d, pid1r, pidstr, pidlean)\\nIt pastes categories from diﬀerent variables together. THEN you\\ncan recode these pasted categories.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 93}, page_content='Managing string/character variables\\nTo split up a string into a list of fragments of the string, use the\\nstrsplit() command.\\n> str_split(response, pattern=\" \")\\n[[1]]\\n[1] \"I\" \"have\" \"the\" \"faintest\" \"idea.\"\\n[6] \"I\" \"really\" \"don’t\" \"watch\" \"the\"\\n[11] \"news\"\\nYou can break the string on any kind of character you type with\\nthepattern argument:\\n> str_split(response, pattern=\"’\")\\n[[1]]\\n[1] \"I have the faintest idea. I really don\"\\n[2] \"t watch the news\"'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 94}, page_content='Managing string/character variables\\nOne annoying thing about how strings are coded sometimes is\\nextraneous white space before and after the text. To remove\\nleading and trailing spaces , use the strtrim() command:\\n> hello <- c(\" whatsup? \")\\n> hello\\n[1] \" whatsup? \"\\n> str_trim(hello)\\n[1] \"whatsup?\"'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 95}, page_content='Managing string/character variables\\nOne of the most useful ways to pull data from a string is to search\\nfor particular words. The strdetect() command returns TRUE\\nif the word is found in the string, FALSE if not. The match must\\nbe exact, and it is case sensitive.\\n> str_detect(response, \"news\")\\n[1] TRUE\\n> str_detect(response, \"have\")\\n[1] TRUE\\n> str_detect(response, \"haven’t\")\\n[1] FALSE\\nThe strdetect() function can also take regular expressions\\ninstead of a single word. We’ll cover that in more detail later in\\nthe semester.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 96}, page_content='Managing string/character variables\\nYou can also search for multiple patterns at the same time with\\nstrdetect() , but the code is a bit strange. There are two ways\\nto proceed.\\nFirst, you can separate diﬀerent words or terms with the |symbol\\ninside the quotes. This only works if there are no spaces or\\ncarriage returns next to any|symbol. So if you are looking for\\none of “larry”, “curly”, and “moe” in the text:\\nThis works: strdetect(response, \"larry|curly|moe\")\\nYou will get a variable that is TRUE if the text contains ”larry” or\\n”curly” or ”moe”\\nDoesn’t work:\\nstrdetect(response, \"larry | curly | moe\")\\nYou won’t get an error but the TRUE and FALSE values won’t be\\nassigned correctly.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 97}, page_content='Managing string/character variables\\nIf you have a long list of words, it’s better to use the following\\ntechnique:\\n1. Put all of the words you want to search for in a character vector\\nobject\\n2. Instead of a word in quotes, type\\npaste(object, collapse = \"|\")\\nwhere object is the vector you made in step 1.\\nThen the variable you create will be TRUE if any of the words in\\nyour vector are present.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 98}, page_content='Managing string/character variables\\nFor example, to search for the names of UVA men’s basketball\\nplayers in a variable named text , type:\\nplayers <- c(\"Marco Anthony\", \"Francesco Badocchi\",\\n\"Francisco Caffaro\", \"Kihei Clark\",\\n\"Mamadi Diakite\", \"Kyle Guy\", \"Jay Huff\",\\n\"De’Andre Hunter\", \"Ty Jerome\",\\n\"Austin Katstra\", \"Braxton Key\",\\n\"Jayden Nixon\", \"Jack Salt\", \"Kody Stattmann\")\\ndata <- mutate(data, mention_player =\\nstr_detect(text, paste(players, collapse=\"|\")))\\nThe variable mention player will be TRUE if text contains any of\\nthese names, and FALSE otherwise.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 99}, page_content='Managing date/time variables\\nThere are lots of data in political science that involve dates and\\ntimes. Unfortunately, there is NOT a consistent way that dates\\nand times are recorded in data.\\nProblem 1 : Sometimes a single date/time is recorded in many\\nvariables: year, month, day, hour, minute might all be separate\\ncolumns .\\nProblem 2 : Sometimes a date/time is recorded in one variable as a\\nmessy and unusable block of text .\\nHow do we deal with each type of coding? How do we get R to\\nrecognize that the variable is a date or time?'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 100}, page_content='Managing date/time variables\\nAs with anything in R, there are many ways to do the same thing.\\nHere we will use the lubridate package:\\nlibrary(lubridate)\\nProblem 1 : the date and time are stored in many variables:\\nyear1 <- 1983; month1 <- 4; day1 <- 14\\nyear2 <- 2018; month2 <- 2; day2 <- 22\\nWe can use the make datetime() command to create single\\nvariables from multiple variables. We specify which variables\\ncontain the year, month, day, etc.\\ndate1 <- make_datetime(year=year1, month=month1, day=day1)\\ndate2 <- make_datetime(year=year2, month=month2, day=day2)\\nWe can also add the hour ,minute , and second arguments if we\\nhave that information.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 101}, page_content='Managing date/time variables\\nNow date1 anddate2 are recognized as dates:\\n> date1\\n[1] \"1983-04-14 UTC\"\\n> date2\\n[1] \"2018-02-22 UTC\"\\nThe class of a date variable is called POSIXct andPOSIXt . It’s a\\ntechnical name, but it just means “date and time”.\\n> class(date1)\\n[1] \"POSIXct\" \"POSIXt\"'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 102}, page_content='Managing date/time variables\\nNow that we have two date variables, we can use basic arithmetic\\nfunctions with them. We can for example measure the length of\\ntime between the two dates:\\n> duration <- date2 - date1\\n> duration\\nTime difference of 12733 days\\n> as.numeric(duration)/365\\n[1] 34.88493\\nThe make datetime() command can work within the mutate()\\ncommand to create new variables.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 103}, page_content='Managing date/time variables\\nProblem 2 : pulling date and time information from a single, messy\\nvariable.\\ntime1 <- \"2018-02-22 10:05:28\"\\ntime2 <- \"02-22-2018 10:05:28\"\\ntime3 <- \"22-02-2018 10:05:28\"\\nThese variables contain the year, month, day, hour, minute, and\\nsecond, all in one variable. They are also currently read as\\ncharacter. How do we get R to read these as dates and times ,\\nand to recognize the correct year, month, etc.?\\nThe ymdhms() command assumes that the data contains the\\nyear, month, day, hour, minute, and second in that order:\\nymd_hms(time1)\\n[1] \"2018-02-22 10:05:28 UTC\"'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 104}, page_content='Managing date/time variables\\nThe time2 variable is diﬀerent: it lists the MONTH ﬁrst, then the\\nDAY, then the YEAR. If we use the ymdhms() command, we\\nparse incorrectly:\\n> ymd_hms(time2)\\n[1] NA\\nWarning message:\\nAll formats failed to parse. No formats found.\\nThe error occurs because it reads 2018 as the day, and that can’t\\nhappen.\\nInstead we can use the mdyhms() command:\\n> mdy_hms(time2)\\n[1] \"2018-02-22 10:05:28 UTC\"'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 105}, page_content='Managing date/time variables\\nThere are many of these commands, and they diﬀer only in the\\norder you write the letters y,m, and d.\\nThese commands automatically recognize the diﬀerent characters\\n(hyphens, slashes, etc.) separate elements of the date, so no need\\nto worry about specifying that.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 106}, page_content='Piping\\nAdvanced R programmers know how to use a pipe.\\n'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 107}, page_content='Piping\\nA pipe is a way to connect consecutive R functions that\\nmanipulate the same data frame.\\nThe code for a pipe is %>%at the end of one command. It means\\n“apply the next command to the data output by this\\ncommand .” Pipes are implemented in the magrittr package, one\\nof the packages included with tidyverse .\\nUsing the pipe is completely optional . It saves some time and\\nspace, and it makes you look like an advanced programmer.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 108}, page_content='Piping\\nHere are some data cleaning commands you know:\\ndata <- read_csv(\"data.csv\")\\ndata <- select(data, vote, age, sex, starts_with(\"party\"))\\ndata <- arrange(data, age)\\ndata <- filter(data, party_ID==\"republican\")\\ndata <- mutate(data, vote = fct_recode(vote,\\n\"Trump\" = \"1. D Trump\",\\n\"Clinton\" = \"2. H Clinton\",\\nNULL = \"3. G Johnson\",\\nNULL = \"4. J Stein\",\\nNULL = \"5. Other\"))\\nNotice how every command begins by calling the data frame we\\nwant to alter.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 109}, page_content='Piping\\nThe exact same commands can be run with the following code:\\ndata <- read_csv(\"data.csv\")\\ndata <- data %>%\\nselect(vote, age, sex, starts_with(\"party\")) %>%\\narrange(age) %>%\\nfilter(party_ID==\"republican\") %>%\\nmutate(vote = fct_recode(vote,\\n\"Trump\" = \"1. D Trump\",\\n\"Clinton\" = \"2. H Clinton\",\\nNULL = \"3. G Johnson\",\\nNULL = \"4. J Stein\",\\nNULL = \"5. Other\"))\\nNotice that the select() ,arrange() ,filter() , and mutate()\\nfunctions no longer need the data frame listed ﬁrst.\\nPipes can be used with any function that takes a data frame as\\nthe ﬁrst argument .'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 110}, page_content='Using group by()\\nSometimes it is necessary to do calculations within groups of\\nobservations. Consider these example data (saved as object\\ntable1 in R once you load tidyverse ):\\ncountry year cases population\\nAfghanistan 1999 745 19987071\\nAfghanistan 2000 2666 20595360\\nBrazil 1999 37737 172006362\\nBrazil 2000 80488 174504898\\nChina 1999 212258 1272915272\\nChina 2000 213766 1280428583\\nWe might want to create a variable with the total number of cases\\nper year, or the average population by country.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 111}, page_content='Using group by()\\nThe group by() function doesn’t do anything by itself, but it\\nallows you to use mutate() orsummarize() to perform\\ncalculations in groups.\\nWithin group by() , specify the variable or variables that denote\\nthe groups.\\nmutate() performs within-group calculations within the existing\\ndata frame.\\nsummarize() deletes rows, and leaves you with one row per\\ngroup .\\nTo turn oﬀ within-group calculations, use the ungroup function.'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 112}, page_content='Using group by()\\nTo calculate the total number of cases per year in table1 use this\\ncode:\\ntable2 <- table1 %>%\\ngroup_by(year) %>%\\nmutate(totalcases = sum(cases))\\nThe data now look like this:\\ncountry year cases population totalcases\\nAfghanistan 1999 745 19987071 250740\\nAfghanistan 2000 2666 20595360 296920\\nBrazil 1999 37737 172006362 250740\\nBrazil 2000 80488 174504898 296920\\nChina 1999 212258 1272915272 250740\\nChina 2000 213766 1280428583 296920'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 113}, page_content='Using group by()\\nTo do this calculation leaving only one row per year, use\\nsummarize() instead of mutate() :\\ntable2 <- table1 %>%\\ngroup_by(year) %>%\\nsummarize(totalcases = sum(cases))\\nThe data now look like this:\\nyear totalcases\\n1999 250740\\n2000 296920'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 114}, page_content='Using group by()\\nTo calculate the average population for each country in table1\\nuse this code:\\ntable2 <- table1 %>%\\ngroup_by(country) %>%\\nmutate(avg.pop = mean(population))\\nThe data now look like this:\\ncountry year cases population avg.pop\\nAfghanistan 1999 745 19987071 20291216\\nAfghanistan 2000 2666 20595360 20291216\\nBrazil 1999 37737 172006362 173255630\\nBrazil 2000 80488 174504898 173255630\\nChina 1999 212258 1272915272 1276671928\\nChina 2000 213766 1280428583 1276671928'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 115}, page_content='Using group by()\\nTo do this calculation leaving only one row per country, use\\nsummarize() instead of mutate() :\\ntable2 <- table1 %>%\\ngroup_by(country) %>%\\nsummarize(avg.pop = mean(population))\\nThe data now look like this:\\ncountry avg.pop\\nAfghanistan 20291216\\nBrazil 173255630\\nChina 1276671928'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 116}, page_content='Using group by()\\nungroup turns oﬀ within-group calculations. To calculate the\\npercent of cases that each country accounts for:\\ntable2 <- table1 %>%\\ngroup_by(year) %>%\\nmutate(totalcases = sum(cases)) %>%\\nungroup %>%\\nmutate(percent_of_cases = 100*cases/totalcases)\\nThe data now look like this:\\ncountry year cases population totalcases percent\\nAfghanistan 1999 745 19987071 250740 0.297\\nAfghanistan 2000 2666 20595360 296920 0.898\\nBrazil 1999 37737 172006362 250740 15.05\\nBrazil 2000 80488 174504898 296920 27.11\\nChina 1999 212258 1272915272 250740 84.65\\nChina 2000 213766 1280428583 296920 71.99'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidydata_reference_Thursday_II.pdf', 'page': 117}, page_content='Using count()\\nThe count() function counts the number of rows within each\\ngroup. It’s not necessary to use group by() when calling\\ncount() .\\nSuppose we have data with terrorist attacks, with variables year\\nandcountry . To create a dataset with a count of the total\\nattacks by country , sorted from most to least, type\\nattacks2 <- attacks %>%\\ncount(country, sort=TRUE)\\nTo create a dataset with a count of the total attacks by in each\\ncountry and year , sorted from most to least, type\\nattacks2 <- attacks %>%\\ncount(country, year, sort=TRUE)'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidy_data_dplyr.pdf', 'page': 0}, page_content='Fun with functions and dplyr\\nBrian Wright\\n1/24/2020\\nBrian Wright Fun with functions and dplyr 1/24/2020 1 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidy_data_dplyr.pdf', 'page': 1}, page_content='Overview of Functions (Advanced R)\\nFunctions are at the core of R language, it’s really a function based\\nlanguage\\n“R, at its heart, is a”functional\" language. This means that it has\\ncertain technical properties, but more importantly that it lends\\nitself to a style of problem solving centred on functions.\" Hadley\\nWickham\\nBrian Wright Fun with functions and dplyr 1/24/2020 2 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidy_data_dplyr.pdf', 'page': 2}, page_content='What is a functional based language?\\nRecently functions have grown in popularity because they can produce\\nefficient and simple solutions to lots of problems. Many of the\\nproblems with performance have been solved.\\nFunctional programming compliments object oriented programming\\nBrian Wright Fun with functions and dplyr 1/24/2020 3 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidy_data_dplyr.pdf', 'page': 3}, page_content='What makes a programming approach “functional”?\\nFunctions can behave like any other data structure\\n▶Assign them to variables, store to lists, pass them as aurguments to\\nother functions, create them inside functions and even produce a\\nfunction as a result of a funcion\\nFunctions need to be “pure” meaning that if you call it again with the\\nsame inputs you get the same results. sys.time() not a “pure”\\nfunction\\nThe execution of the function shouldn’t change global variables, have\\nno side effects.\\nBrian Wright Fun with functions and dplyr 1/24/2020 4 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidy_data_dplyr.pdf', 'page': 4}, page_content='Functions\\nFunction don’t have to be “pure” but it can help to ensure your code\\nis doing what you intend it to do.\\nFunctional programming helps to break a problem down into it’s\\npieces. When working to solve a problem it helps to divide the code\\ninto individually operating functions that solve parts of the problem.\\nBrian Wright Fun with functions and dplyr 1/24/2020 5 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidy_data_dplyr.pdf', 'page': 5}, page_content='Types of Functions\\nFigure 1: Function Types\\nBrian Wright Fun with functions and dplyr 1/24/2020 6 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidy_data_dplyr.pdf', 'page': 6}, page_content='Let’s Build a Function\\nBasically recipes composed of series of R statements\\nname <- funtion (variables){\\n#In here goes the series of R statements\\n}\\nBrian Wright Fun with functions and dplyr 1/24/2020 7 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidy_data_dplyr.pdf', 'page': 7}, page_content='Example, talk out the steps\\nmy_mean <- function (x){\\nSum <- sum(x)#Here we are using a function\\n#inside a function!\\nN <- length (x)\\nreturn (Sum /N)#return is optional but helps with\\n#clarity on some level.\\n}\\nCreate a little list and pass it to the function and see if it works.\\nAlso call the Sum and N variables...does this work?\\nBrian Wright Fun with functions and dplyr 1/24/2020 8 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidy_data_dplyr.pdf', 'page': 8}, page_content='Functional - Will show later, Function Factory\\n(Advanced R)\\npower1 <- function (exp) {\\nfunction (x) {\\nx^exp\\n}\\n}\\n#Assigning the exponentials\\nsquare <- power1 (2)\\ncube <- power1 (3)\\nBrian Wright Fun with functions and dplyr 1/24/2020 9 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidy_data_dplyr.pdf', 'page': 9}, page_content='Run the Created Functions\\nsquare (3)\\n> [1] 9\\ncube (3)\\n> [1] 27\\nBrian Wright Fun with functions and dplyr 1/24/2020 10 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidy_data_dplyr.pdf', 'page': 10}, page_content='Quick Exercise\\nCreate a function that computes the range of a variable and then\\nfornogoodreasonadds100anddividesby10. Writeoutthesteps\\nyou would need first in Pseudocode, then develop the function.\\nBrian Wright Fun with functions and dplyr 1/24/2020 11 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidy_data_dplyr.pdf', 'page': 11}, page_content='dplyrverbs in the tidyverse\\nThe dplyrpackage gives us a few verbs for data manipulation\\nFunction Purpose\\nselect Select columns based on name or position\\nmutate Create or change a column\\nfilter Extract rows based on some criteria\\narrange Re-order rows based on values of variable(s)\\ngroup_by Split a dataset by unique values of a variable\\nsummarize Create summary statistics based on columns\\nBrian Wright Fun with functions and dplyr 1/24/2020 12 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidy_data_dplyr.pdf', 'page': 12}, page_content='select\\nYou can select columns by name or position, of course.\\nYou can also select columns based on some criteria, which are\\nencapsulated in functions.\\nstarts_with(“ \"), ends_with(\" ”), contains(“____”)\\none_of(“____”,“_____”,“______”)\\nThere are others; see help(starts_with) .\\nBrian Wright Fun with functions and dplyr 1/24/2020 13 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidy_data_dplyr.pdf', 'page': 13}, page_content='Example\\nLoad the weather.csv . This contains daily temperature data in 2010 for\\nsome location.\\n> [1] \"C:/Users/Brian Wright/Documents/git_3001/DS-4001/2_R_function_basics\"\\nhead (weather, 2)\\n> # A tibble: 2 x 35\\n> id year month element d1 d2 d3 d4 d5 d6 d7 d8\\n> <chr> <int> <int> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\\n> 1 MX17~ 2010 1 tmax NA NA NA NA NA NA NA NA\\n> 2 MX17~ 2010 1 tmin NA NA NA NA NA NA NA NA\\n> # ... with 23 more variables: d9 <lgl>, d10 <dbl>, d11 <dbl>, d12 <lgl>,\\n> # d13 <dbl>, d14 <dbl>, d15 <dbl>, d16 <dbl>, d17 <dbl>, d18 <lgl>,\\n> # d19 <lgl>, d20 <lgl>, d21 <lgl>, d22 <lgl>, d23 <dbl>, d24 <lgl>,\\n> # d25 <dbl>, d26 <dbl>, d27 <dbl>, d28 <dbl>, d29 <dbl>, d30 <dbl>, d31 <dbl>\\nBrian Wright Fun with functions and dplyr 1/24/2020 14 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidy_data_dplyr.pdf', 'page': 14}, page_content='How would you just select the columns with the daily\\ndata?\\nselect (weather, starts_with (\"d\"))\\nBrian Wright Fun with functions and dplyr 1/24/2020 15 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidy_data_dplyr.pdf', 'page': 15}, page_content='mutate\\nmutatecan either transform a column in place or create a new column in\\na dataset\\nWe’ll use the in-built mpgdataset for this example, We’ll select only the\\ncity and highway mileages. To use this selection later, we will need to\\nassign it to a new name\\nmpg1 <- select (mpg, cty, hwy)\\nBrian Wright Fun with functions and dplyr 1/24/2020 16 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidy_data_dplyr.pdf', 'page': 16}, page_content='mutate\\nWe’ll change the city and highway mileage to km/l from mpg. This will\\ninvolve multiplying it by 1.6 and dividing by 3.8\\nhead (mutate (mpg1, cty = cty *1.6 /3.8,\\nhwy = hwy *1.6/3.8), 5)\\n> # A tibble: 5 x 2\\n> cty hwy\\n> <dbl> <dbl>\\n> 1 7.58 12.2\\n> 2 8.84 12.2\\n> 3 8.42 13.1\\n> 4 8.84 12.6\\n> 5 6.74 10.9\\nThis is in-place replacement\\nBrian Wright Fun with functions and dplyr 1/24/2020 17 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidy_data_dplyr.pdf', 'page': 17}, page_content='New Variable Defined\\nmutate (mpg1, cty1 = cty *1.6/3.8, hwy1 = hwy *1.6/3.8)\\n> # A tibble: 234 x 4\\n> cty hwy cty1 hwy1\\n> <int> <int> <dbl> <dbl>\\n> 1 18 29 7.58 12.2\\n> 2 21 29 8.84 12.2\\n> 3 20 31 8.42 13.1\\n> 4 21 30 8.84 12.6\\n> 5 16 26 6.74 10.9\\n> 6 18 26 7.58 10.9\\n> 7 18 27 7.58 11.4\\n> 8 18 26 7.58 10.9\\n> 9 16 25 6.74 10.5\\n> 10 20 28 8.42 11.8\\n> # ... with 224 more rows\\nThis creates new variables\\nBrian Wright Fun with functions and dplyr 1/24/2020 18 / 28'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\Lectures\\\\\\\\tidy_data_dplyr.pdf', 'page': 18}, page_content='filter\\nfilterextracts rows based on criteria\\nfilter (mpg, cyl ==4)\\n> # A tibble: 81 x 11\\n> manufacturer model displ year cyl trans drv cty hwy fl class\\n> <chr> <chr> <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\\n> 1 audi a4 1.8 1999 4 auto(l~ f 18 29 p comp~\\n> 2 audi a4 1.8 1999 4 manual~ f 21 29 p comp~\\n> 3 audi a4 2 2008 4 manual~ f 20 31 p comp~\\n> 4 audi a4 2 2008 4 auto(a~ f 21 30 p comp~\\n> 5 audi a4 quat~ 1.8 1999 4 manual~ 4 18 26 p comp~\\n> 6 audi a4 quat~ 1.8 1999 4 auto(l~ 4 16 25 p comp~\\n> 7 audi a4 quat~ 2 2008 4 manual~ 4 20 28 p comp~\\n> 8 audi a4 quat~ 2 2008 4 auto(s~ 4 19 27 p comp~\\n> 9 chevrolet malibu 2.4 1999 4 auto(l~ f 19 27 r mids~\\n> 10 chevrolet malibu 2.4 2008 4 auto(l~ f 22 30 r mids~\\n> # ... with 71 more rows\\nThis extracts only 4 cylinder vehicles\\nOther choices might be cyl != 4 ,cyl > 4,year == 1999 ,\\nmanufacturer==\"audi\"Brian Wright Fun with functions and dplyr 1/24/2020 19 / 28'),\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"{len(pages)} Pages in the PDF\")\n",
    "pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview and Data Science\n",
      "Brian Wright\n",
      "brianwright@virginia.edu\n",
      "\n",
      "\n",
      "2Course Administration Everybody Reads Even Computers: Text Mining\n",
      "\n",
      "Final Projects\n",
      "Work individually and use one of the areas below to answer a broad questions related to a given dataset.\n",
      "I’ll provide several datasets for you to potential use, but you are also welcome to chose your own.\n",
      "You can also use any dataset from the class if you choose.\n",
      "Topics we will/have covered that can be a focus of the final project:\n",
      "Data Visualization Fairness/Bias\n",
      "Text Mining\n",
      "KNN\n",
      "Tree based methods\n",
      "Ensemble –Random Forrest – time permitting \n",
      "Final Projects\n",
      "Generate a publishable Rmarkdown document with the following sections:\n",
      "1.Question and background information on the data and why you are asking this question(s).\n",
      "References to previous research/evidence generally would be nice to include.\n",
      "2.Exploratory Data Analysis –Initial summary statistics and graphs with an emphasis on variables you believe to be important for your analysis.\n",
      "3.Methods –Techniques you are using to address your question and the results of those methods.\n",
      "4.Conclusions –What can you say about the results of the methods section as it relates to your question.\n",
      "5.Future work –What additional analysis is needed or what limited your analysis on this project.\n",
      "“Text Mining” \n",
      "Broader field: What is Exploratory Text Analytics?\n",
      "(ETA)\n",
      "Much of the following content is from the Exploratory Text Analytics Class as part of UVA’s MSDS taught by Rafael Alvarado\n",
      "6Text Mining \n",
      "ETA refers to text analytics applied to long -form texts with the purpose of surfacing their latent cognitive, cultural, and social content\n",
      "Texts: Novels, essays, newspaper articles, letters, blogs, journal articles, etc.\n",
      "Content : concepts, categories, themes, emotions, events ...\n",
      "7Text Mining\n",
      "\n",
      "Its called \"exploratory\" because it methods are primarily unsupervised and designed to support human -in-the-loop interpretation\n",
      "“interpretation support”\n",
      "8Text Mining\n",
      "\n",
      "\n",
      "\n",
      "From Kevin Murphy, 2012,  Machine Learning: A Probabilistic Perspective, p.\n",
      "9.\n",
      "Unsupervised learning is about knowledge discovery\n",
      "\n",
      "Some Unsupervised Methods:\n",
      "11Clustering —K-means, hierarchical, etc.\n",
      "Topic Modeling —PCA, LSI/A, NMF, LDA, etc.\n",
      "Word Embedding —SGNS (word2vec), etc.\n",
      "Sentiment Analysis --dictionary- based methods, etc.Text Mining\n",
      "\n",
      "Extracting features from unstructured data to support machine learning\n",
      "E.g.\n",
      "principal components are document features\n",
      "Information retrieval tasks such as document summarization, grouping, classification, and knowledge discovery\n",
      "Provide data to support language modeling, including grammar, syntax, and pragmatics for NLP and computational linguistics\n",
      "Extraction and representation of cultural and social patternsfrom text —\n",
      "See cultural analytics and culturomics\n",
      "12Text Mining: Applications of ETA\n",
      "\n",
      "13Coined by Harvard researchers Jean -Baptiste Michel and Erez Lieberman Aiden , who helped create Google’s NGram Viewer\n",
      "Michel and Aiden (2010):\n",
      "Inferences about culture made from trends in n- gram usageAn n- gram is a sequence of n words\n",
      "Based on Google Books\n",
      "Transformed the field of text analytics\n",
      "Based on application of genomic sequencing techniques to text ( See recent book, Uncharted)Text Mining: Culturomics\n",
      "\n",
      "14\n",
      "Two examples of inferences drawn from n- gram trends\n",
      "(Hand 2011) Text Mining\n",
      "\n",
      "https://books.google.com/ngrams\n",
      "15Text Mining\n",
      "\n",
      "\n",
      "ETA builds on the domain knowledge of textual theory and criticism from history, literary studies, anthropology, sociolinguistics, religious studies, etc.\n",
      "Text is regarded as a first -class object of study,\n",
      "not an incidental container of language data\n",
      "“We” study text as text\n",
      "Text is not necessarily language\n",
      "16Text Mining\n",
      "\n",
      "Text as Text: Langue and Parole\n",
      "Language (langue)\n",
      "Grammar\n",
      "Competence Finite rules (grammar)\n",
      "System\n",
      "Collective\n",
      "Unconscious\n",
      "Structure\n",
      "LatentSpeech (parole)\n",
      "Discourse\n",
      "Performance\n",
      "Indefinite patterns (discourse)\n",
      "Usage\n",
      "Individual\n",
      "Conscious\n",
      "Event\n",
      "Observed\n",
      "17\n",
      "\n",
      "\"Language\" is divided into grammar and discourse\n",
      "Discourse is expressed a speech and writing\n",
      "Writing is \"fixed discourse\"\n",
      "\n",
      "Writing is the direct entextualization of discourse in a document\n",
      "Documents have a material form (medium ) and an \"immaterial\" dimension --the text as structured sequence of symbols\n",
      "Text is not \"unstructured\"!\n",
      "20Text Mining: Some Substantive Properties of Text\n",
      "Above all, texts contain cultural information\n",
      "They function as social genes that encode and express beliefs, opinions, ideas, symbolism, etc.\n",
      "--think of Homer, the Bible, etc.\n",
      "As discourse, distinctive of human beings\n",
      "Texts may also represent events\n",
      "Social media and newspapers are like social sensors\n",
      "As more and more social life becomes entextualized through social media and other conduits (e.g.\n",
      "Internet of Things)\n",
      "Texts contain granular representations of human behavior\n",
      "It is the principal means by which behavioral surplus is captured\n",
      "\n",
      "So, culture is a complex system of human thought and behavior\n",
      "that exhibits a consistent pattern in society It is expressed and communicated\n",
      "by symbolic forms\n",
      "A  primary vehicle in our society for the expression and transmission of symbolic forms is the written word — texts\n",
      "A premise of ETA is that texts “contain” cultural patterns and these may be discovered  through unsupervised methods\n",
      "21\n",
      "\n",
      "ETA Related Fields (Antecedents)\n",
      "22Computational Linguistics (CL)\n",
      "Use of computers to represent and study human language\n",
      "Information Retrieval (IR)\n",
      "Document summarization, retrieval, indexing, classification based on contents and metadata\n",
      "Natural Language Processing (NLP)\n",
      "Get computers to understand and produce human language\n",
      "Text Mining (TM)\n",
      "Convert text -as-unstructured -data into features for data mining + ML\n",
      "Digital Humanities (DH) / Humanities ComputingCreate digital collections of primary textual sources and new forms of scholarship → 1949 Father Busa's Index Thomisticus\n",
      "\n",
      "23\n",
      "A genealogy of ETA\n",
      "\n",
      "Note that text mining (TM) and natural language processing (NLP) are not the same thing\n",
      "Although often used as synonyms, they have different concerns, approaches, and methods\n",
      "They are, however, closely related\n",
      "24Text Mining\n",
      "\n",
      "25Areas of Focus\n",
      "NLP\n",
      "Language models\n",
      "Tokenization\n",
      "Part of speech labeling\n",
      "Named entity recognition\n",
      "Dependency parsing Speech generationTM\n",
      "Text as structured data\n",
      "Document classification\n",
      "Content summarization\n",
      "Network analysis\n",
      "Knowledge discovery\n",
      "Hypothesis discoveryText Mining\n",
      "\n",
      "The functional relationship between NLP and TM\n",
      "26Text Mining\n",
      "\n",
      "27Text Mining: Implementation in R\n",
      "Couple of packages in R that specialize in using text data:\n",
      "tm –fairly popular (used often with the corpus package)\n",
      "quanteda –developed by political scientist to analyze politically oriented text data\n",
      "Tidytext –tidyverse of text analysis –this is what we will focus on this week.\n",
      "textmineR –developed mostly for topic modelling\n",
      "\n",
      "28Text Mining: Implementation in R\n",
      "The first step with conducting text analysis is getting the data loaded into R so we can tokenize the dataset Text data comes in a wide variety of forms and can be difficult to wrangle into a data frame.\n",
      "We are going to use dataset that are in CSV but note this is often not the case.\n",
      "Tokenization means that we take a block of text and separate it into separate observations for each\n",
      "word,\n",
      "combination of 2, 3, or 4 words,\n",
      "sentence,\n",
      "or paragraph.\n",
      "29Text Mining: Implementation in R\n",
      "The first step with conducting text analysis is getting the data loaded into R so we can tokenize the dataset Text data comes in a wide variety of forms and can be difficult to wrangle into a data frame.\n",
      "We are going to use dataset that are in CSV but note this is often not the case.\n",
      "Tokenization means that we take a block of text and separate it into separate observations for each\n",
      "word,\n",
      "combination of 2, 3, or 4 words,\n",
      "sentence,\n",
      "or paragraph.\n",
      "30Text Mining: Implementation in R\n",
      "Switch over to R\n",
      "\n",
      "31Text Mining: Sentiment Analysis\n",
      "Source: Text Mining with R\n",
      "\n",
      "32Text Mining: Sentiment Analysis\n",
      "Source: Text Mining with RSentiment analysis for our purposes considers text to composed of individual words that can have positive or negative meaning.\n",
      "The tidytext package provides access to several lexicons that can work to classify words in our documents in a variety of ways according to sentiment.\n",
      "Examples:\n",
      "AFINN provides a scale from -5 to 5 for included words\n",
      "NRC – classifies words in categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.\n",
      "Bing –Straight poss or neg\n",
      "Let’s take a look….back to R  \n",
      "33Text Mining: Topic Modelling\n",
      "Source: Laten Dirichlet Allocation in R: Martin PonweiserNext Step in the text journal is Topic Modelling\n",
      "Topic models are “[probabilistic] latent variable models of documents that exploit the correlations among the words and latent semantic themes” ( Blei and Lafferty, 2007).\n",
      "The name “topics” signifies the hidden, to be estimated, variable relations (=distributions)that link words in a vocabulary and their occurrence in documents.\n",
      "Essentially think of Topic Modelling as creating clusters of words that are associated with a set of similar documents in a corpus.\n",
      "As an example if you were to gather newspaper articles from across the country from three sections: Politics, Sports and Entertainment.\n",
      "If we ran LDA it would like classify the individual stories into these three topics.\n",
      "Machine Learning Overview, EDA and Clustering\n",
      "Brian Wright\n",
      "brianwright@virginia.edu\n",
      "\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "3Given  D :{2,4,10,12,3,11,20,25,30},  and  k=2 clusters\n",
      "Randomly assign the means:  m1=3, m2=4\n",
      "K1={2,3}, K2={4,10,12,11,20,25,30}, m1=2.5, m2=16\n",
      "K1={2,3,4}, K2={10,12,11,20,25,30}, m1=3, m2=18\n",
      "K1={2,3,4,10}, K2={12,11,20,25,30}, m1=4.75, m2=19.6\n",
      "K1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\n",
      "K1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\n",
      "Stop, since the clusters and the means found in all subsequent iterations will be the same .Example of K -Means\n",
      "\n",
      "1.What is Machine Learning?\n",
      "2.What is exploratory data analysis?\n",
      "3.k-means clustering\n",
      "–Does Congress vote in patterns?\n",
      "4.Multi -dimensional k -means clustering\n",
      "–Are NBA players compensated according to performance?Outline: Intro to Unsupervised ML\n",
      "4\n",
      "\n",
      "•Exploratory data analysis or “EDA” is an approach where the intent is to see what the data can tell us beyond modeling or hypothesis testing\n",
      "–Data visualization is one of the most common forms of EDAWhat is exploratory data analysis?\n",
      "5\n",
      "\n",
      "When data is too big or complex to be analyzed just by visualizing it, these types of analysis can help:\n",
      "1.Clustering: compare pieces of data by measuring similarity among them\n",
      "2.Network analysis: analyze how people, places and entities are connected to evaluate the properties and structure of a network 3.Text mining: analyze what large bodies of unstructured or structured text sayTypes of exploratory data analysis\n",
      "6\n",
      "\n",
      "The data inputs have (x)no target outputs (y)Unsupervised machine learning\n",
      "Input x:\n",
      "VoterOutput y:\n",
      "Not given\n",
      "(To be discovered)?\n",
      "We want to impose structure on the inputs (x)to say something meaningful about the data\n",
      "7\n",
      "\n",
      "\n",
      "1.Technique for finding similarity between groups\n",
      "2.Type of unsupervised machine learning\n",
      "•Not the only class of unsupervised learning        algorithms\n",
      "3.Similarity needs to be defined\n",
      "•Will depend on attributes of data\n",
      "•Usually a distance metricWhat is clustering?\n",
      "8\n",
      "Key assumption: data points that are “closer” together are related or similar\n",
      "\n",
      "•Haimowitz and Schwarz 1997 paper on clustering for credit line optimization\n",
      "–http://www.aaai.org/Papers/Workshops/1997/\n",
      "WS-97-07/WS97- 07-006.pdf\n",
      "•Cluster existing GE Capital customers based on similarity in use of their credit cards to pay bills and customers’ profitability to GE Capital\n",
      "•Resulted in five clusters of consumer credit behavior\n",
      "•Created classification model to predict customer type and offer tailored productsGE Capital case study: grouping clients\n",
      "9\n",
      "\n",
      "Example use case General question Concept\n",
      "Does Congress vote in patterns?\n",
      "Is there a pattern ?\n",
      "Is there structure in unstructured data?k-means clustering\n",
      "Are basketball players \"priced\" efficiently (based on performance)?How to uncover trends with many variables that you    can't easily visualize?k-means clustering in many dimensionsConcept summary\n",
      "12\n",
      "\n",
      "1.Data set consists of 427 members (observations) 2.Members served a full year in 2013\n",
      "3.Three vote types:\n",
      "•“Aye”\n",
      "•“Nay”\n",
      "•“Other”Goal: to understand how polarized the US Congress isPolitical clustering\n",
      "The joint session of Congress on Capitol Hill in Washington\n",
      "13\n",
      "\n",
      "•How do we identify swing votes?\n",
      "–Lobbying\n",
      "–Bridging party lines\n",
      "•Assumption:\n",
      "–Democrats and Republicans vote among partisan lines, which generates clustersEach data point represents a member of CongressFinding voting patterns\n",
      "14\n",
      "\n",
      "Intra vs.\n",
      "inter- cluster distance\n",
      "Intra-Cluster DistanceInter-Cluster Distance\n",
      "Objective: minimize intra -cluster dis tance, maximize inter -cluster distance\n",
      "15\n",
      "\n",
      "•The centroid is the average location of all points in the cluster\n",
      "•Another definition: the centroid minimizes the distance between a central location and all the data points in the cluster\n",
      "Note: Centroids are generally not existing data points, rather locations in spacek-means clustering is based on centroids\n",
      "16\n",
      "\n",
      "1.Randomly choose k data points to be centroids k-means in 4 steps\n",
      "17\n",
      "\n",
      "\n",
      "1.Randomly choose k data points to be centroids 2.Assign each point to closest centroidk-means in 4 steps\n",
      "18\n",
      "\n",
      "\n",
      "1.Randomly choose k data points to be centroids 2.Assign each point to closest centroid\n",
      "3.Recalculate centroids based on current cluster membershipk-means in 4 steps\n",
      "19\n",
      "\n",
      "1.Randomly choose k data points to be centroids 2.Assign each point to closest centroid\n",
      "3.Recalculate centroids based on current cluster membershipk-means in 4 steps\n",
      "204.Repeat steps 2 -3 with the new centroids until the centroids don’t change anymore\n",
      "\n",
      "Step 1: load packages and data\n",
      "# Install packages\n",
      "install.packages(\"e1071\") install.packages(\"ggplot2\" )\n",
      "# Load librarieslibrary(e1071)library(ggplot2)\n",
      "library(help = e1071)Learn about all the functionality of the package, be well informed about what you're doing!\n",
      "21\n",
      "\n",
      "Step 1: load packages and data\n",
      "# Loading house data\n",
      "house_votes_Dem = read_csv (\"house_votes_Dem.csv\")\n",
      "# What does the data look like?\n",
      "View( house_votes_Dem )Script\n",
      "22\n",
      "\n",
      "Step 2: run k -means\n",
      "# Define the columns to be clustered by subsetting the data\n",
      "clust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]\n",
      "# Run an algorithm with 2 centersset.seed(1 )\n",
      "kmeans_obj_Dem = kmeans(clust_data_Dem , centers = 2, algorithm = \"Lloyd\")\n",
      "# What does the new variable kmeans_obj contain?kmeans_obj_Dem\n",
      "# View the results of each output of the kmeans # functionhead( kmeans_obj_Dem)Script\n",
      "1.By placing the set of data we want     after the comma, we tell R we’re   looking for columns 2.kmeans uses a different starting data point each time it runs.\n",
      "To make the results reproducible make R start from the same point every time with set.seed()\n",
      "3.We’re not specifying the number of iterations so R defaults to 10\n",
      "4.We’ll see that kmeans produces a list    of vectors of different lengths.\n",
      "As a result, we cannot use the View() function\n",
      "23\n",
      "\n",
      "Step 2: run k -means\n",
      "1.Number of points each cluster contains\n",
      "2.The “location” of each cluster center is specified by 3 coordinates, one for each column we’re clustering\n",
      "3.The list assigning either cluster 1 or 2 to each data point\n",
      "1.79.5% of the variance between the data points is explained by our clustering, we will discuss this in detail later\n",
      "2.List of other types of data included in kmeans_obj\n",
      "24\n",
      "\n",
      "•cluster: a vector indicating the cluster to which each point is allocated\n",
      "•centers: a matrix of cluster centers\n",
      "•totss: the total sum of squares (sum of distances between all points)\n",
      "•withinss: vector of within -cluster sum of distances, one number per cluster\n",
      "•tot.withinss: total within -cluster sum of distances, i.e.\n",
      "sum of withinss\n",
      "•betweenss: the between -cluster sum of squares, i.e.\n",
      "totss -tot.withinss\n",
      "•size: the number of points in each cluster\n",
      "To learn more about the kmeans function run ?kmeanskmeans outputs\n",
      "26\n",
      "\n",
      "Intra vs.\n",
      "inter- cluster distance\n",
      "Intra-Cluster DistanceInter-Cluster Distance\n",
      "withinssbetweensstotss = withinss +betweenss\n",
      "27\n",
      "\n",
      "Step 3: visualize plot\n",
      "# Tell R to read the cluster labels as factors so that ggplot2 (the # graphing  package) can read them as category labels instead of # continuous variables (numeric variables).party_clusters_Dem = as.factor(kmeans_obj_Dem$cluster)\n",
      "# What does party_clusters look like?View( party_clusters_Dem )\n",
      "View(as.data.frame(party_clusters_Dem))\n",
      "# Set up labels for our data so that we can compare Democrats and # Republicans.party_labels_Dem = house_votes_Dem$partyScript\n",
      "28\n",
      "\n",
      "ggplot(house_votes_Dem, aes(x = aye, y = nay,\n",
      "shape = party_clusters_Dem)) + geom_point(size = 6) +\n",
      "ggtitle(\"Aye vs.\n",
      "Nay votes for Democrat -introduced bills\") +\n",
      "xlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", labels = c(\"Cluster 1\", \"Cluster 2\" ),\n",
      "values = c(\"1\" , \"2\")) +\n",
      "theme_light()Step 3: visualize plotCosmetics layerBase layer\n",
      "Geom Layer\n",
      "Titles and axis\n",
      "Shape\n",
      "Theme\n",
      "29Script\n",
      "\n",
      "Step 3: visualize plot\n",
      "30\n",
      "\n",
      "•Two groups exist\n",
      "•Algorithm identifies voting patternsWhat can we infer about the different clusters?Step 4: analyze results\n",
      "31\n",
      "\n",
      "\n",
      "ggplot(house_votes_Dem, aes(x = yea, y = nay,\n",
      "color= party_labels_Dem,\n",
      "shape = party_clusters_Dem)) + geom_point(size = 6) +\n",
      "ggtitle(\"Aye vs.\n",
      "Nay votes for Democrat -introduced bills\") +\n",
      "xlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", labels = c(\"Cluster 1\", \"Cluster 2\" ),\n",
      "values = c(\"1\" , \"2\")) +\n",
      "scale_color_manual(name = \"Party\", labels = c(\"Democratic\", \"Republican\"),\n",
      "values = c(\"blue\" , \"red\"))+\n",
      "theme_light()Step 5: validate resultsCosmetics layerScript\n",
      "Base layer\n",
      "Geom Layer\n",
      "Titles and axis\n",
      "Color and shape\n",
      "Theme\n",
      "32\n",
      "\n",
      "Step 5: validate results\n",
      "33\n",
      "\n",
      "\n",
      "•Diffuse among Democrats\n",
      "•Republicans more dense\n",
      "•Can gauge “outliers”\n",
      "•Can see the polarization between the two political parties Step 6: interpret results\n",
      "34\n",
      "\n",
      "\n",
      "•Clustering is more powerful than the human eye in3D\n",
      "•Clustering mathematically defines which cluster the peripheral points should be in when it’s not obvious to the human eye\n",
      "•Clustering is helpful when many dimensions / variables exist that you can’t visualize at once\n",
      "–Whiskey similarity example from classification lectureClustering vs.\n",
      "visualizing\n",
      "Aye, Nay and Other Votes\n",
      "in House of Representatives\n",
      "35\n",
      "\n",
      "•Goals of clustering:\n",
      "–Maximize the separation between clusters •i.e.\n",
      "Maximize inter -cluster distance –Keep similar points in a cluster close together •i.e.\n",
      "Minimize intra -cluster distanceHow good is the clustering?\n",
      "36\n",
      "\n",
      "•Look at the variance explained by clusters\n",
      "–In particular, the ratio of inter -cluster variance to total variance\n",
      "•How much of the total variance is explained by the clustering?Assessing how well an algorithm performsHow good is the clustering?\n",
      "Variation explained by clusters\n",
      "= inter-cluster variance / total variance\n",
      "37\n",
      "\n",
      "•cluster: a vector indicating the cluster to which each point is allocated\n",
      "•centers: a matrix of cluster centers\n",
      "•totss: the total sum of squares (sum of distances between all points)\n",
      "•withinss: vector of within -cluster sum of distances, one number per cluster\n",
      "•tot.withinss: total within -cluster sum of distances, i.e.\n",
      "sum of withinss\n",
      "•betweenss: the between -cluster sum of squares, i.e.\n",
      "totss -tot.withinss\n",
      "•size: the number of points in each cluster\n",
      "To learn more about the kmeans function run ?kmeanskmeans outputs\n",
      "38\n",
      "\n",
      "Intra vs.\n",
      "inter- cluster distance\n",
      "Intra-Cluster DistanceInter-Cluster Distance\n",
      "withinssbetweensstotss = withinss +betweenss\n",
      "39\n",
      "\n",
      "How good is the clustering?\n",
      "# Inter-cluster variance,\n",
      "# \"betweenss\" is the sum of the # distances between points from # different clustersnum_Dem= kmeans_obj_Dem$betweenss\n",
      "# Total variance# \"totss\" is the sum of the distances  # between all the points in # the data setdenom_Dem = kmeans_obj_Dem$totss\n",
      "# Variance accounted for by # clustersvar_exp_Dem = num_Dem/ denom_Dem\n",
      "var_exp_Dem[1] 0.7193405Script\n",
      "40\n",
      "\n",
      "\n",
      "•It’s easier when the number of clusters is known ahead of time, but what if we don't know how many clusters we should have?\n",
      "•Since different starting points may generate different clusters, we need a way to assess cluster quality as well.How do we choose the number of clusters (i.e.\n",
      "k)?How good is the clustering?\n",
      "41\n",
      "\n",
      "1.Elbow method\n",
      "–Computes the percentage of variance explained by clusters for a range of cluster numbers\n",
      "–Plots a graph so results are easier to see –Not guaranteed to work!\n",
      "It depends on the data in question\n",
      "2.NbClustHow to select k: two methods\n",
      "–Runs 30 different tests and provides “majority vote” for the best number of clusters (k’s) to use\n",
      "42\n",
      "\n",
      "Elbow method: measure variance\n",
      "# Run algorithm with 3 centers\n",
      "set.seed(1 )\n",
      "kmeans_obj_Dem = kmeans(clust_data_Dem,   centers = 3,\n",
      "algorithm = \"Lloyd\")\n",
      "# Inter- cluster variance\n",
      "num_Dem = kmeans_obj_Dem$ betweenss\n",
      "# Total variance\n",
      "denom_Dem = kmeans_obj_Dem $totss\n",
      "# Variance accounted for by clustersvar_exp_Dem = num_Dem / denom_Dem\n",
      "var_exp_Dem\n",
      "[1] 0.7949741Script\n",
      "43\n",
      "\n",
      "•We want to repeat the variance calculation from the previous slide for several numbers of clusters automatically\n",
      "•We can create a function that contains all the steps we want to automate Automating a step we want to repeat\n",
      "function(data, item to iterate through)\n",
      "44\n",
      "\n",
      "# The function explained_variance wraps our code from previous slides.\n",
      "explained_variance = function( data_in, k){\n",
      "# Running k- means algorithm\n",
      "set.seed(1 )  kmeans_obj = kmeans(data_in, centers = k,\n",
      "algorithm = \"Lloyd\" )\n",
      "# Variance accounted for by clusters\n",
      "var_exp = kmeans_obj $betweenss / kmeans_obj$totss\n",
      "var_exp\n",
      "}Automating a step we want to repeat\n",
      "Script\n",
      "1.A new variable is created and set equal to our function()\n",
      "2.The commands inside the function are wrapped in curly braces {}\n",
      "3.Inside the parentheses, we specify the variables that the user will input and that will then be used inside the function where they appear\n",
      "45\n",
      "\n",
      "# Recall the variable we are using for the # data that we're clustering.\n",
      "clust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]\n",
      "View( clust_data_Dem)\n",
      "# The sapply() function plugs several values # into explained_variance.\n",
      "explained_var_Dem = sapply(1 :10, explained_variance, data_in = clust_data_Dem)\n",
      "View( explained_var_Dem)\n",
      "# Data for ggplot2\n",
      "elbow_data_Dem = data.frame(k = 1:10, explained_var_Dem)\n",
      "View( elbow_data_Dem)Automating a step we want to repeat\n",
      "1.sapply() applies a function to a vector\n",
      "2.We have to tell sapply() that the we want the explained_variance function to use the clust_data data\n",
      "3.Next, we create a data frame that contains both the new variance variable (explained_var_Dem) and the different numbers of k that we used in the previous function (1 through 10)Function we created Script\n",
      "46\n",
      "\n",
      "# Plotting data\n",
      "ggplot(elbow_data_Dem, aes(x = k,  y = explained_var_Dem)) + geom_point(size = 4) +\n",
      "geom_line(size = 1 ) +\n",
      "xlab(\"k\" ) + ylab(\"Intercluster Variance/Total Variance\" ) + theme_light()Elbow method: plotting the graph\n",
      "Script\n",
      "1.geom_point() sets the size of the data points\n",
      "2.geom_line() sets the thickness of the line\n",
      "47\n",
      "\n",
      "Looking for the kink in graph of  inter- cluster variance / total varianceElbow method: measure variance\n",
      "Original data Elbow methodk = 2\n",
      "48\n",
      "\n",
      "\n",
      "•Library: \"NbClust\"\n",
      "Functions:  \"NbClust\"\n",
      "Inputs : •data –data array or data frame\n",
      "•min.nc / max.nc –minimum/maximum number of clusters\n",
      "•method –\"kmeans\"\n",
      "•There are other, more advanced arguments that can be customized but are outside of the scope of this course and are note necessary to for NbClust to workThere are a number of ways to choose the right k.\n",
      "NbClust runs 30 tests and selects k based on majority voteNbClust: k by majority vote\n",
      "NbClust(data, max.nc, method = \"kmeans\")\n",
      "49\n",
      "\n",
      "# Install the package.\n",
      "install.packages(\"NbClust\" )\n",
      "library(NbClust)\n",
      "# Run NbClust.\n",
      "nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")\n",
      "# View the output of NbClust.nbclust_obj_Dem\n",
      "# View the output that shows the number of clusters each # method recommends.View( nbclust_obj_Dem $Best.nc)NbClust : k by majority vote\n",
      "Script\n",
      "50\n",
      "\n",
      "NbClust: k by majority vote\n",
      "> nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")\n",
      "...\n",
      "******************************************************************* * Among all indices:                                                * 14 proposed 2 as the best number of clusters * 3 proposed 3 as the best number of clusters * 1 proposed 4 as the best number of clusters * 3 proposed 6 as the best number of clusters * 1 proposed 9 as the best number of clusters * 1 proposed 10 as the best number of clusters * 1 proposed 15 as the best number of clusters ***** Conclusion *****                            * According to the majority rule, the best number of clusters is  2\n",
      "Note: additional information appears; the above information is most relevant to us for nowConsole\n",
      "51\n",
      "\n",
      "NbClust: k by majority vote\n",
      "> nbclust_obj_Dem Console•nbclust_obj_Dem shows the outputs of NbClust\n",
      "–One of the outputs is Best.nc, which shows the number of clusters                               recommended by each test 52\n",
      "\n",
      "NbClust: k by majority vote\n",
      "•We want to visualize a histogram to make it obvious how many votes there are for each number of clusters 53\n",
      "\n",
      "# Subset the 1st row from Best.nc and convert it  # to a data frame, so ggplot2 can plot it.\n",
      "freq_k_Dem = nbclust_obj_Dem $Best.nc[1 ,]\n",
      "freq_k_Dem = data.frame( freq_k_Dem)\n",
      "View(freq_k_Dem)\n",
      "# Check the maximum number of clusters.\n",
      "max(freq_k_Dem )\n",
      "# Plot as a histogram.\n",
      "ggplot(freq_k_Dem,\n",
      "aes(x = freq_k_Dem)) +\n",
      "geom_bar() +scale_x_continuous(breaks = seq(0 , 15, by = 1)) +\n",
      "scale_y_continuous(breaks = seq(0 , 12, by = 1)) +\n",
      "labs(x = \"Number of Clusters\",\n",
      "y = \"Number of Votes\" ,\n",
      "title = \"Cluster Analysis\")NbClust: k by majority vote\n",
      "Script\n",
      "2 clusters is the winner with 12 votes\n",
      "54\n",
      "\n",
      "•If you’re a lobbyist, which congressperson can you influence for swing votes?\n",
      "•If you’re managing a campaign and your competitor is always voting along party lines, how can you use that information?\n",
      "•If your congressperson is not an active voter, is she representing your interests?\n",
      "•What do the voting patterns look like for Republican -introduced bills?Application of results\n",
      "55\n",
      "\n",
      "•Could see differences between the patterns of Reb lead bills and Democrat lead bills\n",
      "•Could provide information on congressmen that might be see has swing votes.\n",
      "Implications of results\n",
      "56\n",
      "\n",
      "•We are assuming that the patterns correspond with the same bills being voted on –perhaps some Congressmen have the same number of 'aye' and 'nay' votes, but voted on different bills\n",
      "•Network analysis can help determine additional connections between Congressmen\n",
      "•We haven't taken extenuating factors into account – political initiatives, current events, etc.\n",
      "This is a preliminary analysis that gives us initial insights and can help us direct further researchLimitations of results\n",
      "57\n",
      "\n",
      "•The good and bad\n",
      "–+ cheap –NO LABELS , labels are expensive to create and maintain\n",
      "–+/-clustering always works\n",
      "–-Many methods to choose from and knowing the right one can be nontrivial and the differences between many are almost zero, so you need to understand what you're doing\n",
      "•The evil\n",
      "–Curse of dimensionality\n",
      "–Clusters may result from poor data quality\n",
      "–Non-deterministic (e.g.\n",
      "k -means) subject to local minimum.\n",
      "Since it works with averages, k-means does not get much better with Big Data (marginal improvements) –Non spherical data may result in poor clustering (depending on method used)\n",
      "–Unequal cluster sizes may result in poor clustering (depending on method used)The good, bad, and evil\n",
      "58\n",
      "\n",
      "59\n",
      "\n",
      "\n",
      "•Analysts need to ask the following questions\n",
      "–Do you want overlapping or non -overlapping clusters ?\n",
      "–Does your data satisfy the assumptions of the clustering algorithm?\n",
      "–How was the distance measure identified ?\n",
      "–How many clusters and why ?\n",
      "Identifying the number of clusters is a difficult task if the number of class labels is not known beforehand –Does your method scale to the size of the data?\n",
      "–Is the compute time congruent with the temporal budget of your business need (i.e.\n",
      "do you get answers back in time to make meaningful decisions)The good, bad, and evil\n",
      "60\n",
      "\n",
      "Machine Learning Overview, EDA and Clustering\n",
      "Brian Wright\n",
      "brianwright@virginia.edu\n",
      "\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "3Given  D :{2,4,10,12,3,11,20,25,30},  and  k=2 clusters\n",
      "Randomly assign the means:  m1=3, m2=4\n",
      "K1={2,3}, K2={4,10,12,11,20,25,30}, m1=2.5, m2=16\n",
      "K1={2,3,4}, K2={10,12,11,20,25,30}, m1=3, m2=18\n",
      "K1={2,3,4,10}, K2={12,11,20,25,30}, m1=4.75, m2=19.6\n",
      "K1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\n",
      "K1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\n",
      "Stop, since the clusters and the means found in all subsequent iterations will be the same .Example of K -Means\n",
      "\n",
      "1.What is Machine Learning?\n",
      "2.What is exploratory data analysis?\n",
      "3.k-means clustering\n",
      "–Does Congress vote in patterns?\n",
      "4.Multi -dimensional k -means clustering\n",
      "–Are NBA players compensated according to performance?Outline: Intro to Unsupervised ML\n",
      "4\n",
      "\n",
      "•Exploratory data analysis or “EDA” is an approach where the intent is to see what the data can tell us beyond modeling or hypothesis testing\n",
      "–Data visualization is one of the most common forms of EDAWhat is exploratory data analysis?\n",
      "5\n",
      "\n",
      "When data is too big or complex to be analyzed just by visualizing it, these types of analysis can help:\n",
      "1.Clustering: compare pieces of data by measuring similarity among them\n",
      "2.Network analysis: analyze how people, places and entities are connected to evaluate the properties and structure of a network 3.Text mining: analyze what large bodies of unstructured or structured text sayTypes of exploratory data analysis\n",
      "6\n",
      "\n",
      "The data inputs have (x)no target outputs (y)Unsupervised machine learning\n",
      "Input x:\n",
      "VoterOutput y:\n",
      "Not given\n",
      "(To be discovered)?\n",
      "We want to impose structure on the inputs (x)to say something meaningful about the data\n",
      "7\n",
      "\n",
      "\n",
      "1.Technique for finding similarity between groups\n",
      "2.Type of unsupervised machine learning\n",
      "•Not the only class of unsupervised learning        algorithms\n",
      "3.Similarity needs to be defined\n",
      "•Will depend on attributes of data\n",
      "•Usually a distance metricWhat is clustering?\n",
      "8\n",
      "Key assumption: data points that are “closer” together are related or similar\n",
      "\n",
      "•Haimowitz and Schwarz 1997 paper on clustering for credit line optimization\n",
      "–http://www.aaai.org/Papers/Workshops/1997/\n",
      "WS-97-07/WS97- 07-006.pdf\n",
      "•Cluster existing GE Capital customers based on similarity in use of their credit cards to pay bills and customers’ profitability to GE Capital\n",
      "•Resulted in five clusters of consumer credit behavior\n",
      "•Created classification model to predict customer type and offer tailored productsGE Capital case study: grouping clients\n",
      "9\n",
      "\n",
      "Example use case General question Concept\n",
      "Does Congress vote in patterns?\n",
      "Is there a pattern ?\n",
      "Is there structure in unstructured data?k-means clustering\n",
      "Are basketball players \"priced\" efficiently (based on performance)?How to uncover trends with many variables that you    can't easily visualize?k-means clustering in many dimensionsConcept summary\n",
      "12\n",
      "\n",
      "1.Data set consists of 427 members (observations) 2.Members served a full year in 2013\n",
      "3.Three vote types:\n",
      "•“Aye”\n",
      "•“Nay”\n",
      "•“Other”Goal: to understand how polarized the US Congress isPolitical clustering\n",
      "The joint session of Congress on Capitol Hill in Washington\n",
      "13\n",
      "\n",
      "•How do we identify swing votes?\n",
      "–Lobbying\n",
      "–Bridging party lines\n",
      "•Assumption:\n",
      "–Democrats and Republicans vote among partisan lines, which generates clustersEach data point represents a member of CongressFinding voting patterns\n",
      "14\n",
      "\n",
      "Intra vs.\n",
      "inter- cluster distance\n",
      "Intra-Cluster DistanceInter-Cluster Distance\n",
      "Objective: minimize intra -cluster dis tance, maximize inter -cluster distance\n",
      "15\n",
      "\n",
      "•The centroid is the average location of all points in the cluster\n",
      "•Another definition: the centroid minimizes the distance between a central location and all the data points in the cluster\n",
      "Note: Centroids are generally not existing data points, rather locations in spacek-means clustering is based on centroids\n",
      "16\n",
      "\n",
      "1.Randomly choose k data points to be centroids k-means in 4 steps\n",
      "17\n",
      "\n",
      "\n",
      "1.Randomly choose k data points to be centroids 2.Assign each point to closest centroidk-means in 4 steps\n",
      "18\n",
      "\n",
      "\n",
      "1.Randomly choose k data points to be centroids 2.Assign each point to closest centroid\n",
      "3.Recalculate centroids based on current cluster membershipk-means in 4 steps\n",
      "19\n",
      "\n",
      "1.Randomly choose k data points to be centroids 2.Assign each point to closest centroid\n",
      "3.Recalculate centroids based on current cluster membershipk-means in 4 steps\n",
      "204.Repeat steps 2 -3 with the new centroids until the centroids don’t change anymore\n",
      "\n",
      "Step 1: load packages and data\n",
      "# Install packages\n",
      "install.packages(\"e1071\") install.packages(\"ggplot2\" )\n",
      "# Load librarieslibrary(e1071)library(ggplot2)\n",
      "library(help = e1071)Learn about all the functionality of the package, be well informed about what you're doing!\n",
      "21\n",
      "\n",
      "Step 1: load packages and data\n",
      "# Loading house data\n",
      "house_votes_Dem = read_csv (\"house_votes_Dem.csv\")\n",
      "# What does the data look like?\n",
      "View( house_votes_Dem )Script\n",
      "22\n",
      "\n",
      "Step 2: run k -means\n",
      "# Define the columns to be clustered by subsetting the data\n",
      "clust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]\n",
      "# Run an algorithm with 2 centersset.seed(1 )\n",
      "kmeans_obj_Dem = kmeans(clust_data_Dem , centers = 2, algorithm = \"Lloyd\")\n",
      "# What does the new variable kmeans_obj contain?kmeans_obj_Dem\n",
      "# View the results of each output of the kmeans # functionhead( kmeans_obj_Dem)Script\n",
      "1.By placing the set of data we want     after the comma, we tell R we’re   looking for columns 2.kmeans uses a different starting data point each time it runs.\n",
      "To make the results reproducible make R start from the same point every time with set.seed()\n",
      "3.We’re not specifying the number of iterations so R defaults to 10\n",
      "4.We’ll see that kmeans produces a list    of vectors of different lengths.\n",
      "As a result, we cannot use the View() function\n",
      "23\n",
      "\n",
      "Step 2: run k -means\n",
      "1.Number of points each cluster contains\n",
      "2.The “location” of each cluster center is specified by 3 coordinates, one for each column we’re clustering\n",
      "3.The list assigning either cluster 1 or 2 to each data point\n",
      "1.79.5% of the variance between the data points is explained by our clustering, we will discuss this in detail later\n",
      "2.List of other types of data included in kmeans_obj\n",
      "24\n",
      "\n",
      "•cluster: a vector indicating the cluster to which each point is allocated\n",
      "•centers: a matrix of cluster centers\n",
      "•totss: the total sum of squares (sum of distances between all points)\n",
      "•withinss: vector of within -cluster sum of distances, one number per cluster\n",
      "•tot.withinss: total within -cluster sum of distances, i.e.\n",
      "sum of withinss\n",
      "•betweenss: the between -cluster sum of squares, i.e.\n",
      "totss -tot.withinss\n",
      "•size: the number of points in each cluster\n",
      "To learn more about the kmeans function run ?kmeanskmeans outputs\n",
      "26\n",
      "\n",
      "Intra vs.\n",
      "inter- cluster distance\n",
      "Intra-Cluster DistanceInter-Cluster Distance\n",
      "withinssbetweensstotss = withinss +betweenss\n",
      "27\n",
      "\n",
      "Step 3: visualize plot\n",
      "# Tell R to read the cluster labels as factors so that ggplot2 (the # graphing  package) can read them as category labels instead of # continuous variables (numeric variables).party_clusters_Dem = as.factor(kmeans_obj_Dem$cluster)\n",
      "# What does party_clusters look like?View( party_clusters_Dem )\n",
      "View(as.data.frame(party_clusters_Dem))\n",
      "# Set up labels for our data so that we can compare Democrats and # Republicans.party_labels_Dem = house_votes_Dem$partyScript\n",
      "28\n",
      "\n",
      "ggplot(house_votes_Dem, aes(x = aye, y = nay,\n",
      "shape = party_clusters_Dem)) + geom_point(size = 6) +\n",
      "ggtitle(\"Aye vs.\n",
      "Nay votes for Democrat -introduced bills\") +\n",
      "xlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", labels = c(\"Cluster 1\", \"Cluster 2\" ),\n",
      "values = c(\"1\" , \"2\")) +\n",
      "theme_light()Step 3: visualize plotCosmetics layerBase layer\n",
      "Geom Layer\n",
      "Titles and axis\n",
      "Shape\n",
      "Theme\n",
      "29Script\n",
      "\n",
      "Step 3: visualize plot\n",
      "30\n",
      "\n",
      "•Two groups exist\n",
      "•Algorithm identifies voting patternsWhat can we infer about the different clusters?Step 4: analyze results\n",
      "31\n",
      "\n",
      "\n",
      "ggplot(house_votes_Dem, aes(x = yea, y = nay,\n",
      "color= party_labels_Dem,\n",
      "shape = party_clusters_Dem)) + geom_point(size = 6) +\n",
      "ggtitle(\"Aye vs.\n",
      "Nay votes for Democrat -introduced bills\") +\n",
      "xlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", labels = c(\"Cluster 1\", \"Cluster 2\" ),\n",
      "values = c(\"1\" , \"2\")) +\n",
      "scale_color_manual(name = \"Party\", labels = c(\"Democratic\", \"Republican\"),\n",
      "values = c(\"blue\" , \"red\"))+\n",
      "theme_light()Step 5: validate resultsCosmetics layerScript\n",
      "Base layer\n",
      "Geom Layer\n",
      "Titles and axis\n",
      "Color and shape\n",
      "Theme\n",
      "32\n",
      "\n",
      "Step 5: validate results\n",
      "33\n",
      "\n",
      "\n",
      "•Diffuse among Democrats\n",
      "•Republicans more dense\n",
      "•Can gauge “outliers”\n",
      "•Can see the polarization between the two political parties Step 6: interpret results\n",
      "34\n",
      "\n",
      "\n",
      "•Clustering is more powerful than the human eye in3D\n",
      "•Clustering mathematically defines which cluster the peripheral points should be in when it’s not obvious to the human eye\n",
      "•Clustering is helpful when many dimensions / variables exist that you can’t visualize at once\n",
      "–Whiskey similarity example from classification lectureClustering vs.\n",
      "visualizing\n",
      "Aye, Nay and Other Votes\n",
      "in House of Representatives\n",
      "35\n",
      "\n",
      "•Goals of clustering:\n",
      "–Maximize the separation between clusters •i.e.\n",
      "Maximize inter -cluster distance –Keep similar points in a cluster close together •i.e.\n",
      "Minimize intra -cluster distanceHow good is the clustering?\n",
      "36\n",
      "\n",
      "•Look at the variance explained by clusters\n",
      "–In particular, the ratio of inter -cluster variance to total variance\n",
      "•How much of the total variance is explained by the clustering?Assessing how well an algorithm performsHow good is the clustering?\n",
      "Variation explained by clusters\n",
      "= inter-cluster variance / total variance\n",
      "37\n",
      "\n",
      "•cluster: a vector indicating the cluster to which each point is allocated\n",
      "•centers: a matrix of cluster centers\n",
      "•totss: the total sum of squares (sum of distances between all points)\n",
      "•withinss: vector of within -cluster sum of distances, one number per cluster\n",
      "•tot.withinss: total within -cluster sum of distances, i.e.\n",
      "sum of withinss\n",
      "•betweenss: the between -cluster sum of squares, i.e.\n",
      "totss -tot.withinss\n",
      "•size: the number of points in each cluster\n",
      "To learn more about the kmeans function run ?kmeanskmeans outputs\n",
      "38\n",
      "\n",
      "Intra vs.\n",
      "inter- cluster distance\n",
      "Intra-Cluster DistanceInter-Cluster Distance\n",
      "withinssbetweensstotss = withinss +betweenss\n",
      "39\n",
      "\n",
      "How good is the clustering?\n",
      "# Inter-cluster variance,\n",
      "# \"betweenss\" is the sum of the # distances between points from # different clustersnum_Dem= kmeans_obj_Dem$betweenss\n",
      "# Total variance# \"totss\" is the sum of the distances  # between all the points in # the data setdenom_Dem = kmeans_obj_Dem$totss\n",
      "# Variance accounted for by # clustersvar_exp_Dem = num_Dem/ denom_Dem\n",
      "var_exp_Dem[1] 0.7193405Script\n",
      "40\n",
      "\n",
      "\n",
      "•It’s easier when the number of clusters is known ahead of time, but what if we don't know how many clusters we should have?\n",
      "•Since different starting points may generate different clusters, we need a way to assess cluster quality as well.How do we choose the number of clusters (i.e.\n",
      "k)?How good is the clustering?\n",
      "41\n",
      "\n",
      "1.Elbow method\n",
      "–Computes the percentage of variance explained by clusters for a range of cluster numbers\n",
      "–Plots a graph so results are easier to see –Not guaranteed to work!\n",
      "It depends on the data in question\n",
      "2.NbClustHow to select k: two methods\n",
      "–Runs 30 different tests and provides “majority vote” for the best number of clusters (k’s) to use\n",
      "42\n",
      "\n",
      "Elbow method: measure variance\n",
      "# Run algorithm with 3 centers\n",
      "set.seed(1 )\n",
      "kmeans_obj_Dem = kmeans(clust_data_Dem,   centers = 3,\n",
      "algorithm = \"Lloyd\")\n",
      "# Inter- cluster variance\n",
      "num_Dem = kmeans_obj_Dem$ betweenss\n",
      "# Total variance\n",
      "denom_Dem = kmeans_obj_Dem $totss\n",
      "# Variance accounted for by clustersvar_exp_Dem = num_Dem / denom_Dem\n",
      "var_exp_Dem\n",
      "[1] 0.7949741Script\n",
      "43\n",
      "\n",
      "•We want to repeat the variance calculation from the previous slide for several numbers of clusters automatically\n",
      "•We can create a function that contains all the steps we want to automate Automating a step we want to repeat\n",
      "function(data, item to iterate through)\n",
      "44\n",
      "\n",
      "# The function explained_variance wraps our code from previous slides.\n",
      "explained_variance = function( data_in, k){\n",
      "# Running k- means algorithm\n",
      "set.seed(1 )  kmeans_obj = kmeans(data_in, centers = k,\n",
      "algorithm = \"Lloyd\" )\n",
      "# Variance accounted for by clusters\n",
      "var_exp = kmeans_obj $betweenss / kmeans_obj$totss\n",
      "var_exp\n",
      "}Automating a step we want to repeat\n",
      "Script\n",
      "1.A new variable is created and set equal to our function()\n",
      "2.The commands inside the function are wrapped in curly braces {}\n",
      "3.Inside the parentheses, we specify the variables that the user will input and that will then be used inside the function where they appear\n",
      "45\n",
      "\n",
      "# Recall the variable we are using for the # data that we're clustering.\n",
      "clust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]\n",
      "View( clust_data_Dem)\n",
      "# The sapply() function plugs several values # into explained_variance.\n",
      "explained_var_Dem = sapply(1 :10, explained_variance, data_in = clust_data_Dem)\n",
      "View( explained_var_Dem)\n",
      "# Data for ggplot2\n",
      "elbow_data_Dem = data.frame(k = 1:10, explained_var_Dem)\n",
      "View( elbow_data_Dem)Automating a step we want to repeat\n",
      "1.sapply() applies a function to a vector\n",
      "2.We have to tell sapply() that the we want the explained_variance function to use the clust_data data\n",
      "3.Next, we create a data frame that contains both the new variance variable (explained_var_Dem) and the different numbers of k that we used in the previous function (1 through 10)Function we created Script\n",
      "46\n",
      "\n",
      "# Plotting data\n",
      "ggplot(elbow_data_Dem, aes(x = k,  y = explained_var_Dem)) + geom_point(size = 4) +\n",
      "geom_line(size = 1 ) +\n",
      "xlab(\"k\" ) + ylab(\"Intercluster Variance/Total Variance\" ) + theme_light()Elbow method: plotting the graph\n",
      "Script\n",
      "1.geom_point() sets the size of the data points\n",
      "2.geom_line() sets the thickness of the line\n",
      "47\n",
      "\n",
      "Looking for the kink in graph of  inter- cluster variance / total varianceElbow method: measure variance\n",
      "Original data Elbow methodk = 2\n",
      "48\n",
      "\n",
      "\n",
      "•Library: \"NbClust\"\n",
      "Functions:  \"NbClust\"\n",
      "Inputs : •data –data array or data frame\n",
      "•min.nc / max.nc –minimum/maximum number of clusters\n",
      "•method –\"kmeans\"\n",
      "•There are other, more advanced arguments that can be customized but are outside of the scope of this course and are note necessary to for NbClust to workThere are a number of ways to choose the right k.\n",
      "NbClust runs 30 tests and selects k based on majority voteNbClust: k by majority vote\n",
      "NbClust(data, max.nc, method = \"kmeans\")\n",
      "49\n",
      "\n",
      "# Install the package.\n",
      "install.packages(\"NbClust\" )\n",
      "library(NbClust)\n",
      "# Run NbClust.\n",
      "nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")\n",
      "# View the output of NbClust.nbclust_obj_Dem\n",
      "# View the output that shows the number of clusters each # method recommends.View( nbclust_obj_Dem $Best.nc)NbClust : k by majority vote\n",
      "Script\n",
      "50\n",
      "\n",
      "NbClust: k by majority vote\n",
      "> nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")\n",
      "...\n",
      "******************************************************************* * Among all indices:                                                * 14 proposed 2 as the best number of clusters * 3 proposed 3 as the best number of clusters * 1 proposed 4 as the best number of clusters * 3 proposed 6 as the best number of clusters * 1 proposed 9 as the best number of clusters * 1 proposed 10 as the best number of clusters * 1 proposed 15 as the best number of clusters ***** Conclusion *****                            * According to the majority rule, the best number of clusters is  2\n",
      "Note: additional information appears; the above information is most relevant to us for nowConsole\n",
      "51\n",
      "\n",
      "NbClust: k by majority vote\n",
      "> nbclust_obj_Dem Console•nbclust_obj_Dem shows the outputs of NbClust\n",
      "–One of the outputs is Best.nc, which shows the number of clusters                               recommended by each test 52\n",
      "\n",
      "NbClust: k by majority vote\n",
      "•We want to visualize a histogram to make it obvious how many votes there are for each number of clusters 53\n",
      "\n",
      "# Subset the 1st row from Best.nc and convert it  # to a data frame, so ggplot2 can plot it.\n",
      "freq_k_Dem = nbclust_obj_Dem $Best.nc[1 ,]\n",
      "freq_k_Dem = data.frame( freq_k_Dem)\n",
      "View(freq_k_Dem)\n",
      "# Check the maximum number of clusters.\n",
      "max(freq_k_Dem )\n",
      "# Plot as a histogram.\n",
      "ggplot(freq_k_Dem,\n",
      "aes(x = freq_k_Dem)) +\n",
      "geom_bar() +scale_x_continuous(breaks = seq(0 , 15, by = 1)) +\n",
      "scale_y_continuous(breaks = seq(0 , 12, by = 1)) +\n",
      "labs(x = \"Number of Clusters\",\n",
      "y = \"Number of Votes\" ,\n",
      "title = \"Cluster Analysis\")NbClust: k by majority vote\n",
      "Script\n",
      "2 clusters is the winner with 12 votes\n",
      "54\n",
      "\n",
      "•If you’re a lobbyist, which congressperson can you influence for swing votes?\n",
      "•If you’re managing a campaign and your competitor is always voting along party lines, how can you use that information?\n",
      "•If your congressperson is not an active voter, is she representing your interests?\n",
      "•What do the voting patterns look like for Republican -introduced bills?Application of results\n",
      "55\n",
      "\n",
      "•Could see differences between the patterns of Reb lead bills and Democrat lead bills\n",
      "•Could provide information on congressmen that might be see has swing votes.\n",
      "Implications of results\n",
      "56\n",
      "\n",
      "•We are assuming that the patterns correspond with the same bills being voted on –perhaps some Congressmen have the same number of 'aye' and 'nay' votes, but voted on different bills\n",
      "•Network analysis can help determine additional connections between Congressmen\n",
      "•We haven't taken extenuating factors into account – political initiatives, current events, etc.\n",
      "This is a preliminary analysis that gives us initial insights and can help us direct further researchLimitations of results\n",
      "57\n",
      "\n",
      "•The good and bad\n",
      "–+ cheap –NO LABELS , labels are expensive to create and maintain\n",
      "–+/-clustering always works\n",
      "–-Many methods to choose from and knowing the right one can be nontrivial and the differences between many are almost zero, so you need to understand what you're doing\n",
      "•The evil\n",
      "–Curse of dimensionality\n",
      "–Clusters may result from poor data quality\n",
      "–Non-deterministic (e.g.\n",
      "k -means) subject to local minimum.\n",
      "Since it works with averages, k-means does not get much better with Big Data (marginal improvements) –Non spherical data may result in poor clustering (depending on method used)\n",
      "–Unequal cluster sizes may result in poor clustering (depending on method used)The good, bad, and evil\n",
      "58\n",
      "\n",
      "59\n",
      "\n",
      "\n",
      "•Analysts need to ask the following questions\n",
      "–Do you want overlapping or non -overlapping clusters ?\n",
      "–Does your data satisfy the assumptions of the clustering algorithm?\n",
      "–How was the distance measure identified ?\n",
      "–How many clusters and why ?\n",
      "Identifying the number of clusters is a difficult task if the number of class labels is not known beforehand –Does your method scale to the size of the data?\n",
      "–Is the compute time congruent with the temporal budget of your business need (i.e.\n",
      "do you get answers back in time to make meaningful decisions)The good, bad, and evil\n",
      "60\n",
      "\n",
      "Machine Learning Overview, EDA and Clustering\n",
      "Brian Wright\n",
      "brianwright@virginia.edu\n",
      "\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "3Given  D :{2,4,10,12,3,11,20,25,30},  and  k=2 clusters\n",
      "Randomly assign the means:  m1=3, m2=4\n",
      "K1={2,3}, K2={4,10,12,11,20,25,30}, m1=2.5, m2=16\n",
      "K1={2,3,4}, K2={10,12,11,20,25,30}, m1=3, m2=18\n",
      "K1={2,3,4,10}, K2={12,11,20,25,30}, m1=4.75, m2=19.6\n",
      "K1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\n",
      "K1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\n",
      "Stop, since the clusters and the means found in all subsequent iterations will be the same .Example of K -Means\n",
      "\n",
      "1.What is Machine Learning?\n",
      "2.What is exploratory data analysis?\n",
      "3.k-means clustering\n",
      "–Does Congress vote in patterns?\n",
      "4.Multi -dimensional k -means clustering\n",
      "–Are NBA players compensated according to performance?Outline: Intro to Unsupervised ML\n",
      "4\n",
      "\n",
      "•Exploratory data analysis or “EDA” is an approach where the intent is to see what the data can tell us beyond modeling or hypothesis testing\n",
      "–Data visualization is one of the most common forms of EDAWhat is exploratory data analysis?\n",
      "5\n",
      "\n",
      "When data is too big or complex to be analyzed just by visualizing it, these types of analysis can help:\n",
      "1.Clustering: compare pieces of data by measuring similarity among them\n",
      "2.Network analysis: analyze how people, places and entities are connected to evaluate the properties and structure of a network 3.Text mining: analyze what large bodies of unstructured or structured text sayTypes of exploratory data analysis\n",
      "6\n",
      "\n",
      "The data inputs have (x)no target outputs (y)Unsupervised machine learning\n",
      "Input x:\n",
      "VoterOutput y:\n",
      "Not given\n",
      "(To be discovered)?\n",
      "We want to impose structure on the inputs (x)to say something meaningful about the data\n",
      "7\n",
      "\n",
      "\n",
      "1.Technique for finding similarity between groups\n",
      "2.Type of unsupervised machine learning\n",
      "•Not the only class of unsupervised learning        algorithms\n",
      "3.Similarity needs to be defined\n",
      "•Will depend on attributes of data\n",
      "•Usually a distance metricWhat is clustering?\n",
      "8\n",
      "Key assumption: data points that are “closer” together are related or similar\n",
      "\n",
      "•Haimowitz and Schwarz 1997 paper on clustering for credit line optimization\n",
      "–http://www.aaai.org/Papers/Workshops/1997/\n",
      "WS-97-07/WS97- 07-006.pdf\n",
      "•Cluster existing GE Capital customers based on similarity in use of their credit cards to pay bills and customers’ profitability to GE Capital\n",
      "•Resulted in five clusters of consumer credit behavior\n",
      "•Created classification model to predict customer type and offer tailored productsGE Capital case study: grouping clients\n",
      "9\n",
      "\n",
      "•Between 2001 and 2004 most European countries passed legislation that allowed customers to keep their cell phone number if they switched carriers\n",
      "•Telenor, one of the largest telecommunications companies in Norway wanted to ensure it kept its customers\n",
      "–Problem: the promotions the company sent to its clients reminded them that they could leave and resulted in greater defections!\n",
      "–Solution: predict which customers, if contacted, are more likely to stay with the company •Results:\n",
      "–Marketing campaign ROI increased 11x\n",
      "–Customer churn decreased 36%\n",
      "–Marketing campaign costs decreased 40%Telenor case study: predicting behavior\n",
      "10\n",
      "\n",
      "1.What is Machine Learning?\n",
      "2.What is exploratory data analysis?\n",
      "3.k-means clustering\n",
      "–Does Congress vote in patterns?\n",
      "4.Multi -dimensional k -means clustering –Lab –Are NBA players compensated according to performance?Outline: Intro to Unsupervised ML\n",
      "11\n",
      "\n",
      "Example use case General question Concept\n",
      "Does Congress vote in patterns?\n",
      "Is there a pattern ?\n",
      "Is there structure in unstructured data?k-means clustering\n",
      "Are basketball players \"priced\" efficiently (based on performance)?How to uncover trends with many variables that you    can't easily visualize?k-means clustering in many dimensionsConcept summary\n",
      "12\n",
      "\n",
      "1.Data set consists of 427 members (observations) 2.Members served a full year in 2013\n",
      "3.Three vote types:\n",
      "•“Aye”\n",
      "•“Nay”\n",
      "•“Other”Goal: to understand how polarized the US Congress isPolitical clustering\n",
      "The joint session of Congress on Capitol Hill in Washington\n",
      "13\n",
      "\n",
      "•How do we identify swing votes?\n",
      "–Lobbying\n",
      "–Bridging party lines\n",
      "•Assumption:\n",
      "–Democrats and Republicans vote among partisan lines, which generates clustersEach data point represents a member of CongressFinding voting patterns\n",
      "14\n",
      "\n",
      "Intra vs.\n",
      "inter- cluster distance\n",
      "Intra-Cluster DistanceInter-Cluster Distance\n",
      "Objective: minimize intra -cluster dis tance, maximize inter -cluster distance\n",
      "15\n",
      "\n",
      "•The centroid is the average location of all points in the cluster\n",
      "•Another definition: the centroid minimizes the distance between a central location and all the data points in the cluster\n",
      "Note: Centroids are generally not existing data points, rather locations in spacek-means clustering is based on centroids\n",
      "16\n",
      "\n",
      "1.Randomly choose k data points to be centroids k-means in 4 steps\n",
      "17\n",
      "\n",
      "\n",
      "1.Randomly choose k data points to be centroids 2.Assign each point to closest centroidk-means in 4 steps\n",
      "18\n",
      "\n",
      "\n",
      "1.Randomly choose k data points to be centroids 2.Assign each point to closest centroid\n",
      "3.Recalculate centroids based on current cluster membershipk-means in 4 steps\n",
      "19\n",
      "\n",
      "1.Randomly choose k data points to be centroids 2.Assign each point to closest centroid\n",
      "3.Recalculate centroids based on current cluster membershipk-means in 4 steps\n",
      "204.Repeat steps 2 -3 with the new centroids until the centroids don’t change anymore\n",
      "\n",
      "Step 1: load packages and data\n",
      "# Install packages\n",
      "install.packages(\"e1071\") install.packages(\"ggplot2\" )\n",
      "# Load librarieslibrary(e1071)library(ggplot2)\n",
      "library(help = e1071)Learn about all the functionality of the package, be well informed about what you're doing!\n",
      "21\n",
      "\n",
      "Step 1: load packages and data\n",
      "# Loading house data\n",
      "house_votes_Dem = read_csv (\"house_votes_Dem.csv\")\n",
      "# What does the data look like?\n",
      "View( house_votes_Dem )Script\n",
      "22\n",
      "\n",
      "Step 2: run k -means\n",
      "# Define the columns to be clustered by subsetting the data\n",
      "clust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]\n",
      "# Run an algorithm with 2 centersset.seed(1 )\n",
      "kmeans_obj_Dem = kmeans(clust_data_Dem , centers = 2, algorithm = \"Lloyd\")\n",
      "# What does the new variable kmeans_obj contain?kmeans_obj_Dem\n",
      "# View the results of each output of the kmeans # functionhead( kmeans_obj_Dem)Script\n",
      "1.By placing the set of data we want     after the comma, we tell R we’re   looking for columns 2.kmeans uses a different starting data point each time it runs.\n",
      "To make the results reproducible make R start from the same point every time with set.seed()\n",
      "3.We’re not specifying the number of iterations so R defaults to 10\n",
      "4.We’ll see that kmeans produces a list    of vectors of different lengths.\n",
      "As a result, we cannot use the View() function\n",
      "23\n",
      "\n",
      "Step 2: run k -means\n",
      "1.Number of points each cluster contains\n",
      "2.The “location” of each cluster center is specified by 3 coordinates, one for each column we’re clustering\n",
      "3.The list assigning either cluster 1 or 2 to each data point\n",
      "1.79.5% of the variance between the data points is explained by our clustering, we will discuss this in detail later\n",
      "2.List of other types of data included in kmeans_obj\n",
      "24\n",
      "\n",
      "Measuring distance\n",
      "(3,3)\n",
      "(1,2) 21Distance = √(22+12)\n",
      "x\n",
      "25\n",
      "\n",
      "•cluster: a vector indicating the cluster to which each point is allocated\n",
      "•centers: a matrix of cluster centers\n",
      "•totss: the total sum of squares (sum of distances between all points)\n",
      "•withinss: vector of within -cluster sum of distances, one number per cluster\n",
      "•tot.withinss: total within -cluster sum of distances, i.e.\n",
      "sum of withinss\n",
      "•betweenss: the between -cluster sum of squares, i.e.\n",
      "totss -tot.withinss\n",
      "•size: the number of points in each cluster\n",
      "To learn more about the kmeans function run ?kmeanskmeans outputs\n",
      "26\n",
      "\n",
      "Intra vs.\n",
      "inter- cluster distance\n",
      "Intra-Cluster DistanceInter-Cluster Distance\n",
      "withinssbetweensstotss = withinss +betweenss\n",
      "27\n",
      "\n",
      "Step 3: visualize plot\n",
      "# Tell R to read the cluster labels as factors so that ggplot2 (the # graphing  package) can read them as category labels instead of # continuous variables (numeric variables).party_clusters_Dem = as.factor(kmeans_obj_Dem$cluster)\n",
      "# What does party_clusters look like?View( party_clusters_Dem )\n",
      "View(as.data.frame(party_clusters_Dem))\n",
      "# Set up labels for our data so that we can compare Democrats and # Republicans.party_labels_Dem = house_votes_Dem$partyScript\n",
      "28\n",
      "\n",
      "ggplot(house_votes_Dem, aes(x = aye, y = nay,\n",
      "shape = party_clusters_Dem)) + geom_point(size = 6) +\n",
      "ggtitle(\"Aye vs.\n",
      "Nay votes for Democrat -introduced bills\") +\n",
      "xlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", labels = c(\"Cluster 1\", \"Cluster 2\" ),\n",
      "values = c(\"1\" , \"2\")) +\n",
      "theme_light()Step 3: visualize plotCosmetics layerBase layer\n",
      "Geom Layer\n",
      "Titles and axis\n",
      "Shape\n",
      "Theme\n",
      "29Script\n",
      "\n",
      "Step 3: visualize plot\n",
      "30\n",
      "\n",
      "•Two groups exist\n",
      "•Algorithm identifies voting patternsWhat can we infer about the different clusters?Step 4: analyze results\n",
      "31\n",
      "\n",
      "\n",
      "ggplot(house_votes_Dem, aes(x = yea, y = nay,\n",
      "color= party_labels_Dem,\n",
      "shape = party_clusters_Dem)) + geom_point(size = 6) +\n",
      "ggtitle(\"Aye vs.\n",
      "Nay votes for Democrat -introduced bills\") +\n",
      "xlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", labels = c(\"Cluster 1\", \"Cluster 2\" ),\n",
      "values = c(\"1\" , \"2\")) +\n",
      "scale_color_manual(name = \"Party\", labels = c(\"Democratic\", \"Republican\"),\n",
      "values = c(\"blue\" , \"red\"))+\n",
      "theme_light()Step 5: validate resultsCosmetics layerScript\n",
      "Base layer\n",
      "Geom Layer\n",
      "Titles and axis\n",
      "Color and shape\n",
      "Theme\n",
      "32\n",
      "\n",
      "Step 5: validate results\n",
      "33\n",
      "\n",
      "\n",
      "•Diffuse among Democrats\n",
      "•Republicans more dense\n",
      "•Can gauge “outliers”\n",
      "•Can see the polarization between the two political parties Step 6: interpret results\n",
      "34\n",
      "\n",
      "\n",
      "•Clustering is more powerful than the human eye in3D\n",
      "•Clustering mathematically defines which cluster the peripheral points should be in when it’s not obvious to the human eye\n",
      "•Clustering is helpful when many dimensions / variables exist that you can’t visualize at once\n",
      "–Whiskey similarity example from classification lectureClustering vs.\n",
      "visualizing\n",
      "Aye, Nay and Other Votes\n",
      "in House of Representatives\n",
      "35\n",
      "\n",
      "•Goals of clustering:\n",
      "–Maximize the separation between clusters •i.e.\n",
      "Maximize inter -cluster distance –Keep similar points in a cluster close together •i.e.\n",
      "Minimize intra -cluster distanceHow good is the clustering?\n",
      "36\n",
      "\n",
      "•Look at the variance explained by clusters\n",
      "–In particular, the ratio of inter -cluster variance to total variance\n",
      "•How much of the total variance is explained by the clustering?Assessing how well an algorithm performsHow good is the clustering?\n",
      "Variation explained by clusters\n",
      "= inter-cluster variance / total variance\n",
      "37\n",
      "\n",
      "•cluster: a vector indicating the cluster to which each point is allocated\n",
      "•centers: a matrix of cluster centers\n",
      "•totss: the total sum of squares (sum of distances between all points)\n",
      "•withinss: vector of within -cluster sum of distances, one number per cluster\n",
      "•tot.withinss: total within -cluster sum of distances, i.e.\n",
      "sum of withinss\n",
      "•betweenss: the between -cluster sum of squares, i.e.\n",
      "totss -tot.withinss\n",
      "•size: the number of points in each cluster\n",
      "To learn more about the kmeans function run ?kmeanskmeans outputs\n",
      "38\n",
      "\n",
      "Intra vs.\n",
      "inter- cluster distance\n",
      "Intra-Cluster DistanceInter-Cluster Distance\n",
      "withinssbetweensstotss = withinss +betweenss\n",
      "39\n",
      "\n",
      "How good is the clustering?\n",
      "# Inter-cluster variance,\n",
      "# \"betweenss\" is the sum of the # distances between points from # different clustersnum_Dem= kmeans_obj_Dem$betweenss\n",
      "# Total variance# \"totss\" is the sum of the distances  # between all the points in # the data setdenom_Dem = kmeans_obj_Dem$totss\n",
      "# Variance accounted for by # clustersvar_exp_Dem = num_Dem/ denom_Dem\n",
      "var_exp_Dem[1] 0.7193405Script\n",
      "40\n",
      "\n",
      "\n",
      "•It’s easier when the number of clusters is known ahead of time, but what if we don't know how many clusters we should have?\n",
      "•Since different starting points may generate different clusters, we need a way to assess cluster quality as well.How do we choose the number of clusters (i.e.\n",
      "k)?How good is the clustering?\n",
      "41\n",
      "\n",
      "1.Elbow method\n",
      "–Computes the percentage of variance explained by clusters for a range of cluster numbers\n",
      "–Plots a graph so results are easier to see –Not guaranteed to work!\n",
      "It depends on the data in question\n",
      "2.NbClustHow to select k: two methods\n",
      "–Runs 30 different tests and provides “majority vote” for the best number of clusters (k’s) to use\n",
      "42\n",
      "\n",
      "Elbow method: measure variance\n",
      "# Run algorithm with 3 centers\n",
      "set.seed(1 )\n",
      "kmeans_obj_Dem = kmeans(clust_data_Dem,   centers = 3,\n",
      "algorithm = \"Lloyd\")\n",
      "# Inter- cluster variance\n",
      "num_Dem = kmeans_obj_Dem$ betweenss\n",
      "# Total variance\n",
      "denom_Dem = kmeans_obj_Dem $totss\n",
      "# Variance accounted for by clustersvar_exp_Dem = num_Dem / denom_Dem\n",
      "var_exp_Dem\n",
      "[1] 0.7949741Script\n",
      "43\n",
      "\n",
      "•We want to repeat the variance calculation from the previous slide for several numbers of clusters automatically\n",
      "•We can create a function that contains all the steps we want to automate Automating a step we want to repeat\n",
      "function(data, item to iterate through)\n",
      "44\n",
      "\n",
      "# The function explained_variance wraps our code from previous slides.\n",
      "explained_variance = function( data_in, k){\n",
      "# Running k- means algorithm\n",
      "set.seed(1 )  kmeans_obj = kmeans(data_in, centers = k,\n",
      "algorithm = \"Lloyd\" )\n",
      "# Variance accounted for by clusters\n",
      "var_exp = kmeans_obj $betweenss / kmeans_obj$totss\n",
      "var_exp\n",
      "}Automating a step we want to repeat\n",
      "Script\n",
      "1.A new variable is created and set equal to our function()\n",
      "2.The commands inside the function are wrapped in curly braces {}\n",
      "3.Inside the parentheses, we specify the variables that the user will input and that will then be used inside the function where they appear\n",
      "45\n",
      "\n",
      "# Recall the variable we are using for the # data that we're clustering.\n",
      "clust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]\n",
      "View( clust_data_Dem)\n",
      "# The sapply() function plugs several values # into explained_variance.\n",
      "explained_var_Dem = sapply(1 :10, explained_variance, data_in = clust_data_Dem)\n",
      "View( explained_var_Dem)\n",
      "# Data for ggplot2\n",
      "elbow_data_Dem = data.frame(k = 1:10, explained_var_Dem)\n",
      "View( elbow_data_Dem)Automating a step we want to repeat\n",
      "1.sapply() applies a function to a vector\n",
      "2.We have to tell sapply() that the we want the explained_variance function to use the clust_data data\n",
      "3.Next, we create a data frame that contains both the new variance variable (explained_var_Dem) and the different numbers of k that we used in the previous function (1 through 10)Function we created Script\n",
      "46\n",
      "\n",
      "# Plotting data\n",
      "ggplot(elbow_data_Dem, aes(x = k,  y = explained_var_Dem)) + geom_point(size = 4) +\n",
      "geom_line(size = 1 ) +\n",
      "xlab(\"k\" ) + ylab(\"Intercluster Variance/Total Variance\" ) + theme_light()Elbow method: plotting the graph\n",
      "Script\n",
      "1.geom_point() sets the size of the data points\n",
      "2.geom_line() sets the thickness of the line\n",
      "47\n",
      "\n",
      "Looking for the kink in graph of  inter- cluster variance / total varianceElbow method: measure variance\n",
      "Original data Elbow methodk = 2\n",
      "48\n",
      "\n",
      "\n",
      "•Library: \"NbClust\"\n",
      "Functions:  \"NbClust\"\n",
      "Inputs : •data –data array or data frame\n",
      "•min.nc / max.nc –minimum/maximum number of clusters\n",
      "•method –\"kmeans\"\n",
      "•There are other, more advanced arguments that can be customized but are outside of the scope of this course and are note necessary to for NbClust to workThere are a number of ways to choose the right k.\n",
      "NbClust runs 30 tests and selects k based on majority voteNbClust: k by majority vote\n",
      "NbClust(data, max.nc, method = \"kmeans\")\n",
      "49\n",
      "\n",
      "# Install the package.\n",
      "install.packages(\"NbClust\" )\n",
      "library(NbClust)\n",
      "# Run NbClust.\n",
      "nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")\n",
      "# View the output of NbClust.nbclust_obj_Dem\n",
      "# View the output that shows the number of clusters each # method recommends.View( nbclust_obj_Dem $Best.nc)NbClust : k by majority vote\n",
      "Script\n",
      "50\n",
      "\n",
      "NbClust: k by majority vote\n",
      "> nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")\n",
      "...\n",
      "******************************************************************* * Among all indices:                                                * 14 proposed 2 as the best number of clusters * 3 proposed 3 as the best number of clusters * 1 proposed 4 as the best number of clusters * 3 proposed 6 as the best number of clusters * 1 proposed 9 as the best number of clusters * 1 proposed 10 as the best number of clusters * 1 proposed 15 as the best number of clusters ***** Conclusion *****                            * According to the majority rule, the best number of clusters is  2\n",
      "Note: additional information appears; the above information is most relevant to us for nowConsole\n",
      "51\n",
      "\n",
      "NbClust: k by majority vote\n",
      "> nbclust_obj_Dem Console•nbclust_obj_Dem shows the outputs of NbClust\n",
      "–One of the outputs is Best.nc, which shows the number of clusters                               recommended by each test 52\n",
      "\n",
      "NbClust: k by majority vote\n",
      "•We want to visualize a histogram to make it obvious how many votes there are for each number of clusters 53\n",
      "\n",
      "# Subset the 1st row from Best.nc and convert it  # to a data frame, so ggplot2 can plot it.\n",
      "freq_k_Dem = nbclust_obj_Dem $Best.nc[1 ,]\n",
      "freq_k_Dem = data.frame( freq_k_Dem)\n",
      "View(freq_k_Dem)\n",
      "# Check the maximum number of clusters.\n",
      "max(freq_k_Dem )\n",
      "# Plot as a histogram.\n",
      "ggplot(freq_k_Dem,\n",
      "aes(x = freq_k_Dem)) +\n",
      "geom_bar() +scale_x_continuous(breaks = seq(0 , 15, by = 1)) +\n",
      "scale_y_continuous(breaks = seq(0 , 12, by = 1)) +\n",
      "labs(x = \"Number of Clusters\",\n",
      "y = \"Number of Votes\" ,\n",
      "title = \"Cluster Analysis\")NbClust: k by majority vote\n",
      "Script\n",
      "2 clusters is the winner with 12 votes\n",
      "54\n",
      "\n",
      "•If you’re a lobbyist, which congressperson can you influence for swing votes?\n",
      "•If you’re managing a campaign and your competitor is always voting along party lines, how can you use that information?\n",
      "•If your congressperson is not an active voter, is she representing your interests?\n",
      "•What do the voting patterns look like for Republican -introduced bills?Application of results\n",
      "55\n",
      "\n",
      "•Could see differences between the patterns of Reb lead bills and Democrat lead bills\n",
      "•Could provide information on congressmen that might be see has swing votes.\n",
      "Implications of results\n",
      "56\n",
      "\n",
      "•We are assuming that the patterns correspond with the same bills being voted on –perhaps some Congressmen have the same number of 'aye' and 'nay' votes, but voted on different bills\n",
      "•Network analysis can help determine additional connections between Congressmen\n",
      "•We haven't taken extenuating factors into account – political initiatives, current events, etc.\n",
      "This is a preliminary analysis that gives us initial insights and can help us direct further researchLimitations of results\n",
      "57\n",
      "\n",
      "•The good and bad\n",
      "–+ cheap –NO LABELS , labels are expensive to create and maintain\n",
      "–+/-clustering always works\n",
      "–-Many methods to choose from and knowing the right one can be nontrivial and the differences between many are almost zero, so you need to understand what you're doing\n",
      "•The evil\n",
      "–Curse of dimensionality\n",
      "–Clusters may result from poor data quality\n",
      "–Non-deterministic (e.g.\n",
      "k -means) subject to local minimum.\n",
      "Since it works with averages, k-means does not get much better with Big Data (marginal improvements) –Non spherical data may result in poor clustering (depending on method used)\n",
      "–Unequal cluster sizes may result in poor clustering (depending on method used)The good, bad, and evil\n",
      "58\n",
      "\n",
      "59\n",
      "\n",
      "\n",
      "•Analysts need to ask the following questions\n",
      "–Do you want overlapping or non -overlapping clusters ?\n",
      "–Does your data satisfy the assumptions of the clustering algorithm?\n",
      "–How was the distance measure identified ?\n",
      "–How many clusters and why ?\n",
      "Identifying the number of clusters is a difficult task if the number of class labels is not known beforehand –Does your method scale to the size of the data?\n",
      "–Is the compute time congruent with the temporal budget of your business need (i.e.\n",
      "do you get answers back in time to make meaningful decisions)The good, bad, and evil\n",
      "60\n",
      "\n",
      "Introduction to Decision Trees, Background and Application….Ensemble Overview\n",
      "Brian Wright\n",
      "\n",
      "Outline\n",
      "Decision Trees\n",
      "Basics\n",
      "Background Advantages and Limitations\n",
      "Mathematical Approaches and Example\n",
      "Example in R\n",
      "Evaluation\n",
      "2\n",
      "\n",
      "Basics: Graph Elements\n",
      "Tree begins with a Root Node that has no incoming edges and two or more out going edges\n",
      "Internal Node –Has one incoming edge and two or more outgoing and represent test conditions at every given level Leaf Node –One incoming edge and no outgoing edges\n",
      "Edges –Connections between nodes\n",
      "3\n",
      "\n",
      "Basics: Graph Example\n",
      "4\n",
      "Root Node\n",
      "Internal NodesEdges\n",
      "Leaf Nodes\n",
      "\n",
      "1.What is the most important question to move on to a second date?\n",
      "The question with the most amount of relevant information.Basics: Intuition\n",
      "Are you married?What music do you like?>\n",
      "5\n",
      "\n",
      "2.How do you combine questions?\n",
      "Conditional on the first answer -select the next most important question for information gain.Basics: Intuition\n",
      "Belief in a blue colored sky?Are you married?\n",
      "YESNO\n",
      "Stop!Are you married?\n",
      "YES NO\n",
      ">Question 1\n",
      "Question 2\n",
      " What music do you like?\n",
      "6Stop!\n",
      "3.When should you stop asking questions?\n",
      "When the answer no longer provides additional relevant information.Basics: Intuition\n",
      "50% WILL GO ON A SECOND DATE\n",
      "50% WILL GO ON A SECOND DATE\n",
      "What music do you like?\n",
      "7\n",
      "\n",
      "Step 1: Ask the question with the most amount of information, where “most amount of information ”is based on some objective criteria.\n",
      "Step 2: Conditional on the first answer, select the next most important question.\n",
      "Step 3: When the answer no longer provides additional information (no information gain), stop growing the branch.\n",
      "Step 4: Repeat steps 2 and 3 for each question branch.Basics: Building a tree in four steps\n",
      "8\n",
      "\n",
      "Basics\n",
      "Decision trees are a hierarchical technique Meaning that a series of decisions are made until a predetermined metric is met\n",
      "Model is built such that a sequence of ordered decisions concerning values of data features results in assigning class labels\n",
      "Nonparametric\n",
      "Number of parameters is not pre- determined as is the case with linear models that have pre- determine parameters thus limiting their degrees of freedom\n",
      "No assumptions need to be met concerning parameters or distributions\n",
      "Best recognized through graphs produced\n",
      "Type of Acyclic graph -are used to model probabilities, connectivity, and causality.\n",
      "A “graph” in this sense means a structure made from nodes and edges\n",
      "Trees consist of nodes and edges defined by decisions rules applied to the data features\n",
      "9\n",
      "\n",
      "Background\n",
      "10\n",
      " Source: https://www.thehindu.com/features/friday -review/where -sanskrit -meets -computer -science/article7061379.ece\n",
      "\n",
      "Background Uses recursive binary splitting - Considering every possible partition of space is computationally infeasible, a greedy approach is used to divide the space.\n",
      "Greedy algorithm because at each step of the tree building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in the future.\n",
      "Trees can be regression or classification based, but in both instances will use recursive binary splitting\n",
      "The difference is that in regression based trees we are predicting the actual class whereas in classification we are generating the probability of class inclusion as the determinate of the splitting\n",
      "This probability measure that drives the splitting for classification comes in two forms: Gini Index or Entropy 11\n",
      "\n",
      "Background: CART Algorithm and C4.5\n",
      "12Classification and Regression Tree (CART) –1984 Breiman , Friedman, Olshen and Stone –Binary Trees\n",
      "Can be used on numerical or categorical data\n",
      "First splits the training data in two subsets using a single feature k and a threshold tk\n",
      "Searches through all possible pairs (k, tk) to identify the split that produces the purest subsets, based on weighted average of information gain.\n",
      "Stops once it cannot find a split that reduces impurity or by a pre -determine node size (hyperparameter ).\n",
      "C4.5 –Grew out of ID3 (early version) in the late 1980s early 90s both from J.\n",
      "Ross Quinlan, uses gain ratio, accepts cont.\n",
      "and discrete, introduced pruning and the application of different weights to variables C5.0 –Next version of C4.5 –performance improvements, computationally more efficient and allows for boosting\n",
      "\n",
      "Advantages and Limitations\n",
      "13\n",
      " Source: https://socialmediamanager.tweetinggoddess.com/2018/04/20/advantages -and- disadvantages -of-twitter/\n",
      "\n",
      "Advantages Simple to understand and to interpret through visualization.\n",
      "Requires little data preparation.\n",
      "Other techniques often require data normalization, dummy variables need to be created and blank values to be removed.\n",
      "Able to handle both numerical and categorical data.\n",
      "Uses a white box model.\n",
      "If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic.\n",
      "By contrast, in a black box model (some neural network approaches), results may be more difficult to interpret.\n",
      "Fairly straight forward to evaluate and understand reliability of the model.\n",
      "ROC/Hit Rate/Error Rate/\n",
      "14\n",
      "\n",
      "Limitations\n",
      "Decision -tree learners can create over -complex trees that do not generalize the data well.\n",
      "This is called overfitting .\n",
      "Compare terminal nodes to data points, use the depth of the tree to calculate terminal nodes, for example 6 levels = 26 or 64 terminal nodes, if you have 100 data points that’s a lot of single data terminal nodes.\n",
      "Leaf nodes roughly double with every additional level of the tree.\n",
      "Mechanisms such setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree can be used to avoid this problem.\n",
      "Decision trees can be unstable as small variations in the data might result in a completely different tree being generated.\n",
      "This problem is mitigated by using decision trees within an ensemble like Random Forest.\n",
      "15\n",
      "\n",
      "Limitations\n",
      "Practical decision -tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node.\n",
      "Such algorithms cannot guarantee to return the globally optimal decision tree.\n",
      "This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n",
      "Decision tree learners create biased trees if some classes dominate.\n",
      "It is therefore recommended to balance the dataset prior to fitting if necessary 16\n",
      "\n",
      "Mathematical Approaches and Examples\n",
      "17\n",
      "\n",
      "Mathematical Approaches: Node split criterion 18Decision Trees can use several different types of node split criteria depending on the data or data scientist’s preference Regression/MSE –Continuous data\n",
      "Entropy –Binary data splits Gini Coefficient –Most common approach\n",
      "Let’s take a look at each approachBoth Entropy and Gini Coefficient use Information Gain to determine variable split criteria \n",
      "Tree Based Methods\n",
      "Several approaches to tree building CART –Gini Index – Binary Trees\n",
      "ID3 –Information Gain  C4.5 –Gains Ratio –Introduced pruning C5.0 –Improvement on C4.5 –Boosting, computationally efficient 19\n",
      "\n",
      "The formula for entropy is below, where Pi is the probability that a random selection would have a state i\n",
      "(6/6log2 6/6)  = 0 Entropy = sum( -Pi* log2Pi)\n",
      "= 3  = 3  = 2  = 4  = 0\n",
      "= 6  (2/6log2 2/6)  -(4/6log2 4/6)  = 0.92 (3/6 log2 3/6) –(3/6log2 3/6) = 1 Mathematical Approaches: Classification, Entropy (C4.5)\n",
      "\n",
      "Information gain helps us understand how important an attribute is in the data\n",
      "We can use it to decide how to order the nodes of the decision tree\n",
      "Information gain = entropy (parent) –average entropy (children)\n",
      "-(3/6 log2 3/6) –(3/6log2 3/6) = 1\n",
      "-(2/2log2 2/2)\n",
      "= 0-(3/4 log2 3/4) –(1/4log2 1/4)\n",
      "= 0.81entropy (parent) average entropy (children)Mathematical Approaches: Classification, Entropy\n",
      "\n",
      "In order to calculate the average entropy for the split, we need to weigh the split by the number of data points in each node.\n",
      "So we create a weighted average of the entropy of the children nodes.\n",
      "Information gain (ratio) = entropy (parent) – average entropy (children)\n",
      "= (2/6 * 0) + (4/6 * 0.81)\n",
      "= 0.54entropy (parent) Weighted average entropy (children)\n",
      "= 1Mathematical Approaches: Classification, Entropy\n",
      "\n",
      "In order to construct the tree, we need to follow three steps:\n",
      "1.Choose the attribute with the highest information gain\n",
      "2.Construct the child nodes\n",
      "3.Repeat steps 1 and 2 recursively until no more information can be gainedMathematical Approaches: Information gain + entropy: example\n",
      "Outlook Temp Humidity Play\n",
      "Sunny Hot High No\n",
      "Sunny Hot Low No\n",
      "Cloudy Cool High Yes\n",
      "Sunny Cool Low YesShould you play outside?\n",
      "23\n",
      "\n",
      "1.Choose the attribute with the highest information gainMathematical Approaches: Information gain + entropy: example\n",
      "2 yes 2 no\n",
      "Cloudy Sunny\n",
      "1 yes, 2 no 1 yes\n",
      "-(1/3 log2 1/3) –(2/3 log2 2/3)\n",
      "= 0.92-(1/1 log2 1/1) = 0-(2/4 log2 2/4) –(2/4 log2 2/4) = 1\n",
      "I.G.\n",
      "= 1 –((3/4 * 0.92) + (1/4 * 0 )) = 0.31weighted averageOutlook Temp Humidity Play\n",
      "Sunny Hot High No\n",
      "Sunny Hot Low No\n",
      "Cloudy Cool High Yes\n",
      "Sunny Cool Low YesShould you play outside?\n",
      "24\n",
      "\n",
      "1.Choose the attribute with the highest information gainMathematical Approaches: Information gain + entropy: example\n",
      "2 yes 2 no\n",
      "Cool Hot\n",
      "2 no 2 yes\n",
      "-(2/2 log2 2/2)= 0-(2/2 log2 2/2) = 0-(2/4 log2 2/4) –(2/4 log2 2/4) = 1\n",
      "I.G.\n",
      "= 1 –((2/4 * 0) + (2/4 * 0)) = 1weighted averageOutlook Temp Humidity Play\n",
      "Sunny Hot High No\n",
      "Sunny Hot Low No\n",
      "Cloudy Cool High Yes\n",
      "Sunny Cool Low YesShould you play outside?\n",
      "0.31\n",
      "25I.G.\n",
      "1.Choose the attribute with the highest information gainMathematical Approaches: Information gain + entropy: example\n",
      "2 yes 2 no\n",
      "Low High\n",
      "1 yes 1 no 1 yes 1 no-(2/4 log2 2/4) –(2/4log2 2/4) = 1\n",
      "I.G.\n",
      "= 1 –((2/4 * 1) + (2/4 * 1)) = 0weighted averageOutlook Temp Humidity Play\n",
      "Sunny Hot High No\n",
      "Sunny Hot Low No\n",
      "Cloudy Cool High Yes\n",
      "Sunny Cool Low YesShould you play outside?\n",
      "0.31 1-(1/2 log2 1/2) –(1/2log2 1/2)\n",
      "= 1-(1/2 log2 1/2) –(1/2log2 1/2) = 1\n",
      "26I.G.\n",
      "Temp has the highest information gain resulting in an entropy of 1, meaning that this attribute perfectly matches class predictionMathematical Approaches: Information gain + entropy: example\n",
      "2 yes 2 no\n",
      "Cool Hot\n",
      "2 no 2 yesOutlook Temp Humidity Play\n",
      "Sunny Hot High No\n",
      "Sunny Hot Low No\n",
      "Cloudy Cool High Yes\n",
      "Sunny Cool Low YesShould you play outside?\n",
      "0.31 1 0Play\n",
      "27I.G.\n",
      "C4.5/C5.0 and Gains Ratio C4.5/C5.0 –Uses Gain Ratio that extends the previous example but dividing the information gain by the overall split ratio for the feature\n",
      "Gr(S,A) = G(S,A)/ Split (S,A)\n",
      "28Information Gain Split Information \n",
      "1.Choose the attribute with the highest info gain ratio Mathematical Approaches: Information gain + entropy + gains ratio (C4.5)\n",
      "2 yes 2 no\n",
      "Cloudy Sunny\n",
      "1 yes, 2 no 1 yes\n",
      "-(1/3 log2 1/3) –(2/3 log2 2/3)\n",
      "= 0.92-(1/1 log2 1/1) = 0-(2/4 log2 2/4) –(2/4 log2 2/4) = 1\n",
      "I.G.\n",
      "= 1 –((3/4 * 0.92) + (1/4 * 0 )) = 0.31weighted averageOutlook Temp Humidity Play\n",
      "Sunny Hot High No\n",
      "Sunny Hot Low No\n",
      "Cloudy Cool High Yes\n",
      "Sunny Cool Low YesShould you play outside?\n",
      "29Split = -3/4log23/4 –1/4log21/4 = .811 G Ratio = .31/.811 = .38 \n",
      "Mathematical Approaches: Classification, Gini Coefficient (CART)\n",
      "Gini\n",
      "Gini Impurity = 1 – sum[(Pi)2]\n",
      "Pi –Represents the probability that a random selection would have state i(kinda like a target)\n",
      "Same mathematical process as entropy Example:\n",
      "302 yes 2 no\n",
      "1 yes, 2 no 1 yes\n",
      "1 –[ (1/3)2+ (2/3)2] = 0.44 1 –[ (1/1)2]= 01 –[ (2/4)2–(2/4)2] = 0.5\n",
      "Gini Impurity  = 0.5 –((3/4 * 0.44) + (1/4 * 0)) = 0.19weighted average\n",
      "\n",
      "Mathematical Approaches: Regression/MSE\n",
      "31Works to identify the split point in the data set that minimizes mean squared error (MSE) point\n",
      "The average of each of the groups is the term that minimizes the mean squared error\n",
      "MSE –is the average of the difference between the prediction and actual values\n",
      "The decision tree algorithm searches through all variables and all possible split points to identify the point that minimizes error\n",
      "\n",
      "\n",
      "Practice…Poll  32https://www.sli.do/\n",
      "Use # 68881\n",
      "Or https://app.sli.do/event/tyl2nmrk\n",
      "\n",
      "Decision Trees: Overfitting and Hyper -parameters\n",
      "Decision trees are often prone to overfitting, one solution is to utilize the hyper -parameters to control how the tree grows\n",
      "Another option is to use an ensemble method via bagging or what’s known as Random Forest\n",
      "33\n",
      "\n",
      "Decision Trees: Hyper -parameter tuning (Pruning)\n",
      "Minimum samples for a node split Minimum number of samples (or observations) which are required in a node to be considered for splitting.\n",
      "Higher values prevent a model from learning relations which might be highly specific to the particular sample.\n",
      "It should be tuned using cross validation.\n",
      "Minimum samples for a terminal node (leaf) The minimum number of samples (or observations) required in a terminal node or leaf.\n",
      "For imbalanced class problems, a lower value should be used since regions dominant with samples belonging to minority class will be much smaller in number.\n",
      "Maximum depth of tree (vertical depth) The maximum depth of trees, lower values prevent a model from learning relations which might be highly specific to the particular sample.\n",
      "It should be tuned using cross validation.\n",
      "34\n",
      "\n",
      "Decision Trees: Hyper -parameter tuning (Pruning)\n",
      "Maximum number of terminal nodes Also referred as number of leaves .\n",
      "Since binary trees are created, a depth of nwould produce a maximum of 2^n leaves.\n",
      "Maximum features to consider for split The number of features to consider (selected randomly) while searching for a best split.\n",
      "A typical value is the square root of total number of available features.\n",
      "A higher number typically leads to over -fitting but is dependent on the problem as well.\n",
      "35\n",
      "\n",
      "Cross -Validation\n",
      "36\n",
      "\n",
      "\n",
      "Cross -Validation\n",
      "37\n",
      "\n",
      "\n",
      "Decision Trees: Definitions\n",
      "Overfitting –model becomes overly complex and as a result is predicting noise or the space between features (random error) instead of the true relationship.\n",
      "It is in theory possible to create a leaf node for every data point.\n",
      "Ensemble methods – Process of running numerous models and codifying them using a decision rule to choose the optimal model result –example is majority vote on feature inclusion\n",
      "Heuristic algorithms –approaches designed for operational efficiency generating an approximation to the ideal result but does not guarantee the best model\n",
      "38\n",
      "\n",
      "Ensemble Methods\n",
      "A standard error is by definition the standard deviation of the sampling distribution of a parameter estimate, generated by repeated sampling.\n",
      "xerror reflects the mean of the sample means (of the errors) from the ten folds; xstd reflects the standard deviation of the sample means (of the errors) from the ten folds.\n",
      "Thus, xstd is a standard deviation of sample means, which is also known as the standard error of the mean.\n",
      "39\n",
      "\n",
      "Example in R\n",
      "42\n",
      "\n",
      "1\n",
      "Crash Trends in Commercial Vehicles in Virginia Consultants: Students of Practice and Application 4001 Client: Commonwealth of Virginia- Department of Motor Vehicles Description: Crashes involving commercial motor vehicles have increased substantially, especially on certain roadways, in the past five years.\n",
      "The Virginia Department of Motor Vehicles is interested in learning about trends in those crashes and what they can tell us about potential driver training.\n",
      "Objective: Determine factors involved in increasing commercial motor vehicle crashes.\n",
      "Identify trends that can alert us to additional training needs for Commercial Drivers License holders that could then be provided to Commercial Driver Training Schools.\n",
      "2Group Activity 1\n",
      "You have 20ish minutes for this exercise.\n",
      "Read through the project summary again, assign a timekeeper, a note taker, and a presenter then consider and answer the following:\n",
      "1.What is the current situation?\n",
      "Would you like to know more?\n",
      "What would you ask your client if you could?\n",
      "2.Define a solid, measurable goal that you think would satisfy your client.\n",
      "What is your metric of success?\n",
      "3.What data would you like to have (think big)?\n",
      "How would you get this data?\n",
      "How would it have been gathered (sensors, cameras, etc.)?\n",
      "Would retrospective data likely all have been in the same spot?\n",
      "4.What deliverable would you like to hand over to your client at the end of the project (you can think big here –be creative)?\n",
      "3Stop Here for Now…\n",
      "\n",
      "Answer the Four Questions\n",
      "American Pharaoh won the Triple Crown.\n",
      "He won the Belmont, a 12 furlong track, with a time of 2:26.65, the Kentucky Derby, a 10 furlong track, with a time of 2:03.02, and the Preakness Stakes, a 9.5 furlong track, with a time of 1.58.46.\n",
      "What was his average speed in mph?\n",
      "1.What do I know?\n",
      "2.What am I looking for?\n",
      "3.What else do I need to find it?\n",
      "4.What do I expect?\n",
      "Now Solve the Problem\n",
      "Work in your Lab Teams to code the solution with an output that reads “[Horse’s Name] averaged [ x ] mph across the three triple crown races.” Psuedo Code the framework of the function using a google doc then code the answer together sharing one person's screen.\n",
      "American Pharaoh won the Triple Crown.\n",
      "He won the Belmont, a 12 furlong track, with a time of 2:26.65, the Kentucky Derby, a 10 furlong track, with a time of 2:03.02, and the Preakness Stakes, a 9.5 furlong track, with a time of 1.58.46.\n",
      "What was his average speed in mph?\n",
      "Fun with functions and dplyr\n",
      "Brian Wright\n",
      "1/24/2020\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 1 / 28\n",
      "\n",
      "Overview of Functions (Advanced R)\n",
      "Functions are at the core of R language, it’s really a function based\n",
      "language\n",
      "“R, at its heart, is a functional language.\n",
      "This means that it has\n",
      "certain technical properties, but more importantly that it lends\n",
      "itself to a style of problem solving centred on functions.” Hadley\n",
      "Wickham\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 2 / 28\n",
      "\n",
      "What is a functional based language?\n",
      "Recently functions have grown in popularity because they can produce\n",
      "eﬃcient and simple solutions to lots of problems.\n",
      "Many of the\n",
      "problems with performance have been solved.\n",
      "Functional programming compliments object oriented programming\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 3 / 28\n",
      "\n",
      "What makes a programming approach “functional”?\n",
      "Functions can behave like any other data structure\n",
      "▶Assign them to variables, store to lists, pass them as aurguments to\n",
      "other functions, create them inside functions and even produce a\n",
      "function as a result of a funcion\n",
      "Functions need to be “pure” meaning that if you call it again with the\n",
      "same inputs you get the same results.\n",
      "sys.time() not a “pure”\n",
      "function\n",
      "The execution of the function shouldn’t change global variables, have\n",
      "no side eﬀects.\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 4 / 28\n",
      "\n",
      "Functions\n",
      "Function don’t have to be “pure” but it can help to ensure your code\n",
      "is doing what you intend it to do.\n",
      "Functional programming helps to break a problem down into it’s\n",
      "pieces.\n",
      "When working to solve a problem it helps to divide the code\n",
      "into individually operating functions that solve parts of the problem.\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 5 / 28\n",
      "\n",
      "Types of Functions\n",
      "Figure 1: Function Types\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 6 / 28\n",
      "\n",
      "Let’s Build a Function\n",
      "Basically recipes composed of series of R statements\n",
      "name <- funtion (variables){\n",
      "#In here goes the series of R statements\n",
      "}\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 7 / 28\n",
      "\n",
      "Example, talk out the steps\n",
      "my_mean <- function (x){\n",
      "Sum <- sum(x)#Here we are using a function\n",
      "#inside a function!\n",
      "N <- length (x)\n",
      "return (Sum /N)#return is optional but helps with\n",
      "#clarity on some level.\n",
      "}\n",
      "Create a little list and pass it to the function and see if it works.\n",
      "Also call the Sum and N variables...does this work?\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 8 / 28\n",
      "\n",
      "Functional - Will show later, Function Factory\n",
      "(Advanced R)\n",
      "power1 <- function (exp) {\n",
      "function (x) {\n",
      "x^exp\n",
      "}\n",
      "}\n",
      "#Assigning the exponentials\n",
      "square <- power1 (2)\n",
      "cube <- power1 (3)\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 9 / 28\n",
      "\n",
      "Run the Created Functions\n",
      "square (3)\n",
      "> [1] 9\n",
      "cube (3)\n",
      "> [1] 27\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 10 / 28\n",
      "\n",
      "Quick Exercise\n",
      "Create a function that computes the range of a variable and then\n",
      "fornogoodreasonadds100anddividesby10.\n",
      "Writeoutthesteps\n",
      "you would need ﬁrst in Pseudocode, then develop the function.\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 11 / 28\n",
      "\n",
      "dplyrverbs in the tidyverse\n",
      "The dplyrpackage gives us a few verbs for data manipulation\n",
      "Function Purpose\n",
      "select Select columns based on name or position\n",
      "mutate Create or change a column\n",
      "ﬁlter Extract rows based on some criteria\n",
      "arrange Re-order rows based on values of variable(s)\n",
      "group_by Split a dataset by unique values of a variable\n",
      "summarize Create summary statistics based on columns\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 12 / 28\n",
      "\n",
      "select\n",
      "You can select columns by name or position, of course.\n",
      "You can also select columns based on some criteria, which are\n",
      "encapsulated in functions.\n",
      "starts_with(“ \"), ends_with(\" ”), contains(“____”)\n",
      "one_of(“____”,“_____”,“______”)\n",
      "There are others; see help(starts_with) .\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 13 / 28\n",
      "\n",
      "Example\n",
      "Load the weather.csv .\n",
      "This contains daily temperature data in 2010 for\n",
      "some location.\n",
      "head (weather, 2)\n",
      "> # A tibble: 2 x 35\n",
      "> id year month element d1 d2 d3 d4 d5 d6 d7 d8\n",
      "> <chr> <int> <int> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n",
      "> 1 MX17~ 2010 1 tmax NA NA NA NA NA NA NA NA\n",
      "> 2 MX17~ 2010 1 tmin NA NA NA NA NA NA NA NA\n",
      "> # ...\n",
      "with 23 more variables: d9 <lgl>, d10 <dbl>, d11 <dbl>, d12 <lgl>,\n",
      "> # d13 <dbl>, d14 <dbl>, d15 <dbl>, d16 <dbl>, d17 <dbl>, d18 <lgl>,\n",
      "> # d19 <lgl>, d20 <lgl>, d21 <lgl>, d22 <lgl>, d23 <dbl>, d24 <lgl>,\n",
      "> # d25 <dbl>, d26 <dbl>, d27 <dbl>, d28 <dbl>, d29 <dbl>, d30 <dbl>,\n",
      "> # d31 <dbl>\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 14 / 28\n",
      "\n",
      "How would you just select the columns with the daily\n",
      "data?\n",
      "select (weather, starts_with (\"d\"))\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 15 / 28\n",
      "\n",
      "mutate\n",
      "mutatecan either transform a column in place or create a new column in\n",
      "a dataset\n",
      "We’ll use the in-built mpgdataset for this example, We’ll select only the\n",
      "city and highway mileages.\n",
      "To use this selection later, we will need to\n",
      "assign it to a new name\n",
      "mpg1 <- select (mpg, cty, hwy)\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 16 / 28\n",
      "\n",
      "mutate\n",
      "We’ll change the city and highway mileage to km/l from mpg.\n",
      "This will\n",
      "involve multiplying it by 1.6 and dividing by 3.8\n",
      "head (mutate (mpg1, cty = cty *1.6 /3.8,\n",
      "hwy = hwy *1.6/3.8), 5)\n",
      "> # A tibble: 5 x 2\n",
      "> cty hwy\n",
      "> <dbl> <dbl>\n",
      "> 1 7.58 12.2\n",
      "> 2 8.84 12.2\n",
      "> 3 8.42 13.1\n",
      "> 4 8.84 12.6\n",
      "> 5 6.74 10.9\n",
      "This is in-place replacement\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 17 / 28\n",
      "\n",
      "New Variable Deﬁned\n",
      "mutate (mpg1, cty1 = cty *1.6/3.8, hwy1 = hwy *1.6/3.8)\n",
      "> # A tibble: 234 x 4\n",
      "> cty hwy cty1 hwy1\n",
      "> <int> <int> <dbl> <dbl>\n",
      "> 1 18 29 7.58 12.2\n",
      "> 2 21 29 8.84 12.2\n",
      "> 3 20 31 8.42 13.1\n",
      "> 4 21 30 8.84 12.6\n",
      "> 5 16 26 6.74 10.9\n",
      "> 6 18 26 7.58 10.9\n",
      "> 7 18 27 7.58 11.4\n",
      "> 8 18 26 7.58 10.9\n",
      "> 9 16 25 6.74 10.5\n",
      "> 10 20 28 8.42 11.8\n",
      "> # ...\n",
      "with 224 more rows\n",
      "This creates new variables\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 18 / 28\n",
      "\n",
      "ﬁlter\n",
      "filterextracts rows based on criteria\n",
      "filter (mpg, cyl ==4)\n",
      "> # A tibble: 81 x 11\n",
      "> manufacturer model displ year cyl trans drv cty hwy fl class\n",
      "> <chr> <chr> <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\n",
      "> 1 audi a4 1.8 1999 4 auto~ f 18 29 p comp~\n",
      "> 2 audi a4 1.8 1999 4 manu~ f 21 29 p comp~\n",
      "> 3 audi a4 2 2008 4 manu~ f 20 31 p comp~\n",
      "> 4 audi a4 2 2008 4 auto~ f 21 30 p comp~\n",
      "> 5 audi a4 q~ 1.8 1999 4 manu~ 4 18 26 p comp~\n",
      "> 6 audi a4 q~ 1.8 1999 4 auto~ 4 16 25 p comp~\n",
      "> 7 audi a4 q~ 2 2008 4 manu~ 4 20 28 p comp~\n",
      "> 8 audi a4 q~ 2 2008 4 auto~ 4 19 27 p comp~\n",
      "> 9 chevrolet mali~ 2.4 1999 4 auto~ f 19 27 r mids~\n",
      "> 10 chevrolet mali~ 2.4 2008 4 auto~ f 22 30 r mids~\n",
      "> # ...\n",
      "with 71 more rows\n",
      "This extracts only 4 cylinder vehicles\n",
      "Other choices might be cyl != 4 ,cyl > 4,year == 1999 ,\n",
      "manufacturer==\"audi\"Brian Wright Fun with functions and dplyr 1/24/2020 19 / 28\n",
      "\n",
      "Practice Piping\n",
      "admit_df <- read_csv (\"LogReg.csv\")\n",
      "str(admit_df)\n",
      "> Classes /quotesingle.ts1spec_tbl_df /quotesingle.ts1,/quotesingle.ts1tbl_df /quotesingle.ts1,/quotesingle.ts1tbl/quotesingle.ts1and /quotesingle.ts1data.frame /quotesingle.ts1: 400 obs.\n",
      "of 4 variables:\n",
      "> $ admit: num 0 1 1 1 0 1 1 0 1 0 ...\n",
      "> $ gre : num 380 660 800 640 520 760 560 400 540 700 ...\n",
      "> $ gpa : num 3.61 3.67 4 3.19 2.93 3 2.98 3.08 3.39 3.92 ...\n",
      "> $ rank : num 3 3 1 4 4 2 1 2 3 2 ...\n",
      "> - attr(*, \"spec\")=\n",
      "> ..\n",
      "cols(\n",
      "> ..\n",
      "admit = col_double(),\n",
      "> ..\n",
      "gre = col_double(),\n",
      "> ..\n",
      "gpa = col_double(),\n",
      "> ..\n",
      "rank = col_double()\n",
      "> ..\n",
      ")\n",
      "#Do we notice anything that seems a bit off.\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 20 / 28\n",
      "\n",
      "Coercion num to factor\n",
      "admit_df $rank <- as.factor (admit_df $rank)\n",
      "#changes rank to a factor\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 21 / 28\n",
      "\n",
      "Five Basic Classes in R\n",
      "character\n",
      "numeric (double precision ﬂoating point numbers, default)\n",
      "integer (subset of numeric)\n",
      "complex (j = 10 + 5i)\n",
      "logical (True/False)\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 22 / 28\n",
      "\n",
      "All have coercion calls (example from: R Nuts and\n",
      "Bolts)\n",
      "x <- 0 :6\n",
      "class (x)#why\n",
      "> [1] \"integer\"\n",
      "as.numeric (x)\n",
      "> [1] 0 1 2 3 4 5 6\n",
      "as.logical (x)\n",
      "> [1] FALSE TRUE TRUE TRUE TRUE TRUE TRUE\n",
      "as.character (x)\n",
      "> [1] \"0\" \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 23 / 28\n",
      "\n",
      "Functional Example: Pass a function get a vector\n",
      "We can also convert multiple columns using lapply(), great example of\n",
      "functional orientation of R.\n",
      "names <- c(\"admit\",\"rank\")\n",
      "#using names as a index on admit_df,\n",
      "admit_df[,names] <- lapply (admit_df[,names], factor)\n",
      "#Check class of those two variables\n",
      "(as.character (meta_fun <- lapply (subset (admit_df,\n",
      "select = names),\n",
      "class)))\n",
      "> [1] \"factor\" \"factor\"\n",
      "#using a functional with two functions inside that creates a object\n",
      "coerced to a character list...what fun.\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 24 / 28\n",
      "\n",
      "Using the code chunk below to “group_by” rank\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 25 / 28\n",
      "\n",
      "Using the code chunk below to ﬁlter by 1 in the admit\n",
      "column\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 26 / 28\n",
      "\n",
      "Ok now summarise by average GPA\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 27 / 28\n",
      "\n",
      "Now Pipe everything together\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 28 / 28\n",
      "\n",
      "R Data Vis: ggplot\n",
      "Brian Wright, PhD\n",
      "Feb, 2020\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 1 / 137\n",
      "\n",
      "Why visualize data?\n",
      "Anscombe’s data example\n",
      "dataset3 dataset4dataset1 dataset2\n",
      "5 10 15 5 10 155.07.510.012.5\n",
      "5.07.510.012.5\n",
      "xy\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 2 / 137\n",
      "\n",
      "All Have the Summary Stats\n",
      "Statistic Value\n",
      "mean(x) 9\n",
      "mean(y) 7.5\n",
      "var(x) 11\n",
      "var(y) 4.13\n",
      "cor(x,y) 0.82\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 3 / 137\n",
      "\n",
      "The DataSaurus dozen\n",
      "x_shapestar v_lines wide_lineshigh_lines slant_down slant_updino dots h_linesaway bullseye circle\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 4 / 137\n",
      "\n",
      "Same Stats\n",
      "Statistic Value\n",
      "mean(x) 54.3\n",
      "mean(y) 47.8\n",
      "var(x) 281\n",
      "var(y) 725\n",
      "cor(x,y) -0.07\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 5 / 137\n",
      "\n",
      "Bottom line\n",
      "Summary statistics cannot always distinguish datasets\n",
      "Take advantage of humans’ ability to visually recognize and remember\n",
      "patterns\n",
      "Find discrepancies in the data more easily\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 6 / 137\n",
      "\n",
      "What is ggplot2?\n",
      "A second (and ﬁnal) iteration of the ggplot\n",
      "Implementation of Wilkerson’s Grammar of Graphics in R\n",
      "Conceptually, a way to layer diﬀerent elements onto a canvas to create\n",
      "a data visualization\n",
      "Started as Dr.\n",
      "Hadley Wickham’s PhD thesis (with Dr.\n",
      "Dianne Cook)\n",
      "Won the John M.\n",
      "Chambers Statistical Software Award in 2006\n",
      "Mimicked in other software platforms\n",
      "▶ggplotandseaborn in Python\n",
      "▶Translated in plotly\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 7 / 137\n",
      "\n",
      "ggplot2 uses the grammar ofgraphics\n",
      "A grammar .\n",
      ".\n",
      ".\n",
      "compose and re-use small parts\n",
      "build complex structures from simpler units\n",
      "of graphics\n",
      "think of yourself as a painter\n",
      "build a visualization using layers on a canvas\n",
      "draw layers on top of each other\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 8 / 137\n",
      "\n",
      "A dataset\n",
      "library (tidyverse)\n",
      "library (rio)\n",
      "beaches <- import (/quotesingle.ts1beaches.csv /quotesingle.ts1)\n",
      "#> # A tibble: 6 x 12\n",
      "#> date year month day season rainfall temperature enterococci day_num\n",
      "#> <date> <int> <int> <int> <int> <dbl> <dbl> <dbl> <int>\n",
      "#> 1 2013-01-02 2013 1 2 1 0 23.4 6.7 2\n",
      "#> 2 2013-01-06 2013 1 6 1 0 30.3 2 6\n",
      "#> 3 2013-01-12 2013 1 12 1 0 31.4 69.1 12\n",
      "#> 4 2013-01-18 2013 1 18 1 0 46.4 9 18\n",
      "#> 5 2013-01-24 2013 1 24 1 0 27.5 33.9 24\n",
      "#> 6 2013-01-30 2013 1 30 1 0.6 26.6 26.5 30\n",
      "#> # ...\n",
      "with 3 more variables: month_num <int>, month_name <chr>,\n",
      "#> # season_name <chr>\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 9 / 137\n",
      "\n",
      "Building a graph: Start with a blank canvas\n",
      "ggplot ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 10 / 137\n",
      "\n",
      "Add a data set\n",
      "ggplot (\n",
      "data = beaches #<< loaded earlier\n",
      ")\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 11 / 137\n",
      "\n",
      "Add a mapping from data to elements\n",
      "ggplot (\n",
      "data = beaches,\n",
      "mapping = aes(# everytime we add a \"data element\"\n",
      "#we add a aesthetic\n",
      "x = temperature,\n",
      "y = rainfall\n",
      ")\n",
      ")\n",
      "What goes in\n",
      "the x and y axes\n",
      "the color of markers\n",
      "the shape of markers\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 12 / 137\n",
      "\n",
      "0204060\n",
      "20 30 40\n",
      "temperaturerainfallBrian Wright, PhD R Data Vis: ggplot Feb, 2020 13 / 137\n",
      "\n",
      "Add a geometry to draw\n",
      "ggplot (\n",
      "data = beaches,\n",
      "mapping = aes(\n",
      "x = temperature,\n",
      "y = rainfall\n",
      ")\n",
      ")+\n",
      "geom_point ()#?\n",
      "What to draw:\n",
      "Points, lines\n",
      "Histogram, bars, pies, etc.\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 14 / 137\n",
      "\n",
      "0204060\n",
      "20 30 40\n",
      "temperaturerainfallBrian Wright, PhD R Data Vis: ggplot Feb, 2020 15 / 137\n",
      "\n",
      "Add options for the geom\n",
      "ggplot (\n",
      "data = beaches,\n",
      "mapping = aes(\n",
      "x = temperature,\n",
      "y = rainfall\n",
      ")\n",
      ")+\n",
      "geom_point (size = 4) #?\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 16 / 137\n",
      "\n",
      "0204060\n",
      "20 30 40\n",
      "temperaturerainfallBrian Wright, PhD R Data Vis: ggplot Feb, 2020 17 / 137\n",
      "\n",
      "Add a mapping to modify the geom\n",
      "ggplot (\n",
      "data = beaches,\n",
      "mapping = aes(\n",
      "x = temperature,\n",
      "y = rainfall\n",
      ")\n",
      ")+\n",
      "geom_point (\n",
      "mapping = aes(color = season_name),\n",
      "size = 4 #Why do we need the mapping?\n",
      ")\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 18 / 137\n",
      "\n",
      "0204060\n",
      "20 30 40\n",
      "temperaturerainfallseason_name\n",
      "Autumn\n",
      "Spring\n",
      "Summer\n",
      "WinterBrian Wright, PhD R Data Vis: ggplot Feb, 2020 19 / 137\n",
      "\n",
      "Split into facets\n",
      "ggplot (\n",
      "data = beaches,\n",
      "mapping = aes(\n",
      "x = temperature,\n",
      "y = rainfall\n",
      ")\n",
      ")+\n",
      "geom_point (\n",
      "mapping = aes(color = season_name),\n",
      "size = 4\n",
      ")+\n",
      "facet_wrap (~season_name) ###\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 20 / 137\n",
      "\n",
      "Summer WinterAutumn Spring\n",
      "20 30 40 20 30 400204060\n",
      "0204060\n",
      "temperaturerainfallseason_name\n",
      "Autumn\n",
      "Spring\n",
      "Summer\n",
      "WinterBrian Wright, PhD R Data Vis: ggplot Feb, 2020 21 / 137\n",
      "\n",
      "Remove the legend\n",
      "ggplot (\n",
      "data = beaches,\n",
      "mapping = aes(\n",
      "x = temperature,\n",
      "y = rainfall\n",
      ")\n",
      ")+\n",
      "geom_point (\n",
      "mapping = aes(color = season_name),\n",
      "size = 4,\n",
      "show.legend = FALSE ###\n",
      ")+\n",
      "facet_wrap (~season_name)\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 22 / 137\n",
      "\n",
      "Summer WinterAutumn Spring\n",
      "20 30 40 20 30 400204060\n",
      "0204060\n",
      "temperaturerainfallBrian Wright, PhD R Data Vis: ggplot Feb, 2020 23 / 137\n",
      "\n",
      "Change the general theme\n",
      "ggplot (\n",
      "data = beaches,\n",
      "mapping = aes(\n",
      "x = temperature,\n",
      "y = rainfall\n",
      ")\n",
      ")+\n",
      "geom_point (\n",
      "mapping = aes(color = season_name),\n",
      "size = 4,\n",
      "show.legend = FALSE\n",
      ")+\n",
      "facet_wrap (~season_name) +\n",
      "theme_bw ()###\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 24 / 137\n",
      "\n",
      "Summer WinterAutumn Spring\n",
      "20 30 40 20 30 400204060\n",
      "0204060\n",
      "temperaturerainfall##\n",
      "Update the labels\n",
      "ggplot (\n",
      "data = beaches,\n",
      "mapping = aes(\n",
      "x = temperature,\n",
      "y = rainfall\n",
      ")\n",
      ")+\n",
      "geom_point (\n",
      "mapping = aes(color = season_name),\n",
      "size = 4,\n",
      "show.legend = FALSE\n",
      ")+\n",
      "facet_wrap (~season_name) +\n",
      "theme_bw ()+\n",
      "labs (x = /quotesingle.ts1Temperature (C) /quotesingle.ts1, y = /quotesingle.ts1Rainfall (mm) /quotesingle.ts1)###Brian Wright, PhD R Data Vis: ggplot Feb, 2020 25 / 137\n",
      "\n",
      "Summer WinterAutumn Spring\n",
      "20 30 40 20 30 400204060\n",
      "0204060\n",
      "Temperature (C)Rainfall (mm)Brian Wright, PhD R Data Vis: ggplot Feb, 2020 26 / 137\n",
      "\n",
      "Add titles\n",
      "ggplot (\n",
      "data = beaches,\n",
      "mapping = aes(\n",
      "x = temperature,\n",
      "y = rainfall)\n",
      ")+\n",
      "geom_point (\n",
      "mapping = aes(color = season_name),\n",
      "size = 4,\n",
      "show.legend = FALSE\n",
      ")+\n",
      "facet_wrap (~season_name) +\n",
      "theme_bw ()+\n",
      "labs (x = /quotesingle.ts1Temperature (C) /quotesingle.ts1,\n",
      "y = /quotesingle.ts1Rainfall (mm) /quotesingle.ts1,\n",
      "title = /quotesingle.ts1Sydney weather by season /quotesingle.ts1,###\n",
      "subtitle = \"Data from 2013 to 2018\") ###\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 27 / 137\n",
      "\n",
      "Summer WinterAutumn Spring\n",
      "20 30 40 20 30 400204060\n",
      "0204060\n",
      "Temperature (C)Rainfall (mm)Data from 2013 to 2018Sydney weather by seasonBrian Wright, PhD R Data Vis: ggplot Feb, 2020 28 / 137\n",
      "\n",
      "Customize\n",
      "ggplot (\n",
      "data = beaches,\n",
      "mapping = aes(\n",
      "x = temperature,\n",
      "y = rainfall\n",
      ")\n",
      ")+\n",
      "geom_point (\n",
      "mapping = aes(color = season_name),\n",
      "size = 4,\n",
      "show.legend = FALSE\n",
      ")+\n",
      "facet_wrap (~season_name) +\n",
      "theme_bw ()+\n",
      "labs (x = /quotesingle.ts1Temperature (C) /quotesingle.ts1,\n",
      "y = /quotesingle.ts1Rainfall (mm) /quotesingle.ts1,\n",
      "title = /quotesingle.ts1Sydney weather by season /quotesingle.ts1,\n",
      "subtitle = \"Data from 2013 to 2018\") +\n",
      "theme (axis.title = element_text (size = 14), ###\n",
      "axis.text = element_text (size = 12), ###\n",
      "strip.text = element_text (size = 12)) ###\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 29 / 137\n",
      "\n",
      "Summer WinterAutumn Spring\n",
      "20 30 40 20 30 400204060\n",
      "0204060\n",
      "Temperature (C)Rainfall (mm)Data from 2013 to 2018Sydney weather by seasonBrian Wright, PhD R Data Vis: ggplot Feb, 2020 30 / 137\n",
      "\n",
      "The grammar\n",
      "Data\n",
      "Aesthetics (or aesthetic mappings)\n",
      "Geometries (as layers) or Statistics (as computed layers)\n",
      "Facets\n",
      "Themes\n",
      "(Coordinates)\n",
      "(Scales)\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 31 / 137\n",
      "\n",
      "Peeking under the hood ...\n",
      "We input the below items\n",
      "ggplot (\n",
      "data = beaches,\n",
      "aes(x = temperature,\n",
      "y = rainfall)\n",
      ")+\n",
      "geom_point ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 32 / 137\n",
      "\n",
      "What’s really run is ...\n",
      "ggplot (\n",
      "data = beaches,\n",
      "mapping = aes(\n",
      "x = temperature, y = rainfall)\n",
      ")+\n",
      "layer (\n",
      "geom = \"point\",\n",
      "stat = \"identity\",\n",
      "position = \"identity\") +\n",
      "facet_null ()+\n",
      "theme_grey ()+\n",
      "coord_cartesian ()+\n",
      "scale_x_continuous ()+\n",
      "scale_y_continuous ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 33 / 137\n",
      "\n",
      "Exploring aesthetics: Mapping color\n",
      "ggplot (\n",
      "data=beaches,\n",
      "aes(x = date,\n",
      "y = log10 (enterococci),\n",
      "color=season_name)\n",
      ")+\n",
      "geom_line ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 34 / 137\n",
      "\n",
      "−10123\n",
      "2014 2016 2018\n",
      "datelog10(enterococci)season_name\n",
      "Autumn\n",
      "Spring\n",
      "Summer\n",
      "WinterBrian Wright, PhD R Data Vis: ggplot Feb, 2020 35 / 137\n",
      "\n",
      "Adding groups to the mapping\n",
      "ggplot (\n",
      "data=beaches,\n",
      "aes(x = date,\n",
      "y = log10 (enterococci),\n",
      "color = season_name,\n",
      "group = 1) ###\n",
      ")+\n",
      "geom_line ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 36 / 137\n",
      "\n",
      "−10123\n",
      "2014 2016 2018\n",
      "datelog10(enterococci)season_name\n",
      "Autumn\n",
      "Spring\n",
      "Summer\n",
      "WinterBrian Wright, PhD R Data Vis: ggplot Feb, 2020 37 / 137\n",
      "\n",
      "Fixing the legend ordering\n",
      "ggplot (\n",
      "data=beaches,\n",
      "aes(x = date,\n",
      "y = log10 (enterococci),\n",
      "color = fct_relevel (season_name,\n",
      "c(/quotesingle.ts1Spring /quotesingle.ts1,/quotesingle.ts1Summer /quotesingle.ts1,/quotesingle.ts1Autumn /quotesingle.ts1,/quotesingle.ts1Winter /quotesingle.ts1)),\n",
      "group = 1)\n",
      ")+\n",
      "geom_line ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 38 / 137\n",
      "\n",
      "Yikes!\n",
      "−10123\n",
      "2014 2016 2018\n",
      "datelog10(enterococci)fct_relevel(season_name, c(\"Spring\", \"Summer\", \"Autumn\", \"Winter\"))\n",
      "Spring\n",
      "Summer\n",
      "Autumn\n",
      "Winter\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 39 / 137\n",
      "\n",
      "Fixing the legend ordering\n",
      "ggplot (\n",
      "data=beaches,\n",
      "aes(x = date,\n",
      "y = log10 (enterococci),\n",
      "color = fct_relevel (season_name,\n",
      "c(/quotesingle.ts1Spring /quotesingle.ts1,/quotesingle.ts1Summer /quotesingle.ts1,/quotesingle.ts1Autumn /quotesingle.ts1,/quotesingle.ts1Winter /quotesingle.ts1)),\n",
      "group = 1)\n",
      ")+\n",
      "geom_line ()+\n",
      "labs (color = /quotesingle.ts1Seasons /quotesingle.ts1)###\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 40 / 137\n",
      "\n",
      "−10123\n",
      "2014 2016 2018\n",
      "datelog10(enterococci)Seasons\n",
      "Spring\n",
      "Summer\n",
      "Autumn\n",
      "WinterBrian Wright, PhD R Data Vis: ggplot Feb, 2020 41 / 137\n",
      "\n",
      "You can also ﬁll based on data\n",
      "ggplot (\n",
      "data=beaches,\n",
      "aes(x = log10 (enterococci),\n",
      "fill = season_name)\n",
      ")+\n",
      "geom_histogram ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 42 / 137\n",
      "\n",
      "Works a little better\n",
      "0102030\n",
      "−1 0 1 2 3\n",
      "log10(enterococci)countseason_name\n",
      "Autumn\n",
      "Spring\n",
      "Summer\n",
      "Winter\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 43 / 137\n",
      "\n",
      "Exploring geometries: Univariate plots\n",
      "library (tidyverse)\n",
      "library (rio)\n",
      "dat_spine <- import (/quotesingle.ts1Dataset_spine.csv /quotesingle.ts1,\n",
      "check.names = T)\n",
      "ggplot (\n",
      "data=dat_spine,\n",
      "aes(x = Degree.spondylolisthesis)\n",
      ")+\n",
      "geom_histogram ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 44 / 137\n",
      "\n",
      "#> /grave.ts1stat_bin() /grave.ts1using /grave.ts1bins = 30 /grave.ts1.\n",
      "Pick better value with /grave.ts1binwidth /grave.ts1.\n",
      "050100\n",
      "0 100 200 300 400\n",
      "Degree.spondylolisthesiscount\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 45 / 137\n",
      "\n",
      "Histograms\n",
      "ggplot (\n",
      "data=dat_spine,\n",
      "aes(x = Degree.spondylolisthesis)\n",
      ")+\n",
      "geom_histogram (bins = 100)\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 46 / 137\n",
      "\n",
      "Very diﬀerent view of the data\n",
      "0204060\n",
      "0 100 200 300 400\n",
      "Degree.spondylolisthesiscount\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 47 / 137\n",
      "\n",
      "Density plots\n",
      "ggplot (\n",
      "data=dat_spine,\n",
      "aes(x = Degree.spondylolisthesis)\n",
      ")+\n",
      "geom_density ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 48 / 137\n",
      "\n",
      "0.0000.0050.0100.0150.020\n",
      "0 100 200 300 400\n",
      "Degree.spondylolisthesisdensityBrian Wright, PhD R Data Vis: ggplot Feb, 2020 49 / 137\n",
      "\n",
      "Density plots\n",
      "ggplot (\n",
      "data=dat_spine,\n",
      "aes(x = Degree.spondylolisthesis)\n",
      ")+\n",
      "geom_density (adjust = 1 /5)# Use 1/5 the bandwidth\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 50 / 137\n",
      "\n",
      "0.000.010.020.030.04\n",
      "0 100 200 300 400\n",
      "Degree.spondylolisthesisdensityBrian Wright, PhD R Data Vis: ggplot Feb, 2020 51 / 137\n",
      "\n",
      "Layering geometries\n",
      "ggplot (\n",
      "data=dat_spine,\n",
      "aes(x = Degree.spondylolisthesis,\n",
      "y = stat (density))\n",
      ")+# Re-scales histogram\n",
      "geom_histogram (bins = 100, fill= /quotesingle.ts1yellow /quotesingle.ts1)+\n",
      "geom_density (adjust = 1 /5, color = /quotesingle.ts1orange /quotesingle.ts1, size = 2)\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 52 / 137\n",
      "\n",
      "0.000.010.020.030.04\n",
      "0 100 200 300 400\n",
      "Degree.spondylolisthesisdensityBrian Wright, PhD R Data Vis: ggplot Feb, 2020 53 / 137\n",
      "\n",
      "Bar plots (categorical variable)\n",
      "dat_brca <-\n",
      "rio::import (/quotesingle.ts1clinical_data_breast_cancer_modified.csv /quotesingle.ts1,\n",
      "check.names = T)\n",
      "ggplot (\n",
      "data=dat_brca,\n",
      "aes(x = Tumor)\n",
      ")+\n",
      "geom_bar ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 54 / 137\n",
      "\n",
      "0204060\n",
      "T1 T2 T3 T4\n",
      "TumorcountBrian Wright, PhD R Data Vis: ggplot Feb, 2020 55 / 137\n",
      "\n",
      "Bar plots (categorical variable)\n",
      "ggplot (\n",
      "data=dat_brca,\n",
      "aes(x = Tumor,\n",
      "fill = ER.Status)\n",
      ")+#<<\n",
      "geom_bar ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 56 / 137\n",
      "\n",
      "Add additional information via mapping\n",
      "0204060\n",
      "T1 T2 T3 T4\n",
      "TumorcountER.Status\n",
      "Indeterminate\n",
      "Negative\n",
      "Positive\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 57 / 137\n",
      "\n",
      "Bar plots (categorical variable)\n",
      "ggplot (\n",
      "data=dat_brca,\n",
      "aes(x = Tumor,\n",
      "fill = ER.Status)\n",
      ")+\n",
      "geom_bar (position = /quotesingle.ts1dodge /quotesingle.ts1)\n",
      "# Default is position = \"stack\"\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 58 / 137\n",
      "\n",
      "Change the nature of the geometry\n",
      "010203040\n",
      "T1 T2 T3 T4\n",
      "TumorcountER.Status\n",
      "Indeterminate\n",
      "Negative\n",
      "Positive\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 59 / 137\n",
      "\n",
      "Graphing tabulated data\n",
      "(tabulated <- dat_brca %>% count (Tumor))\n",
      "#> # A tibble: 4 x 2\n",
      "#> Tumor n\n",
      "#> <chr> <int>\n",
      "#> 1 T1 15\n",
      "#> 2 T2 65\n",
      "#> 3 T3 19\n",
      "#> 4 T4 6\n",
      "ggplot (\n",
      "data = tabulated,\n",
      "aes(x = Tumor, y = n)\n",
      ")+\n",
      "geom_bar ()\n",
      "#> Error: stat_count() can only have an x or y aesthetic.\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 60 / 137\n",
      "\n",
      "Graphing tabulated data\n",
      "tabulated <- dat_brca %>% count (Tumor)\n",
      "tabulated\n",
      "ggplot (\n",
      "data = tabulated,\n",
      "aes(x = Tumor, y = n)\n",
      ")+\n",
      "geom_bar (stat = /quotesingle.ts1identity /quotesingle.ts1)###\n",
      "Here we need to change the default computation\n",
      "The barplot usually computes the counts ( stat_count )\n",
      "We suppress that here since we have already\n",
      "done the computation\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 61 / 137\n",
      "\n",
      "0204060\n",
      "T1 T2 T3 T4\n",
      "TumornBrian Wright, PhD R Data Vis: ggplot Feb, 2020 62 / 137\n",
      "\n",
      "Peeking under the hood\n",
      "plt <- ggplot (\n",
      "data = tabulated,\n",
      "aes(x = Tumor, y = n)\n",
      ")+\n",
      "geom_bar ()\n",
      "plt$layers\n",
      "#> [[1]]\n",
      "#> geom_bar: width = NULL, na.rm = FALSE, orientation = NA\n",
      "#> stat_count: width = NULL, na.rm = FALSE, orientation = NA\n",
      "#> position_stack\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 63 / 137\n",
      "\n",
      "Peeking under the hood\n",
      "plt <- ggplot (\n",
      "data = tabulated,\n",
      "aes(x = Tumor, y = n)) +\n",
      "geom_bar (stat = /quotesingle.ts1identity /quotesingle.ts1)\n",
      "plt$layers\n",
      "#> [[1]]\n",
      "#> geom_bar: width = NULL, na.rm = FALSE, orientation = NA\n",
      "#> stat_identity: na.rm = FALSE\n",
      "#> position_stack\n",
      "Each layer has a geometry, statistic and position associated with it\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 64 / 137\n",
      "\n",
      "Bivariate plots: Scatter plots\n",
      "ggplot (\n",
      "data = beaches,\n",
      "aes(x = date, y = temperature)\n",
      ")+\n",
      "geom_point ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 65 / 137\n",
      "\n",
      "203040\n",
      "2014 2016 2018\n",
      "datetemperatureBrian Wright, PhD R Data Vis: ggplot Feb, 2020 66 / 137\n",
      "\n",
      "Scatter plots\n",
      "ggplot (\n",
      "data = beaches,\n",
      "aes(x = date, y = temperature, group=1) #Add the group argu.\n",
      ")+\n",
      "geom_point (color= /quotesingle.ts1black /quotesingle.ts1, size = 3) +\n",
      "geom_line (color= /quotesingle.ts1red/quotesingle.ts1,size=2)\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 67 / 137\n",
      "\n",
      "Layer points and lines\n",
      "203040\n",
      "2014 2016 2018\n",
      "datetemperature\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 68 / 137\n",
      "\n",
      "Scatter plots\n",
      "ggplot (\n",
      "data = beaches,\n",
      "aes(x = date, y = temperature,group=1)\n",
      ")+\n",
      "geom_line (color= /quotesingle.ts1red/quotesingle.ts1,size=2) +###\n",
      "geom_point (color= /quotesingle.ts1black /quotesingle.ts1, size = 3) ###\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 69 / 137\n",
      "\n",
      "Order of laying down geometries matters\n",
      "203040\n",
      "2014 2016 2018\n",
      "datetemperature\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 70 / 137\n",
      "\n",
      "Doing some computations\n",
      "ggplot (\n",
      "data = beaches,\n",
      "aes(x = date, y = temperature, group=1)\n",
      ")+\n",
      "geom_point ()+\n",
      "geom_smooth (method= /quotesingle.ts1loess /quotesingle.ts1)\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 71 / 137\n",
      "\n",
      "Averages over 75% of the data\n",
      "#> /grave.ts1geom_smooth() /grave.ts1using formula /quotesingle.ts1y ~ x /quotesingle.ts1\n",
      "203040\n",
      "2014 2016 2018\n",
      "datetemperature\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 72 / 137\n",
      "\n",
      "Doing some computations\n",
      "ggplot (\n",
      "data = beaches,\n",
      "aes(x = date, y = temperature, group=1)\n",
      ")+\n",
      "geom_point ()+\n",
      "geom_smooth (method=\"loess\",span = 0.1) ###\n",
      "#?geom_smooth, kinda funny...?\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 73 / 137\n",
      "\n",
      "Big O!\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 74 / 137\n",
      "\n",
      "Averages over 10% of the data\n",
      "#> /grave.ts1geom_smooth() /grave.ts1using formula /quotesingle.ts1y ~ x /quotesingle.ts1\n",
      "203040\n",
      "2014 2016 2018\n",
      "datetemperature\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 75 / 137\n",
      "\n",
      "Computations over groups\n",
      "ggplot (\n",
      "data = dat_spine,\n",
      "aes(x = Sacral.slope,\n",
      "y = Degree.spondylolisthesis)\n",
      ")+\n",
      "geom_point ()+\n",
      "geom_smooth ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 76 / 137\n",
      "\n",
      "0100200300400\n",
      "25 50 75 100 125\n",
      "Sacral.slopeDegree.spondylolisthesisBrian Wright, PhD R Data Vis: ggplot Feb, 2020 77 / 137\n",
      "\n",
      "Computations over groups\n",
      "ggplot (\n",
      "data = dat_spine,\n",
      "aes(x = Sacral.slope,\n",
      "y = Degree.spondylolisthesis,\n",
      "color = Class.attribute) ##\n",
      ")+\n",
      "geom_point ()+\n",
      "geom_smooth ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 78 / 137\n",
      "\n",
      "Computation is done by groups\n",
      "0100200300400\n",
      "25 50 75 100 125\n",
      "Sacral.slopeDegree.spondylolisthesisClass.attribute\n",
      "Abnormal\n",
      "Normal\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 79 / 137\n",
      "\n",
      "Computations over groups\n",
      "ggplot (\n",
      "data = dat_spine,\n",
      "aes(x = Sacral.slope,\n",
      "y = Degree.spondylolisthesis,\n",
      "color = Class.attribute)\n",
      ")+\n",
      "geom_point ()+\n",
      "geom_smooth ()+\n",
      "xlim (0, 100) #Changing the demonsions of the graphic\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 80 / 137\n",
      "\n",
      "Ignore the outlier for now\n",
      "0100200300400\n",
      "0 25 50 75 100\n",
      "Sacral.slopeDegree.spondylolisthesisClass.attribute\n",
      "Abnormal\n",
      "Normal\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 81 / 137\n",
      "\n",
      "Computations over groups\n",
      "ggplot (\n",
      "data = dat_spine,\n",
      "aes(x = Sacral.slope,\n",
      "y = Degree.spondylolisthesis)\n",
      ")+\n",
      "geom_point ()+\n",
      "geom_smooth (aes(color = Class.attribute)) +#\n",
      "xlim (0, 100)\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 82 / 137\n",
      "\n",
      "Only color-code the smoothers\n",
      "You can change the plot based on where you map the aesthetic\n",
      "0100200300400\n",
      "0 25 50 75 100\n",
      "Sacral.slopeDegree.spondylolisthesisClass.attribute\n",
      "Abnormal\n",
      "Normal\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 83 / 137\n",
      "\n",
      "Computations over groups\n",
      "ggplot (\n",
      "data = dat_spine,\n",
      "aes(x = Sacral.slope,\n",
      "y = Degree.spondylolisthesis)\n",
      ")+\n",
      "geom_point ()+\n",
      "geom_smooth (aes(color = Class.attribute),\n",
      "se = F) +\n",
      "#Turning off the confidence interval\n",
      "xlim (0, 100)\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 84 / 137\n",
      "\n",
      "Looks a little cleaner\n",
      "0100200300400\n",
      "0 25 50 75 100\n",
      "Sacral.slopeDegree.spondylolisthesisClass.attribute\n",
      "Abnormal\n",
      "Normal\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 85 / 137\n",
      "\n",
      "Box Plots\n",
      "ggplot (\n",
      "data = beaches,\n",
      "aes(x = season_name,\n",
      "y = temperature)\n",
      ")+\n",
      "geom_boxplot ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 86 / 137\n",
      "\n",
      "203040\n",
      "Autumn Spring Summer Winter\n",
      "season_nametemperature>What\n",
      "are the components of a boxplot?\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 87 / 137\n",
      "\n",
      "Box Plots\n",
      "ggplot (\n",
      "data = beaches,\n",
      "aes(x = season_name,\n",
      "y = temperature)\n",
      ")+\n",
      "geom_boxplot ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 88 / 137\n",
      "\n",
      "MinimumMaximum (within fence)\n",
      "3rd quartile\n",
      "1st quartileMedian\n",
      "\"Outliers\"\n",
      "203040\n",
      "Autumn Spring Summer Winter\n",
      "season_nametemperatureBrian Wright, PhD R Data Vis: ggplot Feb, 2020 89 / 137\n",
      "\n",
      "Layers, again\n",
      "ggplot (\n",
      "data = beaches,\n",
      "aes(x = season_name,\n",
      "y = temperature)\n",
      ")+\n",
      "geom_boxplot ()+\n",
      "geom_jitter (width = 0.2)\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 90 / 137\n",
      "\n",
      "Layers, again\n",
      "203040\n",
      "Autumn Spring Summer Winter\n",
      "season_nametemperature\n",
      "##\n",
      "Layers, again\n",
      "ggplot (\n",
      "data = beaches,\n",
      "aes(x = season_name,\n",
      "y = temperature)\n",
      ")+\n",
      "geom_violin ()+\n",
      "geom_jitter (width = 0.2)Brian Wright, PhD R Data Vis: ggplot Feb, 2020 91 / 137\n",
      "\n",
      "203040\n",
      "Autumn Spring Summer Winter\n",
      "season_nametemperatureBrian Wright, PhD R Data Vis: ggplot Feb, 2020 92 / 137\n",
      "\n",
      "Exploring grouped data\n",
      "ggplot (\n",
      "data = beaches,\n",
      "aes(x = temperature,\n",
      "fill = season_name)\n",
      ")+\n",
      "geom_density ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 93 / 137\n",
      "\n",
      "Not very useful\n",
      "0.000.050.100.15\n",
      "20 30 40\n",
      "temperaturedensityseason_name\n",
      "Autumn\n",
      "Spring\n",
      "Summer\n",
      "Winter\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 94 / 137\n",
      "\n",
      "Overlaying graphs\n",
      "ggplot (\n",
      "data = beaches,\n",
      "aes(x = temperature,\n",
      "fill = season_name)\n",
      ")+\n",
      "geom_density (alpha = 0.4) # Changes the transparency\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 95 / 137\n",
      "\n",
      "Make graphs more transparent\n",
      "0.000.050.100.15\n",
      "20 30 40\n",
      "temperaturedensityseason_name\n",
      "Autumn\n",
      "Spring\n",
      "Summer\n",
      "Winter\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 96 / 137\n",
      "\n",
      "Exploding graphs\n",
      "ggplot (\n",
      "data = beaches,\n",
      "aes(x = temperature,\n",
      "fill = season_name)\n",
      ")+\n",
      "geom_density ()+\n",
      "facet_wrap (~season_name, ncol = 1) ###\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 97 / 137\n",
      "\n",
      "This is called “small multiples” (Tufte)\n",
      "WinterSummerSpringAutumn\n",
      "20 30 400.000.050.100.15\n",
      "0.000.050.100.15\n",
      "0.000.050.100.15\n",
      "0.000.050.100.15\n",
      "temperaturedensityseason_name\n",
      "Autumn\n",
      "Spring\n",
      "Summer\n",
      "Winter\n",
      ">\n",
      "Notice that all the graphs have the same x-axis.\n",
      "This is a good thing\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 98 / 137\n",
      "\n",
      "Exploding graphs\n",
      "ggplot (\n",
      "data = beaches,\n",
      "aes(x = temperature,\n",
      "fill = season_name)\n",
      ")+\n",
      "geom_density ()+\n",
      "facet_wrap (~season_name, nrow = 1) ###\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 99 / 137\n",
      "\n",
      "We can arrange them the other way too\n",
      "Autumn Spring Summer Winter\n",
      "20 30 40 20 30 40 20 30 40 20 30 400.000.050.100.15\n",
      "temperaturedensityseason_name\n",
      "Autumn\n",
      "Spring\n",
      "Summer\n",
      "Winter\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 100 / 137\n",
      "\n",
      "Order and orientation: Arrests in the USA in 1973\n",
      "arrests <- import (/quotesingle.ts1USArrests.csv /quotesingle.ts1)\n",
      "ggplot (\n",
      "data = arrests,\n",
      "aes(x = State,\n",
      "y = Murder)\n",
      ")+\n",
      "geom_bar (stat = /quotesingle.ts1identity /quotesingle.ts1)\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 101 / 137\n",
      "\n",
      "051015\n",
      "AlabamaAlaskaArizonaArkansasCaliforniaColoradoConnecticutDelawareFloridaGeorgiaHawaiiIdahoIllinoisIndianaIowaKansasKentuckyLouisianaMaineMarylandMassachusettsMichiganMinnesotaMississippiMissouriMontanaNebraskaNevadaNew HampshireNew JerseyNew MexicoNew YorkNorth CarolinaNorth DakotaOhioOklahomaOregonPennsylvaniaRhode IslandSouth CarolinaSouth DakotaTennesseeTexasUtahVermontVirginiaWashingtonWest VirginiaWisconsinWyoming\n",
      "StateMurder>Hard\n",
      "to read, there is no ordering, and x-labels can’t be seen\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 102 / 137\n",
      "\n",
      "Arrests in the USA in 1973\n",
      "ggplot (\n",
      "data = arrests,\n",
      "aes(x = fct_reorder (State, Murder), #Order by murder rate\n",
      "y = Murder)\n",
      ")+\n",
      "geom_bar (stat = /quotesingle.ts1identity /quotesingle.ts1)\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 103 / 137\n",
      "\n",
      "Arrest in the USA in 1973\n",
      "051015\n",
      "North DakotaMaineNew HampshireIowaVermontIdahoWisconsinMinnesotaUtahConnecticutRhode IslandSouth DakotaWashingtonNebraskaMassachusettsOregonHawaiiWest VirginiaDelawareKansasMontanaPennsylvaniaOklahomaWyomingIndianaOhioNew JerseyColoradoArizonaVirginiaArkansasCaliforniaMissouriKentuckyAlaskaIllinoisNew YorkMarylandNew MexicoMichiganNevadaTexasNorth CarolinaAlabamaTennesseeSouth CarolinaFloridaLouisianaMississippiGeorgia\n",
      "fct_reorder(State, Murder)Murder\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 104 / 137\n",
      "\n",
      "Arrests in the USA in 1973\n",
      "ggplot (\n",
      "data = arrests,\n",
      "aes(x = fct_reorder (State, Murder), # Order by murder rate\n",
      "y = Murder)\n",
      ")+\n",
      "geom_bar (stat = /quotesingle.ts1identity /quotesingle.ts1)+\n",
      "coord_flip ()#Flipping the coordinates\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 105 / 137\n",
      "\n",
      "Flipping the axes makes the states readable\n",
      "North DakotaMaineNew HampshireIowaVermontIdahoWisconsinMinnesotaUtahConnecticutRhode IslandSouth DakotaWashingtonNebraskaMassachusettsOregonHawaiiWest VirginiaDelawareKansasMontanaPennsylvaniaOklahomaWyomingIndianaOhioNew JerseyColoradoArizonaVirginiaArkansasCaliforniaMissouriKentuckyAlaskaIllinoisNew YorkMarylandNew MexicoMichiganNevadaTexasNorth CarolinaAlabamaTennesseeSouth CarolinaFloridaLouisianaMississippiGeorgia\n",
      "0 5 10 15\n",
      "Murderfct_reorder(State, Murder)\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 106 / 137\n",
      "\n",
      "Arrests in the USA in 1973\n",
      "ggplot (\n",
      "data = arrests,\n",
      "aes(x = fct_reorder (State, Murder), # Order by murder rate\n",
      "y = Murder)\n",
      ")+\n",
      "geom_bar (stat = /quotesingle.ts1identity /quotesingle.ts1, fill=\"red\") +\n",
      "labs (x = /quotesingle.ts1State /quotesingle.ts1, y = /quotesingle.ts1Murder rate /quotesingle.ts1)+# Adding labels\n",
      "theme_bw ()+# Theme\n",
      "theme (panel.grid.major.y = element_blank (),#\n",
      "panel.grid.minor.x = element_blank ()) +\n",
      "coord_flip ()# Flip last\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 107 / 137\n",
      "\n",
      "Cleaning it up a little\n",
      "North DakotaMaineNew HampshireIowaVermontIdahoWisconsinMinnesotaUtahConnecticutRhode IslandSouth DakotaWashingtonNebraskaMassachusettsOregonHawaiiWest VirginiaDelawareKansasMontanaPennsylvaniaOklahomaWyomingIndianaOhioNew JerseyColoradoArizonaVirginiaArkansasCaliforniaMissouriKentuckyAlaskaIllinoisNew YorkMarylandNew MexicoMichiganNevadaTexasNorth CarolinaAlabamaTennesseeSouth CarolinaFloridaLouisianaMississippiGeorgia\n",
      "0 5 10 15\n",
      "Murder rateState\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 108 / 137\n",
      "\n",
      "Themes\n",
      "ggplot comes with a default color scheme.\n",
      "There are several other schemes\n",
      "available\n",
      "scale_*_brewer uses the ColorBrewer palettes\n",
      "scale_*_gradient uses gradients\n",
      "scale_*_distill uses the ColorBrewer palettes, for continuous\n",
      "outcomes\n",
      "Here * can be color orfill , depending on what you want to\n",
      "color\n",
      "Note color refers to the outline, and fill refers to the inside\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 109 / 137\n",
      "\n",
      "No Theme\n",
      "ggplot (\n",
      "data = beaches,\n",
      "aes(x = date, y = enterococci,\n",
      "color = temperature)\n",
      ")+\n",
      "geom_point ()+\n",
      "scale_y_log10 (name = /quotesingle.ts1Enterococci /quotesingle.ts1,\n",
      "label = scales ::number_format (digits=3))\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 110 / 137\n",
      "\n",
      "No Theme\n",
      "0.101.0010.00100.001 000.00\n",
      "2014 2016 2018\n",
      "dateEnterococci\n",
      "203040temperature\n",
      "##\n",
      "Dark\n",
      "ggplot (\n",
      "data = beaches,\n",
      "aes(x = date, y = enterococci,\n",
      "color = temperature)\n",
      ")+\n",
      "geom_point ()+\n",
      "scale_y_log10 (name = /quotesingle.ts1Enterococci /quotesingle.ts1,\n",
      "label = scales ::number_format (digits=3)) +\n",
      "scale_color_gradient (low = /quotesingle.ts1white /quotesingle.ts1, high= /quotesingle.ts1red/quotesingle.ts1)+\n",
      "theme_dark ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 111 / 137\n",
      "\n",
      "Dark\n",
      "0.101.0010.00100.001 000.00\n",
      "2014 2016 2018\n",
      "dateEnterococci\n",
      "203040temperature\n",
      "##\n",
      "Black and White\n",
      "ggplot (\n",
      "data = beaches,\n",
      "aes(x = date, y = enterococci,\n",
      "color = temperature)\n",
      ")+\n",
      "geom_point ()+\n",
      "scale_y_log10 (name = /quotesingle.ts1Enterococci /quotesingle.ts1,\n",
      "label = scales ::number_format (digits=3)) +\n",
      "scale_color_gradient (low = /quotesingle.ts1blue /quotesingle.ts1, high= /quotesingle.ts1red/quotesingle.ts1)+\n",
      "theme_bw ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 112 / 137\n",
      "\n",
      "Black and White\n",
      "0.101.0010.00100.001 000.00\n",
      "2014 2016 2018\n",
      "dateEnterococci\n",
      "203040temperature\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 113 / 137\n",
      "\n",
      "Themes\n",
      "You can create your own custom themes to keep a uniﬁed look to your\n",
      "graphs\n",
      "ggplot comes with\n",
      "theme_classic\n",
      "theme_bw\n",
      "theme_void\n",
      "theme_dark\n",
      "theme_gray\n",
      "theme_light\n",
      "theme_minimal\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 114 / 137\n",
      "\n",
      "Create your own\n",
      "ggplot (\n",
      "data = dat_spine,\n",
      "aes(x = Sacral.slope, y = Degree.spondylolisthesis,\n",
      "color = Class.attribute)\n",
      ")+\n",
      "geom_point ()+\n",
      "geom_smooth (se = F) +\n",
      "coord_cartesian (xlim = c(0, 100),\n",
      "ylim = c(0,200))\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 115 / 137\n",
      "\n",
      "050100150200\n",
      "0 25 50 75 100\n",
      "Sacral.slopeDegree.spondylolisthesisClass.attribute\n",
      "Abnormal\n",
      "NormalBrian Wright, PhD R Data Vis: ggplot Feb, 2020 116 / 137\n",
      "\n",
      "Create your own\n",
      "my_theme <- function (){\n",
      "theme_bw ()\n",
      "}\n",
      "ggplot (\n",
      "data = dat_spine,\n",
      "aes(x = Sacral.slope, y = Degree.spondylolisthesis,\n",
      "color = Class.attribute)\n",
      ")+\n",
      "geom_point ()+\n",
      "geom_smooth (se = F) +\n",
      "coord_cartesian (xlim = c(0, 100),\n",
      "ylim = c(0,200)) +\n",
      "my_theme ()# Just Black and White\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 117 / 137\n",
      "\n",
      "050100150200\n",
      "0 25 50 75 100\n",
      "Sacral.slopeDegree.spondylolisthesisClass.attribute\n",
      "Abnormal\n",
      "NormalBrian Wright, PhD R Data Vis: ggplot Feb, 2020 118 / 137\n",
      "\n",
      "Create your own\n",
      "my_theme <- function (){\n",
      "theme_bw ()+\n",
      "theme (axis.text = element_text (size = 14),\n",
      "axis.title = element_text (size = 16),\n",
      "panel.grid.minor = element_blank (),\n",
      "strip.text = element_text (size=14),\n",
      "strip.background = element_blank ())\n",
      "}\n",
      "ggplot (\n",
      "data = dat_brca,\n",
      "aes(x = Tumor)) +\n",
      "geom_bar ()+\n",
      "facet_grid (rows = vars (ER.Status),\n",
      "cols = vars (PR.Status))\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 119 / 137\n",
      "\n",
      "Negative PositiveIndeterminate Negative Positive\n",
      "T1 T2 T3 T4 T1 T2 T3 T40102030\n",
      "0102030\n",
      "0102030\n",
      "TumorcountBrian Wright, PhD R Data Vis: ggplot Feb, 2020 120 / 137\n",
      "\n",
      "Create your own\n",
      "ggplot (\n",
      "data = dat_brca,\n",
      "aes(x = Tumor)\n",
      ")+\n",
      "geom_bar ()+\n",
      "facet_grid (rows = vars (ER.Status),\n",
      "cols = vars (PR.Status)) +\n",
      "my_theme ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 121 / 137\n",
      "\n",
      "Negative PositiveIndeterminate Negative Positive\n",
      "T1 T2 T3 T4 T1 T2 T3 T40102030\n",
      "0102030\n",
      "0102030\n",
      "TumorcountBrian Wright, PhD R Data Vis: ggplot Feb, 2020 122 / 137\n",
      "\n",
      "Annotations: Stand-alone stories\n",
      "Data visualization to stand on its own\n",
      "Relevant information should be placed on the graph\n",
      "However, you need to balance the information content with real estate\n",
      "▶Don’t clutter the graph and make it not readable\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 123 / 137\n",
      "\n",
      "Adding derived statistics to a plot: Group means\n",
      "ggplot (iris,\n",
      "aes(x = Sepal.Length,\n",
      "y = Sepal.Width,\n",
      "color = Species)\n",
      ")+\n",
      "geom_point ()+\n",
      "theme_bw ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 124 / 137\n",
      "\n",
      "Adding derived statistics to a plot: Group means\n",
      "2.02.53.03.54.04.5\n",
      "5 6 7 8\n",
      "Sepal.LengthSepal.WidthSpecies\n",
      "setosa\n",
      "versicolor\n",
      "virginica\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 125 / 137\n",
      "\n",
      "Adding derived statistics to a plot: Group means\n",
      "means <- iris %>% group_by (Species) %>%\n",
      "summarize_at (vars (starts_with (/quotesingle.ts1Sepal /quotesingle.ts1)),\n",
      "mean)\n",
      "means\n",
      "#> # A tibble: 3 x 3\n",
      "#> Species Sepal.Length Sepal.Width\n",
      "#> <fct> <dbl> <dbl>\n",
      "#> 1 setosa 5.01 3.43\n",
      "#> 2 versicolor 5.94 2.77\n",
      "#> 3 virginica 6.59 2.97\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 126 / 137\n",
      "\n",
      "Adding derived statistics to a plot: Group means\n",
      "ggplot (iris,\n",
      "aes(x = Sepal.Length,\n",
      "y = Sepal.Width,\n",
      "color = Species)\n",
      ")+\n",
      "geom_point ()+\n",
      "geom_point (data = means,\n",
      "size=5) +\n",
      "theme_bw ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 127 / 137\n",
      "\n",
      "Adding derived statistics to a plot: Group means\n",
      "2.02.53.03.54.04.5\n",
      "5 6 7 8\n",
      "Sepal.LengthSepal.WidthSpecies\n",
      "setosa\n",
      "versicolor\n",
      "virginica\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 128 / 137\n",
      "\n",
      "Adding regression metrics\n",
      "Regress highway mileage on city mileage (data: mpg)\n",
      "mod1 <- lm(hwy ~cty, data = mpg)\n",
      "r2 <- broom ::glance (mod1) %>% pull (r.squared)\n",
      "ggplot (mpg,\n",
      "aes(x = cty, y = hwy)\n",
      ")+\n",
      "geom_point ()+\n",
      "geom_smooth (method = /quotesingle.ts1lm/quotesingle.ts1, se=F) +\n",
      "theme_bw ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 129 / 137\n",
      "\n",
      "Adding regression metrics\n",
      "#> /grave.ts1geom_smooth() /grave.ts1using formula /quotesingle.ts1y ~ x /quotesingle.ts1\n",
      "203040\n",
      "10 15 20 25 30 35\n",
      "ctyhwy\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 130 / 137\n",
      "\n",
      "Regress highway mileage on city mileage\n",
      "mod1 <- lm(hwy ~cty, data = mpg)\n",
      "r2 <- broom ::glance (mod1) %>% pull (r.squared) %>%#pull is part of dplyr\n",
      "round (., 2) #part of base R, rounding behind the .\n",
      "by 2\n",
      "ggplot (mpg,\n",
      "aes(x = cty, y = hwy)) +\n",
      "geom_point ()+\n",
      "geom_smooth (method = /quotesingle.ts1lm/quotesingle.ts1, se=F) +\n",
      "annotate (geom= /quotesingle.ts1text /quotesingle.ts1,\n",
      "x = 15, y = 40,\n",
      "label=glue ::glue (\"R^2 == {r}\",r=r2),\n",
      "size=6,\n",
      "parse=T) +\n",
      "theme_bw ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 131 / 137\n",
      "\n",
      "Glance creates a quick summary of the model\n",
      "broom ::glance (mod1)\n",
      "#> # A tibble: 1 x 12\n",
      "#> r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC\n",
      "#> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n",
      "#> 1 0.914 0.913 1.75 2459.\n",
      "1.87e-125 1 -462.\n",
      "931.\n",
      "941.\n",
      "#> # ...\n",
      "with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 132 / 137\n",
      "\n",
      "Nice Addition\n",
      "#> /grave.ts1geom_smooth() /grave.ts1using formula /quotesingle.ts1y ~ x /quotesingle.ts1\n",
      "R2=0.91\n",
      "203040\n",
      "10 15 20 25 30 35\n",
      "ctyhwy\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 133 / 137\n",
      "\n",
      "Highlighting regions\n",
      "mpg %>%\n",
      "mutate (cyl = as.factor (cyl)) %>%\n",
      "ggplot (aes(x = cyl, y = hwy)\n",
      ")+\n",
      "geom_boxplot ()+\n",
      "theme_bw ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 134 / 137\n",
      "\n",
      "Highlight regions\n",
      "203040\n",
      "4 5 6 8\n",
      "cylhwy\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 135 / 137\n",
      "\n",
      "Highlighting regions\n",
      "mpg %>%\n",
      "mutate (cyl = as.factor (cyl)) %>%\n",
      "ggplot (aes(x = cyl, y = hwy)\n",
      ")+\n",
      "geom_boxplot ()+\n",
      "theme_bw ()+\n",
      "annotate (geom = /quotesingle.ts1rect /quotesingle.ts1,\n",
      "xmin=3.75,xmax=4.25,\n",
      "ymin = 22, ymax = 28,\n",
      "fill = /quotesingle.ts1red/quotesingle.ts1,\n",
      "alpha = 0.2) +\n",
      "annotate (/quotesingle.ts1text /quotesingle.ts1,\n",
      "x = 4.5, y = 25,\n",
      "label = /quotesingle.ts1Outliers?\n",
      "/quotesingle.ts1,\n",
      "hjust = 0) +\n",
      "coord_cartesian (xlim = c(0,5)) +\n",
      "theme_bw ()\n",
      "Brian Wright, PhD R Data Vis: ggplot Feb, 2020 136 / 137\n",
      "\n",
      "Highlighting regions\n",
      "Outliers?\n",
      "203040\n",
      "4 5 6 8\n",
      "cylhwy\n",
      "################EXAMPLES for in\n",
      "CLASS##################Brian Wright, PhD R Data Vis: ggplot Feb, 2020 137 / 137\n",
      "\n",
      "Markdown and the knitr Package in R\n",
      "Practice of Data Science\n",
      "Communicating with Code\n",
      "\n",
      "Reproducible Research\n",
      "Baggerly and Coombes’ Replication Work\n",
      "A Strategy for Reproducible Research\n",
      "The Markdown Language\n",
      "Choosing an Output and Compiling\n",
      "The Header\n",
      "Publishing the HTML Output\n",
      "Syntax\n",
      "Bold, Italics, etc.\n",
      "Sectioning\n",
      "Tabbing\n",
      "Equations\n",
      "Lists\n",
      "Tables (by hand)\n",
      "Graphics (from external ﬁles)\n",
      "Embedding Code\n",
      "In-line Code for Display Only\n",
      "In-line Code that gets Evaluated\n",
      "Code Chunks\n",
      "Code Chunk Options\n",
      "Caching\n",
      "Tables and Figures (from code)\n",
      "\n",
      "Baggerly and Coombes\n",
      "There’s a really infamous talk from 2010, called\n",
      "The Importance of Reproducible Research in High-Throughput\n",
      "Biology: Case Studies in Forensic Bioinformatics :\n",
      "https://www.youtube.com/watch?v=7gYIs7uYbMo\n",
      "by Keith Baggerly and Kevin R.\n",
      "Coombes.\n",
      "Their talk illustrates what can go seriously wrong when people\n",
      "working with data are not transparent about how they get their\n",
      "results.\n",
      "Baggerly and Coombes\n",
      "Baggerly and Coombes are bioinformaticians who study cancer.\n",
      "They attempted to replicate this study:\n",
      "This paper claims to show how treatments for childhood leukemia\n",
      "should be tailored to patients based on the speciﬁc genomic\n",
      "information in the patient’s DNA.\n",
      "This was an important ﬁnding and was published in a prominent\n",
      "journal.\n",
      "Baggerly and Coombes\n",
      "In order to replicate the study, Baggerly and Coombes started with\n",
      "the same raw dataset that the study’s authors used.\n",
      "Their goal\n",
      "was to use the data to reproduce the study’s results.\n",
      "But the problem was that the authors did not provide any\n",
      "scripts or discussion of what they did to the data prior to running\n",
      "the tests.\n",
      "As a result, Baggerly and Coombes had to reproduce the results in\n",
      "a “forensic” way: ﬁguring out after-the-fact what the authors must\n",
      "have done to get these results.\n",
      "Baggerly and Coombes\n",
      "Eventually, they were able to reproduce the results, but found the\n",
      "authors had made two errors:\n",
      "1.\n",
      "The rows in the data refer to particular genes.\n",
      "Someone\n",
      "copy-and-pasted the cells in a way that left oﬀ the column headers.\n",
      "That created an oﬀ-by-one error where the data for a gene were\n",
      "listed one row above the gene name.\n",
      "2.\n",
      "The treatment was coded as 1 or 2, but someone along the way\n",
      "confused what 1 and 2 meant.\n",
      "So the treated patients were\n",
      "reported as control, and vice versa.\n",
      "That means that the reported positive eﬀect for the treatment\n",
      "group is actually a positive eﬀect for the control group.\n",
      "In other\n",
      "words, the treatment harms people .\n",
      "Baggerly and Coombes\n",
      "By the time Baggerly and Coombes made this discovery, the\n",
      "research had moved forward to the stage of clinical drug trials.\n",
      "Children with leukemia were being given harmful treatments\n",
      "based on mistaken research.\n",
      "In his talk, Baggerly describes all the ways that he and Coombes\n",
      "tried to sound the alarm on this research.\n",
      "But the stakeholders\n",
      "were reluctant to retract and end the research because of the\n",
      "implications for reputation and grant money.\n",
      "The study continued for many months.\n",
      "It was only stopped when\n",
      "it was revealed that the principal investigator on the original study\n",
      "had lied on his CV about being a Rhodes Scholar.\n",
      "Baggerly and Coombes\n",
      "So, what exactly went so wrong here?\n",
      "1.\n",
      "Dumb, typical mistakes that people make with spreadsheets all\n",
      "the time (the oﬀ-by-one error, confusing the 1s and 2s).\n",
      "2.\n",
      "Laziness: no eﬀort to document the steps that were taken to\n",
      "prepare the data for analysis.\n",
      "3.\n",
      "Self-interest and ego: covering up mistakes instead of risking the\n",
      "penalties of correcting them, thereby making the mistakes worse.\n",
      "4.\n",
      "Magical thinking: because the work involves data, there’s a\n",
      "tendency by most people to simply believe that the work is correct\n",
      "without digging in to it (not Baggerly and Coombes though!)\n",
      "\n",
      "Baggerly and Coombes\n",
      "If we are going to be working with data, how can we avoid the\n",
      "mistakes that Baggerly and Coombes discovered?\n",
      "1.\n",
      "Mistakes are inevitable.\n",
      "But, if we use code instead of\n",
      "point-and-clicking, it’s easier to see mistakes and to go back and\n",
      "correct them.\n",
      "2.\n",
      "If we document our steps as we go along, we’ll be transparent\n",
      "and able to show anyone exactly what we did with the data.\n",
      "3.\n",
      "We can feed our egos in a diﬀerent way: clear and professional\n",
      "documentation looks impressive to others.\n",
      "4.\n",
      "Working with code and explaining what each part of the code\n",
      "does goes a long way towards dispelling the anxiety people have\n",
      "about data, and overcomes magical thinking.\n",
      "Reproducible Research\n",
      "Our goal : to give you the skills and practice you need to work with\n",
      "data in a way that\n",
      "▶Is easy to document\n",
      "▶Allows you to combine code, results, and text to better\n",
      "convey the context of what you are doing\n",
      "▶Looks really good\n",
      "This morning we will discuss R markdown, one of the best tools we\n",
      "have for conducting transparent research.\n",
      "This afternoon we will walk through an entire research pipeline\n",
      "using R markdown, documenting everything we need to do to raw\n",
      "data to prepare it for analysis, and including the ﬁnal results in the\n",
      "document.\n",
      "Reproducible Research\n",
      "Reproducibility — the ability to get the same research results as an\n",
      "original study by using the raw data and computer code provided\n",
      "by researchers.\n",
      "There have been serious crises in many ﬁelds, including medicine,\n",
      "economics, and political science, because researchers failed to\n",
      "make their code and data available.\n",
      "We are going to learn how to make a document that shows ALL\n",
      "the steps involved in a project, going from raw data to ﬁnal\n",
      "results .\n",
      "We also want this document to be as easy to read and\n",
      "understand as possible.\n",
      "Practice on your own computer as we discuss the steps for\n",
      "creating a markdown document.\n",
      "There are two ways to save R code\n",
      "The ﬁrst way is to save code in an R script .\n",
      "▶Pros : Scripts are easy to work with, run, save, and share.\n",
      "▶Cons : Scripts can be hard for even an advanced R user to\n",
      "read and understand.\n",
      "The second way is with an R markdown document compiled with\n",
      "theknitr package.\n",
      "(If you’ve never used knitr before, install it\n",
      "by typing install.packages(\"knitr\") into the console.)\n",
      "▶Pros : Creates a beautiful, readable document by placing text,\n",
      "code, and the output of the code all in the same document\n",
      "(this is also called weaving: hence the name knitr ).\n",
      "Able to\n",
      "create HTML, PDF, or Word ﬁles.\n",
      "▶Cons : More syntax to learn in addition to R code.\n",
      "Might take\n",
      "a while to compile documents.\n",
      "The Markdown Language\n",
      "You need to become experienced using both methods to save\n",
      "code.\n",
      "They are each useful in diﬀerent situations.\n",
      "Markdown is a programming language for formatting text, using\n",
      "minimal programming syntax.\n",
      "It’s basically a lightwight version of\n",
      "HTML code.\n",
      "It’s not just for R — it’s for anything that involves\n",
      "text together with some other kind of code.\n",
      "To start a markdown ﬁle : open R Studio, click File, then New File,\n",
      "then R Markdown.\n",
      "Give the document a title and author, and\n",
      "choose whether you want this code to produce an HTML, PDF, or\n",
      "Word ﬁle.\n",
      "This will call up an example page with some text and code already\n",
      "in it.\n",
      "(You will end up deleting this example text and code and\n",
      "writing your own.)\n",
      "\n",
      "Choosing an Output and Compiling\n",
      "While R scripts are saved as “ .R” ﬁles, these markdown ﬁles are\n",
      "saved as “ .Rmd ” ﬁles.\n",
      "This ﬁle is separate from the HTML, PDF,\n",
      "or Word document you will create with this code.\n",
      "First notice the “ Knit ” button.\n",
      "This button compiles the\n",
      "document – produces the desired output.\n",
      "To change the type of\n",
      "output, click the arrow to the right of Knit.\n",
      "At the top of the .Rmd ﬁle, there’s code separated on the top and\n",
      "bottom by three dashes.\n",
      "This is the header.\n",
      "You can change the\n",
      "title, author, etc.\n",
      "here.\n",
      "You can also change the output here:\n",
      "▶For HTML output, output: html document\n",
      "▶For PDF output, output: pdf document\n",
      "▶For Word output, output: word document\n",
      "\n",
      "Using the Header to Set Options\n",
      "To create a table of contents that lists up to 5 levels of sectioning:\n",
      "output:\n",
      "html_document:\n",
      "toc: true\n",
      "toc_depth: 5\n",
      "By default the table of contents appears at the top of the\n",
      "document, just under the title.\n",
      "But, you can also use a ﬂoating\n",
      "and collapsable table of contents window like this:\n",
      "output:\n",
      "html_document:\n",
      "toc: true\n",
      "toc_depth: 5\n",
      "toc_float: true\n",
      "toc_collapsed: true\n",
      "\n",
      "Using the Header to Set Options\n",
      "Important : You will see three dashes --- at the beginning and\n",
      "end of the header.\n",
      "Do not delete these!\n",
      "The document will not\n",
      "compile if you do.\n",
      "(It won’t even give you an intelligible error\n",
      "message)\n",
      "To change the overall theme (colors, fonts, etc.) of the\n",
      "document, add the theme argument, like this:\n",
      "output:\n",
      "html_document:\n",
      "theme: cerulean\n",
      "Diﬀerent themes are illustrated here:\n",
      "https://www.datadreaming.org/post/r-markdown-theme-gallery/\n",
      "Try a few right now.\n",
      "Other options for the header are listed here:\n",
      "http://rmarkdown.rstudio.com/html document format.html\n",
      "\n",
      "Publishing the HTML Output\n",
      "If you choose to compile your markdown code as an HTML ﬁle, it\n",
      "displays in R Studio’s makeshift browser.\n",
      "That’s good enough for\n",
      "checking your work.\n",
      "But remember, HTML code creates webpages.\n",
      "So you can click\n",
      "the “ Open in Browser ” button to see your code displayed in a\n",
      "web browser.\n",
      "If you have your own webpage, you can post this HTML output to\n",
      "your webpage.\n",
      "If you need space on the web to host this page, click on\n",
      "“Publish ”, then click “ RPubs ”.\n",
      "RPubs is a free service, run by R\n",
      "Studio, that provides server space for your markdown documents.\n",
      "If you post online using RPubs, you can use a URL to share your\n",
      "work with your audience.\n",
      "Markdown syntax\n",
      "To insert text into the output, just type into the .Rmd ﬁle and\n",
      "compile.\n",
      "The goal of markdown is to give you an easy way to create stylized\n",
      "text, sections, equations, lists, tables, etc.\n",
      "quickly by typing .\n",
      "Foritalicized text , place ONE star * before and after the text.\n",
      "Forbold text , place TWO stars ** before and after the text.\n",
      "Forstruck through text, place TWO tildes ∼∼ before and after the\n",
      "text.\n",
      "*this will be italicized*\n",
      "**this will be bold**\n",
      "~~this will be struck out~~\n",
      "\n",
      "Markdown syntax\n",
      "To start a new paragraph, push Enter/Return TWICE, so that\n",
      "there is a blank line separating the paragraphs.\n",
      "For a hyperlink , either type the address itself (it will automatically\n",
      "become a link), or use syntax like this to place the link on top of\n",
      "other text:\n",
      "[The best web comic](https://smbc-comics.com/)\n",
      "For block quotes, push Enter/Return TWICE then start every line\n",
      "of the quote with >and a space.\n",
      "Here's a profound quote:\n",
      "> I'd rather have this bottle in front of me\n",
      "> than a frontal lobotomy\n",
      "\n",
      "Sectioning\n",
      "One of the most important ways to make a document readable is\n",
      "to use sectioning to organize the document.\n",
      "Two hashtags (pound signs) ##followed by a name denote a\n",
      "section.\n",
      "Three hashtags ### followed by a name denote a subsection.\n",
      "Four hashtags #### followed by a name denote a sub-subsection,\n",
      "and so on.\n",
      "One hashtag is used for titles, but generally, these titles appear too\n",
      "large and look weird, so most people start at two hashtags.\n",
      "If thetoc: true option is speciﬁed, the section titles will\n",
      "appear in the table of contents automatically.\n",
      "Tabbing\n",
      "Sometimes it makes sense for only one of several sub-sections to\n",
      "appear at a time.\n",
      "For example, suppose I wrote a paragraph in\n",
      "English and Spanish.\n",
      "I could have the two sections represented by\n",
      "tabs, in which the user can switch between the two tabs.\n",
      "To create tabs, type {.tabset}immediately after the section\n",
      "title.\n",
      "Then all sub-sections within this section will exist in tabs.\n",
      "{.tabset .tabset-fade }does the same thing, but include a\n",
      "nice fade-in animation when switching between tabs.\n",
      "{.tabset .tabset-fade .tabset-pills }places the tabs into\n",
      "squares with rounded-edges.\n",
      "Equations\n",
      "In statistics and data science, sometimes it makes sense to include\n",
      "mathematical equations in the document.\n",
      "In Microsoft Word, you\n",
      "have to point-and-click every symbol, but in Markdown, you can\n",
      "quickly type out equations .\n",
      "To include an equation in-line, place a dollar sign $before and\n",
      "after the equation.\n",
      "To place the equation on its own line, place\n",
      "TWO dollar signs and a space before and after the equation.\n",
      "Some special math characters:\n",
      "▶^exponentiation\n",
      "▶ subscripts▶\\sqrt{5}√\n",
      "5\n",
      "▶\\frac{1}{2}1\n",
      "2\n",
      "A list of the code for many other math symbols is here:\n",
      "http://reu.dimacs.rutgers.edu/Symbols.pdf\n",
      "\n",
      "Equations\n",
      "For example, to include the quadratic formula in your document,\n",
      "x=−b±√\n",
      "b2−4ac\n",
      "2a\n",
      "type\n",
      "$$ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$\n",
      "(Here the\\pmrefers to the “plus or minus” symbol)\n",
      "\n",
      "Lists\n",
      "To create an unordered list (just bullets), type:\n",
      "* Item 1\n",
      "* Item 2\n",
      "* Item 2a\n",
      "* Item 2b\n",
      "To create an ordered list (numbered), type:\n",
      "1.\n",
      "Item 1\n",
      "2.\n",
      "Item 2\n",
      "3.\n",
      "Item 3\n",
      "a.\n",
      "Item 3a\n",
      "b.\n",
      "Item 3b\n",
      "\n",
      "Lists\n",
      "Some weird things about lists in R Markdown:\n",
      "1.\n",
      "It won’t compile as a list unless there’s an entire blank line\n",
      "(push enter twice) separating the list from the preceding text\n",
      "2.\n",
      "To create a sublist, you need to tab twice .\n",
      "Once won’t register\n",
      "diﬀerently from the other items\n",
      "3.\n",
      "For numbered lists, the top level must be regular (Arabic)\n",
      "numbers (1,2,3,...).\n",
      "The next levels can be lowercase letters or\n",
      "lowercase Roman numerals\n",
      "4.\n",
      "You can change the ﬁrst number to anything you want.\n",
      "But the\n",
      "following numbers will always count up by 1 from the ﬁrst\n",
      "number, no matter what you type there\n",
      "\n",
      "Creating tables by hand\n",
      "There are two kinds of tables: ones you enter by hand and ones\n",
      "you can create with R.\n",
      "To create a nice-looking table by hand,\n",
      "markdown has a peculiar notation.\n",
      "In general, I don’t like memorizing the speciﬁcs of this notation.\n",
      "Instead I use https://tableconvert.com/ (or another website like\n",
      "it).\n",
      "Then:\n",
      "1.\n",
      "In the top toolbar, click the “Table” button to choose the\n",
      "desired dimensions\n",
      "2.\n",
      "Fill in the table however you want\n",
      "3.\n",
      "Select “Copy”\n",
      "4.\n",
      "And paste the syntax into your R markdown document\n",
      "The table will appear in a neatly formatted way when you compile.\n",
      "Graphics (from external ﬁles)\n",
      "Sometimes you might want to display a graphic in your document\n",
      "that you aren’t using R to create.\n",
      "For example, what kind of\n",
      "monster wouldn’t want to include this graphic?\n",
      "You can include graphics if you have the ﬁle on your local machine:\n",
      "![A caption, if wanted, goes here](duck.jpg)\n",
      "Or pull them directly oﬀ the web by entering the image’s URL:\n",
      "![](http://1funny.com/wp-content/uploads/2010/08/Baby-Animals-24.jpg)\n",
      "\n",
      "Embedding Code\n",
      "Remember that the purpose of an R markdown ﬁle is to weave\n",
      "text, code, and the results of code together in one, readable\n",
      "document.\n",
      "There are three ways to include code in a document:\n",
      "1.\n",
      "in-line for display only,\n",
      "2.\n",
      "in-line and evaluated,\n",
      "3.\n",
      "and evaluated inside a “code chunk.”\n",
      "In-line code for display only is just for referring to speciﬁc R\n",
      "commands as you are writing.\n",
      "Markdown will place these pieces of\n",
      "code in a diﬀerent, computery font with a grey background.\n",
      "But\n",
      "this code WILL NOT be evaluated or run.\n",
      "To write in-line code, use single, forward-sloping quotes ‘(on the\n",
      "same key as the tilde).\n",
      "Then if you write about the lm()\n",
      "function, or the ggplot2 package, it will appear in this diﬀerent\n",
      "font and have a grey background.\n",
      "In-line Code that is Evaluated\n",
      "If you use the single, forward-sloping quotes, the code is displayed\n",
      "but not run.\n",
      "Alternatively, if you type rprior to any code within\n",
      "the quotes, markdown will evaluate the code and display the\n",
      "output in the text.\n",
      "For example, try typing the following into your markdown\n",
      "document:\n",
      "The current date and time are `r Sys.time()`\n",
      "Sys.time() is R’s internal clock, so this syntax should display\n",
      "the current date and time as regular text, not as computer code.\n",
      "This feature is great for ﬁlling in details about the data into your\n",
      "text automatically.\n",
      "Code Chunks\n",
      "A code chuck contains several lines of code, on separate lines from\n",
      "the text.\n",
      "It gets run by R , and the output appears in the\n",
      "document as well.\n",
      "Start a code chunk like this:\n",
      "```{r nameofthischunk}\n",
      "First, type three forward single-quotes.\n",
      "Then within curly braces,\n",
      "the letter r— this tells markdown that this is R code, then a\n",
      "distinct name you give this code chunk.\n",
      "Naming the chunks is\n",
      "optional, but will help you isolate errors if there are any.\n",
      "End a code chunk by typing three more forward single-quotes\n",
      "on a new line:\n",
      "```\n",
      "\n",
      "Code Chunks\n",
      "You can type as many lines of R code as you want inside one code\n",
      "chunk.\n",
      "But, best practice is to only write a few lines at a time in\n",
      "one code chunk.\n",
      "The reason is that you are trying to bring a reader along and\n",
      "explain your code.\n",
      "It’s easier to explain a few lines at a time.\n",
      "Also, If there’s multiple outputs from one chunk, they will all be\n",
      "displayed after the chunk.\n",
      "Keeping the chunk small helps us keep\n",
      "track of which output goes with which code.\n",
      "Take a moment to try out some code chunks in your markdown\n",
      "document.\n",
      "Code Chunk Options\n",
      "You can write options inside the curly braces, separated by\n",
      "commas, to change the behavior of a code chunk.\n",
      "Here are some options you can use:\n",
      "echo=FALSE — don’t display the code\n",
      "eval=FALSE — don’t display the results\n",
      "warning=FALSE, message=FALSE, error=FALSE — don’t\n",
      "display warnings, messages, or errors (I always use these options for\n",
      "the code chunk that loads the packages I need)\n",
      "Other options are listed here: https://yihui.name/knitr/options/\n",
      "\n",
      "Code Chunk Options\n",
      "A new .Rmd ﬁle has this code chunk at the top of the document:\n",
      "```{r setup, include=FALSE}\n",
      "knitr::opts_chunk$set(echo = TRUE)\n",
      "```\n",
      "This code chunk contains global options to be applied to all\n",
      "subsequent chunks.\n",
      "By default, it sets echo=TRUE , which tells all the chunks to\n",
      "display the code in the document in addition to the output.\n",
      "If this\n",
      "were instead echo=FALSE , the output would get displayed but the\n",
      "R code that generates the output would not be displayed.\n",
      "You can set other global options here if you want them applied to\n",
      "all code chunks.\n",
      "Just write knitr::opts chunk$set( option)in\n",
      "this chunk.\n",
      "Caching\n",
      "One extremely useful code chunk option is caching the output of\n",
      "the code.\n",
      "Caching means that R saves the output of the chunk so that\n",
      "the next time the document compiles, it can load the saved results\n",
      "instead of running the code again.\n",
      "To cache the output of a code chunk, use the option\n",
      "cache=TRUE .\n",
      "This option saves A LOT OF TIME when using commands that\n",
      "take a while to run, such as loading a big dataset or running a\n",
      "complicated model.\n",
      "But don’t use this option for every chunk, as it\n",
      "can cause problems with the keeping results accurate as code\n",
      "changes.\n",
      "Tables and Figures (from code)\n",
      "A big reason why markdown makes sense for R is the ability to\n",
      "cleanly display tables and ﬁgures you produce with R code.\n",
      "To convert tables from R output to nice looking HTML, use the\n",
      "kable() function.\n",
      "Just place the code that creates the table\n",
      "inside of kable() .\n",
      "For ﬁgures, such as ggplot graphics, no extra function is needed to\n",
      "display.\n",
      "But use the fig.width andfig.height options on\n",
      "the code chunk to control the size of the ﬁgure in the output.\n",
      "For example:\n",
      "```{r plot, fig.width=6, fig.height=8}\n",
      "ggplot(mtcars, aes(x=wgt, y=mpg)) + geom_point()\n",
      "```\n",
      "\n",
      "KNN and Probability \n",
      "Classification\n",
      "Definition\n",
      "Classification can take two distinct meanings in Machine Learning\n",
      "Unsupervised Learning We may be given a set of observations with the aim of establishing the existence of classes or clusters in the data\n",
      "Supervised Learning We may know for certain that there are so many classes, and the aim is to establish a rule that we can use to classify a new observation into one of the existing classes\n",
      "k-NN is a supervised method for classification 2\n",
      "\n",
      "Classification\n",
      "A few classification examples:\n",
      "A Person arrives at the emergency room with a set of symptoms that could possibly be attributed to one of three medical conditions.\n",
      "Which of the three conditions does the individual have?\n",
      "An online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user’s IP address, past transaction history, and so forth.\n",
      "On the basis of DN sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are disease -causing and which are not.\n",
      "3\n",
      "\n",
      "Classification\n",
      "There are many possible techniques that a classifier might use to predict a qualitative response.\n",
      "Today we will discuss k -NN\n",
      "k-Nearest Neighbors\n",
      "Naïve Bayes Logistic Regression\n",
      "Tree –based methods\n",
      "4\n",
      "\n",
      "Classification\n",
      "A few issues to keep in mind when building a classifier\n",
      "Accuracy.\n",
      "There is the reliability of the rule, usually represented by the proportion of correct classifications, although it may be that some errors are more serious than others, and it may be important to control the error rate for some key class.\n",
      "Speed.\n",
      "In some circumstances, the speed of the classifier is a major issue.\n",
      "A classifier that is 85% accurate may be preferred over one that is 95% accurate if it is 100 times faster in testing (and such differences in time -scales are not uncommon in neural networks for example).\n",
      "Such considerations would be important for the automatic reading of postal codes, or automatic fault detection of items on a production line for example.\n",
      "5\n",
      "\n",
      "Classification\n",
      "A few issues to keep in mind when building a classifier\n",
      "Comprehensibility.\n",
      "If it is a human operator that must apply the classification procedure, the procedure must be easily understood else mistakes will be made in applying the rule.\n",
      "It is important also, that human operators believe the system.\n",
      "Training Time.\n",
      "Especially in a rapidly changing environment, it may be necessary to learn a classification rule quickly, or make adjustments to an existing rule in real time.\n",
      "“Quickly” might imply also that we need only a small number of observations to establish our rule.\n",
      "6\n",
      "\n",
      "K-Nearest Neighbors\n",
      "7\n",
      "\n",
      "Simple approach for k -NN\n",
      "Simple goal:\n",
      "Predict the label of a data point by:\n",
      "Looking at the ‘k’ closest labeled data points (neighbors)\n",
      "Uses a majority vote\n",
      "One of the easiest algorithms to interpret, oftentimes used as a baseline for measuring model performance\n",
      "Memory -Based Learning\n",
      "Also known as “case -based” or “example -based” learning\n",
      "Intuition behind memory -based learning\n",
      "Similar inputs map to similar outputs\n",
      "If true, we just have to define “similar”\n",
      "Not all similarities created equal…\n",
      "8\n",
      "\n",
      "Memory -Based Learning\n",
      "How do we determine “similar”?\n",
      "For instance, if we wanted to:\n",
      "Predict Brian’s weight\n",
      "Who are the similar people?\n",
      "Similar age, diet, height, waistline, activity level …\n",
      "Predict Brian’s IQ\n",
      "Similar occupation, writing style, undergraduate degree, SAT score, …\n",
      "How do we calculate variously ranges in similarity?\n",
      "Need some metric…\n",
      "Distance\n",
      "9\n",
      "\n",
      "k-NN Approach\n",
      "Define a distance 𝑑𝑑(𝑥𝑥1,𝑥𝑥2)between any 2 examples\n",
      "Examples are essentially rows\n",
      "Sowe could just use Euclidean distance … Training\n",
      "Index the training examples for fast lookup (build a “database”)\n",
      "Test\n",
      "Given a new 𝑥𝑥, find the closest neighbor (k=1) from training index\n",
      "Classify x the same as its closest neighbor\n",
      "10\n",
      "\n",
      "k-NN Approach\n",
      "Euclidean Distance Equation:\n",
      "q is the current vector and p is new, think of q as the training and p as the test.\n",
      "Smaller values mean lines or more similar in value or closer together on a graph.\n",
      "11\n",
      "x1 x2 x3 x4 x5 x6 x7 x8 Result\n",
      "q 3 2 2 5 6 2 1 5\n",
      "p 4 3 3 6 4 4 5 6\n",
      "Euclidean 1 1 1 1 4 4 16 1 5.39\n",
      "z 3 2 2 5 6 2 1 5\n",
      "Euclidean0 0 0 0 0 0 0 0 0Subtracting then squaring row p with q.\n",
      "Then summing and taking the sqrt.\n",
      "Do the same with z and compare\n",
      "\n",
      "kNN Decision Boundaries\n",
      "kNN can learn complex decision boundaries\n",
      "12\n",
      "\n",
      "\n",
      "Instead of picking the (1) nearest neighbor, what if we picked the k-Nearest Neighbors and have them vote?\n",
      "Choosing k points is more reliable in the following cases:\n",
      "Noise in training vectors x\n",
      "Noise in training labels y\n",
      "Overlapping classeskNN\n",
      "13x_1x_2\n",
      "+++\n",
      "++++++o\n",
      "oo\n",
      "o\n",
      "ooooooo\n",
      "++\n",
      "+ o\n",
      "ook=1\n",
      "\n",
      "kNN\n",
      "Instead of picking the (1) nearest neighbor, what if we picked the k-Nearest Neighbors and have them vote?\n",
      "Choosing k points is more reliable in the following cases:\n",
      "Noise in training vectors x\n",
      "Noise in training labels y\n",
      "Overlapping classes\n",
      "Why?\n",
      "14x_1x_2+++\n",
      "++++++o\n",
      "oo\n",
      "o\n",
      "ooooooo\n",
      "++\n",
      "+ o\n",
      "ook=1X\n",
      "\n",
      "kNN Decision Boundaries\n",
      "Consider this example with R,G,B classes with significant overlap\n",
      "15\n",
      "\n",
      "\n",
      "kNN Decision Boundaries\n",
      "Consider this example with R,G,B classes with significant overlap\n",
      "k=1 Decision Boundary\n",
      "Looks complex\n",
      "Overfitting?\n",
      "16\n",
      "\n",
      "\n",
      "kNN Decision Boundaries\n",
      "k=1 Decision Boundary\n",
      "Looks complex\n",
      "Overfitting?\n",
      "What if we were to increase k?\n",
      "17\n",
      "\n",
      "\n",
      "kNN Decision Boundaries\n",
      "k=1 Decision Boundary\n",
      "Looks complex\n",
      "Overfitting?\n",
      "What if we were to increase k?\n",
      "K=15 Decision boundary\n",
      "18\n",
      "\n",
      "\n",
      "kNN Decision Boundaries\n",
      "k=1 Decision Boundary\n",
      "Looks complex\n",
      "Overfitting?\n",
      "What if we were to increase k?\n",
      "K=15 Decision boundary\n",
      "Smoother boundaries\n",
      "19\n",
      "\n",
      "\n",
      "kNN Decision Boundaries\n",
      "k=1 Decision Boundary\n",
      "Looks complex\n",
      "Overfitting?\n",
      "What if we were to increase k?\n",
      "K=15 Decision boundary\n",
      "Smoother boundaries\n",
      "Generalizes better on unseen data\n",
      "20\n",
      "\n",
      "\n",
      "kNN Decision Boundaries\n",
      "k=1 Decision Boundary\n",
      "Looks complex\n",
      "Overfitting?\n",
      "What if we were to increase k?\n",
      "K=15 Decision boundary\n",
      "Smoother boundaries\n",
      "Generalizes better on unseen data\n",
      "What makes the boundaries smoother?\n",
      "21\n",
      "\n",
      "\n",
      "kNN Decision Boundaries\n",
      "k=1 Decision Boundary\n",
      "Looks complex\n",
      "Overfitting?\n",
      "What if we were to increase k?\n",
      "K=15 Decision boundary\n",
      "Smoother boundaries\n",
      "Generalizes better on unseen data\n",
      "What makes the boundaries smoother?\n",
      "Let’s look at a two -class (binary) example\n",
      "22\n",
      "\n",
      "\n",
      "k-NN Graphical Example\n",
      "23\n",
      "Consider this two -dimensional dataset with points classified as Red or Green \n",
      "k-NN Graphical Example\n",
      "24\n",
      "Consider this two -dimensional dataset with points classified as Red or Green We want to Classify this point\n",
      "\n",
      "k-NN Graphical Example\n",
      "25\n",
      "Consider this two -dimensional dataset with points classified as Red or Green We want to Classify this point\n",
      "If we consider k=3 neighbors \n",
      "k-NN Graphical Example\n",
      "26\n",
      "Consider this two -dimensional dataset with points classified as Red or Green We want to Classify this point\n",
      "If we consider k=3 neighbors Measured by some distance\n",
      "d\n",
      "\n",
      "k-NN Graphical Example\n",
      "27\n",
      "Consider this two -dimensional dataset with points classified as Red or Green We want to Classify this point\n",
      "If we consider k=3 neighbors Measured by some distance\n",
      "The point is classified as Red\n",
      "\n",
      "k-NN Graphical Example\n",
      "28\n",
      "Consider this two -dimensional dataset with points classified as Red or Green We want to Classify this point\n",
      "If we consider k=3 neighbors Measured by some distance\n",
      "The point is classified as Red\n",
      "If we consider k=5 neighbors\n",
      "\n",
      "k-NN Graphical Example\n",
      "29\n",
      "Consider this two -dimensional dataset with points classified as Red or Green We want to Classify this point\n",
      "If we consider k=3 neighbors Measured by some distance\n",
      "The point is classified as Red\n",
      "If we consider k=5 neighbors\n",
      "Measured by some distanced\n",
      "\n",
      "k-NN Graphical Example\n",
      "30\n",
      "Consider this two -dimensional dataset with points classified as Red or Green We want to Classify this point\n",
      "If we consider k=3 neighbors Measured by some distance\n",
      "The point is classified as Red\n",
      "If we consider k=5 neighbors\n",
      "Measured by some distance\n",
      "The point is classified as Green\n",
      "\n",
      "k-NN Graphical Example\n",
      "31\n",
      "Consider this two -dimensional dataset with points classified as Red or Green We want to Classify this point\n",
      "If we consider k=3 neighbors Measured by some distance\n",
      "The point is classified as Red\n",
      "If we consider k=5 neighbors\n",
      "Measured by some distance\n",
      "The point is classified as Green\n",
      "So, how do we know what k to choose?\n",
      "How to choose “k”\n",
      "Odd k (often 1, 3, or 5):\n",
      "Avoids problem of breaking ties (in a binary classifier)\n",
      "Large k:\n",
      "Less sensitive to noise (particularly class noise)\n",
      "Better probability estimates for discrete classes\n",
      "Larger training sets allow larger values of k\n",
      "Small k:\n",
      "Captures fine structure of problem space better\n",
      "May be necessary with small training sets\n",
      "Balance between large and small k\n",
      "32\n",
      "\n",
      "kNN distance problem\n",
      "Problem:\n",
      "What if the input represents weight in milligrams?\n",
      "Then small differences in physical weight dimension have a huge effect on distances, overwhelming other features\n",
      "Should really correct for these arbitrary “scaling” issues\n",
      "This leads to Standard Scaling\n",
      "Rescale weights so that standard deviation = 1\n",
      "33+oo\n",
      "+\n",
      "weight (lb)attribute_2++\n",
      "+++++o\n",
      "oooooo\n",
      "oo\n",
      "weight (mg)attribute_2\n",
      "++++\n",
      "++ ++\n",
      "+o\n",
      "ooo\n",
      "oooo\n",
      "o o\n",
      "obad\n",
      "\n",
      "More kNN Details\n",
      "Nonparametric -makes no explicit assumptions about the underlying distribution of the input\n",
      "Instance/memory -based learning means that this algorithm doesn’t explicitly learn a model.\n",
      "Instead, it chooses to memorize the training instances which are subsequently used as “knowledge” for the prediction phase\n",
      "Learns arbitrarily complicated decision boundaries\n",
      "Lazy learner -method that generalizes data in the testing (deployment) phase, rather than during the training phase –designed to be continuously updating as new data comes in\n",
      "A benefit of lazy learning is that it can quickly adapt to changes, Think Netflix recommendations, new options are appearing constantly so have a static training set it’s really valuable .\n",
      "Very fast training time, but very slow prediction ( has to search for the nearest neighbors)\n",
      "34\n",
      "\n",
      "Advantages of k -NN\n",
      "Simple and fast to deploy\n",
      "Little to no training time\n",
      "Easy to interpret/explain\n",
      "Naturally handles multiclass datasets\n",
      "Non-parametric Does not assume any probability distributions on the input data\n",
      "35\n",
      "\n",
      "Disadvantages of k -NN\n",
      "Storage of model takes a lot of disk space (contains entire training dataset)\n",
      "Curse of Dimensionality -often works best with 25 or fewer dimensions\n",
      "There is little difference between the nearest and farthest neighbor in high dimensional data (starts to normalize to 1)\n",
      "Computationally expensive predictions (large search problem to find nearest neighbors)\n",
      "Might be impractical in industry settings\n",
      "Need to normalize -suffers from skewed class distributions\n",
      "If one type of category occurs much more than another, classifying an input will be more biased towards that one category (dominates the majority vote since it is more likely to be neighbors with the input) 36\n",
      "\n",
      "kNN Exercise\n",
      "37Switch to R\n",
      "\n",
      "Understanding Probability Classifiers\n",
      "To be able to unpack the probability classifiers, we need a good grasp on the following topics\n",
      "Random variables\n",
      "Distributions\n",
      "Continuous Discrete\n",
      "Statistical Independence\n",
      "Probability\n",
      "Conditional Probability\n",
      "Joint Probability\n",
      "Marginal Probability\n",
      "38\n",
      "\n",
      "Random Variables\n",
      "A random variable is a random number determined by chance, or more formally, drawn according to a probability distribution which specifies the probability that its value falls in any given interval.\n",
      "Discrete Random Variable\n",
      "Taking any of a specified finite or countable list of values, endowed with aprobability mass function characteristic of the random variable's probability distribution\n",
      "Continuous\n",
      "Taking any numerical value in an interval or collection of intervals, via aprobability density function that is characteristic of the random variable's probability distribution\n",
      "39\n",
      "\n",
      "Random Variables\n",
      "Why do we care about Random Variables?\n",
      "Our goal is to predict the target/class\n",
      "We are not given the true (presumably deterministic) function\n",
      "We are only given observations\n",
      "Uncertainty arises through:\n",
      "Noisy measurements\n",
      "Finite size of data sets\n",
      "Ambiguity: The word “bank” can mean (1) a financial institution, (2) the side of a river,or (3) tilting an airplane.\n",
      "Which meaning was intended, based on the words that appear nearby?\n",
      "Probability theory provides a consistent framework for the quantification and manipulation of uncertainty\n",
      "Allows us to make optimal predictions given all the information available to us, even though that information may be incomplete or ambiguous\n",
      "40\n",
      "\n",
      "Probability\n",
      "Probabilities assign numbers to possibilities\n",
      "A probability needs to satisfy three properties (Kolmogorov, 1956):\n",
      "A probability must be nonnegative\n",
      "The sum of the probabilities across all events in the entire sample space must be 1\n",
      "For any two mutually exclusive events, the probability that one or the other occurs is the sum of their individual probabilities\n",
      "For example, the probability that a fair six -sided die comes up 3 OR 4 is 1/6 + 1/6 = 2/6.\n",
      "41\n",
      "\n",
      "Probability Distributions\n",
      "A probability distribution is simply a list of all possible events and their corresponding probabilities\n",
      "There are two kinds of probability distributions\n",
      "Discrete Distribution:\n",
      "Probability of heads or tails\n",
      "Continuous Distribution:\n",
      "Probabilities of people’s heights\n",
      "42\n",
      "\n",
      "Discrete Probability Distribution\n",
      "When the sample space consists of discrete outcomes (e.g., heads or tails), the probability distribution is a list of probabilities of the outcomes\n",
      "The probability of a discrete outcome is called a probability mass\n",
      "The sum of the probability masses across the sample space must be 1\n",
      "43\n",
      "\n",
      "Discrete Probability Example\n",
      "Example\n",
      "Consider the simple experiment of tossing a coin three times.\n",
      "Let X = number of times the coin comes up heads.\n",
      "The 8 possible elementary events and the corresponding values for X are:\n",
      "44Elementary Event Count of Heads (X)\n",
      "TTT 0\n",
      "TTH 1\n",
      "THT 1\n",
      "HTT 1\n",
      "THH 2\n",
      "HTH 2\n",
      "HHT 2\n",
      "HHH 3\n",
      "\n",
      "Discrete Probability Example\n",
      "Example\n",
      "Therefore, the probability distribution for the number of heads occurring in three coin tosses is\n",
      "45Count of Heads (X) p(x) F(x)\n",
      "0 1/8 1/8\n",
      "1 3/8 4/8\n",
      "2 3/8 7/8\n",
      "3 1/8 1\n",
      "\n",
      "Discrete Probability Example\n",
      "46Count of Heads (X) p(x) F(x)\n",
      "0 1/8 1/8\n",
      "1 3/8 4/8\n",
      "2 3/8 7/8\n",
      "3 1/8 1\n",
      "\n",
      "\n",
      "Continuous Probability Distribution\n",
      "When the sample space consists of continuous outcomes (ex: people’s heights) we cannot use probability mass for a specific outcome.\n",
      "Why not?\n",
      "47\n",
      "\n",
      "Continuous Probability Distribution –Probability Density\n",
      "When the sample space consists of continuous outcomes (ex: people’s heights) we cannot use probability mass for a specific outcome.\n",
      "Why not?\n",
      "Because the probability mass for a specific outcome will be zero\n",
      "In other words, the probability of someone’s height being exactly 67.2141390842076153…\n",
      "Instead, we can:\n",
      "Discretize the space into a finite set of mutually exclusive and exhaustive intervals\n",
      "Calculate the probability mass in each interval\n",
      "Use the ratio of probability mass to interval width\n",
      "This ratio is called the Probability Density\n",
      "48\n",
      "\n",
      "Probability Density\n",
      "The top panel of this figure shows the discretized intervals and probability mass in each interval\n",
      "The second panel shows the probability density\n",
      "The third panel shows the narrower intervals and probability mass in each interval\n",
      "The bottom panel shows the probability density corresponding to the more narrow intervals\n",
      "Generally, the skinnier the intervals are, the more accurate the probability density is\n",
      "49\n",
      "\n",
      "\n",
      "Probability Density\n",
      "While probability mass cannot exceed 1, probability densities can\n",
      "The upper panel of this figure shows that most of the probability mass is concentrated around 84\n",
      "Consequently, the probability density near 84 exceeds 1.0, as shown in the lower panel\n",
      "This simply means that there is a high concentration of probability mass relative to the width of the interval\n",
      "50\n",
      "\n",
      "\n",
      "Properties of Probability Density Functions\n",
      "We need to define some notations first\n",
      "Let:\n",
      "𝑥𝑥be the continuous variable\n",
      "Δ𝑥𝑥be the width of an interval on 𝑥𝑥\n",
      "𝑖𝑖be an index for the intervals\n",
      "[𝑥𝑥𝑖𝑖,𝑥𝑥𝑖𝑖+∆𝑥𝑥]be the interval between 𝑥𝑥𝑖𝑖and 𝑥𝑥𝑖𝑖+∆𝑥𝑥\n",
      "𝑃𝑃([𝑥𝑥𝑖𝑖,𝑥𝑥𝑖𝑖+∆𝑥𝑥])be the probability mass of the 𝑖𝑖th interval\n",
      "Then the sum of those probability masses must be 1:\n",
      "�\n",
      "𝑖𝑖𝑃𝑃𝑥𝑥𝑖𝑖,𝑥𝑥𝑖𝑖+∆𝑥𝑥=1\n",
      "We can rewrite the equation above in terms of the density of each interval, by  dividing and multiplying by x:\n",
      "�\n",
      "𝑖𝑖∆𝑥𝑥∗𝑃𝑃𝑥𝑥𝑖𝑖,𝑥𝑥𝑖𝑖+∆𝑥𝑥\n",
      "∆𝑥𝑥=1\n",
      "51\n",
      "\n",
      "Properties of Probability Density Functions\n",
      "In the limit, as the interval width becomes infinitesimal, we denote:\n",
      "Summation as ∫ instead of ∑\n",
      "Then, the previous equation (in terms of density) can be rewritten as:\n",
      "�\n",
      "𝑖𝑖∆𝑥𝑥∗𝑃𝑃𝑥𝑥𝑖𝑖,𝑥𝑥𝑖𝑖+∆𝑥𝑥\n",
      "∆𝑥𝑥=1⇒�𝑑𝑑𝑥𝑥𝑝𝑝𝑥𝑥=1\n",
      "We use𝑝𝑝(𝑥𝑥)to represent the probability mass when 𝑥𝑥 is discrete\n",
      "Thus, what 𝑝𝑝(𝑥𝑥)represents depends on the context\n",
      "52\n",
      "\n",
      "The Normal Probability Density Functions\n",
      "Perhaps the most famous probability density function is the normal distribution, also known as the Gaussian distribution\n",
      "The probability density function of normal distribution is\n",
      "p𝑥𝑥=1\n",
      "𝜎𝜎√2𝜋𝜋𝑒𝑒−1\n",
      "2𝑥𝑥−𝜇𝜇\n",
      "𝜎𝜎2\n",
      "Recall, what are 𝜎𝜎and 𝜇𝜇?\n",
      "what do they control?\n",
      "An example of the probability density is shown in the figure where the x axis is divided into a dense comb of small intervals\n",
      "The figure also shows that the area under the curve is, in fact, 1\n",
      "53\n",
      "\n",
      "\n",
      "Example -Continuous Normal Distribution\n",
      "Example of the continuous distribution of weights\n",
      "The continuous normal distribution can describe the distribution of weight of adult males.\n",
      "For example, you can calculate the probability that a man weighs between 160 and 170 pounds.\n",
      "The area of this range is 0.136; therefore, the probability that a randomly selected man weighs between 160 and 170 pounds is 13.6%.\n",
      "The entire area under the curve equals 1.0\n",
      "�\n",
      "−∞+∞\n",
      "𝑝𝑝𝑥𝑥𝑑𝑑𝑥𝑥=1\n",
      "54\n",
      "\n",
      "\n",
      "Joint Probability\n",
      "Joint Probability\n",
      "Knowing that y occurred reduces the sample space to y\n",
      "The part of y where x also occurred, or the probability of x and y occurring, is:\n",
      "𝑃𝑃𝑥𝑥,𝑦𝑦=𝑃𝑃(𝑥𝑥∩𝑦𝑦)\n",
      "Order does not matter:\n",
      "𝑃𝑃(𝑥𝑥,𝑦𝑦)=𝑃𝑃(𝑦𝑦,𝑥𝑥)\n",
      "55\n",
      "Disjoint Sets Mutually Exclusive Events\n",
      "𝑥𝑥∩𝑦𝑦=∅Intersecting sets\n",
      "\n",
      "Joint Probability and Marginal Probability\n",
      "56This table shows the probabilities of various combinations of people’s eye/hair color\n",
      "Each entry indicates the joint probability of particular combinations of eye color (𝑒𝑒)and hair color (ℎ), denoted by 𝑝𝑝(𝑒𝑒,ℎ)\n",
      "The right margin of the table shows the probabilities of the eye colors overall, collapsed across hair colors\n",
      "Such probabilities are called marginal probability , denoted by 𝑝𝑝(𝑒𝑒):\n",
      "𝑝𝑝𝑒𝑒=�\n",
      "ℎ𝑝𝑝(𝑒𝑒,ℎ)\n",
      "The marginal probabilities of the hair colors, 𝑝𝑝(ℎ), are indicated on the lower margin of the table:\n",
      "𝑝𝑝ℎ=�\n",
      "𝑒𝑒𝑝𝑝(𝑒𝑒,ℎ)\n",
      "\n",
      "Conditional Probability\n",
      "Conditional Probability\n",
      "P𝑥𝑥𝑦𝑦is the probability of the occurrence of event 𝑥𝑥, given that 𝑦𝑦occurred is given as:\n",
      "P𝑥𝑥𝑦𝑦=𝑃𝑃(𝑥𝑥∩𝑦𝑦)\n",
      "𝑃𝑃(𝑦𝑦)=𝑃𝑃(𝑥𝑥,𝑦𝑦)\n",
      "𝑃𝑃(𝑦𝑦)\n",
      "Answers the question: How does the probability of an event change if we have extra information?\n",
      "57\n",
      "\n",
      "\n",
      "Conditional Probability Example\n",
      "Coin Toss Example:\n",
      "Toss a fair coin 3 times\n",
      "What is the probability of 3 heads?\n",
      "Answer: 𝑆𝑆𝑆𝑆𝑆𝑆𝑝𝑝𝑆𝑆𝑒𝑒𝑆𝑆𝑝𝑝𝑆𝑆𝑆𝑆𝑒𝑒={𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 }\n",
      "All outcomes are equally likely (if the coin is fair)\n",
      "𝑃𝑃(𝐻𝐻𝐻𝐻𝐻𝐻 )=1\n",
      "8\n",
      "Suppose we are told that the first toss was heads\n",
      "Given this information, how should we compute the probability of {HHH}?\n",
      "Answer: We have a new (reduced) 𝑆𝑆𝑆𝑆𝑆𝑆𝑝𝑝𝑆𝑆𝑒𝑒𝑆𝑆𝑝𝑝𝑆𝑆𝑆𝑆𝑒𝑒={𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 }\n",
      "All outcomes are still equally likely (the coin is still fair)\n",
      "𝑃𝑃(𝐻𝐻𝐻𝐻𝐻𝐻 )=14\n",
      "58\n",
      "\n",
      "Conditional Probability Example\n",
      "We can visualize the conditional probability as follows\n",
      "Think of 𝑃𝑃(𝐴𝐴)as the proportion of the area of the whole sample space taken up by A\n",
      "For 𝑃𝑃(𝐴𝐴|𝐵𝐵)we restrict our attention to B\n",
      "𝑃𝑃(𝐴𝐴|𝐵𝐵)is the proportion of B taken up by A\n",
      "𝑃𝑃𝐴𝐴𝐵𝐵=𝑃𝑃(𝐴𝐴∩𝐵𝐵)\n",
      "𝑃𝑃(𝐵𝐵)\n",
      "59\n",
      "\n",
      "\n",
      "Statistical Independence\n",
      "Independent Events\n",
      "If x and y are independent then they are unconnected and not related to each other\n",
      "We have:\n",
      "P𝑥𝑥𝑦𝑦=𝑃𝑃𝑥𝑥\n",
      "From there it follows that\n",
      "𝑃𝑃𝑥𝑥,𝑦𝑦=𝑃𝑃𝑥𝑥∗𝑃𝑃(𝑦𝑦)\n",
      "In other words, knowing that y occurred does not change the probability that x occurs (and vice versa)\n",
      "Examples of absolute independence include:\n",
      "Eye color and height\n",
      "Hair color and weight\n",
      "60\n",
      "\n",
      "Statistical Independence Example\n",
      "Independent Events\n",
      "If we want to calculate the joint probability of two independent events, we can simply multiply each probability together to get the joint probability\n",
      "“Joint Distribution” = “Product Distribution”\n",
      "P𝑥𝑥,𝑦𝑦=𝑃𝑃𝑥𝑥∗𝑃𝑃(𝑦𝑦)\n",
      "For Example:\n",
      "Probability of tossing a coin and getting “Heads”:\n",
      "𝑃𝑃𝐻𝐻𝑒𝑒𝑆𝑆𝑑𝑑𝐻𝐻=𝑃𝑃𝑥𝑥=1\n",
      "2\n",
      "Probability of rolling a dice and getting “3”:\n",
      "𝑃𝑃𝑅𝑅𝑅𝑅𝑆𝑆𝑆𝑆“3” =𝑃𝑃𝑦𝑦=16\n",
      "𝑃𝑃𝐻𝐻𝑒𝑒𝑆𝑆𝑑𝑑𝐻𝐻∗𝑃𝑃𝑅𝑅𝑅𝑅𝑆𝑆𝑆𝑆“3” =P𝑥𝑥,𝑦𝑦=12∗16=1\n",
      "12\n",
      "61\n",
      "\n",
      "Linear regression finds the straight line, called the least squares regression line or LSRL, that best represents observations in a data set.\n",
      "Suppose Yis a dependent variable, and Xis an independent variable.\n",
      "Then, the equation for the regression line would be: ŷ = b0+ b1xRegression Review\n",
      "\n",
      "\n",
      "63\n",
      "George Washington University, Intro to Data ScienceAssumptions\n",
      "Linear relationship between dependent and independent -Plot\n",
      "Multicollinearity –occurs when independent variables are not independent –checked with VIF Auto -correlation –Occurs when residuals are not independent from each other – stock prices example -Durbin- Watson d tests\n",
      "Homoscedasticity –Error term along the regression line are equal –Scatter Plot –can convert the dependent variable.\n",
      "64\n",
      "George Washington University, Intro to Data ScienceRegression\n",
      "Works best with numeric/continuous data to increase the inferential power\n",
      "We are trying to model or explain the relationship between a single variable Y (response, output, dependent) and one or more X1…..\n",
      "(predictor, input, independent, explanatory, regessors..etc.)\n",
      "Y must be a continuous variable but X categorical, continuous, Centering (scale/ Zscores )variables so that the predictors have mean 0, is often recommended.\n",
      "The intercept term is then interpreted as the expected value of Yiwhen the predictor values are set to their means .\n",
      "Otherwise, the intercept is interpreted as the expected value of Yi when the predictors are set to 0, which may not be a realistic or interpretable situation (e.g.\n",
      "what if the predictors were height and weight?).\n",
      "65\n",
      "George Washington University, Intro to Data ScienceWhat Does r2(Coefficient of Determination) Mean?\n",
      "This statistic quantifies the proportion of the variance of one variable “explained” (in a statistical sense, not a causal sense) by the other.\n",
      "As an example at data set in R (.2697) = 27% of the variability in trunk Height is explained by Girth\n",
      "\n",
      "\n",
      "66\n",
      "George Washington University, Intro to Data ScienceR Regression Output: Coefficients\n",
      "Estimate?\n",
      "Coefficients for the regression equation\n",
      "Standard Error?\n",
      "Measures the average amount that the coefficient estimates vary from the actual average value of our response variable, can be used to generate confidence intervals.\n",
      "P value?\n",
      "A small p -value indicates that it is unlikely we will observe a relationship between the predictor (Girth) and response (Height) variables due to chance, a p-value of .05 or less is considered statistical significant Formula Call?\n",
      "Regression equation\n",
      "\n",
      "67\n",
      "George Washington University, Intro to Data ScienceR Regression Output\n",
      "Residual Standard Error\n",
      "The average amount that the response will deviate from the true regression line.\n",
      "Used to evaluate our model.\n",
      "In our example we have a residual error of 5.538 in the predication of tree height.\n",
      "The average tree height is 76 meaning we could be off by roughly 7%.\n",
      "Multiple R -squared?\n",
      "Provides a measure of how well the model is fitting the actual data, percentage of variance explained by the predictors, ours example is 27%\n",
      "\n",
      "68\n",
      "George Washington University, Intro to Data ScienceVisual \n",
      "\n",
      "69\n",
      "George Washington University, Intro to Data Science\n",
      "Bike Share Output\n",
      "\n",
      "Another Definition: Functional Approximation\n",
      "What is a functional approximation problem?\n",
      "Target variable: Dependent: What we are trying to predict\n",
      "Other Variables: Independent: Using to Predict\n",
      "Functional approximation is a approach that uses the other variables we have access to approximate the dependent and does so through the function development\n",
      "We will use regression and which assumes that we have a numeric target variable, for classification it’s often a bi- variate or class level variable 70\n",
      "\n",
      "Assessment Measures Assessing Regression Models: MSE, RMSE and MAE\n",
      "MSE –The difference between the predicted values and the actual values squared\n",
      "RSME –Same as above only the square root is taken to put the error back in terms of the dependent variable\n",
      "Can also normalize the RSME to the range of the data in order to be able to compare RSME outputs that include different data ranges MAE –The same approach only taking the absolute value instead of squaring 71\n",
      "\n",
      "Equations 72nX X\n",
      "RMSEn\n",
      "iidelmo iobs∑=−\n",
      "=12\n",
      ", , ) (\n",
      "min, max, obs obs X XRMSENRMSE\n",
      "−=\n",
      "\n",
      "\n",
      "Linear Regression\n",
      "Best used in situations where the data being deploy lacks a high level of complexity Trains very fast but isn’t able to create complex decision boundaries when data is heterogeneous.\n",
      "Also as compared to most ML approaches OLS does not have a built in function that can control for overfitting However not all hope is lost, we can use forward stepwise regression –which we discussed in Intro to DS or\n",
      "Ridge/Penalized Regression 73\n",
      "\n",
      "Regression Said another way basic linear regression has a Prediction Accuracy Problem:\n",
      "Has a low bias (overfitting) but a high variance\n",
      "This can be improved by injecting some level of bias into the equation by reducing the impact of certain coefficients This can improve overall accuracy by reducing variance Draw Dart Board –\n",
      "Another issue is interpretation –with a large number of predictor variables and large data sets it often hard to identify variable importance and explain model outcomes\n",
      "74\n",
      "\n",
      "Regression and Sparsity\n",
      "Often we have more features than observations in the world of big data\n",
      "What type of problem is this?\n",
      "So we strive to have Sparse models\n",
      "What do we mean by Sparse?\n",
      "75\n",
      "\n",
      "Bias Versus Variance 76\n",
      "\n",
      "\n",
      "Ridge Regression Injecting bias can be done through a regulator or utilization of a penalizing attribute.\n",
      "two common examples are Ridge and Lasso, let’s start with Ridge\n",
      "77\n",
      "Basic Regression Equation\n",
      "\n",
      "78\n",
      "\n",
      "\n",
      "Ridge Penalty In this example if 𝛼𝛼 is 0 we simply have normal regression equations\n",
      "If 𝛼𝛼is large or ∞ then the 𝛽𝛽approaches zero and only the constant term is available to predict y\n",
      "Essentially provides a weight on the squared residuals during the normal OLS process We want to train the regulator to fall between 0 and 1 using cross -validation in such a way that it minimize mean square error\n",
      "79\n",
      "Essentially the square of the Euclidean norm (magnitude) of 𝛽𝛽which is the vector of coefficientsor\n",
      "𝛼𝛼\n",
      "\n",
      "Gradient Decent Penalized regression methods use a very common algorithm called gradient decent\n",
      "It is essentially a step function that works to minimize some cost function through a iterative process\n",
      "80\n",
      "\n",
      "\n",
      "Gradient Decent 81\n",
      "\n",
      "\n",
      "82\n",
      "\n",
      "\n",
      "L2 Norm or Euclidean Norm or Euclidean Length\n",
      "Square of all the elements in matrix Sum the values together\n",
      "Take the square root Ridge also squares the final result of this, so we are taking the square of the Euclidean norm and multiply this number by the penalty to weight the coefficients 83In words, the L2 norm is defined as, 1) square all the elements in the vector together; 2) sum these squared values; and, 3) take the square root o f this sum.\n",
      "Ridge Visualization\n",
      "84\n",
      "\n",
      "\n",
      "Measuring the Magnitude of Vectors, L2 and L1\n",
      "𝛽𝛽=𝛽𝛽0\n",
      "𝛽𝛽1\n",
      "L2 –Norm : II𝛽𝛽II2 = 𝛽𝛽02+ 𝛽𝛽12 : What we used for ridge, essentially calculates the magnitude of the coefficient vector of the regression equation, gives us a bias minimum L1 85\n",
      "\n",
      "Measuring the Magnitude of Vectors, L2 and L1\n",
      "𝛽𝛽=𝛽𝛽0\n",
      "𝛽𝛽1\n",
      "L1 –Norm : II𝛽𝛽II1 = I𝛽𝛽0I + I𝛽𝛽1I : What we used for ridge, essentially calculates the magnitude of the coefficient vector of the regression equation, gives us a bias minimum.\n",
      "Cartesian Distance or Euclidean Distance of our vector\n",
      "86\n",
      "\n",
      "Measuring the Magnitude of Vectors, L2 and L1\n",
      "87\n",
      "\n",
      "Lasso Equation\n",
      "88\n",
      "\n",
      "\n",
      "Machine Learning Overview\n",
      "Brian Wright, PhD\n",
      "\n",
      "2Themes\n",
      "Machine Learning Lifecycle\n",
      "Are you ready for Machine Learning?\n",
      "Terms and Phases \n",
      "Engineering of Machine Learning Algos versus Software Development Source: https://towardsdatascience.com/stoend -to-\n",
      "end-data -science -life-cycle -6387523b5afc\n",
      "\n",
      "Brian’s Version of Data Science Lifecycle\n",
      "4Question IDBusiness UnderstandingData Acquisition -\n",
      "ETLInitial Model\n",
      "EvaluationData Understanding -\n",
      "EDAInitial Model(s) Building\n",
      "Evaluation Criteria Value Metric Model Creation & Training Feature Engineering and Evaluation\n",
      "Optimization –\n",
      "Hyperpara and EvaluationModel Deployment\n",
      "Data Drift AnalysisModel Performance –Evaluation Value Metric\n",
      "Model Drift Analysis –Model Evaluation\n",
      "Reports –Dashboards -Products G1 G2\n",
      "G3\n",
      "Gate Reviews\n",
      "\n",
      "5Machine Learning Time\n",
      "\n",
      "6“A field of Computer Science that gives computers the ability to learn\n",
      "without being explicitly programmed.”\n",
      "-Arthur Samuel (Coined the term in 1959 at IBM)\n",
      "“The ability [for systems] to acquire their own knowledge, by\n",
      "extracting patterns from raw data.”\n",
      "-Deep Learning, Goodfellow et al\n",
      "“A computer program is said to learn from experience E with respect\n",
      "to some set of tasks T and performance measure P if its performance\n",
      "tasks in T, as measured by P, improves with experience E.”\n",
      "-Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)\n",
      "\n",
      "Machine vs.\n",
      "human\n",
      "Machine Human\n",
      "Understanding context ✔\n",
      "Thinking through the problem ✔\n",
      "Asking the right questions ✔\n",
      "Selecting the right tools ✔\n",
      "Performing calculations quickly ✔\n",
      "Performing repetitive tasks ✔\n",
      "Following pre-defined rules ✔\n",
      "Interpreting results ✔\n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "Pattern discovery when inputs (x) and outputs (y) are knownSupervised machine learning\n",
      "Input x:\n",
      "VoterOutput y:\n",
      "Political affiliation\n",
      "Examples: Classification and regression are supervised machine learning 9\n",
      "\n",
      "The data inputs (x)have no target outputs (y)Unsupervised machine learning\n",
      "Input x:\n",
      "VoterOutput y:\n",
      "Not given\n",
      "(to be discovered)?\n",
      "We want to impose structure on the inputs (x)to say something meaningful about the data\n",
      "10\n",
      "\n",
      "\n",
      "11\n",
      "\n",
      "12Machine Learning is a general use technology what does that mean?\n",
      "Machine Learning Overview Ageneral -purpose technology orGPT is a term coined to describe a new method of producing and inventing that is important enough to have a protracted aggregate impact.\n",
      "Similar to electricity or the internet, in that it can be applied across domains and work to improve market outcomes.\n",
      "13\n",
      "\n",
      "Machine Learning Overview Twitter Data Usage\n",
      "Error rates on ImageNet (10,000 labelled images) have been driven down from 30% in 2010 to less than 3% today.\n",
      "Below 5% is important why?\n",
      "Chess: Deep Blue (IBM AI) searched some 200 million positions per second, Kasparov was searching not more than 5– 10 positions probably, per second.\n",
      "Yet he played almost at the same level….why?\n",
      "14\n",
      "\n",
      "Machine Learning Overview However, before we all turn into robots consider two important facts: 1.We remain remarkably far away from what would be consider a similar general intelligence that can be compared to humans\n",
      "2.Machines cannot do the full range of tasks that humans can do\n",
      "We can then refer to jobs or activities that might be good cases for Machine Learning as SML or Suitable for Machine Learning\n",
      "15What are examples of tasks that might be SML and how do we know if our organizations are ready?\n",
      "Machine Learning Overview Successful implementation of ML requires very detailed specifications on what is to be learned and data to support that learning activity.\n",
      "Including the development of engineering features through a series of trial-and- error and..\n",
      "Then most importantly embedding these products into normal business operations in such a way that efficiencies can be realized.\n",
      "16\n",
      "\n",
      "Machine Learning Overview What tasks are most suitable for ML to take over: Most recent successes are predicated on supervised learning Competency is narrow as compared to the complexity of human decision making  1.Learning a function that maps well -defined inputs to well- defined outputs\n",
      "oIf can predict Y given any value of X –still might not produce the actual causal effect\n",
      "2.Large Data is present or can be created containing input -output pairs\n",
      "oThe more training data available the more arcuate the model\n",
      "3.Task provides clear feedback with well definable goals and metrics oIf we know what to achieve –(optimize flight patterns not a single flight)\n",
      "4.Where reasoning and diverse background knowledge is not necessary\n",
      "oGood at empirical associations but terrible at decision making that requires common sense of historical knowledge\n",
      "5.No need for why the decision was made to be clear\n",
      "oNN could use millions numerical weights\n",
      "17\n",
      "\n",
      "Machine Learning Overview 6.A tolerance for error or sub- optimal solutions\n",
      "oML use probabilistic outputs which means some error is always assumed\n",
      "7.Function of item being learned should not change rapidly over time\n",
      "oWork best when the distribution of future test examples is the same roughly as the training set over time\n",
      "oIf not the case systems need to be in place to refresh algorithms 18\n",
      "\n",
      "How do machines learn?\n",
      "The basic machine learning process can be divided into three parts.\n",
      "Data Input: Past data or information is utilized as a basis for future decision-making\n",
      "Abstraction: The input data is represented in a broader way through the underlying algorithm\n",
      "Generalization: The abstracted representation is generalized to form a framework for making decisions\n",
      "19(reference Introduction to ML by Subramanian Chandramouli , Saikat Dutt , Amit Kumar Das\n",
      "(https://learning.oreilly.com/library/view/machine -learning/9789389588132/xhtml/chapter001.xhtml#ch1_1)\n",
      "\n",
      "20\n",
      "\n",
      "\n",
      "Brian’s Version of Data Science Lifecycle\n",
      "21Question ID\n",
      "Business UnderstandingData Acquisition/ RepresentationInitial Model\n",
      "EvaluationData Understanding -\n",
      "EDAInitial Model(s) Building\n",
      "Evaluation Criteria/Value Metric Model Creation & Training Feature Engineering and Evaluation\n",
      "Final Model DevelopmentModel Deployment\n",
      "Data Drift AnalysisModel Performance –Evaluation Value Metric\n",
      "Model Drift Analysis –Model Evaluation\n",
      "Reports – Dashboards -Products G1 G2\n",
      "G3\n",
      "Gate Reviews\n",
      "\n",
      "Overview of SOME Key ML Methods/Terms Phase –1 Idea Development Prediction versus Inference Independent Metric for Business Value\n",
      "Target Variable and features Classification versus Regression\n",
      "Probabilistic Interpretation\n",
      "Data acquisition/gathering   Phase –2 Data Prep and Problem Exploration\n",
      "Variable types and data types\n",
      "Scaling and/or Normalizing Data/One -Hot Encoding\n",
      "Missing Data Baseline –prevalence\n",
      "Data Partitioning/Sampling EDA and (Summary Stats and Visuals)\n",
      "Cross Validation Phase –3 –Solution Model Development\n",
      "Parameters versus Hyperparameters\n",
      "Thresholding Feature Engineering\n",
      "Bias versus Variance Tradeoff Model Evaluation Non -parametric modelling (random state)\n",
      "22\n",
      "\n",
      "23Phase I\n",
      "\n",
      "# Prediction versus Inference 24\n",
      "\n",
      "# Prediction versus Inference\n",
      "Goals of prediction are not centered on how the features are interacting or resulting in an event but are instead focused on the ability of the model to predicted an event.\n",
      "Almost all ML methods are focused on predication not causation or inference.\n",
      "This is why model performance is based largely on how well a model predicts not necessarily how much individual variables are contributing to error reduction.\n",
      "25\n",
      "\n",
      "Overview of Key ML Methods/Terms Independent Metric for Business Value A key part of building a solution using Machine Learning Techniques is having a metric that is independent of the model that can be used to determine if the model is providing value.\n",
      "Examples\n",
      "Recommender Engine for Netflix: Number of user clicks\n",
      "Spam Block Predictor: Number of viruses in the network Market Clustering: Did sales increase\n",
      "Others?\n",
      "26\n",
      "\n",
      "Overview of Key ML Methods/Terms: Target Variable versus Features Target variable –Is the variable that includes the patterns the machine learning algorithm is trying to learn.\n",
      "It is the variable of interest and key to evaluating the model output.\n",
      "More simply it is the variable we are trying to predict.\n",
      "Feature variables – Are the variables the model will use to learn the patterns of the target variable.\n",
      "The process of feature engineering can result in additional features.\n",
      "More simply these are the variables used for predicting the target\n",
      "27\n",
      "\n",
      "Overview of Key ML Methods/Terms: Classification versus Regression Classification is the process of developing a model to predict whether a target variable is in defined categories.\n",
      "This is driven by having either a binary or multi- level categorical variable as the target variable.\n",
      "Examples: Predicting whether someone is male, or female based on 1,000s of pictures.\n",
      "Predicting whether a team will have a winning season or not based on player performance Predicting whether a person will default on a loan or not\n",
      "Key point: The predications of the model are not binary (1s or 0s) but are given as percentages indicating the likelihood that any one row of data belongs to any one category.\n",
      "In the case of target variables with multiple categories each row will get the same number of percent predictions as categories.\n",
      "28\n",
      "\n",
      "Overview of Key ML Methods/Terms: Classification versus Regression Regression is the process of developing a model to predict a specific number or range of numbers.\n",
      "This is driven by having a continuous variable as the target variable for the model\n",
      "Examples: Predicting the score given the players playing a game.\n",
      "Predicting an amount of rain given weather conditions Predicting a persons weight based on various personal statistics\n",
      "29\n",
      "\n",
      "Overview of Key ML Methods/Terms: Probabilistic Interpretation A significant portion of this class will focus on building models for classification.\n",
      "Classification is a much more common machine learning goal versus regression.\n",
      "We all know the range of values for probabilities, 0 to 100, the key to understanding these outputs is to think of them as risk measures, with 100 being no risk and 0 being all the risk!\n",
      "How the outputs are used will depend on your question.\n",
      "Example: How certain do you want to be that a drug is effective as compared to whether a customer will open a marketing email?\n",
      "The results could both yield 75% probabilities but is that high enough?\n",
      "Could also think of the outputs as a quantification of uncertainty, the question becomes given your problem how much uncertainty are you willing to accept?\n",
      "30\n",
      "\n",
      "Overview of Key ML Methods/Terms: Data Brainstorming Data to Concept –Does the data available support the algo target and goal\n",
      "How difficult is the data to gather?\n",
      "Is the data large enough?\n",
      "What is the rate of change of the data?\n",
      "Do we believe this is the correct source and data content to address the problem?\n",
      "Learning Difficulty –How complex or vague is the target variable?\n",
      "Are there imbalances in the classes?\n",
      "Does the data clearly link to the problem?\n",
      "Has this data been used in the past, to what success?\n",
      "Is the target difficult to measure or break into smaller components?\n",
      "What risk level are you willing to accept given the question?31\n",
      "\n",
      "32Phase II\n",
      "\n",
      "Brian’s Version of Data Science Lifecycle\n",
      "33Question ID\n",
      "Business UnderstandingData Acquisition/ RepresentationInitial Model\n",
      "EvaluationData Understanding -\n",
      "EDAInitial Model(s) Building\n",
      "Evaluation Criteria/Value Metric Model Creation & Training Feature Engineering and Evaluation\n",
      "Final Model DevelopmentModel Deployment\n",
      "Data Drift AnalysisModel Performance –Evaluation Value Metric\n",
      "Model Drift Analysis –Model Evaluation\n",
      "Reports – Dashboards -Products G1 G2\n",
      "G3\n",
      "Gate Reviews\n",
      "\n",
      "Overview of SOME Key ML Methods/Terms Phase –1 Idea Development Prediction versus Inference Independent Metric for Business Value\n",
      "Target Variable and features Classification versus Regression\n",
      "Probabilistic Interpretation Data acquisition/gathering     Phase –2 Data Prep and Problem Exploration\n",
      "Variable types and data types\n",
      "Baseline –prevalence\n",
      "Scaling and/or Normalizing Data/One -Hot Encoding\n",
      "Missing Data Data Partitioning/Sampling EDA (Summary Stats and Visuals)\n",
      "Cross Validation Phase –3 –Solution Model Development\n",
      "Parameters versus Hyperparameters\n",
      "Thresholding Feature Engineering\n",
      "Bias versus Variance Tradeoff Model Evaluation\n",
      "Non -parametric modelling (random state)\n",
      "34\n",
      "\n",
      "Overview of Key ML Methods/Terms Variable Types and Data Types\n",
      "Five Atomic Variable Types in R\n",
      "Numeric –number unlimited size\n",
      "Integer –number with constraints on size  Complex –numbers and characters\n",
      "Character –words\n",
      "Factor –unique character class that is limited in the number of categories\n",
      "Logical –True or False Data Types List -A list is an R -object containing different types of elements inside it like vectors, functions, and even another list inside it.\n",
      "Vector -Avector in R is a series of data items of the same basic type (from above)\n",
      "Array -is alistorvector with two or more dimensions\n",
      "Matrix -A matrix is a two -dimensional rectangular data structure, created through the use of matrix function.\n",
      "Usually numeric, can’t have different data types, think of it as many vectors Dataframe –A two dimensional object that can contain multiple variable types 35\n",
      "\n",
      "Overview of Key ML Methods/Terms Some useful variable and data type  str()\n",
      "class()\n",
      "names()\n",
      "length()\n",
      "dim()\n",
      "Open up Rstudio and try these functions out on the mtcars dataset.\n",
      "See if you agree with the output.\n",
      "36\n",
      "\n",
      "Overview of Key ML Methods/Terms Baseline –prevalence The proportion of a particular population found to be in the positive class at a specific time.\n",
      "“Positive class” in this example is the class to which we are trying to learn.\n",
      "Percentage split across classes of our target variable.\n",
      "Using mtcars again, what is the prevalence of vs variable?\n",
      "37\n",
      "\n",
      "Overview of Key ML Methods/Terms Scaling and/or Normalizing Data/One -Hot Encoding\n",
      "Many DS approaches require the data to be normalized or placed into a standard format so comparison between variables is possible.\n",
      "For factor variables this measure creating individual columns for each level that are logical or boolien 1s and 0s.\n",
      "We will mostly use a min max scaler that will maintain the variance of the values but re -calculate them to be between 1 and 0.\n",
      "Use the minmax scaling function in the gradDescent package and scale the mtcars dataset setting the results to a new object.\n",
      "What happens?\n",
      "What class is the object?\n",
      "Can you view the data.frame?\n",
      "38\n",
      "\n",
      "Overview of Key ML Methods/Terms Missing Data Large area of study concerning missing data.\n",
      "Here we just need to be aware of how to check for missing data and quick solutions R comes with several functions/packages that handle missing data we are going to focus on the MICE package.\n",
      "First you need to try to detect if there are patterns of missing data, is it random or not.\n",
      "If you detect patterns than you have to develop a strategy to deal with that issue.\n",
      "MCAR –missing completely at random\n",
      "MNAR  -missing not at random\n",
      "Start with the summary() function on a data frame Load in the beaches dataframe from the data file and find the columns that have missing data using the summary function\n",
      "Generally variables with more the about 5% missing values should be deleted or imputation needs to occur Dig a little deeper and use the md.pattern () function in the Mice package.\n",
      "Since there doesn’t appear to be a pattern we will use complete cases to remove the NAs.\n",
      "39\n",
      "\n",
      "Overview of Key ML Methods/Terms Missing Data Complete.cases () function creates aindex to remove missing values remove missing values from a vector x <-x[complete.cases(x)]\n",
      "remove from a data.frame\n",
      "df <-df[complete.cases(df), ]\n",
      "remove from individual rows df <-df[complete.cases(df[ , c(row1, row2, ….)]), ]\n",
      "Try the dataframe version on the beaches dataset, then use summary() to see if the missing datapoints are gone.\n",
      "MICE package can also do imputation (NA replacement) very easily, lots of examples online on how to do these in very robust ways.\n",
      "40\n",
      "\n",
      "Overview of Key ML Methods/Terms Partitioning and Sampling\n",
      "We need to split our data into three sections (in most cases) to build machine learning models Training –What we use to build the original model Tune –Data used to evaluate initial outputs of a model after it’s been modified (example: changing the k in kNN ) (Feature Engineering)\n",
      "Test –Very last step to evaluate the quality of the model after training and tune\n",
      "The function we will be using throughout the course will be the createDataPartition() function in the caret package.\n",
      "The problem is that it’s not great at creating multiple partition, so we essentially use it twice to create a sample, then a sample of a sample.\n",
      "Need to make sure to use the target variable to do stratified sampling, otherwise we could create imbalances in our samples.\n",
      "41\n",
      "\n",
      "Cross -Validation\n",
      "42\n",
      "\n",
      "\n",
      "Cross -Validation\n",
      "43\n",
      "\n",
      "\n",
      "44Phase III\n",
      "\n",
      "Overview of SOME Key ML Methods/Terms 45Phase –1 Idea Development Prediction versus Inference Independent Metric for Business Value\n",
      "Target Variable and features Classification versus Regression\n",
      "Probabilistic Interpretation Data acquisition/gathering     Phase –2 Data Prep and Problem Exploration\n",
      "Variable types and data types\n",
      "Baseline –prevalence\n",
      "Scaling and/or Normalizing Data/One -Hot Encoding\n",
      "Missing Data Data Partitioning/Sampling EDA (Summary Stats and Visuals)\n",
      "Cross Validation Phase –3 –Solution Model Development\n",
      "Parameters versus Hyperparameters\n",
      "Thresholding Feature Engineering\n",
      "Bias versus Variance Tradeoff Model Evaluation\n",
      "Non- parametric modelling (random state)\n",
      "\n",
      "Overview of Key ML Methods/Terms # Feature Engineering – Combining or exploring different levels of variable that best work in your model.\n",
      "Likely going to dedicate a week to just this topic.\n",
      "46\n",
      "\n",
      "Overview of Key ML Methods/Terms Thresholding –The percentage point where our models will predict the result to be either a 0 or 1, in the typical binary case.\n",
      "Adjust the threshold associated with indication of a positive class.\n",
      "The default is 50%, could be that we want to be extra careful and instead adjust that measure up to 75% or 90%.\n",
      "47\n",
      "\n",
      "Overview of Key ML Methods/Terms Evaluation –The metrics you use to assess model quality.\n",
      "There are a ton of this measures, and we are dedicating an entire week to the exploring these further.\n",
      "I’ll show some examples in the code for this week.\n",
      "48\n",
      "\n",
      "Bias Versus Variance 49\n",
      "\n",
      "\n",
      "50Extra Material \n",
      "51Bookings.com  \n",
      "52Lesson Learned: Booking.com\n",
      "\n",
      "Bookings.com\n",
      "Swiss Army Knife –Their approach to ML is highly adoptable , meaning it can be used in a variety of settings –generate specific results or more generalizable depending on the inputs (data)\n",
      "Offline Health Check– Use Randomized Control Trails (RCT) to test model outputs aligned with normative business metrics to assess quality (customer conversion)\n",
      "Increase model performance doesn’t necessary translate to better gain in value\n",
      "53\n",
      "\n",
      "Bookings.com\n",
      "Make a Target Before you Shoot –Develop a clear understanding of the business case and target variable (what is date flexibility) Learning Difficulty –How complex or vague is the target\n",
      "Data to Concept –Does the data available support the algo target and goal\n",
      "Selection Bias –Does the model perform better for a subset of the target Speed Kills –ML algos, even simple ones, take a lot of computing power –to reduce user weight time (latency) measures should be taken\n",
      "See page 1748 (sparsity, model redundancy, caching…etc.)\n",
      "54\n",
      "\n",
      "Machine Learning Overview Keep a watchful eye –Used specialized monitoring tools to understand how the models are performing in practice (even when the result was unclear)\n",
      "Traditional Research Methods (Experimental Design) is a Best Practice Approach to ML –\n",
      "“Experimentation through Randomized Controlled Trials is ingrained into Booking.com culture”\n",
      "55\n",
      "\n",
      "Machine Learning Overview\n",
      "Brian Wright, PhD\n",
      "\n",
      "2Themes\n",
      "Machine Learning Lifecycle\n",
      "Are you ready for Machine Learning?\n",
      "Terms and Phases \n",
      "Engineering of Machine Learning Algos versus Software Development Source: https://towardsdatascience.com/stoend -to-\n",
      "end-data -science -life-cycle -6387523b5afc\n",
      "\n",
      "Brian’s Version of Data Science Lifecycle\n",
      "4Question IDBusiness UnderstandingData Acquisition -\n",
      "ETLInitial Model\n",
      "EvaluationData Understanding -\n",
      "EDAInitial Model(s) Building\n",
      "Evaluation Criteria Value Metric Model Creation & Training Feature Engineering and Evaluation\n",
      "Optimization –\n",
      "Hyperpara and EvaluationModel Deployment\n",
      "Data Drift AnalysisModel Performance –Evaluation Value Metric\n",
      "Model Drift Analysis –Model Evaluation\n",
      "Reports –Dashboards -Products G1 G2\n",
      "G3\n",
      "Gate Reviews\n",
      "\n",
      "5Machine Learning Time\n",
      "\n",
      "6“A field of Computer Science that gives computers the ability to learn\n",
      "without being explicitly programmed.”\n",
      "-Arthur Samuel (Coined the term in 1959 at IBM)\n",
      "“The ability [for systems] to acquire their own knowledge, by\n",
      "extracting patterns from raw data.”\n",
      "-Deep Learning, Goodfellow et al\n",
      "“A computer program is said to learn from experience E with respect\n",
      "to some set of tasks T and performance measure P if its performance\n",
      "tasks in T, as measured by P, improves with experience E.”\n",
      "-Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)\n",
      "\n",
      "Machine vs.\n",
      "human\n",
      "Machine Human\n",
      "Understanding context ✔\n",
      "Thinking through the problem ✔\n",
      "Asking the right questions ✔\n",
      "Selecting the right tools ✔\n",
      "Performing calculations quickly ✔\n",
      "Performing repetitive tasks ✔\n",
      "Following pre-defined rules ✔\n",
      "Interpreting results ✔\n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "Pattern discovery when inputs (x) and outputs (y) are knownSupervised machine learning\n",
      "Input x:\n",
      "VoterOutput y:\n",
      "Political affiliation\n",
      "Examples: Classification and regression are supervised machine learning 9\n",
      "\n",
      "The data inputs (x)have no target outputs (y)Unsupervised machine learning\n",
      "Input x:\n",
      "VoterOutput y:\n",
      "Not given\n",
      "(to be discovered)?\n",
      "We want to impose structure on the inputs (x)to say something meaningful about the data\n",
      "10\n",
      "\n",
      "\n",
      "11\n",
      "\n",
      "12Machine Learning is a general use technology what does that mean?\n",
      "Machine Learning Overview Ageneral -purpose technology orGPT is a term coined to describe a new method of producing and inventing that is important enough to have a protracted aggregate impact.\n",
      "Similar to electricity or the internet, in that it can be applied across domains and work to improve market outcomes.\n",
      "13\n",
      "\n",
      "Machine Learning Overview Twitter Data Usage\n",
      "Error rates on ImageNet (10,000 labelled images) have been driven down from 30% in 2010 to less than 3% today.\n",
      "Below 5% is important why?\n",
      "Chess: Deep Blue (IBM AI) searched some 200 million positions per second, Kasparov was searching not more than 5– 10 positions probably, per second.\n",
      "Yet he played almost at the same level….why?\n",
      "14\n",
      "\n",
      "Machine Learning Overview However, before we all turn into robots consider two important facts: 1.We remain remarkably far away from what would be consider a similar general intelligence that can be compared to humans\n",
      "2.Machines cannot do the full range of tasks that humans can do\n",
      "We can then refer to jobs or activities that might be good cases for Machine Learning as SML or Suitable for Machine Learning\n",
      "15What are examples of tasks that might be SML and how do we know if our organizations are ready?\n",
      "Machine Learning Overview Successful implementation of ML requires very detailed specifications on what is to be learned and data to support that learning activity.\n",
      "Including the development of engineering features through a series of trial-and- error and..\n",
      "Then most importantly embedding these products into normal business operations in such a way that efficiencies can be realized.\n",
      "16\n",
      "\n",
      "Machine Learning Overview What tasks are most suitable for ML to take over: Most recent successes are predicated on supervised learning Competency is narrow as compared to the complexity of human decision making  1.Learning a function that maps well -defined inputs to well- defined outputs\n",
      "oIf can predict Y given any value of X –still might not produce the actual causal effect\n",
      "2.Large Data is present or can be created containing input -output pairs\n",
      "oThe more training data available the more arcuate the model\n",
      "3.Task provides clear feedback with well definable goals and metrics oIf we know what to achieve –(optimize flight patterns not a single flight)\n",
      "4.Where reasoning and diverse background knowledge is not necessary\n",
      "oGood at empirical associations but terrible at decision making that requires common sense of historical knowledge\n",
      "5.No need for why the decision was made to be clear\n",
      "oNN could use millions numerical weights\n",
      "17\n",
      "\n",
      "Machine Learning Overview 6.A tolerance for error or sub- optimal solutions\n",
      "oML use probabilistic outputs which means some error is always assumed\n",
      "7.Function of item being learned should not change rapidly over time\n",
      "oWork best when the distribution of future test examples is the same roughly as the training set over time\n",
      "oIf not the case systems need to be in place to refresh algorithms 18\n",
      "\n",
      "How do machines learn?\n",
      "The basic machine learning process can be divided into three parts.\n",
      "Data Input: Past data or information is utilized as a basis for future decision-making\n",
      "Abstraction: The input data is represented in a broader way through the underlying algorithm\n",
      "Generalization: The abstracted representation is generalized to form a framework for making decisions\n",
      "19(reference Introduction to ML by Subramanian Chandramouli , Saikat Dutt , Amit Kumar Das\n",
      "(https://learning.oreilly.com/library/view/machine -learning/9789389588132/xhtml/chapter001.xhtml#ch1_1)\n",
      "\n",
      "20\n",
      "\n",
      "\n",
      "Brian’s Version of Data Science Lifecycle\n",
      "21Question ID\n",
      "Business UnderstandingData Acquisition/ RepresentationInitial Model\n",
      "EvaluationData Understanding -\n",
      "EDAInitial Model(s) Building\n",
      "Evaluation Criteria/Value Metric Model Creation & Training Feature Engineering and Evaluation\n",
      "Final Model DevelopmentModel Deployment\n",
      "Data Drift AnalysisModel Performance –Evaluation Value Metric\n",
      "Model Drift Analysis –Model Evaluation\n",
      "Reports – Dashboards -Products G1 G2\n",
      "G3\n",
      "Gate Reviews\n",
      "\n",
      "Overview of SOME Key ML Methods/Terms Phase –1 Idea Development Prediction versus Inference Independent Metric for Business Value\n",
      "Target Variable and features Classification versus Regression\n",
      "Probabilistic Interpretation\n",
      "Data acquisition/gathering   Phase –2 Data Prep and Problem Exploration\n",
      "Variable types and data types\n",
      "Scaling and/or Normalizing Data/One -Hot Encoding\n",
      "Missing Data Baseline –prevalence\n",
      "Data Partitioning/Sampling EDA and (Summary Stats and Visuals)\n",
      "Cross Validation Phase –3 –Solution Model Development\n",
      "Parameters versus Hyperparameters\n",
      "Thresholding Feature Engineering\n",
      "Bias versus Variance Tradeoff Model Evaluation Non -parametric modelling (random state)\n",
      "22\n",
      "\n",
      "23Phase I\n",
      "\n",
      "# Prediction versus Inference 24\n",
      "\n",
      "# Prediction versus Inference\n",
      "Goals of prediction are not centered on how the features are interacting or resulting in an event but are instead focused on the ability of the model to predicted an event.\n",
      "Almost all ML methods are focused on predication not causation or inference.\n",
      "This is why model performance is based largely on how well a model predicts not necessarily how much individual variables are contributing to error reduction.\n",
      "25\n",
      "\n",
      "Overview of Key ML Methods/Terms Independent Metric for Business Value A key part of building a solution using Machine Learning Techniques is having a metric that is independent of the model that can be used to determine if the model is providing value.\n",
      "Examples\n",
      "Recommender Engine for Netflix: Number of user clicks\n",
      "Spam Block Predictor: Number of viruses in the network Market Clustering: Did sales increase\n",
      "Others?\n",
      "26\n",
      "\n",
      "Overview of Key ML Methods/Terms: Target Variable versus Features Target variable –Is the variable that includes the patterns the machine learning algorithm is trying to learn.\n",
      "It is the variable of interest and key to evaluating the model output.\n",
      "More simply it is the variable we are trying to predict.\n",
      "Feature variables – Are the variables the model will use to learn the patterns of the target variable.\n",
      "The process of feature engineering can result in additional features.\n",
      "More simply these are the variables used for predicting the target\n",
      "27\n",
      "\n",
      "Overview of Key ML Methods/Terms: Classification versus Regression Classification is the process of developing a model to predict whether a target variable is in defined categories.\n",
      "This is driven by having either a binary or multi- level categorical variable as the target variable.\n",
      "Examples: Predicting whether someone is male, or female based on 1,000s of pictures.\n",
      "Predicting whether a team will have a winning season or not based on player performance Predicting whether a person will default on a loan or not\n",
      "Key point: The predications of the model are not binary (1s or 0s) but are given as percentages indicating the likelihood that any one row of data belongs to any one category.\n",
      "In the case of target variables with multiple categories each row will get the same number of percent predictions as categories.\n",
      "28\n",
      "\n",
      "Overview of Key ML Methods/Terms: Classification versus Regression Regression is the process of developing a model to predict a specific number or range of numbers.\n",
      "This is driven by having a continuous variable as the target variable for the model\n",
      "Examples: Predicting the score given the players playing a game.\n",
      "Predicting an amount of rain given weather conditions Predicting a persons weight based on various personal statistics\n",
      "29\n",
      "\n",
      "Overview of Key ML Methods/Terms: Probabilistic Interpretation A significant portion of this class will focus on building models for classification.\n",
      "Classification is a much more common machine learning goal versus regression.\n",
      "We all know the range of values for probabilities, 0 to 100, the key to understanding these outputs is to think of them as risk measures, with 100 being no risk and 0 being all the risk!\n",
      "How the outputs are used will depend on your question.\n",
      "Example: How certain do you want to be that a drug is effective as compared to whether a customer will open a marketing email?\n",
      "The results could both yield 75% probabilities but is that high enough?\n",
      "Could also think of the outputs as a quantification of uncertainty, the question becomes given your problem how much uncertainty are you willing to accept?\n",
      "30\n",
      "\n",
      "Overview of Key ML Methods/Terms: Data Brainstorming Data to Concept –Does the data available support the algo target and goal\n",
      "How difficult is the data to gather?\n",
      "Is the data large enough?\n",
      "What is the rate of change of the data?\n",
      "Do we believe this is the correct source and data content to address the problem?\n",
      "Learning Difficulty –How complex or vague is the target variable?\n",
      "Are there imbalances in the classes?\n",
      "Does the data clearly link to the problem?\n",
      "Has this data been used in the past, to what success?\n",
      "Is the target difficult to measure or break into smaller components?\n",
      "What risk level are you willing to accept given the question?31\n",
      "\n",
      "32Phase II\n",
      "\n",
      "Brian’s Version of Data Science Lifecycle\n",
      "33Question ID\n",
      "Business UnderstandingData Acquisition/ RepresentationInitial Model\n",
      "EvaluationData Understanding -\n",
      "EDAInitial Model(s) Building\n",
      "Evaluation Criteria/Value Metric Model Creation & Training Feature Engineering and Evaluation\n",
      "Final Model DevelopmentModel Deployment\n",
      "Data Drift AnalysisModel Performance –Evaluation Value Metric\n",
      "Model Drift Analysis –Model Evaluation\n",
      "Reports – Dashboards -Products G1 G2\n",
      "G3\n",
      "Gate Reviews\n",
      "\n",
      "Overview of SOME Key ML Methods/Terms Phase –1 Idea Development Prediction versus Inference Independent Metric for Business Value\n",
      "Target Variable and features Classification versus Regression\n",
      "Probabilistic Interpretation Data acquisition/gathering     Phase –2 Data Prep and Problem Exploration\n",
      "Variable types and data types\n",
      "Baseline –prevalence\n",
      "Scaling and/or Normalizing Data/One -Hot Encoding\n",
      "Missing Data Data Partitioning/Sampling EDA (Summary Stats and Visuals)\n",
      "Cross Validation Phase –3 –Solution Model Development\n",
      "Parameters versus Hyperparameters\n",
      "Thresholding Feature Engineering\n",
      "Bias versus Variance Tradeoff Model Evaluation\n",
      "Non -parametric modelling (random state)\n",
      "34\n",
      "\n",
      "Overview of Key ML Methods/Terms Variable Types and Data Types\n",
      "Five Atomic Variable Types in R\n",
      "Numeric –number unlimited size\n",
      "Integer –number with constraints on size  Complex –numbers and characters\n",
      "Character –words\n",
      "Factor –unique character class that is limited in the number of categories\n",
      "Logical –True or False Data Types List -A list is an R -object containing different types of elements inside it like vectors, functions, and even another list inside it.\n",
      "Vector -Avector in R is a series of data items of the same basic type (from above)\n",
      "Array -is alistorvector with two or more dimensions\n",
      "Matrix -A matrix is a two -dimensional rectangular data structure, created through the use of matrix function.\n",
      "Usually numeric, can’t have different data types, think of it as many vectors Dataframe –A two dimensional object that can contain multiple variable types 35\n",
      "\n",
      "Overview of Key ML Methods/Terms Some useful variable and data type  str()\n",
      "class()\n",
      "names()\n",
      "length()\n",
      "dim()\n",
      "Open up Rstudio and try these functions out on the mtcars dataset.\n",
      "See if you agree with the output.\n",
      "36\n",
      "\n",
      "Overview of Key ML Methods/Terms Baseline –prevalence The proportion of a particular population found to be in the positive class at a specific time.\n",
      "“Positive class” in this example is the class to which we are trying to learn.\n",
      "Percentage split across classes of our target variable.\n",
      "Using mtcars again, what is the prevalence of vs variable?\n",
      "37\n",
      "\n",
      "Overview of Key ML Methods/Terms Scaling and/or Normalizing Data/One -Hot Encoding\n",
      "Many DS approaches require the data to be normalized or placed into a standard format so comparison between variables is possible.\n",
      "For factor variables this measure creating individual columns for each level that are logical or boolien 1s and 0s.\n",
      "We will mostly use a min max scaler that will maintain the variance of the values but re -calculate them to be between 1 and 0.\n",
      "Use the minmax scaling function in the gradDescent package and scale the mtcars dataset setting the results to a new object.\n",
      "What happens?\n",
      "What class is the object?\n",
      "Can you view the data.frame?\n",
      "38\n",
      "\n",
      "Overview of Key ML Methods/Terms Missing Data Large area of study concerning missing data.\n",
      "Here we just need to be aware of how to check for missing data and quick solutions R comes with several functions/packages that handle missing data we are going to focus on the MICE package.\n",
      "First you need to try to detect if there are patterns of missing data, is it random or not.\n",
      "If you detect patterns than you have to develop a strategy to deal with that issue.\n",
      "MCAR –missing completely at random\n",
      "MNAR  -missing not at random\n",
      "Start with the summary() function on a data frame Load in the beaches dataframe from the data file and find the columns that have missing data using the summary function\n",
      "Generally variables with more the about 5% missing values should be deleted or imputation needs to occur Dig a little deeper and use the md.pattern () function in the Mice package.\n",
      "Since there doesn’t appear to be a pattern we will use complete cases to remove the NAs.\n",
      "39\n",
      "\n",
      "Overview of Key ML Methods/Terms Missing Data Complete.cases () function creates aindex to remove missing values remove missing values from a vector x <-x[complete.cases(x)]\n",
      "remove from a data.frame\n",
      "df <-df[complete.cases(df), ]\n",
      "remove from individual rows df <-df[complete.cases(df[ , c(row1, row2, ….)]), ]\n",
      "Try the dataframe version on the beaches dataset, then use summary() to see if the missing datapoints are gone.\n",
      "MICE package can also do imputation (NA replacement) very easily, lots of examples online on how to do these in very robust ways.\n",
      "40\n",
      "\n",
      "Overview of Key ML Methods/Terms Partitioning and Sampling\n",
      "We need to split our data into three sections (in most cases) to build machine learning models Training –What we use to build the original model Tune –Data used to evaluate initial outputs of a model after it’s been modified (example: changing the k in kNN ) (Feature Engineering)\n",
      "Test –Very last step to evaluate the quality of the model after training and tune\n",
      "The function we will be using throughout the course will be the createDataPartition() function in the caret package.\n",
      "The problem is that it’s not great at creating multiple partition, so we essentially use it twice to create a sample, then a sample of a sample.\n",
      "Need to make sure to use the target variable to do stratified sampling, otherwise we could create imbalances in our samples.\n",
      "41\n",
      "\n",
      "Cross -Validation\n",
      "42\n",
      "\n",
      "\n",
      "Cross -Validation\n",
      "43\n",
      "\n",
      "\n",
      "44Phase III\n",
      "\n",
      "Overview of SOME Key ML Methods/Terms 45Phase –1 Idea Development Prediction versus Inference Independent Metric for Business Value\n",
      "Target Variable and features Classification versus Regression\n",
      "Probabilistic Interpretation Data acquisition/gathering     Phase –2 Data Prep and Problem Exploration\n",
      "Variable types and data types\n",
      "Baseline –prevalence\n",
      "Scaling and/or Normalizing Data/One -Hot Encoding\n",
      "Missing Data Data Partitioning/Sampling EDA (Summary Stats and Visuals)\n",
      "Cross Validation Phase –3 –Solution Model Development\n",
      "Parameters versus Hyperparameters\n",
      "Thresholding Feature Engineering\n",
      "Bias versus Variance Tradeoff Model Evaluation\n",
      "Non- parametric modelling (random state)\n",
      "\n",
      "Overview of Key ML Methods/Terms # Feature Engineering – Combining or exploring different levels of variable that best work in your model.\n",
      "Likely going to dedicate a week to just this topic.\n",
      "46\n",
      "\n",
      "Overview of Key ML Methods/Terms Thresholding –The percentage point where our models will predict the result to be either a 0 or 1, in the typical binary case.\n",
      "Adjust the threshold associated with indication of a positive class.\n",
      "The default is 50%, could be that we want to be extra careful and instead adjust that measure up to 75% or 90%.\n",
      "47\n",
      "\n",
      "Overview of Key ML Methods/Terms Evaluation –The metrics you use to assess model quality.\n",
      "There are a ton of this measures, and we are dedicating an entire week to the exploring these further.\n",
      "I’ll show some examples in the code for this week.\n",
      "48\n",
      "\n",
      "Bias Versus Variance 49\n",
      "\n",
      "\n",
      "50Extra Material \n",
      "51Bookings.com  \n",
      "52Lesson Learned: Booking.com\n",
      "\n",
      "Bookings.com\n",
      "Swiss Army Knife –Their approach to ML is highly adoptable , meaning it can be used in a variety of settings –generate specific results or more generalizable depending on the inputs (data)\n",
      "Offline Health Check– Use Randomized Control Trails (RCT) to test model outputs aligned with normative business metrics to assess quality (customer conversion)\n",
      "Increase model performance doesn’t necessary translate to better gain in value\n",
      "53\n",
      "\n",
      "Bookings.com\n",
      "Make a Target Before you Shoot –Develop a clear understanding of the business case and target variable (what is date flexibility) Learning Difficulty –How complex or vague is the target\n",
      "Data to Concept –Does the data available support the algo target and goal\n",
      "Selection Bias –Does the model perform better for a subset of the target Speed Kills –ML algos, even simple ones, take a lot of computing power –to reduce user weight time (latency) measures should be taken\n",
      "See page 1748 (sparsity, model redundancy, caching…etc.)\n",
      "54\n",
      "\n",
      "Machine Learning Overview Keep a watchful eye –Used specialized monitoring tools to understand how the models are performing in practice (even when the result was unclear)\n",
      "Traditional Research Methods (Experimental Design) is a Best Practice Approach to ML –\n",
      "“Experimentation through Randomized Controlled Trials is ingrained into Booking.com culture”\n",
      "55\n",
      "\n",
      "Machine Learning Overview\n",
      "Brian Wright, PhD\n",
      "\n",
      "2Themes\n",
      "Machine Learning Lifecycle\n",
      "Are you ready for Machine Learning?\n",
      "Terms and Phases \n",
      "Engineering of Machine Learning Algos versus Software Development Source: https://towardsdatascience.com/stoend -to-\n",
      "end-data -science -life-cycle -6387523b5afc\n",
      "\n",
      "Brian’s Version of Data Science Lifecycle\n",
      "4Question IDBusiness UnderstandingData Acquisition -\n",
      "ETLInitial Model\n",
      "EvaluationData Understanding -\n",
      "EDAInitial Model(s) Building\n",
      "Evaluation Criteria Value Metric Model Creation & Training Feature Engineering and Evaluation\n",
      "Optimization –\n",
      "Hyperpara and EvaluationModel Deployment\n",
      "Data Drift AnalysisModel Performance –Evaluation Value Metric\n",
      "Model Drift Analysis –Model Evaluation\n",
      "Reports –Dashboards -Products G1 G2\n",
      "G3\n",
      "Gate Reviews\n",
      "\n",
      "5Machine Learning Time\n",
      "\n",
      "6“A field of Computer Science that gives computers the ability to learn\n",
      "without being explicitly programmed.”\n",
      "-Arthur Samuel (Coined the term in 1959 at IBM)\n",
      "“The ability [for systems] to acquire their own knowledge, by\n",
      "extracting patterns from raw data.”\n",
      "-Deep Learning, Goodfellow et al\n",
      "“A computer program is said to learn from experience E with respect\n",
      "to some set of tasks T and performance measure P if its performance\n",
      "tasks in T, as measured by P, improves with experience E.”\n",
      "-Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)\n",
      "\n",
      "Machine vs.\n",
      "human\n",
      "Machine Human\n",
      "Understanding context ✔\n",
      "Thinking through the problem ✔\n",
      "Asking the right questions ✔\n",
      "Selecting the right tools ✔\n",
      "Performing calculations quickly ✔\n",
      "Performing repetitive tasks ✔\n",
      "Following pre-defined rules ✔\n",
      "Interpreting results ✔\n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "Pattern discovery when inputs (x) and outputs (y) are knownSupervised machine learning\n",
      "Input x:\n",
      "VoterOutput y:\n",
      "Political affiliation\n",
      "Examples: Classification and regression are supervised machine learning 9\n",
      "\n",
      "The data inputs (x)have no target outputs (y)Unsupervised machine learning\n",
      "Input x:\n",
      "VoterOutput y:\n",
      "Not given\n",
      "(to be discovered)?\n",
      "We want to impose structure on the inputs (x)to say something meaningful about the data\n",
      "10\n",
      "\n",
      "\n",
      "11\n",
      "\n",
      "12Machine Learning is a general use technology what does that mean?\n",
      "Machine Learning Overview Ageneral -purpose technology orGPT is a term coined to describe a new method of producing and inventing that is important enough to have a protracted aggregate impact.\n",
      "Similar to electricity or the internet, in that it can be applied across domains and work to improve market outcomes.\n",
      "13\n",
      "\n",
      "Machine Learning Overview Twitter Data Usage\n",
      "Error rates on ImageNet (10,000 labelled images) have been driven down from 30% in 2010 to less than 3% today.\n",
      "Below 5% is important why?\n",
      "Chess: Deep Blue (IBM AI) searched some 200 million positions per second, Kasparov was searching not more than 5– 10 positions probably, per second.\n",
      "Yet he played almost at the same level….why?\n",
      "14\n",
      "\n",
      "Machine Learning Overview However, before we all turn into robots consider two important facts: 1.We remain remarkably far away from what would be consider a similar general intelligence that can be compared to humans\n",
      "2.Machines cannot do the full range of tasks that humans can do\n",
      "We can then refer to jobs or activities that might be good cases for Machine Learning as SML or Suitable for Machine Learning\n",
      "15What are examples of tasks that might be SML and how do we know if our organizations are ready?\n",
      "Machine Learning Overview Successful implementation of ML requires very detailed specifications on what is to be learned and data to support that learning activity.\n",
      "Including the development of engineering features through a series of trial-and- error and..\n",
      "Then most importantly embedding these products into normal business operations in such a way that efficiencies can be realized.\n",
      "16\n",
      "\n",
      "Machine Learning Overview What tasks are most suitable for ML to take over: Most recent successes are predicated on supervised learning Competency is narrow as compared to the complexity of human decision making  1.Learning a function that maps well -defined inputs to well- defined outputs\n",
      "oIf can predict Y given any value of X –still might not produce the actual causal effect\n",
      "2.Large Data is present or can be created containing input -output pairs\n",
      "oThe more training data available the more arcuate the model\n",
      "3.Task provides clear feedback with well definable goals and metrics oIf we know what to achieve –(optimize flight patterns not a single flight)\n",
      "4.Where reasoning and diverse background knowledge is not necessary\n",
      "oGood at empirical associations but terrible at decision making that requires common sense of historical knowledge\n",
      "5.No need for why the decision was made to be clear\n",
      "oNN could use millions numerical weights\n",
      "17\n",
      "\n",
      "Machine Learning Overview 6.A tolerance for error or sub- optimal solutions\n",
      "oML use probabilistic outputs which means some error is always assumed\n",
      "7.Function of item being learned should not change rapidly over time\n",
      "oWork best when the distribution of future test examples is the same roughly as the training set over time\n",
      "oIf not the case systems need to be in place to refresh algorithms 18\n",
      "\n",
      "How do machines learn?\n",
      "The basic machine learning process can be divided into three parts.\n",
      "Data Input: Past data or information is utilized as a basis for future decision-making\n",
      "Abstraction: The input data is represented in a broader way through the underlying algorithm\n",
      "Generalization: The abstracted representation is generalized to form a framework for making decisions\n",
      "19(reference Introduction to ML by Subramanian Chandramouli , Saikat Dutt , Amit Kumar Das\n",
      "(https://learning.oreilly.com/library/view/machine -learning/9789389588132/xhtml/chapter001.xhtml#ch1_1)\n",
      "\n",
      "20\n",
      "\n",
      "\n",
      "Brian’s Version of Data Science Lifecycle\n",
      "21Question ID\n",
      "Business UnderstandingData Acquisition/ RepresentationInitial Model\n",
      "EvaluationData Understanding -\n",
      "EDAInitial Model(s) Building\n",
      "Evaluation Criteria/Value Metric Model Creation & Training Feature Engineering and Evaluation\n",
      "Final Model DevelopmentModel Deployment\n",
      "Data Drift AnalysisModel Performance –Evaluation Value Metric\n",
      "Model Drift Analysis –Model Evaluation\n",
      "Reports – Dashboards -Products G1 G2\n",
      "G3\n",
      "Gate Reviews\n",
      "\n",
      "Overview of SOME Key ML Methods/Terms Phase –1 Idea Development Prediction versus Inference Independent Metric for Business Value\n",
      "Target Variable and features Classification versus Regression\n",
      "Probabilistic Interpretation\n",
      "Data acquisition/gathering   Phase –2 Data Prep and Problem Exploration\n",
      "Variable types and data types\n",
      "Scaling and/or Normalizing Data/One -Hot Encoding\n",
      "Missing Data Baseline –prevalence\n",
      "Data Partitioning/Sampling EDA and (Summary Stats and Visuals)\n",
      "Cross Validation Phase –3 –Solution Model Development\n",
      "Parameters versus Hyperparameters\n",
      "Thresholding Feature Engineering\n",
      "Bias versus Variance Tradeoff Model Evaluation Non -parametric modelling (random state)\n",
      "22\n",
      "\n",
      "23Phase I\n",
      "\n",
      "# Prediction versus Inference 24\n",
      "\n",
      "# Prediction versus Inference\n",
      "Goals of prediction are not centered on how the features are interacting or resulting in an event but are instead focused on the ability of the model to predicted an event.\n",
      "Almost all ML methods are focused on predication not causation or inference.\n",
      "This is why model performance is based largely on how well a model predicts not necessarily how much individual variables are contributing to error reduction.\n",
      "25\n",
      "\n",
      "Overview of Key ML Methods/Terms Independent Metric for Business Value A key part of building a solution using Machine Learning Techniques is having a metric that is independent of the model that can be used to determine if the model is providing value.\n",
      "Examples\n",
      "Recommender Engine for Netflix: Number of user clicks\n",
      "Spam Block Predictor: Number of viruses in the network Market Clustering: Did sales increase\n",
      "Others?\n",
      "26\n",
      "\n",
      "Overview of Key ML Methods/Terms: Target Variable versus Features Target variable –Is the variable that includes the patterns the machine learning algorithm is trying to learn.\n",
      "It is the variable of interest and key to evaluating the model output.\n",
      "More simply it is the variable we are trying to predict.\n",
      "Feature variables – Are the variables the model will use to learn the patterns of the target variable.\n",
      "The process of feature engineering can result in additional features.\n",
      "More simply these are the variables used for predicting the target\n",
      "27\n",
      "\n",
      "Overview of Key ML Methods/Terms: Classification versus Regression Classification is the process of developing a model to predict whether a target variable is in defined categories.\n",
      "This is driven by having either a binary or multi- level categorical variable as the target variable.\n",
      "Examples: Predicting whether someone is male, or female based on 1,000s of pictures.\n",
      "Predicting whether a team will have a winning season or not based on player performance Predicting whether a person will default on a loan or not\n",
      "Key point: The predications of the model are not binary (1s or 0s) but are given as percentages indicating the likelihood that any one row of data belongs to any one category.\n",
      "In the case of target variables with multiple categories each row will get the same number of percent predictions as categories.\n",
      "28\n",
      "\n",
      "Overview of Key ML Methods/Terms: Classification versus Regression Regression is the process of developing a model to predict a specific number or range of numbers.\n",
      "This is driven by having a continuous variable as the target variable for the model\n",
      "Examples: Predicting the score given the players playing a game.\n",
      "Predicting an amount of rain given weather conditions Predicting a persons weight based on various personal statistics\n",
      "29\n",
      "\n",
      "Overview of Key ML Methods/Terms: Probabilistic Interpretation A significant portion of this class will focus on building models for classification.\n",
      "Classification is a much more common machine learning goal versus regression.\n",
      "We all know the range of values for probabilities, 0 to 100, the key to understanding these outputs is to think of them as risk measures, with 100 being no risk and 0 being all the risk!\n",
      "How the outputs are used will depend on your question.\n",
      "Example: How certain do you want to be that a drug is effective as compared to whether a customer will open a marketing email?\n",
      "The results could both yield 75% probabilities but is that high enough?\n",
      "Could also think of the outputs as a quantification of uncertainty, the question becomes given your problem how much uncertainty are you willing to accept?\n",
      "30\n",
      "\n",
      "Overview of Key ML Methods/Terms: Data Brainstorming Data to Concept –Does the data available support the algo target and goal\n",
      "How difficult is the data to gather?\n",
      "Is the data large enough?\n",
      "What is the rate of change of the data?\n",
      "Do we believe this is the correct source and data content to address the problem?\n",
      "Learning Difficulty –How complex or vague is the target variable?\n",
      "Are there imbalances in the classes?\n",
      "Does the data clearly link to the problem?\n",
      "Has this data been used in the past, to what success?\n",
      "Is the target difficult to measure or break into smaller components?\n",
      "What risk level are you willing to accept given the question?31\n",
      "\n",
      "32Phase II\n",
      "\n",
      "Brian’s Version of Data Science Lifecycle\n",
      "33Question ID\n",
      "Business UnderstandingData Acquisition/ RepresentationInitial Model\n",
      "EvaluationData Understanding -\n",
      "EDAInitial Model(s) Building\n",
      "Evaluation Criteria/Value Metric Model Creation & Training Feature Engineering and Evaluation\n",
      "Final Model DevelopmentModel Deployment\n",
      "Data Drift AnalysisModel Performance –Evaluation Value Metric\n",
      "Model Drift Analysis –Model Evaluation\n",
      "Reports – Dashboards -Products G1 G2\n",
      "G3\n",
      "Gate Reviews\n",
      "\n",
      "Overview of SOME Key ML Methods/Terms Phase –1 Idea Development Prediction versus Inference Independent Metric for Business Value\n",
      "Target Variable and features Classification versus Regression\n",
      "Probabilistic Interpretation Data acquisition/gathering     Phase –2 Data Prep and Problem Exploration\n",
      "Variable types and data types\n",
      "Baseline –prevalence\n",
      "Scaling and/or Normalizing Data/One -Hot Encoding\n",
      "Missing Data Data Partitioning/Sampling EDA (Summary Stats and Visuals)\n",
      "Cross Validation Phase –3 –Solution Model Development\n",
      "Parameters versus Hyperparameters\n",
      "Thresholding Feature Engineering\n",
      "Bias versus Variance Tradeoff Model Evaluation\n",
      "Non -parametric modelling (random state)\n",
      "34\n",
      "\n",
      "Overview of Key ML Methods/Terms Variable Types and Data Types\n",
      "Five Atomic Variable Types in R\n",
      "Numeric –number unlimited size\n",
      "Integer –number with constraints on size  Complex –numbers and characters\n",
      "Character –words\n",
      "Factor –unique character class that is limited in the number of categories\n",
      "Logical –True or False Data Types List -A list is an R -object containing different types of elements inside it like vectors, functions, and even another list inside it.\n",
      "Vector -Avector in R is a series of data items of the same basic type (from above)\n",
      "Array -is alistorvector with two or more dimensions\n",
      "Matrix -A matrix is a two -dimensional rectangular data structure, created through the use of matrix function.\n",
      "Usually numeric, can’t have different data types, think of it as many vectors Dataframe –A two dimensional object that can contain multiple variable types 35\n",
      "\n",
      "Overview of Key ML Methods/Terms Some useful variable and data type  str()\n",
      "class()\n",
      "names()\n",
      "length()\n",
      "dim()\n",
      "Open up Rstudio and try these functions out on the mtcars dataset.\n",
      "See if you agree with the output.\n",
      "36\n",
      "\n",
      "Overview of Key ML Methods/Terms Baseline –prevalence The proportion of a particular population found to be in the positive class at a specific time.\n",
      "“Positive class” in this example is the class to which we are trying to learn.\n",
      "Percentage split across classes of our target variable.\n",
      "Using mtcars again, what is the prevalence of vs variable?\n",
      "37\n",
      "\n",
      "Overview of Key ML Methods/Terms Scaling and/or Normalizing Data/One -Hot Encoding\n",
      "Many DS approaches require the data to be normalized or placed into a standard format so comparison between variables is possible.\n",
      "For factor variables this measure creating individual columns for each level that are logical or boolien 1s and 0s.\n",
      "We will mostly use a min max scaler that will maintain the variance of the values but re -calculate them to be between 1 and 0.\n",
      "Use the minmax scaling function in the gradDescent package and scale the mtcars dataset setting the results to a new object.\n",
      "What happens?\n",
      "What class is the object?\n",
      "Can you view the data.frame?\n",
      "38\n",
      "\n",
      "Overview of Key ML Methods/Terms Missing Data Large area of study concerning missing data.\n",
      "Here we just need to be aware of how to check for missing data and quick solutions R comes with several functions/packages that handle missing data we are going to focus on the MICE package.\n",
      "First you need to try to detect if there are patterns of missing data, is it random or not.\n",
      "If you detect patterns than you have to develop a strategy to deal with that issue.\n",
      "MCAR –missing completely at random\n",
      "MNAR  -missing not at random\n",
      "Start with the summary() function on a data frame Load in the beaches dataframe from the data file and find the columns that have missing data using the summary function\n",
      "Generally variables with more the about 5% missing values should be deleted or imputation needs to occur Dig a little deeper and use the md.pattern () function in the Mice package.\n",
      "Since there doesn’t appear to be a pattern we will use complete cases to remove the NAs.\n",
      "39\n",
      "\n",
      "Overview of Key ML Methods/Terms Missing Data Complete.cases () function creates aindex to remove missing values remove missing values from a vector x <-x[complete.cases(x)]\n",
      "remove from a data.frame\n",
      "df <-df[complete.cases(df), ]\n",
      "remove from individual rows df <-df[complete.cases(df[ , c(row1, row2, ….)]), ]\n",
      "Try the dataframe version on the beaches dataset, then use summary() to see if the missing datapoints are gone.\n",
      "MICE package can also do imputation (NA replacement) very easily, lots of examples online on how to do these in very robust ways.\n",
      "40\n",
      "\n",
      "Overview of Key ML Methods/Terms Partitioning and Sampling\n",
      "We need to split our data into three sections (in most cases) to build machine learning models Training –What we use to build the original model Tune –Data used to evaluate initial outputs of a model after it’s been modified (example: changing the k in kNN ) (Feature Engineering)\n",
      "Test –Very last step to evaluate the quality of the model after training and tune\n",
      "The function we will be using throughout the course will be the createDataPartition() function in the caret package.\n",
      "The problem is that it’s not great at creating multiple partition, so we essentially use it twice to create a sample, then a sample of a sample.\n",
      "Need to make sure to use the target variable to do stratified sampling, otherwise we could create imbalances in our samples.\n",
      "41\n",
      "\n",
      "Cross -Validation\n",
      "42\n",
      "\n",
      "\n",
      "Cross -Validation\n",
      "43\n",
      "\n",
      "\n",
      "44Phase III\n",
      "\n",
      "Overview of SOME Key ML Methods/Terms 45Phase –1 Idea Development Prediction versus Inference Independent Metric for Business Value\n",
      "Target Variable and features Classification versus Regression\n",
      "Probabilistic Interpretation Data acquisition/gathering     Phase –2 Data Prep and Problem Exploration\n",
      "Variable types and data types\n",
      "Baseline –prevalence\n",
      "Scaling and/or Normalizing Data/One -Hot Encoding\n",
      "Missing Data Data Partitioning/Sampling EDA (Summary Stats and Visuals)\n",
      "Cross Validation Phase –3 –Solution Model Development\n",
      "Parameters versus Hyperparameters\n",
      "Thresholding Feature Engineering\n",
      "Bias versus Variance Tradeoff Model Evaluation\n",
      "Non- parametric modelling (random state)\n",
      "\n",
      "Overview of Key ML Methods/Terms # Feature Engineering – Combining or exploring different levels of variable that best work in your model.\n",
      "Likely going to dedicate a week to just this topic.\n",
      "46\n",
      "\n",
      "Overview of Key ML Methods/Terms Thresholding –The percentage point where our models will predict the result to be either a 0 or 1, in the typical binary case.\n",
      "Adjust the threshold associated with indication of a positive class.\n",
      "The default is 50%, could be that we want to be extra careful and instead adjust that measure up to 75% or 90%.\n",
      "47\n",
      "\n",
      "Overview of Key ML Methods/Terms Evaluation –The metrics you use to assess model quality.\n",
      "There are a ton of this measures, and we are dedicating an entire week to the exploring these further.\n",
      "I’ll show some examples in the code for this week.\n",
      "48\n",
      "\n",
      "Bias Versus Variance 49\n",
      "\n",
      "\n",
      "50Extra Material \n",
      "51Bookings.com  \n",
      "52Lesson Learned: Booking.com\n",
      "\n",
      "Bookings.com\n",
      "Swiss Army Knife –Their approach to ML is highly adoptable , meaning it can be used in a variety of settings –generate specific results or more generalizable depending on the inputs (data)\n",
      "Offline Health Check– Use Randomized Control Trails (RCT) to test model outputs aligned with normative business metrics to assess quality (customer conversion)\n",
      "Increase model performance doesn’t necessary translate to better gain in value\n",
      "53\n",
      "\n",
      "Bookings.com\n",
      "Make a Target Before you Shoot –Develop a clear understanding of the business case and target variable (what is date flexibility) Learning Difficulty –How complex or vague is the target\n",
      "Data to Concept –Does the data available support the algo target and goal\n",
      "Selection Bias –Does the model perform better for a subset of the target Speed Kills –ML algos, even simple ones, take a lot of computing power –to reduce user weight time (latency) measures should be taken\n",
      "See page 1748 (sparsity, model redundancy, caching…etc.)\n",
      "54\n",
      "\n",
      "Machine Learning Overview Keep a watchful eye –Used specialized monitoring tools to understand how the models are performing in practice (even when the result was unclear)\n",
      "Traditional Research Methods (Experimental Design) is a Best Practice Approach to ML –\n",
      "“Experimentation through Randomized Controlled Trials is ingrained into Booking.com culture”\n",
      "55\n",
      "\n",
      "Machine Learning Overview\n",
      "Brian Wright, PhD\n",
      "\n",
      "2Themes\n",
      "Machine Learning Lifecycle\n",
      "Are you ready for Machine Learning?\n",
      "Terms and Phases \n",
      "Engineering of Machine Learning Algos versus Software Development Source: https://towardsdatascience.com/stoend -to-\n",
      "end-data -science -life-cycle -6387523b5afc\n",
      "\n",
      "Brian’s Version of Data Science Lifecycle\n",
      "4Question IDBusiness UnderstandingData Acquisition -\n",
      "ETLInitial Model\n",
      "EvaluationData Understanding -\n",
      "EDAInitial Model(s) Building\n",
      "Evaluation Criteria Value Metric Model Creation & Training Feature Engineering and Evaluation\n",
      "Optimization –\n",
      "Hyperpara and EvaluationModel Deployment\n",
      "Data Drift AnalysisModel Performance –Evaluation Value Metric\n",
      "Model Drift Analysis –Model Evaluation\n",
      "Reports –Dashboards -Products G1 G2\n",
      "G3\n",
      "Gate Reviews\n",
      "\n",
      "5Machine Learning Time\n",
      "\n",
      "6“A field of Computer Science that gives computers the ability to learn\n",
      "without being explicitly programmed.”\n",
      "-Arthur Samuel (Coined the term in 1959 at IBM)\n",
      "“The ability [for systems] to acquire their own knowledge, by\n",
      "extracting patterns from raw data.”\n",
      "-Deep Learning, Goodfellow et al\n",
      "“A computer program is said to learn from experience E with respect\n",
      "to some set of tasks T and performance measure P if its performance\n",
      "tasks in T, as measured by P, improves with experience E.”\n",
      "-Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)\n",
      "\n",
      "Machine vs.\n",
      "human\n",
      "Machine Human\n",
      "Understanding context ✔\n",
      "Thinking through the problem ✔\n",
      "Asking the right questions ✔\n",
      "Selecting the right tools ✔\n",
      "Performing calculations quickly ✔\n",
      "Performing repetitive tasks ✔\n",
      "Following pre-defined rules ✔\n",
      "Interpreting results ✔\n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "Pattern discovery when inputs (x) and outputs (y) are knownSupervised machine learning\n",
      "Input x:\n",
      "VoterOutput y:\n",
      "Political affiliation\n",
      "Examples: Classification and regression are supervised machine learning 9\n",
      "\n",
      "The data inputs (x)have no target outputs (y)Unsupervised machine learning\n",
      "Input x:\n",
      "VoterOutput y:\n",
      "Not given\n",
      "(to be discovered)?\n",
      "We want to impose structure on the inputs (x)to say something meaningful about the data\n",
      "10\n",
      "\n",
      "\n",
      "11\n",
      "\n",
      "12Machine Learning is a general use technology what does that mean?\n",
      "Machine Learning Overview Ageneral -purpose technology orGPT is a term coined to describe a new method of producing and inventing that is important enough to have a protracted aggregate impact.\n",
      "Similar to electricity or the internet, in that it can be applied across domains and work to improve market outcomes.\n",
      "13\n",
      "\n",
      "Machine Learning Overview Twitter Data Usage\n",
      "Error rates on ImageNet (10,000 labelled images) have been driven down from 30% in 2010 to less than 3% today.\n",
      "Below 5% is important why?\n",
      "Chess: Deep Blue (IBM AI) searched some 200 million positions per second, Kasparov was searching not more than 5– 10 positions probably, per second.\n",
      "Yet he played almost at the same level….why?\n",
      "14\n",
      "\n",
      "Machine Learning Overview However, before we all turn into robots consider two important facts: 1.We remain remarkably far away from what would be consider a similar general intelligence that can be compared to humans\n",
      "2.Machines cannot do the full range of tasks that humans can do\n",
      "We can then refer to jobs or activities that might be good cases for Machine Learning as SML or Suitable for Machine Learning\n",
      "15What are examples of tasks that might be SML and how do we know if our organizations are ready?\n",
      "Machine Learning Overview Successful implementation of ML requires very detailed specifications on what is to be learned and data to support that learning activity.\n",
      "Including the development of engineering features through a series of trial-and- error and..\n",
      "Then most importantly embedding these products into normal business operations in such a way that efficiencies can be realized.\n",
      "16\n",
      "\n",
      "Machine Learning Overview What tasks are most suitable for ML to take over: Most recent successes are predicated on supervised learning Competency is narrow as compared to the complexity of human decision making  1.Learning a function that maps well -defined inputs to well- defined outputs\n",
      "oIf can predict Y given any value of X –still might not produce the actual causal effect\n",
      "2.Large Data is present or can be created containing input -output pairs\n",
      "oThe more training data available the more arcuate the model\n",
      "3.Task provides clear feedback with well definable goals and metrics oIf we know what to achieve –(optimize flight patterns not a single flight)\n",
      "4.Where reasoning and diverse background knowledge is not necessary\n",
      "oGood at empirical associations but terrible at decision making that requires common sense of historical knowledge\n",
      "5.No need for why the decision was made to be clear\n",
      "oNN could use millions numerical weights\n",
      "17\n",
      "\n",
      "Machine Learning Overview 6.A tolerance for error or sub- optimal solutions\n",
      "oML use probabilistic outputs which means some error is always assumed\n",
      "7.Function of item being learned should not change rapidly over time\n",
      "oWork best when the distribution of future test examples is the same roughly as the training set over time\n",
      "oIf not the case systems need to be in place to refresh algorithms 18\n",
      "\n",
      "How do machines learn?\n",
      "The basic machine learning process can be divided into three parts .\n",
      "Data Input: Past data or information is utilized as a basis for future decision-making\n",
      "Abstraction : The input data is represented in a broader way through the underlying algorithm\n",
      "Generalization : The abstracted representation is generalized to form a framework for making decisions\n",
      "19(reference Introduction to ML by Subramanian Chandramouli , Saikat Dutt , Amit Kumar Das\n",
      "(https://learning.oreilly.com/library/view/machine -learning/9789389588132/xhtml/chapter001.xhtml#ch1_1)\n",
      "\n",
      "20\n",
      "\n",
      "\n",
      "Brian’s Version of Data Science Lifecycle\n",
      "21Question ID\n",
      "Business UnderstandingData Acquisition/ RepresentationInitial Model\n",
      "EvaluationData Understanding -\n",
      "EDAInitial Model(s) Building\n",
      "Evaluation Criteria/Value Metric Model Creation & Training Feature Engineering and Evaluation\n",
      "Final Model DevelopmentModel Deployment\n",
      "Data Drift AnalysisModel Performance –Evaluation Value Metric\n",
      "Model Drift Analysis –Model Evaluation\n",
      "Reports – Dashboards -Products G1 G2\n",
      "G3\n",
      "Gate Reviews\n",
      "\n",
      "Overview of SOME Key ML Methods/Terms Phase –1 Idea Development Prediction versus Inference Independent Metric for Business Value\n",
      "Target Variable and features Classification versus Regression\n",
      "Probabilistic Interpretation\n",
      "Data acquisition/gathering   Phase –2 Data Prep and Problem Exploration\n",
      "Variable classes/types\n",
      "Scaling and/or Normalizing Data/One -Hot Encoding\n",
      "Missing Data Baseline –prevalence\n",
      "Data Partitioning/Sampling EDA and (Summary Stats and Visuals)\n",
      "Phase –3 –Solution Model Development\n",
      "Parameters versus Hyperparameters\n",
      "Thresholding Feature Engineering\n",
      "Bias versus Variance Tradeoff Model Evaluation Non -parametric modelling (random state) 22\n",
      "\n",
      "23Phase I\n",
      "\n",
      "# Prediction versus Inference 24\n",
      "\n",
      "# Prediction versus Inference\n",
      "Goals of prediction are not centered on how the features are interacting or resulting in an event but are instead focused on the ability of the model to predicted an event.\n",
      "Almost all ML methods are focused on predication not causation or inference.\n",
      "This is why model performance is based largely on how well a model predicts not necessarily how much individual variables are contributing to error reduction.\n",
      "25\n",
      "\n",
      "Overview of Key ML Methods/Terms Independent Metric for Business Value A key part of building a solution using Machine Learning Techniques is having a metric that is independent of the model that can be used to determine if the model is providing value.\n",
      "Examples\n",
      "Recommender Engine for Netflix: Number of user clicks\n",
      "Spam Block Predictor: Number of viruses in the network Market Clustering: Did sales increase\n",
      "Others?\n",
      "26\n",
      "\n",
      "Overview of Key ML Methods/Terms: Target Variable versus Features Target variable –Is the variable that includes the patterns the machine learning algorithm is trying to learn.\n",
      "It is the variable of interest and key to evaluating the model output.\n",
      "More simply it is the variable we are trying to predict.\n",
      "Feature variables – Are the variables the model will use to learn the patterns of the target variable.\n",
      "The process of feature engineering can result in additional features.\n",
      "More simply these are the variables used for predicting the target\n",
      "27\n",
      "\n",
      "Overview of Key ML Methods/Terms: Classification versus Regression Classification is the process of developing a model to predict whether a target variable is in defined categories.\n",
      "This is driven by having either a binary or multi- level categorical variable as the target variable.\n",
      "Examples: Predicting whether someone is male, or female based on 1,000s of pictures.\n",
      "Predicting whether a team will have a winning season or not based on player performance Predicting whether a person will default on a loan or not\n",
      "Key point: The predications of the model are not binary (1s or 0s) but are given as percentages indicating the likelihood that any one row of data belongs to any one category.\n",
      "In the case of target variables with multiple categories each row will get the same number of percent predictions as categories.\n",
      "28\n",
      "\n",
      "Overview of Key ML Methods/Terms: Classification versus Regression Regression is the process of developing a model to predict a specific number or range of numbers.\n",
      "This is driven by having a continuous variable as the target variable for the model\n",
      "Examples: Predicting the score given the players playing a game.\n",
      "Predicting an amount of rain given weather conditions Predicting a persons weight based on various personal statistics\n",
      "29\n",
      "\n",
      "Overview of Key ML Methods/Terms: Probabilistic Interpretation A significant portion of this class will focus on building models for classification.\n",
      "Classification is a much more common machine learning goal versus regression.\n",
      "We all know the range of values for probabilities, 0 to 100, the key to understanding these outputs is to think of them as risk measures, with 100 being no risk and 0 being all the risk!\n",
      "How the outputs are used will depend on your question.\n",
      "Example: How certain do you want to be that a drug is effective as compared to whether a customer will open a marketing email?\n",
      "The results could both yield 75% probabilities but is that high enough?\n",
      "Could also think of the outputs as a quantification of uncertainty, the question becomes given your problem how much uncertainty are you willing to accept?\n",
      "30\n",
      "\n",
      "Overview of Key ML Methods/Terms: Data Brainstorming Data to Concept –Does the data available support the algo target and goal\n",
      "How difficult is the data to gather?\n",
      "Is the data large enough?\n",
      "What is the rate of change of the data?\n",
      "Do we believe this is the correct source and data content to address the problem?\n",
      "Learning Difficulty –How complex or vague is the target variable?\n",
      "Are there imbalances in the classes?\n",
      "Does the data clearly link to the problem?\n",
      "Has this data been used in the past, to what success?\n",
      "Is the target difficult to measure or break into smaller components?\n",
      "What risklevel areyou willing toaccept given thequestion?31\n",
      "\n",
      "32Phase II\n",
      "\n",
      "Overview of SOME Key ML Methods/Terms Phase –1 Idea Development Prediction versus Inference Independent Metric for Business Value\n",
      "Target Variable and features Classification versus Regression\n",
      "Probabilistic Interpretation Data acquisition/gathering     Phase –2 Data Prep and Problem Exploration\n",
      "Variable classes/types\n",
      "Scaling and/or Normalizing Data/One -Hot Encoding\n",
      "Missing Data Baseline –prevalence\n",
      "Data Partitioning/Sampling EDA (Summary Stats and Visuals)\n",
      "Phase –3 –Solution Model Development\n",
      "Parameters versus Hyperparameters\n",
      "Thresholding Feature Engineering\n",
      "Bias versus Variance Tradeoff Model Evaluation\n",
      "Non -parametric modelling (random state) 33\n",
      "\n",
      "Overview of Key ML Methods/Terms # Baseline – prevalence 34\n",
      "\n",
      "35Phase III\n",
      "\n",
      "Overview of SOME Key ML Methods/Terms Phase –1 Idea Development Prediction versus Inference Independent Metric for Business Value\n",
      "Target Variable and features Classification versus Regression\n",
      "Probabilistic Interpretation\n",
      "Data acquisition/gathering      Phase –2 Data Prep and Problem Exploration\n",
      "Variable classes/types\n",
      "Scaling and/or Normalizing Data/One -Hot Encoding\n",
      "Missing Data Baseline –prevalence\n",
      "Data Partitioning/Sampling EDA (Summary Stats and Visuals)\n",
      "Phase –3 –Solution Model Development\n",
      "Parameters versus Hyperparameters\n",
      "Thresholding Feature Engineering\n",
      "Bias versus Variance Tradeoff Model Evaluation Non -parametric modelling (random state) 36\n",
      "\n",
      "Overview of Key ML Methods/Terms # Feature Engineering – Combining or exploring different levels of variable that best work in your model.\n",
      "Likely going to dedicate a week to just this topic.\n",
      "37\n",
      "\n",
      "Overview of Key ML Methods/Terms Thresholding –The percentage point where our models will predict the result to be either a 0 or 1, in the typical binary case.\n",
      "Adjust the threshold associated with indication of a positive class.\n",
      "The default is 50%, could be that we want to be extra careful and instead adjust that measure up to 75% or 90%.\n",
      "38\n",
      "\n",
      "Overview of Key ML Methods/Terms Evaluation –The metrics you use to assess model quality.\n",
      "There are a ton of this measures, and we are dedicating an entire week to the exploring these further.\n",
      "I’ll show some examples in the code for this week.\n",
      "39\n",
      "\n",
      "Bias Versus Variance 40\n",
      "\n",
      "\n",
      "41Extra Material \n",
      "42Bookings.com  \n",
      "43Lesson Learned: Booking.com\n",
      "\n",
      "Bookings.com\n",
      "Swiss Army Knife –Their approach to ML is highly adoptable , meaning it can be used in a variety of settings –generate specific results or more generalizable depending on the inputs (data)\n",
      "Offline Health Check– Use Randomized Control Trails (RCT) to test model outputs aligned with normative business metrics to assess quality (customer conversion)\n",
      "Increase model performance doesn’t necessary translate to better gain in value\n",
      "44\n",
      "\n",
      "Bookings.com\n",
      "Make a Target Before you Shoot –Develop a clear understanding of the business case and target variable (what is date flexibility) Learning Difficulty –How complex or vague is the target\n",
      "Data to Concept –Does the data available support the algo target and goal\n",
      "Selection Bias –Does the model perform better for a subset of the target Speed Kills –ML algos, even simple ones, take a lot of computing power –to reduce user weight time (latency) measures should be taken\n",
      "See page 1748 (sparsity, model redundancy, caching…etc.)\n",
      "45\n",
      "\n",
      "Machine Learning Overview Keep a watchful eye –Used specialized monitoring tools to understand how the models are performing in practice (even when the result was unclear)\n",
      "Traditional Research Methods (Experimental Design) is a Best Practice Approach to ML –\n",
      "“Experimentation through Randomized Controlled Trials is ingrained into Booking.com culture”\n",
      "46\n",
      "\n",
      "Machine Learning Overview, EDA and Clustering\n",
      "Brian Wright\n",
      "brianwright@virginia.edu\n",
      "\n",
      "\n",
      "1.What is Machine Learning ?\n",
      "2.What is exploratory data analysis?\n",
      "3.k-means clustering\n",
      "–Does Congress vote in patterns?\n",
      "4.Multi -dimensional k -means clustering\n",
      "–Are NBA players compensated according to performance?Outline: Intro to Unsupervised ML\n",
      "2\n",
      "\n",
      "“A field of Computer Science that gives computers the ability to learn\n",
      "without being explicitly programmed.”\n",
      "-Arthur Samuel (Coined the term in 1959 at IBM)\n",
      "“The ability [for systems] to acquire their own knowledge, by\n",
      "extracting patterns from raw data.”\n",
      "-Deep Learning , Goodfellow et al\n",
      "\n",
      "Machine vs.\n",
      "human\n",
      "Machine Human\n",
      "Understanding context ✔\n",
      "Thinking through the problem ✔\n",
      "Asking the right questions ✔\n",
      "Selecting the right tools ✔\n",
      "Performing calculations quickly ✔\n",
      "Performing repetitive tasks ✔\n",
      "Following pre-defined rules ✔\n",
      "Interpreting results ✔\n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "Pattern discovery when inputs (x) and outputs (y) are knownSupervised machine learning\n",
      "Input x:\n",
      "VoterOutput y:\n",
      "Political affiliation\n",
      "Examples: Classification and regression are supervised machine learning 6\n",
      "\n",
      "The data inputs (x)have no target outputs (y)Unsupervised machine learning\n",
      "Input x:\n",
      "VoterOutput y:\n",
      "Not given\n",
      "(to be discovered)?\n",
      "We want to impose structure on the inputs (x) to say something meaningful about the data\n",
      "7\n",
      "\n",
      "\n",
      "8\n",
      "\n",
      "9Given  D :{2,4,10,12,3,11,20,25,30},  and  k=2 clusters\n",
      "Randomly assign the means:  m1=3, m2=4\n",
      "K1={2,3}, K2={4,10,12,11,20,25,30}, m1=2.5, m2=16\n",
      "K1={2,3,4}, K2={10,12,11,20,25,30}, m1=3, m2=18\n",
      "K1={2,3,4,10}, K2={12,11,20,25,30}, m1=4.75, m2=19.6\n",
      "K1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\n",
      "K1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\n",
      "Stop, since the clusters and the means found in all subsequent iterations will be the same .Example of K -Means\n",
      "\n",
      "•Value of more accurate information = incremental improvement of results above status quo\n",
      "•What is the value of being 5% more accurate ?\n",
      "–For Wal -Mart: on ~$470B of revenues, $23B of potential revenue increase by more accurately predicting demand or recommending the right products\n",
      "–For the IRS: on ~$21B of annual tax fraud, $1B of potential incremental tax collections\n",
      "•~30% of Amazon.com sales come from product recommendationsMaking $ with Machine Learnin g\n",
      "Source: Predictive Analytics by Eric Siegel; NBC News\n",
      "10\n",
      "\n",
      "•The value of data mining and predictive analytics is in how you use these tools\n",
      "•Insights on their own can have little value without intelligent applicationHow does data mining make $?\n",
      "11\n",
      "\n",
      "Engineering of Machine Learning Algos versus Software Development Source: https://towardsdatascience.com/stoend -to-\n",
      "end-data -science -life-cycle -6387523b5afc\n",
      "\n",
      "Data Science Life Cycle (Everything includes Evaluation\n",
      "13\n",
      "Train\n",
      "Feature Engineer\n",
      "Test\n",
      " Deploy\n",
      "Evaluate\n",
      " Evaluate\n",
      " Evaluate\n",
      " Evaluate\n",
      "Monitor\n",
      "\n",
      "1.What is Machine Learning?\n",
      "2.What is exploratory data analysis?\n",
      "3.k-means clustering\n",
      "–Does Congress vote in patterns?\n",
      "4.Multi -dimensional k -means clustering\n",
      "–Are NBA players compensated according to performance?Outline: Intro to Unsupervised ML\n",
      "14\n",
      "\n",
      "•Exploratory data analysis or “EDA” is an approach where the intent is to see what the data can tell us beyond modeling or hypothesis testing\n",
      "–Data visualization is one of the most common forms of EDAWhat is exploratory data analysis?\n",
      "15\n",
      "\n",
      "When data is too big or complex to be analyzed just by visualizing it, these types of analysis can help:\n",
      "1.Clustering: compare pieces of data by measuring similarity among them\n",
      "2.Network analysis: analyze how people, places and entities are connected to evaluate the properties and structure of a network 3.Text mining: analyze what large bodies of unstructured or structured text sayTypes of exploratory data analysis\n",
      "16\n",
      "\n",
      "The data inputs have (x)no target outputs (y)Unsupervised machine learning\n",
      "Input x:\n",
      "VoterOutput y:\n",
      "Not given\n",
      "(To be discovered)?\n",
      "We want to impose structure on the inputs (x)to say something meaningful about the data\n",
      "17\n",
      "\n",
      "\n",
      "1.Technique for finding similarity between groups\n",
      "2.Type of unsupervised machine learning\n",
      "•Not the only class of unsupervised learning        algorithms\n",
      "3.Similarity needs to be defined\n",
      "•Will depend on attributes of data\n",
      "•Usually a distance metricWhat is clustering?\n",
      "18\n",
      "Key assumption: data points that are “closer” together are related or similar\n",
      "\n",
      "•Haimowitz and Schwarz 1997 paper on clustering for credit line optimization\n",
      "–http://www.aaai.org/Papers/Workshops/1997/W\n",
      "S-97-07/WS97- 07-006.pdf\n",
      "•Cluster existing GE Capital customers based on similarity in use of their credit cards to pay bills and customers’ profitability to GE Capital\n",
      "•Resulted in five clusters of consumer credit behavior\n",
      "•Created classification model to predict customer type and offer tailored productsGE Capital case study: grouping clients\n",
      "19\n",
      "\n",
      "•Between 2001 and 2004 most European countries passed legislation that allowed customers to keep their cell phone number if they switched carriers\n",
      "•Telenor, one of the largest telecommunications companies in Norway wanted to ensure it kept its customers\n",
      "–Problem: the promotions the company sent to its clients reminded them that they could leave and resulted in greater defections !\n",
      "–Solution: predict which customers, if contacted, are more likely to stay with the company •Results:\n",
      "–Marketing campaign ROI increased 11x\n",
      "–Customer churn decreased 36%\n",
      "–Marketing campaign costs decreased 40%Telenor case study: predicting behavior\n",
      "20\n",
      "\n",
      "1.What is Machine Learning?\n",
      "2.What is exploratory data analysis?\n",
      "3.k-means clustering\n",
      "–Does Congress vote in patterns?\n",
      "4.Multi -dimensional k -means clustering –Lab –Are NBA players compensated according to performance?Outline: Intro to Unsupervised ML\n",
      "21\n",
      "\n",
      "Example use case General question Concept\n",
      "Does Congress vote in patterns?\n",
      "Is there a pattern ?\n",
      "Is there structure in unstructured data?k-means clustering\n",
      "Are basketball players \"priced\" efficiently (based on performance)?How to uncover trends with many variables that you    can't easily visualize?k-means clustering in many dimensionsConcept summary\n",
      "22\n",
      "\n",
      "1.Data set consists of 427 members (observations) 2.Members served a full year in 2013\n",
      "3.Three vote types:\n",
      "•“Aye”\n",
      "•“Nay”\n",
      "•“Other”Goal: to understand how polarized the US Congress isPolitical clustering\n",
      "The joint session of Congress on Capitol Hill in Washington\n",
      "23\n",
      "\n",
      "•How do we identify swing votes?\n",
      "–Lobbying\n",
      "–Bridging party lines\n",
      "•Assumption:\n",
      "–Democrats and Republicans vote among partisan lines, which generates clustersEach data point represents a member of CongressFinding voting patterns\n",
      "24\n",
      "\n",
      "Intra vs.\n",
      "inter- cluster distance\n",
      "Intra-Cluster DistanceInter-Cluster Distance\n",
      "Objective: minimize intra -cluster distan ce, maximize inter -cluster distance\n",
      "25\n",
      "\n",
      "•The centroid is the average location of all points in the cluster\n",
      "•Another definition: the centroid minimizes the distance between a central location and all the data points in the cluster\n",
      "Note: Centroids are generally not existing data points, rather locations in spacek-means clustering is based on centroids\n",
      "26\n",
      "\n",
      "1.Randomly choose k data points to be centroids k-means in 4 steps\n",
      "27\n",
      "\n",
      "\n",
      "1.Randomly choose k data points to be centroids 2.Assign each point to closest centroidk-means in 4 steps\n",
      "28\n",
      "\n",
      "\n",
      "1.Randomly choose k data points to be centroids 2.Assign each point to closest centroid\n",
      "3.Recalculate centroids based on current cluster membershipk-means in 4 steps\n",
      "29\n",
      "\n",
      "1.Randomly choose k data points to be centroids 2.Assign each point to closest centroid\n",
      "3.Recalculate centroids based on current cluster membershipk-means in 4 steps\n",
      "304.Repeat steps 2 -3 with the new centroids until the centroids don’t change anymore\n",
      "\n",
      "Step 1: load packages and data\n",
      "# Install packages\n",
      "install.packages(\"e1071\") install.packages(\"ggplot2\" )\n",
      "# Load librarieslibrary(e1071)library(ggplot2)\n",
      "library(help = e1071)Learn about all the functionality of the package, be well informed about what you're doing!\n",
      "31\n",
      "\n",
      "Step 1: load packages and data\n",
      "# Loading house data\n",
      "house_votes_Dem = read_csv (\"house_votes_Dem.csv\")\n",
      "# What does the data look like?\n",
      "View( house_votes_Dem )Script\n",
      "32\n",
      "\n",
      "Step 2: run k -means\n",
      "# Define the columns to be clustered by subsetting the data\n",
      "clust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]\n",
      "# Run an algorithm with 2 centersset.seed(1 )\n",
      "kmeans_obj_Dem = kmeans(clust_data_Dem , centers = 2, algorithm = \"Lloyd\")\n",
      "# What does the new variable kmeans_obj contain?kmeans_obj_Dem\n",
      "# View the results of each output of the kmeans # functionhead( kmeans_obj_Dem)Script\n",
      "1.By placing the set of data we want     after the comma, we tell R we’re   looking for columns 2.kmeans uses a different starting data point each time it runs.\n",
      "To make the results reproducible make R start from the same point every time with set.seed()\n",
      "3.We’re not specifying the number of iterations so R defaults to 10\n",
      "4.We’ll see that kmeans produces a list    of vectors of different lengths.\n",
      "As a result, we cannot use the View() function\n",
      "33\n",
      "\n",
      "Step 2: run k -means\n",
      "1.Number of points each cluster contains\n",
      "2.The “location” of each cluster center is specified by 3 coordinates, one for each column we’re clustering\n",
      "3.The list assigning either cluster 1 or 2 to each data point\n",
      "1.79.5% of the variance between the data points is explained by our clustering, we will discuss this in detail later\n",
      "2.List of other types of data included in kmeans_obj\n",
      "34\n",
      "\n",
      "Measuring distance\n",
      "(3,3)\n",
      "(1,2) 21Distance = √(22+12)\n",
      "x\n",
      "35\n",
      "\n",
      "•cluster: a vector indicating the cluster to which each point is allocated\n",
      "•centers: a matrix of cluster centers\n",
      "•totss: the total sum of squares (sum of distances between all points)\n",
      "•withinss: vector of within -cluster sum of distances, one number per cluster\n",
      "•tot.withinss: total within -cluster sum of distances, i.e.sum of withinss\n",
      "•betweenss: the between -cluster sum of squares, i.e.\n",
      "totss -tot.withinss\n",
      "•size: the number of points in each cluster\n",
      "To learn more about the kmeans function run ?kmeanskmeans outputs\n",
      "36\n",
      "\n",
      "Intra vs.\n",
      "inter- cluster distance\n",
      "Intra-Cluster DistanceInter-Cluster Distance\n",
      "withinssbetweensstotss = withinss +betweenss\n",
      "37\n",
      "\n",
      "Step 3: visualize plot\n",
      "# Tell R to read the cluster labels as factors so that ggplot2 (the # graphing  package) can read them as category labels instead of # continuous variables (numeric variables).party_clusters_Dem = as.factor(kmeans_obj_Dem$cluster)\n",
      "# What does party_clusters look like?View( party_clusters_Dem )\n",
      "View(as.data.frame(party_clusters_Dem))\n",
      "# Set up labels for our data so that we can compare Democrats and # Republicans.party_labels_Dem = house_votes_Dem$partyScript\n",
      "38\n",
      "\n",
      "ggplot(house_votes_Dem, aes(x = aye, y = nay,\n",
      "shape = party_clusters_Dem)) + geom_point(size = 6) +\n",
      "ggtitle(\"Aye vs.\n",
      "Nay votes for Democrat -introduced bills\") +\n",
      "xlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", labels = c(\"Cluster 1\", \"Cluster 2\" ),\n",
      "values = c(\"1\" , \"2\")) +\n",
      "theme_light()Step 3: visualize plotCosmetics layerBase layer\n",
      "Geom Layer\n",
      "Titles and axis\n",
      "Shape\n",
      "Theme\n",
      "39Script\n",
      "\n",
      "Step 3: visualize plot\n",
      "40\n",
      "\n",
      "•Two groups exist\n",
      "•Algorithm identifies voting patternsWhat can we infer about the different clusters?Step 4: analyze results\n",
      "41\n",
      "\n",
      "ggplot(house_votes_Dem, aes(x = yea, y = nay,\n",
      "color= party_labels_Dem,\n",
      "shape = party_clusters_Dem)) + geom_point(size = 6) +\n",
      "ggtitle(\"Aye vs.\n",
      "Nay votes for Democrat -introduced bills\") +\n",
      "xlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", labels = c(\"Cluster 1\", \"Cluster 2\" ),\n",
      "values = c(\"1\" , \"2\")) +\n",
      "scale_color_manual(name = \"Party\", labels = c(\"Democratic\", \"Republican\"),\n",
      "values = c(\"blue\" , \"red\"))+\n",
      "theme_light()Step 5: validate resultsCosmetics layerScript\n",
      "Base layer\n",
      "Geom Layer\n",
      "Titles and axis\n",
      "Color and shape\n",
      "Theme\n",
      "42\n",
      "\n",
      "Step 5: validate results\n",
      "43\n",
      "\n",
      "•Diffuse among Democrats\n",
      "•Republicans more dense\n",
      "•Can gauge “outliers”\n",
      "•Can see the polarization between the two political parties Step 6: interpret results\n",
      "outlieroutlier\n",
      "44\n",
      "\n",
      "•Clustering is more powerful than the human eye in3D\n",
      "•Clustering mathematically defines which cluster the peripheral points should be in when it’s not obvious to the human eye\n",
      "•Clustering is helpful when many dimensions / variables exist that you can’t visualize at once\n",
      "–Whiskey similarity example from classification lectureClustering vs.\n",
      "visualizing\n",
      "Aye, Nay and Other Votes\n",
      "in House of Representatives\n",
      "45\n",
      "\n",
      "•Goals of clustering:\n",
      "–Maximize the separation between clusters •i.e.Maximize inter -cluster distance –Keep similar points in a cluster close together •i.e.Minimize intra-cluster distanceHow good is the clustering?\n",
      "46\n",
      "\n",
      "•Look at the variance explained by clusters\n",
      "–In particular, the ratio of inter -cluster variance to total variance\n",
      "•How much of the total variance is explained by the clustering?Assessing how well an algorithm performsHow good is the clustering?\n",
      "Variation explained by clusters\n",
      "= inter-cluster variance / total variance\n",
      "47\n",
      "\n",
      "•cluster: a vector indicating the cluster to which each point is allocated\n",
      "•centers: a matrix of cluster centers\n",
      "•totss: the total sum of squares (sum of distances between all points)\n",
      "•withinss: vector of within -cluster sum of distances, one number per cluster\n",
      "•tot.withinss: total within -cluster sum of distances, i.e.sum of withinss\n",
      "•betweenss: the between -cluster sum of squares, i.e.\n",
      "totss -tot.withinss\n",
      "•size: the number of points in each cluster\n",
      "To learn more about the kmeans function run ?kmeanskmeans outputs\n",
      "48\n",
      "\n",
      "Intra vs.\n",
      "inter- cluster distance\n",
      "Intra-Cluster DistanceInter-Cluster Distance\n",
      "withinssbetweensstotss = withinss +betweenss\n",
      "49\n",
      "\n",
      "How good is the clustering?\n",
      "# Inter-cluster variance,\n",
      "# \"betweenss\" is the sum of the # distances between points from # different clustersnum_Dem= kmeans_obj_Dem$betweenss\n",
      "# Total variance# \"totss\" is the sum of the distances  # between all the points in # the data setdenom_Dem = kmeans_obj_Dem$totss\n",
      "# Variance accounted for by # clustersvar_exp_Dem = num_Dem/ denom_Dem\n",
      "var_exp_Dem[1] 0.7952692Script\n",
      "50\n",
      "\n",
      "•It’s easier when the number of clusters is known ahead of time, but what if we don't know how many clusters we should have?\n",
      "•Since different starting points may generate different clusters, we need a way to assess cluster quality as well.How do we choose the number of clusters ( i.e.k)?How good is the clustering?\n",
      "51\n",
      "\n",
      "1.Elbow method\n",
      "–Computes the percentage of variance explained by clusters for a range of cluster numbers\n",
      "–Plots a graph so results are easier to see –Not guaranteed to work!\n",
      "It depends on the data in question\n",
      "2.NbClustHow to select k: two methods\n",
      "–Runs 30 different tests and provides “majority vote” for the best number of clusters (k’s) to use\n",
      "52\n",
      "\n",
      "Elbow method: measure variance\n",
      "# Run algorithm with 3 centers\n",
      "set.seed(1 )\n",
      "kmeans_obj_Dem = kmeans(clust_data_Dem,   centers = 3,\n",
      "algorithm = \"Lloyd\")\n",
      "# Inter- cluster variance\n",
      "num_Dem = kmeans_obj_Dem$ betweenss\n",
      "# Total variance\n",
      "denom_Dem = kmeans_obj_Dem $totss\n",
      "# Variance accounted for by clustersvar_exp_Dem = num_Dem / denom_Dem\n",
      "var_exp_Dem\n",
      "[1] 0.8463623Script\n",
      "53\n",
      "\n",
      "•We want to repeat the variance calculation from the previous slide for several numbers of clusters automatically\n",
      "•We can create a function that contains all the steps we want to automate Automating a step we want to repeat\n",
      "function(data, item to iterate through)\n",
      "54\n",
      "\n",
      "# The function explained_variance wraps our code from previous slides.\n",
      "explained_variance = function( data_in, k){\n",
      "# Running k- means algorithm\n",
      "set.seed(1 )  kmeans_obj = kmeans(data_in, centers = k,\n",
      "algorithm = \"Lloyd\" )\n",
      "# Variance accounted for by clusters\n",
      "var_exp = kmeans_obj $betweenss / kmeans_obj$totss\n",
      "var_exp\n",
      "}Automating a step we want to repeat\n",
      "Script\n",
      "1.A new variable is created and set equal to our function()\n",
      "2.The commands inside the function are wrapped in curly braces {}\n",
      "3.Inside the parentheses, we specify the variables that the user will input and that will then be used inside the function where they appear\n",
      "55\n",
      "\n",
      "# Recall the variable we are using for the # data that we're clustering.\n",
      "clust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]\n",
      "View( clust_data_Dem)\n",
      "# The sapply() function plugs several values # into explained_variance.\n",
      "explained_var_Dem = sapply(1 :10, explained_variance, data_in = clust_data_Dem)\n",
      "View( explained_var_Dem)\n",
      "# Data for ggplot2\n",
      "elbow_data_Dem = data.frame(k = 1:10, explained_var_Dem)\n",
      "View( elbow_data_Dem)Automating a step we want to repeat\n",
      "1.sapply() applies a function to a vector\n",
      "2.We have to tell sapply() that the we want the explained_variance function to use the clust_data data\n",
      "3.Next, we create a data frame that contains both the new variance variable (explained_var_Dem) and the different numbers of k that we used in the previous function (1 through 10)Function we created Script\n",
      "56\n",
      "\n",
      "# Plotting data\n",
      "ggplot(elbow_data_Dem, aes(x = k,  y = explained_var_Dem)) + geom_point(size = 4) +\n",
      "geom_line(size = 1 ) +\n",
      "xlab(\"k\" ) + ylab(\"Intercluster Variance/Total Variance\" ) + theme_light()Elbow method: plotting the graph\n",
      "Script\n",
      "1.geom_point() sets the size of the data points\n",
      "2.geom_line() sets the thickness of the line\n",
      "57\n",
      "\n",
      "Looking for the kink in graph of  inter- cluster variance / total varianceElbow method: measure variance\n",
      "Original data Elbow methodk = 2\n",
      "58\n",
      "\n",
      "Elbow method: measure variance\n",
      "k 1 2 3\n",
      "Inter-cluster variance/ total variance~0% 79.5% 86.4%\n",
      "k =1 k =2 k =3\n",
      "59\n",
      "\n",
      "•Library: \"NbClust\"\n",
      "Functions:  \"NbClust\"\n",
      "Inputs : •data –data array or data frame\n",
      "•min.nc / max.nc –minimum/maximum number of clusters\n",
      "•method –\"kmeans\"\n",
      "•There are other, more advanced arguments that can be customized but are outside of the scope of this course and are note necessary to for NbClust to workThere are a number of ways to choose the right k.\n",
      "NbClust runs 30 tests and selects k based on majority voteNbClust: k by majority vote\n",
      "NbClust(data, max.nc, method = \"kmeans\")\n",
      "60\n",
      "\n",
      "# Install the package.\n",
      "install.packages(\"NbClust\" )\n",
      "library(NbClust)\n",
      "# Run NbClust.\n",
      "nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")\n",
      "# View the output of NbClust.nbclust_obj_Dem\n",
      "# View the output that shows the number of clusters each # method recommends.View( nbclust_obj_Dem $Best.nc)NbClust : k by majority vote\n",
      "Script\n",
      "61\n",
      "\n",
      "NbClust: k by majority vote\n",
      "> nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")\n",
      "...\n",
      "******************************************************************* * Among all indices:                                                * 12 proposed 2 as the best number of clusters * 4 proposed 3 as the best number of clusters ...\n",
      "***** Conclusion *****                            * According to the majority rule, the best number of clusters is  2 *******************************************************************\n",
      "Note: additional information appears; the above information is most relevant to us for nowConsole\n",
      "62\n",
      "\n",
      "NbClust: k by majority vote\n",
      "> nbclust_obj_Dem Console•nbclust_obj_Dem shows the outputs of NbClust\n",
      "–One of the outputs is Best.nc, which shows the number of clusters                               recommended by each test 63\n",
      "\n",
      "NbClust: k by majority vote\n",
      "•We want to visualize a histogram to make it obvious how many votes there are for each number of clusters 64\n",
      "\n",
      "# Subset the 1st row from Best.nc and convert it  # to a data frame, so ggplot2 can plot it.\n",
      "freq_k_Dem = nbclust_obj_Dem $Best.nc[1 ,]\n",
      "freq_k_Dem = data.frame( freq_k_Dem)\n",
      "View(freq_k_Dem)\n",
      "# Check the maximum number of clusters.\n",
      "max(freq_k_Dem )\n",
      "# Plot as a histogram.\n",
      "ggplot(freq_k_Dem,\n",
      "aes(x = freq_k_Dem)) +\n",
      "geom_bar() +scale_x_continuous(breaks = seq(0 , 15, by = 1)) +\n",
      "scale_y_continuous(breaks = seq(0 , 12, by = 1)) +\n",
      "labs(x = \"Number of Clusters\",\n",
      "y = \"Number of Votes\" ,\n",
      "title = \"Cluster Analysis\")NbClust: k by majority vote\n",
      "Script\n",
      "2 clusters is the winner with 12 votes\n",
      "65\n",
      "\n",
      "•If you’re a lobbyist, which congressperson can you influence for swing votes?\n",
      "•If you’re managing a campaign and your competitor is always voting along party lines, how can you use that information?\n",
      "•If your congressperson is not an active voter, is she representing your interests?\n",
      "•What do the voting patterns look like for Republican-introduced bills?Application of results\n",
      "66\n",
      "\n",
      "•Could see differences between the patterns of Reb lead bills and Democrat lead bills\n",
      "•Could provide information on congressmen that might be see has swing votes.\n",
      "Implications of results\n",
      "67\n",
      "\n",
      "•We are assuming that the patterns correspond with the same bills being voted on –perhaps some Congressmen have the same number of 'aye' and 'nay' votes, but voted on different bills\n",
      "•Network analysis can help determine additional connections between Congressmen\n",
      "•We haven't taken extenuating factors into account –political initiatives, current events, etc.\n",
      "This is a preliminary analysis that gives us initial insights and can help us direct further researchLimitations of results\n",
      "68\n",
      "\n",
      "Problem 1: clusters overlap when data points are unequally distributedCommon pitfalls with clustering\n",
      "Original data k-means clustering\n",
      "69\n",
      "\n",
      "Problem 2: clusters should have roughly the same density or else they may split!Common pitfalls with clustering\n",
      "Original data k-means clustering\n",
      "70\n",
      "\n",
      "Problem 3: clusters should be circular / elliptical.\n",
      "We can’t use this method for particular shapesCommon pitfalls with clustering\n",
      "Original data k-means clustering\n",
      "71\n",
      "\n",
      "Problem 4: a bad starting point can lead to bad clustering!Common problems with clustering\n",
      "Original data k-means clustering\n",
      "72\n",
      "\n",
      "Lab, Part 1 73•Go through the same process only using bills introduced by republican congressmen.\n",
      "–What is the ideal number of clusters’ (You can probably guess this answer)\n",
      "–Are the patterns the same between the parties or do they vary?\n",
      "–Evaluate the model, is it better or worse than the model using the democratic introduced bills.\n",
      "Which clustering algorithm to use?\n",
      "74\n",
      "\n",
      "\n",
      "Which clustering algorithm to you use?\n",
      "Method Parameters Scalability Use case Geometry (metric used)\n",
      "K-means Number of clusters Very large n_samples, medium n_clusters with MiniBatch codeGeneral purpose, even cluster size, flat geometry, not too  many clustersDistances between points\n",
      "Affinity propagation Damping, sample preferenceNot scalable with n_samplesMany clusters, uneven cluster size, non- flat geometryGraph distance ( e.g.\n",
      "nearest -neighbor graph)\n",
      "Mean -shift Bandwidth Not scalable with n_samplesMany clusters, uneven cluster size, non- flat geometryDistances between points\n",
      "Spectral clustering Number of clusters Medium n_samples, small n_clustersFew clusters, even cluster size, non- flat geometryGraph distances ( e.g.\n",
      "nearest -neighbor graph)\n",
      "Ward hierarchical\n",
      "clusteringNumber of clusters Large n_samples and n_clustersMany clusters, possibly connectivity constraintsDistances between points\n",
      "Agglomerative clusteringNumber of clusters, linkage type, distanceLarge n_samples and n_clustersMany clusters, possiblyconnectivity constraints, non -\n",
      "Euclidean distancesAny pairwise distance\n",
      "DBSCAN Neighborhood size Very large n_samples,   medium n_clustersNon-flatgeometry, uneven cluster sizesDistances between nearest points\n",
      "Gaussian mixtures Many Not scalable Flatgeometry, good for density estimationMahalanobis distances to centers\n",
      "Birch Branching factor, threshold, optional global clusterLarge n_samples and n_clustersLarge dataset, outlier removal, data reductionEuclidean distance between points\n",
      "75\n",
      "\n",
      "•The good and bad\n",
      "–+ cheap –NO LABELS , labels are expensive to create and maintain\n",
      "–+/-clustering always works\n",
      "–-Many methods to choose from and knowing the right one can be nontrivial and the differences between many are almost zero, so you need to understand what you're doing\n",
      "•The evil\n",
      "–Curse of dimensionality\n",
      "–Clusters may result from poor data quality\n",
      "–Non-deterministic ( e.g.k-means) subject to local minimum.\n",
      "Since it works with averages, k -means does not get much better with Big Data (marginal improvements) but luckily is naïve to parallelize –Non spherical data may result in poor clustering (depending on method used)\n",
      "–Unequal cluster sizes may result in poor clustering (depending on method used)The good, bad, and evil\n",
      "76\n",
      "\n",
      "•Analysts need to ask the following questions\n",
      "–Do you want overlapping or non- overlapping clusters ?\n",
      "–Does your data satisfy the assumptions of the clustering algorithm?\n",
      "–How was the distance measure identified ?\n",
      "–How many clusters and why ?\n",
      "Identifying the number of clusters is a difficult task if the number of class labels is not known beforehand –Does your method scale to the size of the data?\n",
      "–Is the compute time congruent with the temporal budget of your business need ( i.e.\n",
      "do you get answers back in time to make meaningful decisions)The good, bad, and evil\n",
      "77\n",
      "\n",
      "\n",
      "\n",
      "Supervised Machine Learning\n",
      "Computer learns through examples how to classify events/objects/instances.\n",
      "Supervised Machine Learning\n",
      "Computer learns through examples how to classify events/objects/instances.\n",
      "Supervised Machine Learning\n",
      "Computer learns through examples how to classify events/objects/instances.\n",
      "Performance Metrics\n",
      "\n",
      "Supervised Machine Learning\n",
      "Computer learns through examples how to classify events/objects/instances.\n",
      "•What metrics should we use?\n",
      "•Accuracy may not be enough\n",
      "•How reliable are the predicted values from your model?\n",
      "•Are errors on the training data a good indication of errors on future data?\n",
      "•optimisticEvaluation of Performance\n",
      "\n",
      "\n",
      "8\n",
      "Source: https://developers.google.com/machine -learning/crash- course/classification/true -false -positive -negative\n",
      "\n",
      "Machine Learning Bias and Social Science MethodsConfusion Matrix, ROC (receiver operating curve) and AUC (area under the curve) are very common approaches for measuring the performance of classification models\n",
      "Classification models output percentages that an individual input will belong to a specific class, usually a 1 or 0, with one being a positive attribute.\n",
      "Likelihood of email spam/fraud is an example.\n",
      "The higher the model percentage prediction on any one email the higher chance it is fraud.\n",
      "Essentially both measure the misclassification error rate associated with your model\n",
      "A Confusion Matrix is a good tool for understanding how accurate you model is classifying and is used to build ROC\n",
      "\n",
      "Machine Learning Bias and Social Science MethodsLet's use intruder/fraud detection as an example\n",
      "Say we have 135 emails entering our system and we are trying to detect whether they are fraudulent or not We use lots of criteria –source, subject, if they came from a prince…\n",
      "Generate probability measures as a result for a tree -based classifier to determine the likelihood that any one of these emails is fraudulent The cutoff point that is predetermined in the tree (and is a universal standard) is 50% but can be modified as an input if needed\n",
      "\n",
      "Machine Learning Bias and Social Science MethodsBelow is are the results of our model in a Confusion Matrix.\n",
      "They center on the positive and negative classifications in sub-categories of true and false positive.\n",
      "Keep in mind we know because of the labels, what is fraud and not, so we can measure how good the model is classifying.\n",
      "Both true negative and true positive are good, false negative and false positive are errors.\n",
      "1 = Fraud/Spam\n",
      "0 = Not Fraud/SpamPredicted Class\n",
      "Positive Fraud Pred (1)Negative\n",
      "Not Fraud Pred (0)\n",
      "Actual ClassPositive Fraud Actual (1)True Positive10False Negatives\n",
      "22\n",
      "Negative Not Fraud Actual (0)False Positives7True Negative\n",
      "96\n",
      "\n",
      "Machine Learning Bias and Social Science MethodsLet’s consider the extremes: what if we set the threshold to 0?\n",
      "Means that everything is captured as Fraud and no ever gets an email again!\n",
      "1 = Fraud/Spam\n",
      "0 = Not Fraud/SpamPredicted Class\n",
      "Positive Fraud Pred (1)Negative\n",
      "Not Fraud Pred (0)\n",
      "Actual ClassPositive Fraud Actual (1)True Positive32False Negatives\n",
      "0\n",
      "Negative Not Fraud Actual (0)False Positives103True Negative0\n",
      "\n",
      "Machine Learning Bias and Social Science Methods Let’s consider the other extreme: what if we set the threshold to 100?\n",
      "Means nothing is fraud and now everyone is getting rich off of Arabian princes\n",
      "1 = Fraud/Spam\n",
      "0 = Not Fraud/SpamPredicted Class\n",
      "Positive Fraud Pred (1)NegativeNot Fraud Pred (0)\n",
      "Actual ClassPositive Fraud Actual (1)True Positive0False Negatives\n",
      "32\n",
      "Negative Not Fraud Actual (0)False Positives0True Negative103\n",
      "\n",
      "Machine Learning Bias and Social Science MethodsWe can further assess our model by generating classification rates:\n",
      "True Positive Rate (TPR) (Sensitivity) = TP/(TP+FN) = 10/(10+22) = .31\n",
      "% of fraud correctly labeled as fraud\n",
      "False Positive Rate (FPR) (1- Specificity) = FP/(FP+TN) = 7/(7+96) = .06\n",
      "% of emails labelled not fraud that were false positives\n",
      "1 = Fraud/Spam\n",
      "0 = Not Fraud/SpamPredicted Class\n",
      "Positive Fraud Pred (1)Negative\n",
      "Not Fraud Pred (0)\n",
      "Actual ClassPositive Fraud Actual (1)True Positive10False Negatives\n",
      "22\n",
      "Negative Not Fraud Actual (0)False Positives7True Negative96\n",
      "\n",
      "Machine Learning Bias and Social Science MethodsThese two data points can used to begin to develop a Receiver Operating Characteristic Curve or ROC curve\n",
      "True Positive Rate (TPR) = 10/(10+22) = .31 = y-axis\n",
      "False Positive Rate (FPR) = 7/(7+96) = .06 = x-axis\n",
      "1 = Fraud/Spam\n",
      "0 = Not Fraud/SpamPredicted Class\n",
      "Positive Fraud Pred (1)Negative\n",
      "Not Fraud Pred (0)\n",
      "Actual ClassPositive Fraud Actual (1)True Positive10False Negatives\n",
      "22\n",
      "Negative Not Fraud Actual (0)False Positives7True Negative\n",
      "96\n",
      "\n",
      "Machine Learning Bias and Social Science MethodsROC curve is essentially a graphical representation of the adjusted threshold values of the confusion matrix, below are two examples\n",
      "AUC\n",
      "\n",
      "Machine Learning Bias and Social Science MethodsROC curve generates the area under the curve as a percentage of the total graph under the curve.\n",
      "The Area Under the Curve (AUC) is indicative of performance.\n",
      "AUC:\n",
      "0.9 –1.0 = Excellent\n",
      "0.8 –0.9 = Good\n",
      "0.7 –0.8 = Fair\n",
      "0.6 –0.7 = Poor\n",
      "0.5 –0.6 = Fail\n",
      "\n",
      "\n",
      "19\n",
      "Source: https://developers.google.com/machine -learning/crash- course/classification/true -false -positive -negative\n",
      "\n",
      "Machine Learning Bias and Social Science MethodsAdditional Performance Measures\n",
      "Accuracy –(TP+TN )/(TP+FP+FN+TN)\n",
      "Prevalance –The percentage of the positive class in the\n",
      "test data set Detection Rate -The rate of true events also predicted to be events\n",
      "Balanced Accuracy -(sensitivity+specificity )/2\n",
      "Precision -TP/TP+FP –When predicting True Positives , what percentage is correct?\n",
      "(no FP, precision = 1)\n",
      "Recall –TP/TP+FN (same as sensitivity) –What proportion of Actual Positives where identified correctly?\n",
      "F1 Score –Harmonic mean of Precision and Recall, where accuracy is used when True Positives and True Negatives are important, F1 is used when False Negatives and False Positives are more of a concern.\n",
      "Also really bestused on unbalanced datasets \n",
      "\n",
      "Machine Learning Bias and Social Science MethodsAdditional Performance Measures\n",
      "Log Loss - log loss measures the UNCERTAINTY of the probabilities of your model by comparing them to the true labels –CLASSIFICATION.\n",
      "It heavily penalizes classifications that are highly confident in the wrong direction.\n",
      "So, seeing a log loss of 1 can be expected in the case when our model only gives less than a ~ 40 % probability estimate for selecting the actual class.\n",
      "Knowing the baseline rate (prevalence) here is important!\n",
      "Machine Learning Bias and Social Science MethodsAdditional Performance Measures\n",
      "Kappa - Landis and Koch (1977) provide a way to characterize values.\n",
      "According to their scheme a value < 0 is indicating no agreement , 0–0.20 as slight, 0.21–0.40 as fair, 0.41–0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1 as almost perfect agreement.\n",
      "Indicates how much better our classifier is performing over the performance of a classifier that would just guess at random according to the frequency of each class.\n",
      "Its especially useful for multi -class models as many of the metrics we have reviewed are better suited to binary examples.\n",
      "Machine Learning Bias and Fairness These metrics can be used to assess the fairness of the machine learning models\n",
      "We will review these topics again later in the semester, as there’s much more to be learning but having a basic understand will help when we walk through the fairness formulas.\n",
      "Another Definition: Functional Approximation\n",
      "•What is a functional approximation problem?\n",
      "•Target variable: Dependent: What we are trying to predict\n",
      "•Other Variables: Independent: Using to Predict\n",
      "•Functional approximation is an approach that uses the other variables we have access to approximate the dependent and does so through the function development\n",
      "•We will use regression, which assumes that we have a numeric target variable, for classification it’s often a bi- variate or class level variable 24\n",
      "\n",
      "Assessment Measures •Assessing Regression Models: MSE, RMSE and MAE and Log Loss •MSE –The difference between the predicted values and the actual values squared\n",
      "•RSME –Same as above only the square root is taken to put the error back in terms of the dependent variable\n",
      "•Can also normalize the RSME to the range of the data in order to be able to compare RSME outputs that include different data ranges •MAE –The same approach only taking the absolute value instead of squaring\n",
      "25\n",
      "\n",
      "Equations 26nX X\n",
      "RMSEn\n",
      "iidelmo iobs∑=−\n",
      "=12\n",
      ", , ) (\n",
      "min, max, obs obs X XRMSENRMSE\n",
      "−=\n",
      "\n",
      "\n",
      "Regression •Said another way basic linear regression has a Prediction Accuracy Problem:\n",
      "•Has a low bias (overfitting) but a high variance\n",
      "•This can be improved by injecting some level of bias into the equation by reducing the impact of certain coefficients •This can improve overall accuracy by reducing variance •Draw Dart Board –\n",
      "•Another issue is interpretation –with a large number of predictor variables and large data sets it often hard to identify variable importance and explain model outcomes\n",
      "27\n",
      "\n",
      "Regression and Sparsity\n",
      "•Often we have more features than observations in the world of big data\n",
      "•What type of problem is this?\n",
      "•So we strive to have Sparse models\n",
      "•What do we mean by Sparse?\n",
      "28\n",
      "\n",
      "Bias Versus Variance 29\n",
      "\n",
      "\n",
      "The Confusion Matrix and Accuracy\n",
      "\n",
      "\n",
      "Most widely used metricAccuracy•Accuracy is better measured using test data that was not used to build the classifier.\n",
      "•Referred to as the overall recognition rate of the classifier\n",
      "•Error rate or misclassification rate:\n",
      "1-Accuracy\n",
      "•When training data are used to compute the accuracy, the error rate is called resubstitution error .\n",
      "Accuracy may not be enough.Consider this two- class case.\n",
      "Consider the accuracy if a model predicts that the outcome is always class a.\n",
      "28/30 = 93% accuracy\n",
      "This is misleading.\n",
      "Sensitivity and Specificity\n",
      "\n",
      "\n",
      "Receiver Operating Characteristic (ROC) Curve\n",
      "1-Specificity\n",
      "False Negative RateSensitivity\n",
      "True Positive Rate\n",
      "0 101\n",
      "\n",
      "The ROC Curve\n",
      "\n",
      "\n",
      "Cleaning the Data and Documenting the\n",
      "Process Using R Markdown\n",
      "Introduction to R Programming\n",
      "District Data Labs\n",
      "\n",
      "The Data “Pipeline”\n",
      "Document Your Data Cleaning Process\n",
      "Tidy Data\n",
      "Data cleaning methods\n",
      "Opening data ﬁles in R and viewing them\n",
      "Working with data frames\n",
      "Dropping variables\n",
      "Renaming variables\n",
      "Logical operators\n",
      "Dropping observations\n",
      "Reshaping\n",
      "Reshaping from wide to long\n",
      "Reshaping from long to wide\n",
      "Creating new variables\n",
      "\n",
      "Replacing existing variables\n",
      "Managing string/character variables\n",
      "Merging\n",
      "Binding columns or rows\n",
      "Matching on ID variables\n",
      "Checking the merge\n",
      "Sorting rows\n",
      "Additional data management techniques\n",
      "Managing the workspace\n",
      "Deleting objects\n",
      "Saving/loading the workspace\n",
      "Opening ASCII data ﬁles\n",
      "\n",
      "Saving ASCII data ﬁles\n",
      "Arranging columns\n",
      "Missing data\n",
      "Managing categorical variables\n",
      "Recoding/labeling values\n",
      "Combining categories\n",
      "Reordering categories\n",
      "Combining categories across multiple variables\n",
      "More methods for managing string/character variables\n",
      "Managing date/time variables\n",
      "Piping\n",
      "Collapsing and within-group calculations\n",
      "Using group by()\n",
      "Using count()\n",
      "\n",
      "Document Your Data Cleaning Process\n",
      "Remember the work by Baggerly and Coombes.\n",
      "They had to\n",
      "spend months trying to replicate a study in a “forensic” way\n",
      "because the authors did not document their data cleaning steps.\n",
      "They had to deal with this:\n",
      "Instead, we will learn how to do this:\n",
      "\n",
      "\n",
      "Document Your Data Cleaning Process\n",
      "Real-world data are always messy: that is, original data are never\n",
      "immediately ready for analysis.\n",
      "Data must be cleaned in order to\n",
      "be useful.\n",
      "But small mistakes in the data cleaning stage can have profound\n",
      "eﬀects for the analysis.\n",
      "If your data are corrupted, no amount of\n",
      "statistical-machine-learning-artiﬁcial-intelligence-magic will save\n",
      "the analysis.\n",
      "This afternoon, we will review the tidyverse packages, and all the\n",
      "methods we have for cleaning data.\n",
      "We will also introduce a\n",
      "couple new methods.\n",
      "We will work through how to do this work using R markdown, so\n",
      "that we document our steps as we go along .\n",
      "We can catch errors\n",
      "this way, and make them easier to ﬁx.\n",
      "What is Tidy Data?\n",
      "“Happy families are all alike; every unhappy family is unhappy in\n",
      "its own way.\n",
      "” — Leo Tolstoy\n",
      "“Like families, tidy datasets are all alike but every messy dataset is\n",
      "messy in its own way.\n",
      "” — Hadley Wickham\n",
      "Data can come in many shapes and formats.\n",
      "The data can be\n",
      "assembled in one place or might be in many diﬀerent places.\n",
      "Tidy data is a philosophy about the best way to code a dataset to\n",
      "make data easy to analyze in a reproducible way.\n",
      "What is Tidy Data?\n",
      "Atidy dataset meets the following minimum standards :\n",
      "1.\n",
      "The dataset exists as one table.\n",
      "2.\n",
      "It’s square: every row has an entry for every column (even if it\n",
      "is missing).\n",
      "3.\n",
      "The rows represent observations.\n",
      "4.\n",
      "The columns represent variables.\n",
      "5.\n",
      "The observations are comparable units (for example: people,\n",
      "countries, but not a mix of the two)\n",
      "\n",
      "What is Tidy Data?\n",
      "Even better tidy data also:\n",
      "6.\n",
      "Sorts the rows and columns into a logical order\n",
      "7.\n",
      "Has descriptive variable names\n",
      "8.\n",
      "Creates new variables to convey the important information\n",
      "9.\n",
      "Deletes irrelevant variables (and sometime observations)\n",
      "10.\n",
      "Reads date variables as dates\n",
      "11.\n",
      "Uses consistent codes for missing values\n",
      "These are all things to do with the edited, working data.\n",
      "Never\n",
      "overwrite the original data ﬁle.\n",
      "We may need to return to the\n",
      "original data many times.\n",
      "Opening data ﬁles in R and viewing them\n",
      "The most common type of data ﬁle is CSV (comma separated\n",
      "values).\n",
      "CSVs are space eﬃcient, portable, and can be transferred\n",
      "to any data/statistics software.\n",
      "To load a CSV ﬁle called data.csv , load the tidyverse package\n",
      "by typing\n",
      "library(tidyverse)\n",
      "Then type\n",
      "data <- read_csv(\"data.csv\")\n",
      "Then to look at the data spreadsheet type View(data) in the\n",
      "console .\n",
      "(Please note the capital V.)\n",
      "Looking at the data is the most useful way to understand the\n",
      "challenges ahead of you for preparing the data for analysis.\n",
      "Working with data frames\n",
      "All of the following commands start by assigning the data\n",
      "management step to a data frame.\n",
      "If you assign the step to a new name, you create a new object .\n",
      "The step is applied to the NEW data frame, but the step is NOT\n",
      "applied to the original data frame object.\n",
      "Example : If the data frame is called data ,\n",
      "▶data <- command overwrites the data object,\n",
      "▶data2 <- command creates another object, data2 , in\n",
      "which the command was applied.\n",
      "It leaves the data object\n",
      "alone.\n",
      "In general : assign to the same object over and over, UNLESS you\n",
      "don’t want the step to be permanent.\n",
      "Dropping variables\n",
      "Sometimes datasets have hundreds or thousands of variables.\n",
      "Only a small portion of them might be relevant to your work.\n",
      "You\n",
      "may have to curate the data by deleting variables.\n",
      "Use the select() command to keep only the variables you\n",
      "specify.\n",
      "To keep only the country name andyear variables, type\n",
      "vdem <- select(vdem, country_name, year)\n",
      "You can keep country name ,year , and every variable that\n",
      "starts with “v2x” by typing\n",
      "vdem <- select(vdem, country_name, year,\n",
      "starts_with(\"v2x\"))\n",
      "\n",
      "Dropping variables\n",
      "You can keep country name ,year , and every variable that ends\n",
      "with “ocracy” by typing\n",
      "vdem <- select(vdem, country_name, year,\n",
      "ends_with(\"ocracy\"))\n",
      "You can also specify which variables to drop instead of keep by\n",
      "using a minus sign:\n",
      "vdem <- select(vdem, -ends_with(\"error\"))\n",
      "\n",
      "Renaming variables\n",
      "Tidy data should have descriptive variable names so you actually\n",
      "know what each variable is.\n",
      "It’s crazy how often the data collectors\n",
      "use dumb names like XAB14G7B!\n",
      "To rename the variable v2xpolyarchy todemocracy , type:\n",
      "vdem <- rename(vdem, democracy=v2x_polyarchy)\n",
      "Note that the new name comes ﬁrst, and the old name comes\n",
      "second.\n",
      "You can rename many variables with one command.\n",
      "Just separate\n",
      "these statements with commas:\n",
      "vdem <- rename(vdem, democracy=v2x_polyarchy,\n",
      "corrupt=v2x_corr,\n",
      "civil_lib=v2x_civlib)\n",
      "\n",
      "Logical operators\n",
      "A logical statement is one that can only take on two values: true\n",
      "or false.\n",
      "Logical operators are used to build logical statements.\n",
      "These operators are\n",
      "▶== is equal to?\n",
      "▶>is greater than?\n",
      "▶>=is greater than or\n",
      "equal to?\n",
      "▶<is less than?▶<=is less than or equal\n",
      "to?\n",
      "▶%in% is an element of the\n",
      "set?\n",
      "A!sign in front of >,>=,<, or<= means “not greater than”,\n",
      "“not greater than or equal to”, etc.\n",
      "Also, != means “not equal to”, and !(3 %in% c(4,5,6))\n",
      "means “3 is not an element of the set 4, 5, 6”\n",
      "\n",
      "Logical operators\n",
      "Two symbols are used to combine logical operators:\n",
      "▶&“and”\n",
      "▶|“or” (shift + the button just above enter)\n",
      "Parentheses work here too to designate an order of evaluation.\n",
      "Examples:\n",
      "1==1 true 2==3 false\n",
      "(1==1)|(2==3) true (1==1)&(2==3) false\n",
      "(1!=1)|(2!=3) true !((1==1)&(2==3)) true\n",
      "(4*3-2) %in% c(10, 11, 12) true !(4*3-2) %in% c(10, 11, 12) false\n",
      "If variables are used in a logical statement, R puts the value TRUE\n",
      "on the rows for which the statement is true, and FALSE on the\n",
      "rows for which the statement is false.\n",
      "If you change the class of a logical variable to numeric, the variable\n",
      "will be binary (0,1).\n",
      "Dropping observations\n",
      "Thefilter() command works just like select() , except it keeps\n",
      "observations instead of variables, and you use logical statements\n",
      "to identify rows instead of variable names.\n",
      "To keep only the rows corresponding to the year 2014, type\n",
      "vdem <- filter(vdem, year==2014)\n",
      "To keep only the rows in 2014 when the democracy variable is\n",
      "greater than .5, type\n",
      "vdem <- filter(vdem, year==2014 & democracy > .5)\n",
      "To keep only the rows for 1997, 2003, and 2011, type\n",
      "vdem <- filter(vdem, year %in% c(1997, 2003, 2011))\n",
      "\n",
      "Reshaping\n",
      "Reshaping is a way to transform a dataset by turning columns into\n",
      "rows, or rows into columns.\n",
      "There are two kinds of reshapes :\n",
      "1.gather() : turning columns into rows.\n",
      "(In Stata, this is called\n",
      "areshape long .)\n",
      "2.spread() : turning rows into columns.\n",
      "(In Stata, this is called\n",
      "areshape wide .)\n",
      "Remember: tidy data requires that rows are observations and\n",
      "columns are variables .\n",
      "Years are observations, not variables.\n",
      "Reshaping is tricky.\n",
      "But any command can be broken down to its\n",
      "simple logical elements.\n",
      "We just need to examine the rules on\n",
      "which the gather() andspread() commands operate.\n",
      "Reshaping\n",
      "Here’s an untidy dataset because observations are in the columns:\n",
      "country 1999 2000\n",
      "1 Afghanistan 745 2666\n",
      "2 Brazil 37737 80488\n",
      "3 China 212258 213766\n",
      "The gather() command brings the observations to the rows.\n",
      "This\n",
      "command has the following syntax:\n",
      "gather(data, ...\n",
      ", key, value)\n",
      "Let’s clearly deﬁne what each argument does.\n",
      "Reshaping\n",
      "gather(data, ...\n",
      ", key, value)\n",
      "data is the name of the data frame you want to reshape.\n",
      ".\n",
      ".\n",
      ".represents the names of the variables that you will move to the\n",
      "rows.\n",
      "▶You can type all the raw variable names here, separated by\n",
      "commas\n",
      "▶If the variable names are numeric (as with years), place\n",
      "forward-slanted single quotes around the names\n",
      "▶You can type something like var1:var10 to refer to all\n",
      "variables in between between var1 andvar10 in the data\n",
      "frame.\n",
      "Reshaping\n",
      "gather(data, ...\n",
      ", key, value)\n",
      "key is the name of the variable that will contain the names of the\n",
      "old variables you are moving.\n",
      "This variable doesn’t yet exist.\n",
      "If the\n",
      "column names are numeric years, you should type key=\"year\" .\n",
      "value is the name of the variable that will contain the data inside\n",
      "the old variables you are moving.\n",
      "This variable also doesn’t yet\n",
      "exist.\n",
      "Reshaping\n",
      "This data is an object named table4a in R:\n",
      "country 1999 2000\n",
      "1 Afghanistan 745 2666\n",
      "2 Brazil 37737 80488\n",
      "3 China 212258 213766\n",
      "The data inside the columns represent a variable named cases.\n",
      "Reshaping\n",
      "To reshape this data, we type:\n",
      "gather(table4a, ‘1999‘, ‘2000‘, key=\"year\", value=\"cases\")\n",
      "The data now looks like\n",
      "country year cases\n",
      "Afghanistan 1999 745\n",
      "Brazil 1999 37737\n",
      "China 1999 212258\n",
      "Afghanistan 2000 2666\n",
      "Brazil 2000 80488\n",
      "China 2000 213766\n",
      "\n",
      "Reshaping\n",
      "Here’s an untidy dataset because variables are in the rows:\n",
      "country year type count\n",
      "Afghanistan 1999 cases 745\n",
      "Afghanistan 1999 population 19987071\n",
      "Afghanistan 2000 cases 2666\n",
      "Afghanistan 2000 population 20595360\n",
      "Brazil 1999 cases 37737\n",
      "Brazil 1999 population 172006362\n",
      "Brazil 2000 cases 80488\n",
      "Brazil 2000 population 174504898\n",
      "China 1999 cases 212258\n",
      "China 1999 population 1272915272\n",
      "China 2000 cases 213766\n",
      "China 2000 population 1280428583\n",
      "\n",
      "Reshaping\n",
      "To clean this dataset, we use the spread() command, which\n",
      "works similarly (but not exactly like) the gather() command:\n",
      "spread(data, key, value)\n",
      "The notable diﬀerence here is the lack of an .\n",
      ".\n",
      ".argument.\n",
      "We\n",
      "don’t have to specify columns, because the variables are already in\n",
      "the rows, and R knows what to place in the columns\n",
      "automatically by looking at the levels of the factor (named type in\n",
      "this data).\n",
      "data is the name of the data frame you want to reshape.\n",
      "Reshaping\n",
      "spread(data, key, value)\n",
      "key is the name of the existing variable that contains the names of\n",
      "the variables you will move to the columns.\n",
      "value is the name of the existing variable that contains the data\n",
      "inside the variables you will move to the columns.\n",
      "To reshape the data (named table2 in R), we type:\n",
      "spread(table2, key=\"type\", value=\"count\")\n",
      "\n",
      "Reshaping\n",
      "The data now looks like this:\n",
      "country year cases population\n",
      "1 Afghanistan 1999 745 19987071\n",
      "2 Afghanistan 2000 2666 20595360\n",
      "3 Brazil 1999 37737 172006362\n",
      "4 Brazil 2000 80488 174504898\n",
      "5 China 1999 212258 1272915272\n",
      "6 China 2000 213766 1280428583\n",
      "\n",
      "Creating new variables\n",
      "Use the mutate() command to create new variables.\n",
      "Write the new variable name, one equal sign, then what the new\n",
      "variable equals.\n",
      "You can use math and logic operators and the\n",
      "names of existing variables.\n",
      "For example:\n",
      "vdem <- mutate(vdem,\n",
      "after2001 = (year>=2001),\n",
      "adjusted_dem = democracy - corruption,\n",
      "avg = (democracy + civil_rights)/2)\n",
      "If you use logical operators , the new variable will be BINARY to\n",
      "express whether the logical statement is TRUE or FALSE.\n",
      "Replacing existing variables\n",
      "You can also use the mutate() command to replace old variables.\n",
      "The diﬀerence is that now you write old variable names instead of\n",
      "new variables.\n",
      "In this example, the variables democracy andcorruption\n",
      "already exist :\n",
      "vdem <- mutate(vdem,\n",
      "democracy = 100*democracy,\n",
      "adjusted_dem = democracy - corruption)\n",
      "The ﬁrst transformation says “replace each value of democracy\n",
      "with 100 times its old value.”\n",
      "The second transformation says “create adjusted dem to be the\n",
      "diﬀerence of democracy andcorruption .”\n",
      "But there’s a problem here.\n",
      "See it?\n",
      "Replacing existing variables\n",
      "vdem <- mutate(vdem,\n",
      "democracy = 100*democracy,\n",
      "adjusted_dem = democracy - corruption)\n",
      "These transformations are done one at a time and in order.\n",
      "In the second transformation, our intention was to use the old\n",
      "version ofdemocracy .\n",
      "Instead it uses the new version (×100).\n",
      "This command instead does what we want:\n",
      "vdem <- mutate(vdem,\n",
      "adjusted_dem = democracy - corruption\n",
      "democracy = 100*democracy)\n",
      "So pay attention to the order in which you create/replace variables.\n",
      "Managing string/character variables\n",
      "Some variables are coded as character (words), but in reality, they\n",
      "are categorical variables.\n",
      "We’ve covered how to deal with variables\n",
      "like that.\n",
      "But other variables are character because they really are\n",
      "open-ended responses .\n",
      "Open-ended questions contain a LOT of data, some of it novel and\n",
      "unexpected, since these responses are not constrained to follow any\n",
      "multiple choices.\n",
      "But how to extract this data?\n",
      "Managing string/character variables\n",
      "The following commands are part of the stringr library, which is\n",
      "part of the tidyverse packages.\n",
      "Here’s an example of a string:\n",
      "response <- \"I have the faintest idea.\n",
      "I really don’t watch the news\"\n",
      "String is just another word for character, text, words, etc.\n",
      "It has\n",
      "the character class in R:\n",
      "> class(response)\n",
      "[1] \"character\"\n",
      "\n",
      "Managing string/character variables\n",
      "The length of a string is the number of characters in the string,\n",
      "including spaces:\n",
      "> str_length(response)\n",
      "[1] 55\n",
      "To pull out a subsection of this string, use the strsub()\n",
      "command.\n",
      "Specify the string variable to work with, the NUMBER\n",
      "of the character that starts the substring, and the NUMBER of the\n",
      "last character of the substring:\n",
      "> str_sub(response, 1, 10)\n",
      "[1] \"I have the\"\n",
      "> str_sub(response, 25, 37)\n",
      "[1] \".\n",
      "I really do\"\n",
      "\n",
      "Managing string/character variables\n",
      "Negative numbers with the strsub() command tell R to count\n",
      "backwards .\n",
      "To get a substring starting 15 characters from the\n",
      "end, until the end, type:\n",
      "> str_sub(response, -15, -1)\n",
      "[1] \" watch the news\"\n",
      "It might be useful to convert all letters to UPPERCASE, so we\n",
      "don’t have to worry about case sensitivity when looking for speciﬁc\n",
      "patterns and words:\n",
      "> str_to_upper(response)\n",
      "[1] \"I HAVE THE FAINTEST IDEA.\n",
      "I REALLY DON’T WATCH\n",
      "THE NEWS\"\n",
      "\n",
      "Merging\n",
      "Merging is the technique of combining two data frames.\n",
      "There are\n",
      "a few diﬀerent ways to perform a merge.\n",
      "(1) Adding columns, without trying to match observations .\n",
      "Thebind cols() (or the older cbind() ) function pastes two data\n",
      "frames together, side by side, exactly as they are .\n",
      "So if you sort\n",
      "one or both data frames, it changes the result of the bind cols()\n",
      "command.\n",
      "For example, consider data1 anddata2 :\n",
      "country x y\n",
      "USA -0.10 -0.90\n",
      "Canada 0.18 -0.88\n",
      "UK 0.50 0.92country x y\n",
      "China -0.61 -1.23\n",
      "Japan -0.24 -1.05\n",
      "S.\n",
      "Korea -0.11 -1.10\n",
      "\n",
      "Merging\n",
      "Using bind cols() to combine these data frames has the\n",
      "following result:\n",
      "cbind(data1,data2)\n",
      "country x y country1 x1 y1\n",
      "USA -0.10 -0.90 China -0.61 -1.23\n",
      "Canada 0.18 -0.88 Japan -0.24 -1.05\n",
      "UK 0.50 0.92 S.\n",
      "Korea -0.11 -1.10\n",
      "To avoid duplicate column names, R placed an arbitrary “1” after\n",
      "the variable names for data2 .\n",
      "R pasted the two data frames together without any regard for the\n",
      "data’s structure or meaning.\n",
      "In general, this is NOT what we want\n",
      "to do.\n",
      "Merging\n",
      "(2) Adding rows (in Stata this is called “appending”)\n",
      "The bind rows() (or the older rbind() ) function is similar to\n",
      "bind cols() , but with two important diﬀerences:\n",
      "1.bind rows() pastes data frames together by pasting one on\n",
      "top of the other, resulting in more rows but not more columns\n",
      "2.bind rows() rearranges the columns of each data frame to\n",
      "try to match corresponding variables.\n",
      "This is a good option to group observations that belong together,\n",
      "but are stored in separate places for whatever reason.\n",
      "Merging\n",
      "Using bind rows() to combine these data frames has the\n",
      "following result:\n",
      "rbind(data1,data2)\n",
      "country x y\n",
      "1 USA -0.10 -0.90\n",
      "2 Canada 0.18 -0.88\n",
      "3 UK 0.50 0.92\n",
      "4 China -0.61 -1.23\n",
      "5 Japan -0.24 -1.05\n",
      "6 S.\n",
      "Korea -0.11 -1.10\n",
      "\n",
      "Merging\n",
      "(3) Adding columns, matching observations that share the\n",
      "same ID variable(s) .\n",
      "This is the most important kind of merge.\n",
      "It’s also the trickiest to\n",
      "do correctly.\n",
      "Suppose you have two data frames that look like this:\n",
      "country x y\n",
      "USA 0.97 0.76\n",
      "China -0.96 -2.12\n",
      "Russia 1.10 0.24country w z\n",
      "China 0.99 0.33\n",
      "USA 0.37 -0.64\n",
      "Russia 0.10 -0.49\n",
      "How do you combine the two data frames?\n",
      "Merging\n",
      "You can sort both data frames alphabetically by country, then use\n",
      "cbind() .\n",
      "But while that works here, that approach won’t work in\n",
      "general.\n",
      "We need a way for R to recognize that country is the ID\n",
      "variable , and that the two data frames share IDs.\n",
      "Then R needs to\n",
      "match rows that share the same ID .\n",
      "The most important merge command is full join() .\n",
      "Applying\n",
      "this command gives the following result:\n",
      "full_join(data1, data2)\n",
      "country x y w z\n",
      "USA 0.97 0.76 0.37 -0.64\n",
      "China -0.96 -2.12 0.99 0.33\n",
      "Russia 1.10 0.24 0.10 -0.49\n",
      "\n",
      "Merging\n",
      "Here’s what full join() does :\n",
      "1.\n",
      "It looks at the variable names for each dataset, and\n",
      "determines whether there is any overlap in these names.\n",
      "2.\n",
      "It assumes that the shared variable names are the ID variables\n",
      "you want to use to match the observations in each data frame.\n",
      "3.\n",
      "It combines observations with the same IDs.\n",
      "full join() is very useful and easy to use compared to merge\n",
      "commands in other packages and software.\n",
      "But it tends to mess up without displaying any warnings or\n",
      "errors !\n",
      "You have to understand how full join() works, and you\n",
      "have to visualize what you want the data to look like after using\n",
      "full join() .\n",
      "Merging\n",
      "If an observation has no match in the other data frame, it still\n",
      "appears in the merged data with NAvalues for the variables from\n",
      "the other data frame.\n",
      "There are related commands that treat unmatched observations\n",
      "diﬀerently:\n",
      "▶inner join() : drops all unmatched observations\n",
      "▶left join(data1, data2) : keeps all observations from\n",
      "data1 , but drops all unmatched observations in data2\n",
      "▶right join(data1, data2) : keeps all observations from\n",
      "data2 , but drops all unmatched observations in data1\n",
      "In general, it is safer to stick with full join() .\n",
      "These other\n",
      "commands might delete observations you don’t want to delete.\n",
      "Merging\n",
      "Suppose for example we want to merge these two data frames:\n",
      "country x y\n",
      "USA 0.43 -0.57\n",
      "China -0.70 -1.38\n",
      "Russia 1.64 0.60\n",
      "France -0.38 -0.30country w z\n",
      "USA 0.99 -1.27\n",
      "China 1.06 0.13\n",
      "Russia 1.65 0.72\n",
      "Japan 0.28 -0.09\n",
      "Note that some, but not all, of the ID values have a match in the\n",
      "other data frame.\n",
      "Merging\n",
      "full join() keeps all observations that appear in either data\n",
      "frame, but creates missing values for unmatched observations:\n",
      "data3 <- full_join(data1, data2)\n",
      "country x y w z\n",
      "USA 0.43 -0.57 0.99 -1.27\n",
      "China -0.70 -1.38 1.06 0.13\n",
      "Russia 1.64 0.60 1.65 0.72\n",
      "France -0.38 -0.30 NA NA\n",
      "Japan NA NA 0.28 -0.09\n",
      "\n",
      "Merging\n",
      "inner join() keeps the observations that have a match in both\n",
      "observations, and deletes all other observations:\n",
      "data3 <- inner_join(data1, data2)\n",
      "country x y w z\n",
      "USA 0.43 -0.57 0.99 -1.27\n",
      "China -0.70 -1.38 1.06 0.13\n",
      "Russia 1.64 0.60 1.65 0.72\n",
      "\n",
      "Merging\n",
      "left join() keeps all observations from data1 (the data frame\n",
      "typed FIRST), keeps all observations from data2 (the data frame\n",
      "typed SECOND) that have a match in data1 , and deletes\n",
      "observations from data2 without a match in data1 :\n",
      "data3 <- left_join(data1, data2)\n",
      "country x y w z\n",
      "USA 0.43 -0.57 0.99 -1.27\n",
      "China -0.70 -1.38 1.06 0.13\n",
      "Russia 1.64 0.60 1.65 0.72\n",
      "France -0.38 -0.30 NA NA\n",
      "\n",
      "Merging\n",
      "right join() keeps all observations from data2 (the data frame\n",
      "typed SECOND), keeps all observations from data1 (the data\n",
      "frame typed FIRST) that have a match in data2 , and deletes\n",
      "observations from data1 without a match in data2 :\n",
      "data3 <- left_join(data1, data2)\n",
      "country x y w z\n",
      "USA 0.43 -0.57 0.99 -1.27\n",
      "China -0.70 -1.38 1.06 0.13\n",
      "Russia 1.64 0.60 1.65 0.72\n",
      "Japan NA NA 0.28 -0.09\n",
      "\n",
      "Merging\n",
      "Sometimes there is more than one ID variable that identiﬁes an\n",
      "observation.\n",
      "For example, we might have data on\n",
      "▶U.S States within years\n",
      "▶Political parties within countries within years\n",
      "▶Students within classes within schools within school districts\n",
      "within states\n",
      "A set of ID variables are called unique identiﬁers of the rows if no\n",
      "two rows share the same values of every ID variable.\n",
      "Two rows\n",
      "might represent China at diﬀerent years.\n",
      "So country alone is not a\n",
      "unique identiﬁer, but country and year TOGETHER are.\n",
      "The full join() command can match on multiple ID variables,\n",
      "as long as corresponding ID variables have the same name in\n",
      "each data frame .\n",
      "Merging\n",
      "Suppose we want to merge the following two data frames:\n",
      "country year x y\n",
      "USA 2014 0.53 0.26\n",
      "USA 2015 -1.12 -1.68\n",
      "USA 2016 0.75 -1.24\n",
      "China 2014 -0.01 0.17\n",
      "China 2015 -0.09 0.14\n",
      "China 2016 -0.32 0.07\n",
      "Russia 2014 1.49 0.83\n",
      "Russia 2015 -0.96 1.04\n",
      "Russia 2016 2.50 -0.43country year w z\n",
      "USA 2014 0.27 -1.95\n",
      "USA 2015 0.88 0.14\n",
      "USA 2016 -0.06 0.49\n",
      "China 2014 -0.25 -1.02\n",
      "China 2015 0.72 0.22\n",
      "China 2016 0.89 1.20\n",
      "Russia 2014 0.59 0.45\n",
      "Russia 2015 0.06 -1.57\n",
      "Russia 2016 -0.93 -0.01\n",
      "There are two ID variables, country and year.\n",
      "Because the have\n",
      "the same name in each data frame, we can use full join() like\n",
      "we did before.\n",
      "Merging\n",
      "We use the following command:\n",
      "data3 <- full_join(data1, data2)\n",
      "country year x y w z\n",
      "USA 2014 0.53 0.26 0.27 -1.95\n",
      "USA 2015 -1.12 -1.68 0.88 0.14\n",
      "USA 2016 0.75 -1.24 -0.06 0.49\n",
      "China 2014 -0.01 0.17 -0.25 -1.02\n",
      "China 2015 -0.09 0.14 0.72 0.22\n",
      "China 2016 -0.32 0.07 0.89 1.20\n",
      "Russia 2014 1.49 0.83 0.59 0.45\n",
      "Russia 2015 -0.96 1.04 0.06 -1.57\n",
      "Russia 2016 2.50 -0.43 -0.93 -0.01\n",
      "\n",
      "Merging\n",
      "Sometimes one data frame has more unique identifying variables\n",
      "than the other.\n",
      "For example:\n",
      "country year x y\n",
      "USA 2014 0.53 0.26\n",
      "USA 2015 -1.12 -1.68\n",
      "USA 2016 0.75 -1.24\n",
      "China 2014 -0.01 0.17\n",
      "China 2015 -0.09 0.14\n",
      "China 2016 -0.32 0.07\n",
      "Russia 2014 1.49 0.83\n",
      "Russia 2015 -0.96 1.04\n",
      "Russia 2016 2.50 -0.43country w z\n",
      "USA 0.99 -1.27\n",
      "China 1.06 0.13\n",
      "Russia 1.65 0.72\n",
      "\n",
      "Merging\n",
      "full join() handles this situation too.\n",
      "It recognizes the shared\n",
      "ID of country, and places the data for a country in data2 on every\n",
      "row for that country:\n",
      "data3 <- full_join(data1, data2)\n",
      "country year x y w z\n",
      "USA 2014 0.53 0.26 0.99 -1.27\n",
      "USA 2015 -1.12 -1.68 0.99 -1.27\n",
      "USA 2016 0.75 -1.24 0.99 -1.27\n",
      "China 2014 -0.01 0.17 1.06 0.13\n",
      "China 2015 -0.09 0.14 1.06 0.13\n",
      "China 2016 -0.32 0.07 1.06 0.13\n",
      "Russia 2014 1.49 0.83 1.65 0.72\n",
      "Russia 2015 -0.96 1.04 1.65 0.72\n",
      "Russia 2016 2.50 -0.43 1.65 0.72\n",
      "\n",
      "What can go wrong while merging\n",
      "full join() doesn’t always know if something went wrong.\n",
      "So\n",
      "there’s no automated procedure to check errors.\n",
      "But there are\n",
      "common problems you can check for before merging the data.\n",
      "Prior to merging, we will perform three checks :\n",
      "1.ID name check : do the shared ID variables have the same\n",
      "name?\n",
      "Are these variables the ONLY ones that share the\n",
      "same name in both data frames?\n",
      "2.Unique ID check : do we expect the ID variables to be\n",
      "unique identiﬁers in one or both data frames?\n",
      "If so, are they?\n",
      "3.ID value check : are there discrepancies in the values of the\n",
      "ID variable?\n",
      "If all three checks pass, then the merge will work without problems.\n",
      "What can go wrong while merging\n",
      "ID name check : First, look at each data frame with the View() ,\n",
      "summary() , and head() commands.\n",
      "Decide on what the unique\n",
      "ID variables are in each data frame, and which you will use to\n",
      "match in the merge.\n",
      "Use the names() command to display the names for each data\n",
      "frame, and use the intersect() command to see which variable\n",
      "names are shared by both data frames:\n",
      "> names(data1)\n",
      "[1] \"country\" \"year\" \"x\" \"y\"\n",
      "> names(data2)\n",
      "[1] \"country\" \"year\" \"w\" \"z\"\n",
      "> intersect(names(data1), names(data2))\n",
      "[1] \"country\" \"year\"\n",
      "There are two problems this check can reveal.\n",
      "What can go wrong while merging\n",
      "Problem 1 : the ID variables don’t have the same name in each data\n",
      "frame.\n",
      "In this case R won’t know which variable to match on.\n",
      "That can happen if one data frame has “state” while the other has\n",
      "“State” (case sensitive!).\n",
      "Or “year” vs.\n",
      "“yr”, “countrycode” vs.\n",
      "“ccode”, etc.\n",
      "Solution :prior to merging , rename the ID variable in one of the\n",
      "two data frames, so that they have the same name.\n",
      "Problem 2 : non-ID variables in each data frame unexpectedly have\n",
      "the same name.\n",
      "In this case, R will mistakenly think this variable is\n",
      "an ID.\n",
      "Solution :prior to merging , rename the non-ID variable in one of\n",
      "the two data frames, so that they have they DON’T same name\n",
      "anymore.\n",
      "What can go wrong while merging\n",
      "Unique ID check : If the ID variables are not unique identiﬁers in\n",
      "either data frame, then R places ALL combinations of matching\n",
      "observations in the merged data.\n",
      "Here’s a simple example:\n",
      "name x\n",
      "A 2\n",
      "A 7\n",
      "A 1name y\n",
      "A 6\n",
      "A 5\n",
      "A 8\n",
      "These two data frames share an ID variable named “name”.\n",
      "But\n",
      "name is NOT a unique ID in either data frame since multiple rows\n",
      "have the same value of “A”.\n",
      "There’s no second ID variable like year that we can use to identify\n",
      "the rows.\n",
      "What can go wrong while merging\n",
      "If we just go ahead and merge, the result is\n",
      "data3 <- full_join(data1, data2)\n",
      "name x y\n",
      "A 2 6\n",
      "A 2 5\n",
      "A 2 8\n",
      "A 7 6\n",
      "A 7 5\n",
      "A 7 8\n",
      "A 1 6\n",
      "A 1 5\n",
      "A 1 8\n",
      "\n",
      "What can go wrong while merging\n",
      "R didn’t have enough information to match each row to a single\n",
      "row in the other data frame, so it matched each row to every\n",
      "possible row.\n",
      "That’s not what we want.\n",
      "To check whether this is a problem :\n",
      "1.\n",
      "Create a second data frame with just the ID variables\n",
      "2.\n",
      "Use the unique() command to keep only the non-repeated\n",
      "rows\n",
      "3.\n",
      "Use nrow() to compare the number of rows of the unique ID\n",
      "data frame to the number of rows in the original data.\n",
      "If\n",
      "these numbers are the same, then the ID variables are unique.\n",
      "What can go wrong while merging\n",
      "For example, to check whether country andyear are unique IDs\n",
      "in this data frame:\n",
      "country year x y\n",
      "USA 2014 0.53 0.26\n",
      "USA 2015 -1.12 -1.68\n",
      "USA 2016 0.75 -1.24\n",
      "China 2014 -0.01 0.17\n",
      "China 2015 -0.09 0.14\n",
      "China 2016 -0.32 0.07\n",
      "Russia 2014 1.49 0.83\n",
      "Russia 2015 -0.96 1.04\n",
      "Russia 2016 2.50 -0.43\n",
      "\n",
      "What can go wrong while merging\n",
      "(1)Create a second data frame with just the ID variables\n",
      "data.temp <- select(data1, country, year)\n",
      "(2)Use the unique() command to keep only the non-repeated\n",
      "rows\n",
      "data.temp <- unique(data.temp)\n",
      "(3)Usenrow() to compare the number of rows of the unique ID\n",
      "data frame to the number of rows in the original data.\n",
      "If these\n",
      "numbers are the same, then the ID variables are unique.\n",
      "> nrow(data.temp)\n",
      "[1] 9\n",
      "> nrow(data1)\n",
      "[1] 9\n",
      "Because the numbers of rows are equal, country and year are\n",
      "unique IDs.\n",
      "What can go wrong while merging\n",
      "IDs don’t always have to be unique.\n",
      "In this example:\n",
      "country year x y\n",
      "USA 2014 0.53 0.26\n",
      "USA 2015 -1.12 -1.68\n",
      "USA 2016 0.75 -1.24\n",
      "China 2014 -0.01 0.17\n",
      "China 2015 -0.09 0.14\n",
      "China 2016 -0.32 0.07\n",
      "Russia 2014 1.49 0.83\n",
      "Russia 2015 -0.96 1.04\n",
      "Russia 2016 2.50 -0.43country w z\n",
      "USA 0.99 -1.27\n",
      "China 1.06 0.13\n",
      "Russia 1.65 0.72\n",
      "We merge on country .\n",
      "We didn’t expect country to be unique in\n",
      "the ﬁrst data frame, just the second.\n",
      "That’s okay!\n",
      "Make sure the\n",
      "results of this test match your expectations.\n",
      "What can go wrong while merging\n",
      "ID value check : It’s common for two diﬀerent datasets to code\n",
      "the same observations with slightly diﬀerent IDs.\n",
      "Some examples:\n",
      "▶“District of Columbia” vs.\n",
      "“DC”\n",
      "▶“South Korea” vs.\n",
      "“S.\n",
      "Korea” vs.\n",
      "“ROK”\n",
      "▶1998 vs 98\n",
      "If you don’t catch these discrepancies, the data won’t get\n",
      "matched for this observation .\n",
      "Also, two datasets might not cover the exact same cases.\n",
      "Or they\n",
      "might use diﬀerent time frames.\n",
      "You have to decide whether or\n",
      "not that’s okay.\n",
      "Sometimes data contain thousands of unique IDs.\n",
      "That’s too\n",
      "many to read through manually.\n",
      "We need a reliable way to identify\n",
      "the unmatched IDs.\n",
      "What can go wrong while merging\n",
      "If we merge two data frames, data1 anddata2 , there are two\n",
      "ways for an ID value to be unmatched:\n",
      "▶An ID appears in data1 but not data2 , or\n",
      "▶An ID appears in data2 but not data1\n",
      "We can look at each type of unmatched ID separately.\n",
      "The anti join() command is a kind of “reverse merging”.\n",
      "It\n",
      "drops the matched observations and leaves the unmatched ones.\n",
      "Speciﬁcally:\n",
      "▶anti join(data1, data2) keeps only observations in\n",
      "data1 that have no match in data2\n",
      "▶anti join(data2, data1) keeps only observations in\n",
      "data2 that have no match in data1\n",
      "\n",
      "What can go wrong while merging\n",
      "To identify the unmatched observations, type:\n",
      "check1 <- anti_join(data1, data2, by=c(\"id1\", \"id2\"))\n",
      "check2 <- anti_join(data2, data1, by=c(\"id1\", \"id2\"))\n",
      "Unlike full join() , these commands require the byargument,\n",
      "which takes a character vector with the names of the ID variables\n",
      "to match on.\n",
      "(If there’s only one ID, type its name in quotes\n",
      "without the c() command.)\n",
      "Now check1 has all the observations in data1 that have no match\n",
      "indata2 , and check2 has all the observations in data2 that have\n",
      "no match in data1 .\n",
      "You can use View() to see these observations, and nrow() to\n",
      "count the number of observations in check1 andcheck2 .\n",
      "If both\n",
      "have 0 observations, then every observation was matched .\n",
      "Sorting rows\n",
      "Sorting means rearranging the rows/columns in a way that does\n",
      "not break any row or column apart.\n",
      "You can sort rows based on the values of one variable numerically\n",
      "(from smallest to largest, or from largest to smallest) or\n",
      "alphabetically .\n",
      "You can move the columns to appear in any order you like.\n",
      "These steps are mostly cosmetic (they won’t change statistical\n",
      "results) but they make the data much easier to look at .\n",
      "Sorting rows\n",
      "To sort observations, use the arrange() function.\n",
      "Example : The state legislature data lists estimates of left/right\n",
      "ideologies for U.S.\n",
      "state legislatures from 1993-2014.\n",
      "To sort the\n",
      "rows from the most liberal (smallest value) to the most\n",
      "conservative (largest value) state house, type\n",
      "stateleg <- arrange(stateleg, hou_chamber)\n",
      "To sort from most conservative (largest value) to most liberal\n",
      "(smallest value), use a minus sign in front of the sorting variable:\n",
      "stateleg <- arrange(stateleg, -hou_chamber)\n",
      "Sometimes I get an error when I use the minus sign.\n",
      "If that\n",
      "happens, this should work instead:\n",
      "stateleg <- arrange(stateleg, desc(hou_chamber))\n",
      "\n",
      "Sorting rows\n",
      "You can specify more than one sorting variable:\n",
      "1.\n",
      "The data are sorted by the ﬁrst variable.\n",
      "2.\n",
      "If there are ties , they are broken by sorting the second\n",
      "variable within values of the ﬁrst.\n",
      "3.\n",
      "The third variable breaks ties with the ﬁrst two variables, and\n",
      "so on.\n",
      "To sort alphabetically by state name, then sort the observations\n",
      "from the same state by year, type\n",
      "stateleg <- arrange(stateleg, st, year)\n",
      "\n",
      "Deleting Objects\n",
      "The memory that R sets aside for all the objects you load and\n",
      "create is called the workspace.\n",
      "To see all of the objects that currently exist in the workspace, type\n",
      "ls() .\n",
      "To delete an object, use rm() .\n",
      "To delete an object named data ,\n",
      "type\n",
      "rm(data)\n",
      "To delete three objects named data ,polity , and cow, type\n",
      "rm(list=c(\"data\", \"polity\", \"cow\"))\n",
      "Note : you need to put object names in quotes here.\n",
      "Deleting Objects\n",
      "When you start a new R session (by closing and restarting R),\n",
      "there are no objects in the workspace.\n",
      "Sometimes you switch from working on one project to another.\n",
      "When you do, it might be a good idea to delete ALL objects in\n",
      "the workspace .\n",
      "That way, there’s less chance of confusing new\n",
      "objects with old ones.\n",
      "To delete all objects, type\n",
      "rm(list=ls())\n",
      "Sometimes this command is compared to clear in Stata.\n",
      "It’s\n",
      "similar, but two big diﬀerences:\n",
      "1.clear closes datasets only, rm(list=ls()) removes any\n",
      "object.\n",
      "2.clear closes a ﬁle without saving it.\n",
      "rm(list=ls())\n",
      "DELETES objects.\n",
      "Saving and Loading the Workspace\n",
      "The workspace (the set of all of the objects you’ve made) is\n",
      "temporary.\n",
      "But, when you close R and R Studio, they always ask you if you\n",
      "want to save the “workspace image”, even when there’s nothing\n",
      "in it .\n",
      "You can also save the whole workspace without being prompted by\n",
      "typing\n",
      "save(list=ls(), file=\"filename.Rdata\")\n",
      "If you save the workspace, it gets saved as an .Rdata ﬁle in your\n",
      "working directory.\n",
      "This is NOT the same as a data ﬁle.\n",
      "It can only be read by R\n",
      "and R Studio.\n",
      "It contains many objects, potentially, not just data\n",
      "frames.\n",
      "Managing the workspace\n",
      "You can save just a few of the objects by typing something like\n",
      "save(list=c(\"data\", \"polity\", \"cow\"),\n",
      "file=\"filename.Rdata\")\n",
      "Caution : if “ﬁlename.Rdata” already exists in the working\n",
      "directory, this command OVERWRITES it.\n",
      "Be very careful if the\n",
      "raw data is stored in an .Rdata ﬁle.\n",
      "You can load a saved workspace when you start a new R session by\n",
      "typing\n",
      "load(\"filename.Rdata\")\n",
      "Now all the objects you had saved are in the workspace again.\n",
      "Managing the workspace\n",
      "I don’t recommend relying on the save() andload() commands\n",
      "or.Rdata ﬁles.\n",
      "Here’s why :\n",
      "▶.Rdata ﬁles are speciﬁc to R and do not transfer to other\n",
      "programs.\n",
      "▶.Rdata ﬁles are too easy to overwrite .\n",
      "▶.Rdata ﬁles encourage saving data in disparate objects\n",
      "instead of in a clean dataset.\n",
      "Avoiding .Rdata ﬁles is counter-intuitive since it’s natural to\n",
      "equate .Rdata ﬁles to .dta ﬁles for Stata or .xls ﬁles for Excel.\n",
      "I use .Rdata ﬁles only when there is a very speciﬁc reason for\n",
      "saving objects instead of a data frame.\n",
      "Much more often, I use commands to load and save data in ASCII\n",
      "format ﬁles .\n",
      "Opening and Saving ASCII Data Files\n",
      "By default, read csv() assumes row 1 contains the variable\n",
      "names.\n",
      "But if the data does not have names in row 1, type\n",
      "data <- read_csv(\"data.csv\", col_names=FALSE)\n",
      "To replace the given variable names with names you choose , type\n",
      "something like\n",
      "data <- read_csv(\"data.csv\",\n",
      "col_names=c(\"id\", \"vote\", \"age\"))\n",
      "Sometimes data ﬁles have a few lines of text at the top with\n",
      "information like authors, title, grant number, etc.\n",
      "To skip 3 lines\n",
      "at the top, type\n",
      "data <- read_csv(\"data.csv\", skip=3)\n",
      "\n",
      "Opening and Saving ASCII Data Files\n",
      "Sometimes certain rows in the ASCII ﬁle are commented out with\n",
      "some symbol the data’s authors chose.\n",
      "To avoid reading rows that\n",
      "are marked with % at the beginning as data, type\n",
      "data <- read_csv(\"data.csv\", comment=\"%\")\n",
      "You might need to convert missing codes.\n",
      "For example, Stata\n",
      "marks missing values as .and R marks them as NA.\n",
      "To read the .\n",
      "markers as missing, type\n",
      "data <- read_csv(\"data.csv\", na=\".\")\n",
      "\n",
      "Opening and Saving ASCII Data Files\n",
      "read csv2() is for semi-colon separated ﬁles.\n",
      "read tsv() is for tab separated ﬁles.\n",
      "read table() is for ﬁles where datapoints are separated by white\n",
      "space, not necessary tab.\n",
      "read fwf() is for ﬁxed width ﬁles.\n",
      "You will have to also specify\n",
      "which characters (counting left to right) correspond to which\n",
      "variables.\n",
      "To read data that is delimited in any other way (by &, for\n",
      "example), type\n",
      "data <- read_delim(\"data.txt\", delim=\"&\")\n",
      "\n",
      "Saving CSV data ﬁles\n",
      "Remember the workﬂow : First, load the raw data.\n",
      "Then do stuﬀ.\n",
      "End by saving the cleaned/edited data under a diﬀerent\n",
      "name , and NEVER overwrite the original data.\n",
      "To save a data frame object data as a new CSV ﬁle, type\n",
      "write_csv(data, \"different_name.csv\")\n",
      "To save a data frame object data as a new tab separated ﬁle, type\n",
      "write_tsv(data, \"different_name.txt\")\n",
      "These two commands will be enough the vast majority of the time.\n",
      "SeeR for Data Science for some commands for very special\n",
      "circumstances.\n",
      "Arranging columns\n",
      "You can arrange columns from left to right.\n",
      "Mostly, this task just\n",
      "makes the data nicer to look at when you use View() .\n",
      "Note, however, that it will change the column numbers .\n",
      "So\n",
      "stateleg[,15] no longer refers to the same variable as before.\n",
      "To arrange the stateleg data so that state name comes ﬁrst,\n",
      "then year, then everything else, type:\n",
      "stateleg <- select(stateleg, st, year, everything())\n",
      "Careful : The select() command is also used to delete variables.\n",
      "If you forget to include everything() in the above command:\n",
      "stateleg <- select(stateleg, st, year)\n",
      "then every variable EXCEPT for standyear gets deleted.\n",
      "Missing data\n",
      "A data point is missing if for some reason we can’t know what it\n",
      "is.\n",
      "R denotes missing values as NA.\n",
      "“Missing values are contagious.\n",
      "” Since we don’t know what an\n",
      "NAis, we can’t know what operations with an NAare either:\n",
      "1 + NA=NA\n",
      "If you want to see the mean of a variable, this might happen:\n",
      "> mean(stateleg$hou_dem)\n",
      "[1] NA\n",
      "That happened because the variable houdem has at least one\n",
      "missing value.\n",
      "To ignore missing values when using functions like\n",
      "the mean, use the na.rm=TRUE argument:\n",
      "> mean(stateleg$hou_dem, na.rm=TRUE)\n",
      "[1] -0.7418481\n",
      "\n",
      "Missing data\n",
      "The summary() command for a data frame will show you the\n",
      "number of missing values for each variable.\n",
      "To see a giant matrix of TRUE/FALSE values, indicating whether\n",
      "each data point is missing, type\n",
      "is.na(stateleg)\n",
      "Toforcibly delete any row with at least one missing value, type\n",
      "stateleg <- na.omit(stateleg)\n",
      "In general, I don’t recommend doing that.\n",
      "It’s heavy handed and\n",
      "there are better approaches to handling missing data.\n",
      "Some notes on missing data\n",
      "Data may be missing for a lot of reasons.\n",
      "On a survey, respondents\n",
      "might say “don’t know,” may refuse to answer, or the question\n",
      "might not be applicable.\n",
      "Sometimes surveys will place a marker value to indicate a missing\n",
      "value.(For example, using 998 for “don’t know”).\n",
      "If you do not\n",
      "recode these values, then it messes up mathematical functions like\n",
      "averages.\n",
      "“Missing values are contagious.”\n",
      "Any mathematical function that includes even one missing value\n",
      "will be missing.\n",
      "1 + NA = NA , 1 + NA̸= 1\n",
      "In other words, don’t confuse missing with 0!\n",
      "0 is information,\n",
      "and missing is a lack of information.\n",
      "Missing values for continuous variables\n",
      "We can use logical statements to deal with situations in which\n",
      "missing values for continuous variables are given numeric codes.\n",
      "For example, in the data data, thermometer scores are continuous,\n",
      "coded 0 to 100 scales .\n",
      "But if a respondent says “don’t know”,\n",
      "the score is marked as 998.\n",
      "If we don’t replace these values, all\n",
      "results with these variables are thrown oﬀ.\n",
      "The logical statement data$ft obama == 998 returns a vector\n",
      "of logical values: TRUE if the ftobama variable is 998, FALSE if\n",
      "not.\n",
      "This command replaces these values with NAs:\n",
      "data$ft_obama[data$ft_obama == 998] <- NA\n",
      "\n",
      "Recoding values\n",
      "Recoding values means replacing many values of a categorical\n",
      "variable with new values, simultaneously.\n",
      "There are a few reasons why you may want to recode :\n",
      "1.\n",
      "Change the labels of values.\n",
      "Make it easier to see and\n",
      "remember what each value means.\n",
      "Better to code as \"Male\",\n",
      "\"Female\" than 1, 2.\n",
      "2.\n",
      "Replace missing codes with NA.\n",
      "3.\n",
      "Change the order of categories for display in graphs, or in case\n",
      "you want to treat the variable as ordinal and categories are\n",
      "out of order.\n",
      "Labeling categorical values\n",
      "Suppose you have a categorical variable with values 1, 2, 3, 4, 8\n",
      "and 9 .\n",
      "You look in the data’s codebook (hopefully it has one) and see that\n",
      "Category Meaning\n",
      "1 I speak Spanish primarily\n",
      "2 I speak both Spanish and English equally\n",
      "3 I speak English primarily but can speak Spanish\n",
      "4 I can not speak Spanish\n",
      "8 refused\n",
      "9 skipped\n",
      "We can replace the numeric values with their written meanings,\n",
      "without changing the way the categorical data is treated in R.\n",
      "Using fctrecode()\n",
      "There are many ways to replace numeric categories with their\n",
      "written meanings.\n",
      "The easiest method uses the fctrecode()\n",
      "function from the forcats package (one of the tidyverse ).\n",
      "Here’s an example of how to use fctrecode() :\n",
      "Let’s break down the elements of this code:\n",
      "\n",
      "Using fctrecode()\n",
      "This function must be only ever used inside the mutate()\n",
      "function.\n",
      "Type mutate() , then the data frame, then the name of the\n",
      "categorical variable you are editing, then an equal sign.\n",
      "Parentheses will appear automatically and will indent correctly\n",
      "when you push enter.\n",
      "Leave the closing parentheses alone.\n",
      "Using fctrecode()\n",
      "Then type fctrecode()\n",
      "The ﬁrst argument of fctrecode() is the categorical variable,\n",
      "which must be of the factor class.\n",
      "If it is not (here it is numeric),\n",
      "useas.factor() to coerce the variable:\n",
      "\n",
      "\n",
      "Using fctrecode()\n",
      "The press enter, and on each new line write the new categorical\n",
      "text label, in quotes, equal to the old categorical label, also in\n",
      "quotes.\n",
      "Remember, as with the rename() function: new ﬁrst, then old .\n",
      "This code works whether the old labels are numbers or text.\n",
      "Using fctrecode()\n",
      "Finally, for the categories you want to set to be missing , write the\n",
      "new category labels as NULL , with no quotes:\n",
      "This code is more space-consuming than other approaches.\n",
      "But\n",
      "the advantage is that we can more easily keep track of the new\n",
      "and old categories, minimizing the risk of confusing which label\n",
      "goes with which number.\n",
      "Using fctrecode()\n",
      "One more nice thing that fctrecode() can do: combine\n",
      "categories.\n",
      "To combine two old categories into the same new category, just\n",
      "use the same new category label for multiple old categories.\n",
      "For example, suppose we want to group every category in which a\n",
      "person knows at least some Spanish as Yes, and the category in\n",
      "which a person knows no Spanish as No.\n",
      "We can write:\n",
      "\n",
      "\n",
      "Reordering categories\n",
      "The categories of a factor variable have a built-in order.\n",
      "The\n",
      "order controls a few things:\n",
      "1.\n",
      "The order of the categories appear in any table\n",
      "2.\n",
      "The order the categories appear left-to-right in any graph\n",
      "3.\n",
      "The meaning of the variable when used in a regression model\n",
      "Sometimes the categories have a natural ordering: for example, we\n",
      "can arrange categories in order of how much Spanish a person\n",
      "speaks.\n",
      "Sometimes the categories don’t have a natural ordering, but it\n",
      "makes sense to choose a particular order because it makes a table\n",
      "or graph looks better, or to change the base category in a\n",
      "regression.\n",
      "Reordering categories\n",
      "To change the order, use the fctrelevel() function.\n",
      "It works a\n",
      "lot like fctrecode() , only instead of writing old categories,\n",
      "simply write the existing categories in the order you want.\n",
      "For example:\n",
      "We’ll talk more about why it matters to change the order in more\n",
      "detail over the next few weeks.\n",
      "Reordering categories\n",
      "Also, note that both the fctrecode() and fctrelevel()\n",
      "functions can be called within the same call to mutate() :\n",
      "If you have multiple categorical variables to edit in this way, you\n",
      "can place all the calls to fctrecode() and fctrelevel() in\n",
      "the same mutate() command.\n",
      "Combining categories across multiple variables\n",
      "Sometimes a survey will store categorical data in multiple\n",
      "variables.\n",
      "For example:\n",
      "▶Variable 1 : are you a Democrat, Republican, or neither?\n",
      "▶Variable 2 : (if Democrat/Republican) are you a strong\n",
      "Democrat/Republican?\n",
      "▶Variable 3 : (if neither) do you lean towards one party or the\n",
      "other?\n",
      "To combine categorical variables, use the unite() command.\n",
      "data <- unite(data, pid, pid1d, pid1r, pidstr, pidlean)\n",
      "It pastes categories from diﬀerent variables together.\n",
      "THEN you\n",
      "can recode these pasted categories.\n",
      "Managing string/character variables\n",
      "To split up a string into a list of fragments of the string, use the\n",
      "strsplit() command.\n",
      "> str_split(response, pattern=\" \")\n",
      "[[1]]\n",
      "[1] \"I\" \"have\" \"the\" \"faintest\" \"idea.\"\n",
      "[6] \"I\" \"really\" \"don’t\" \"watch\" \"the\"\n",
      "[11] \"news\"\n",
      "You can break the string on any kind of character you type with\n",
      "thepattern argument:\n",
      "> str_split(response, pattern=\"’\")\n",
      "[[1]]\n",
      "[1] \"I have the faintest idea.\n",
      "I really don\"\n",
      "[2] \"t watch the news\"\n",
      "\n",
      "Managing string/character variables\n",
      "One annoying thing about how strings are coded sometimes is\n",
      "extraneous white space before and after the text.\n",
      "To remove\n",
      "leading and trailing spaces , use the strtrim() command:\n",
      "> hello <- c(\" whatsup?\n",
      "\")\n",
      "> hello\n",
      "[1] \" whatsup?\n",
      "\"\n",
      "> str_trim(hello)\n",
      "[1] \"whatsup?\"\n",
      "\n",
      "Managing string/character variables\n",
      "One of the most useful ways to pull data from a string is to search\n",
      "for particular words.\n",
      "The strdetect() command returns TRUE\n",
      "if the word is found in the string, FALSE if not.\n",
      "The match must\n",
      "be exact, and it is case sensitive.\n",
      "> str_detect(response, \"news\")\n",
      "[1] TRUE\n",
      "> str_detect(response, \"have\")\n",
      "[1] TRUE\n",
      "> str_detect(response, \"haven’t\")\n",
      "[1] FALSE\n",
      "The strdetect() function can also take regular expressions\n",
      "instead of a single word.\n",
      "We’ll cover that in more detail later in\n",
      "the semester.\n",
      "Managing string/character variables\n",
      "You can also search for multiple patterns at the same time with\n",
      "strdetect() , but the code is a bit strange.\n",
      "There are two ways\n",
      "to proceed.\n",
      "First, you can separate diﬀerent words or terms with the |symbol\n",
      "inside the quotes.\n",
      "This only works if there are no spaces or\n",
      "carriage returns next to any|symbol.\n",
      "So if you are looking for\n",
      "one of “larry”, “curly”, and “moe” in the text:\n",
      "This works: strdetect(response, \"larry|curly|moe\")\n",
      "You will get a variable that is TRUE if the text contains ”larry” or\n",
      "”curly” or ”moe”\n",
      "Doesn’t work:\n",
      "strdetect(response, \"larry | curly | moe\")\n",
      "You won’t get an error but the TRUE and FALSE values won’t be\n",
      "assigned correctly.\n",
      "Managing string/character variables\n",
      "If you have a long list of words, it’s better to use the following\n",
      "technique:\n",
      "1.\n",
      "Put all of the words you want to search for in a character vector\n",
      "object\n",
      "2.\n",
      "Instead of a word in quotes, type\n",
      "paste(object, collapse = \"|\")\n",
      "where object is the vector you made in step 1.\n",
      "Then the variable you create will be TRUE if any of the words in\n",
      "your vector are present.\n",
      "Managing string/character variables\n",
      "For example, to search for the names of UVA men’s basketball\n",
      "players in a variable named text , type:\n",
      "players <- c(\"Marco Anthony\", \"Francesco Badocchi\",\n",
      "\"Francisco Caffaro\", \"Kihei Clark\",\n",
      "\"Mamadi Diakite\", \"Kyle Guy\", \"Jay Huff\",\n",
      "\"De’Andre Hunter\", \"Ty Jerome\",\n",
      "\"Austin Katstra\", \"Braxton Key\",\n",
      "\"Jayden Nixon\", \"Jack Salt\", \"Kody Stattmann\")\n",
      "data <- mutate(data, mention_player =\n",
      "str_detect(text, paste(players, collapse=\"|\")))\n",
      "The variable mention player will be TRUE if text contains any of\n",
      "these names, and FALSE otherwise.\n",
      "Managing date/time variables\n",
      "There are lots of data in political science that involve dates and\n",
      "times.\n",
      "Unfortunately, there is NOT a consistent way that dates\n",
      "and times are recorded in data.\n",
      "Problem 1 : Sometimes a single date/time is recorded in many\n",
      "variables: year, month, day, hour, minute might all be separate\n",
      "columns .\n",
      "Problem 2 : Sometimes a date/time is recorded in one variable as a\n",
      "messy and unusable block of text .\n",
      "How do we deal with each type of coding?\n",
      "How do we get R to\n",
      "recognize that the variable is a date or time?\n",
      "Managing date/time variables\n",
      "As with anything in R, there are many ways to do the same thing.\n",
      "Here we will use the lubridate package:\n",
      "library(lubridate)\n",
      "Problem 1 : the date and time are stored in many variables:\n",
      "year1 <- 1983; month1 <- 4; day1 <- 14\n",
      "year2 <- 2018; month2 <- 2; day2 <- 22\n",
      "We can use the make datetime() command to create single\n",
      "variables from multiple variables.\n",
      "We specify which variables\n",
      "contain the year, month, day, etc.\n",
      "date1 <- make_datetime(year=year1, month=month1, day=day1)\n",
      "date2 <- make_datetime(year=year2, month=month2, day=day2)\n",
      "We can also add the hour ,minute , and second arguments if we\n",
      "have that information.\n",
      "Managing date/time variables\n",
      "Now date1 anddate2 are recognized as dates:\n",
      "> date1\n",
      "[1] \"1983-04-14 UTC\"\n",
      "> date2\n",
      "[1] \"2018-02-22 UTC\"\n",
      "The class of a date variable is called POSIXct andPOSIXt .\n",
      "It’s a\n",
      "technical name, but it just means “date and time”.\n",
      "> class(date1)\n",
      "[1] \"POSIXct\" \"POSIXt\"\n",
      "\n",
      "Managing date/time variables\n",
      "Now that we have two date variables, we can use basic arithmetic\n",
      "functions with them.\n",
      "We can for example measure the length of\n",
      "time between the two dates:\n",
      "> duration <- date2 - date1\n",
      "> duration\n",
      "Time difference of 12733 days\n",
      "> as.numeric(duration)/365\n",
      "[1] 34.88493\n",
      "The make datetime() command can work within the mutate()\n",
      "command to create new variables.\n",
      "Managing date/time variables\n",
      "Problem 2 : pulling date and time information from a single, messy\n",
      "variable.\n",
      "time1 <- \"2018-02-22 10:05:28\"\n",
      "time2 <- \"02-22-2018 10:05:28\"\n",
      "time3 <- \"22-02-2018 10:05:28\"\n",
      "These variables contain the year, month, day, hour, minute, and\n",
      "second, all in one variable.\n",
      "They are also currently read as\n",
      "character.\n",
      "How do we get R to read these as dates and times ,\n",
      "and to recognize the correct year, month, etc.?\n",
      "The ymdhms() command assumes that the data contains the\n",
      "year, month, day, hour, minute, and second in that order:\n",
      "ymd_hms(time1)\n",
      "[1] \"2018-02-22 10:05:28 UTC\"\n",
      "\n",
      "Managing date/time variables\n",
      "The time2 variable is diﬀerent: it lists the MONTH ﬁrst, then the\n",
      "DAY, then the YEAR.\n",
      "If we use the ymdhms() command, we\n",
      "parse incorrectly:\n",
      "> ymd_hms(time2)\n",
      "[1] NA\n",
      "Warning message:\n",
      "All formats failed to parse.\n",
      "No formats found.\n",
      "The error occurs because it reads 2018 as the day, and that can’t\n",
      "happen.\n",
      "Instead we can use the mdyhms() command:\n",
      "> mdy_hms(time2)\n",
      "[1] \"2018-02-22 10:05:28 UTC\"\n",
      "\n",
      "Managing date/time variables\n",
      "There are many of these commands, and they diﬀer only in the\n",
      "order you write the letters y,m, and d.\n",
      "These commands automatically recognize the diﬀerent characters\n",
      "(hyphens, slashes, etc.) separate elements of the date, so no need\n",
      "to worry about specifying that.\n",
      "Piping\n",
      "Advanced R programmers know how to use a pipe.\n",
      "Piping\n",
      "A pipe is a way to connect consecutive R functions that\n",
      "manipulate the same data frame.\n",
      "The code for a pipe is %>%at the end of one command.\n",
      "It means\n",
      "“apply the next command to the data output by this\n",
      "command .” Pipes are implemented in the magrittr package, one\n",
      "of the packages included with tidyverse .\n",
      "Using the pipe is completely optional .\n",
      "It saves some time and\n",
      "space, and it makes you look like an advanced programmer.\n",
      "Piping\n",
      "Here are some data cleaning commands you know:\n",
      "data <- read_csv(\"data.csv\")\n",
      "data <- select(data, vote, age, sex, starts_with(\"party\"))\n",
      "data <- arrange(data, age)\n",
      "data <- filter(data, party_ID==\"republican\")\n",
      "data <- mutate(data, vote = fct_recode(vote,\n",
      "\"Trump\" = \"1.\n",
      "D Trump\",\n",
      "\"Clinton\" = \"2.\n",
      "H Clinton\",\n",
      "NULL = \"3.\n",
      "G Johnson\",\n",
      "NULL = \"4.\n",
      "J Stein\",\n",
      "NULL = \"5.\n",
      "Other\"))\n",
      "Notice how every command begins by calling the data frame we\n",
      "want to alter.\n",
      "Piping\n",
      "The exact same commands can be run with the following code:\n",
      "data <- read_csv(\"data.csv\")\n",
      "data <- data %>%\n",
      "select(vote, age, sex, starts_with(\"party\")) %>%\n",
      "arrange(age) %>%\n",
      "filter(party_ID==\"republican\") %>%\n",
      "mutate(vote = fct_recode(vote,\n",
      "\"Trump\" = \"1.\n",
      "D Trump\",\n",
      "\"Clinton\" = \"2.\n",
      "H Clinton\",\n",
      "NULL = \"3.\n",
      "G Johnson\",\n",
      "NULL = \"4.\n",
      "J Stein\",\n",
      "NULL = \"5.\n",
      "Other\"))\n",
      "Notice that the select() ,arrange() ,filter() , and mutate()\n",
      "functions no longer need the data frame listed ﬁrst.\n",
      "Pipes can be used with any function that takes a data frame as\n",
      "the ﬁrst argument .\n",
      "Using group by()\n",
      "Sometimes it is necessary to do calculations within groups of\n",
      "observations.\n",
      "Consider these example data (saved as object\n",
      "table1 in R once you load tidyverse ):\n",
      "country year cases population\n",
      "Afghanistan 1999 745 19987071\n",
      "Afghanistan 2000 2666 20595360\n",
      "Brazil 1999 37737 172006362\n",
      "Brazil 2000 80488 174504898\n",
      "China 1999 212258 1272915272\n",
      "China 2000 213766 1280428583\n",
      "We might want to create a variable with the total number of cases\n",
      "per year, or the average population by country.\n",
      "Using group by()\n",
      "The group by() function doesn’t do anything by itself, but it\n",
      "allows you to use mutate() orsummarize() to perform\n",
      "calculations in groups.\n",
      "Within group by() , specify the variable or variables that denote\n",
      "the groups.\n",
      "mutate() performs within-group calculations within the existing\n",
      "data frame.\n",
      "summarize() deletes rows, and leaves you with one row per\n",
      "group .\n",
      "To turn oﬀ within-group calculations, use the ungroup function.\n",
      "Using group by()\n",
      "To calculate the total number of cases per year in table1 use this\n",
      "code:\n",
      "table2 <- table1 %>%\n",
      "group_by(year) %>%\n",
      "mutate(totalcases = sum(cases))\n",
      "The data now look like this:\n",
      "country year cases population totalcases\n",
      "Afghanistan 1999 745 19987071 250740\n",
      "Afghanistan 2000 2666 20595360 296920\n",
      "Brazil 1999 37737 172006362 250740\n",
      "Brazil 2000 80488 174504898 296920\n",
      "China 1999 212258 1272915272 250740\n",
      "China 2000 213766 1280428583 296920\n",
      "\n",
      "Using group by()\n",
      "To do this calculation leaving only one row per year, use\n",
      "summarize() instead of mutate() :\n",
      "table2 <- table1 %>%\n",
      "group_by(year) %>%\n",
      "summarize(totalcases = sum(cases))\n",
      "The data now look like this:\n",
      "year totalcases\n",
      "1999 250740\n",
      "2000 296920\n",
      "\n",
      "Using group by()\n",
      "To calculate the average population for each country in table1\n",
      "use this code:\n",
      "table2 <- table1 %>%\n",
      "group_by(country) %>%\n",
      "mutate(avg.pop = mean(population))\n",
      "The data now look like this:\n",
      "country year cases population avg.pop\n",
      "Afghanistan 1999 745 19987071 20291216\n",
      "Afghanistan 2000 2666 20595360 20291216\n",
      "Brazil 1999 37737 172006362 173255630\n",
      "Brazil 2000 80488 174504898 173255630\n",
      "China 1999 212258 1272915272 1276671928\n",
      "China 2000 213766 1280428583 1276671928\n",
      "\n",
      "Using group by()\n",
      "To do this calculation leaving only one row per country, use\n",
      "summarize() instead of mutate() :\n",
      "table2 <- table1 %>%\n",
      "group_by(country) %>%\n",
      "summarize(avg.pop = mean(population))\n",
      "The data now look like this:\n",
      "country avg.pop\n",
      "Afghanistan 20291216\n",
      "Brazil 173255630\n",
      "China 1276671928\n",
      "\n",
      "Using group by()\n",
      "ungroup turns oﬀ within-group calculations.\n",
      "To calculate the\n",
      "percent of cases that each country accounts for:\n",
      "table2 <- table1 %>%\n",
      "group_by(year) %>%\n",
      "mutate(totalcases = sum(cases)) %>%\n",
      "ungroup %>%\n",
      "mutate(percent_of_cases = 100*cases/totalcases)\n",
      "The data now look like this:\n",
      "country year cases population totalcases percent\n",
      "Afghanistan 1999 745 19987071 250740 0.297\n",
      "Afghanistan 2000 2666 20595360 296920 0.898\n",
      "Brazil 1999 37737 172006362 250740 15.05\n",
      "Brazil 2000 80488 174504898 296920 27.11\n",
      "China 1999 212258 1272915272 250740 84.65\n",
      "China 2000 213766 1280428583 296920 71.99\n",
      "\n",
      "Using count()\n",
      "The count() function counts the number of rows within each\n",
      "group.\n",
      "It’s not necessary to use group by() when calling\n",
      "count() .\n",
      "Suppose we have data with terrorist attacks, with variables year\n",
      "andcountry .\n",
      "To create a dataset with a count of the total\n",
      "attacks by country , sorted from most to least, type\n",
      "attacks2 <- attacks %>%\n",
      "count(country, sort=TRUE)\n",
      "To create a dataset with a count of the total attacks by in each\n",
      "country and year , sorted from most to least, type\n",
      "attacks2 <- attacks %>%\n",
      "count(country, year, sort=TRUE)\n",
      "\n",
      "Fun with functions and dplyr\n",
      "Brian Wright\n",
      "1/24/2020\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 1 / 28\n",
      "\n",
      "Overview of Functions (Advanced R)\n",
      "Functions are at the core of R language, it’s really a function based\n",
      "language\n",
      "“R, at its heart, is a”functional\" language.\n",
      "This means that it has\n",
      "certain technical properties, but more importantly that it lends\n",
      "itself to a style of problem solving centred on functions.\" Hadley\n",
      "Wickham\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 2 / 28\n",
      "\n",
      "What is a functional based language?\n",
      "Recently functions have grown in popularity because they can produce\n",
      "efficient and simple solutions to lots of problems.\n",
      "Many of the\n",
      "problems with performance have been solved.\n",
      "Functional programming compliments object oriented programming\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 3 / 28\n",
      "\n",
      "What makes a programming approach “functional”?\n",
      "Functions can behave like any other data structure\n",
      "▶Assign them to variables, store to lists, pass them as aurguments to\n",
      "other functions, create them inside functions and even produce a\n",
      "function as a result of a funcion\n",
      "Functions need to be “pure” meaning that if you call it again with the\n",
      "same inputs you get the same results.\n",
      "sys.time() not a “pure”\n",
      "function\n",
      "The execution of the function shouldn’t change global variables, have\n",
      "no side effects.\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 4 / 28\n",
      "\n",
      "Functions\n",
      "Function don’t have to be “pure” but it can help to ensure your code\n",
      "is doing what you intend it to do.\n",
      "Functional programming helps to break a problem down into it’s\n",
      "pieces.\n",
      "When working to solve a problem it helps to divide the code\n",
      "into individually operating functions that solve parts of the problem.\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 5 / 28\n",
      "\n",
      "Types of Functions\n",
      "Figure 1: Function Types\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 6 / 28\n",
      "\n",
      "Let’s Build a Function\n",
      "Basically recipes composed of series of R statements\n",
      "name <- funtion (variables){\n",
      "#In here goes the series of R statements\n",
      "}\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 7 / 28\n",
      "\n",
      "Example, talk out the steps\n",
      "my_mean <- function (x){\n",
      "Sum <- sum(x)#Here we are using a function\n",
      "#inside a function!\n",
      "N <- length (x)\n",
      "return (Sum /N)#return is optional but helps with\n",
      "#clarity on some level.\n",
      "}\n",
      "Create a little list and pass it to the function and see if it works.\n",
      "Also call the Sum and N variables...does this work?\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 8 / 28\n",
      "\n",
      "Functional - Will show later, Function Factory\n",
      "(Advanced R)\n",
      "power1 <- function (exp) {\n",
      "function (x) {\n",
      "x^exp\n",
      "}\n",
      "}\n",
      "#Assigning the exponentials\n",
      "square <- power1 (2)\n",
      "cube <- power1 (3)\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 9 / 28\n",
      "\n",
      "Run the Created Functions\n",
      "square (3)\n",
      "> [1] 9\n",
      "cube (3)\n",
      "> [1] 27\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 10 / 28\n",
      "\n",
      "Quick Exercise\n",
      "Create a function that computes the range of a variable and then\n",
      "fornogoodreasonadds100anddividesby10.\n",
      "Writeoutthesteps\n",
      "you would need first in Pseudocode, then develop the function.\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 11 / 28\n",
      "\n",
      "dplyrverbs in the tidyverse\n",
      "The dplyrpackage gives us a few verbs for data manipulation\n",
      "Function Purpose\n",
      "select Select columns based on name or position\n",
      "mutate Create or change a column\n",
      "filter Extract rows based on some criteria\n",
      "arrange Re-order rows based on values of variable(s)\n",
      "group_by Split a dataset by unique values of a variable\n",
      "summarize Create summary statistics based on columns\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 12 / 28\n",
      "\n",
      "select\n",
      "You can select columns by name or position, of course.\n",
      "You can also select columns based on some criteria, which are\n",
      "encapsulated in functions.\n",
      "starts_with(“ \"), ends_with(\" ”), contains(“____”)\n",
      "one_of(“____”,“_____”,“______”)\n",
      "There are others; see help(starts_with) .\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 13 / 28\n",
      "\n",
      "Example\n",
      "Load the weather.csv .\n",
      "This contains daily temperature data in 2010 for\n",
      "some location.\n",
      "> [1] \"C:/Users/Brian Wright/Documents/git_3001/DS-4001/2_R_function_basics\"\n",
      "head (weather, 2)\n",
      "> # A tibble: 2 x 35\n",
      "> id year month element d1 d2 d3 d4 d5 d6 d7 d8\n",
      "> <chr> <int> <int> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n",
      "> 1 MX17~ 2010 1 tmax NA NA NA NA NA NA NA NA\n",
      "> 2 MX17~ 2010 1 tmin NA NA NA NA NA NA NA NA\n",
      "> # ...\n",
      "with 23 more variables: d9 <lgl>, d10 <dbl>, d11 <dbl>, d12 <lgl>,\n",
      "> # d13 <dbl>, d14 <dbl>, d15 <dbl>, d16 <dbl>, d17 <dbl>, d18 <lgl>,\n",
      "> # d19 <lgl>, d20 <lgl>, d21 <lgl>, d22 <lgl>, d23 <dbl>, d24 <lgl>,\n",
      "> # d25 <dbl>, d26 <dbl>, d27 <dbl>, d28 <dbl>, d29 <dbl>, d30 <dbl>, d31 <dbl>\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 14 / 28\n",
      "\n",
      "How would you just select the columns with the daily\n",
      "data?\n",
      "select (weather, starts_with (\"d\"))\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 15 / 28\n",
      "\n",
      "mutate\n",
      "mutatecan either transform a column in place or create a new column in\n",
      "a dataset\n",
      "We’ll use the in-built mpgdataset for this example, We’ll select only the\n",
      "city and highway mileages.\n",
      "To use this selection later, we will need to\n",
      "assign it to a new name\n",
      "mpg1 <- select (mpg, cty, hwy)\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 16 / 28\n",
      "\n",
      "mutate\n",
      "We’ll change the city and highway mileage to km/l from mpg.\n",
      "This will\n",
      "involve multiplying it by 1.6 and dividing by 3.8\n",
      "head (mutate (mpg1, cty = cty *1.6 /3.8,\n",
      "hwy = hwy *1.6/3.8), 5)\n",
      "> # A tibble: 5 x 2\n",
      "> cty hwy\n",
      "> <dbl> <dbl>\n",
      "> 1 7.58 12.2\n",
      "> 2 8.84 12.2\n",
      "> 3 8.42 13.1\n",
      "> 4 8.84 12.6\n",
      "> 5 6.74 10.9\n",
      "This is in-place replacement\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 17 / 28\n",
      "\n",
      "New Variable Defined\n",
      "mutate (mpg1, cty1 = cty *1.6/3.8, hwy1 = hwy *1.6/3.8)\n",
      "> # A tibble: 234 x 4\n",
      "> cty hwy cty1 hwy1\n",
      "> <int> <int> <dbl> <dbl>\n",
      "> 1 18 29 7.58 12.2\n",
      "> 2 21 29 8.84 12.2\n",
      "> 3 20 31 8.42 13.1\n",
      "> 4 21 30 8.84 12.6\n",
      "> 5 16 26 6.74 10.9\n",
      "> 6 18 26 7.58 10.9\n",
      "> 7 18 27 7.58 11.4\n",
      "> 8 18 26 7.58 10.9\n",
      "> 9 16 25 6.74 10.5\n",
      "> 10 20 28 8.42 11.8\n",
      "> # ...\n",
      "with 224 more rows\n",
      "This creates new variables\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 18 / 28\n",
      "\n",
      "filter\n",
      "filterextracts rows based on criteria\n",
      "filter (mpg, cyl ==4)\n",
      "> # A tibble: 81 x 11\n",
      "> manufacturer model displ year cyl trans drv cty hwy fl class\n",
      "> <chr> <chr> <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\n",
      "> 1 audi a4 1.8 1999 4 auto(l~ f 18 29 p comp~\n",
      "> 2 audi a4 1.8 1999 4 manual~ f 21 29 p comp~\n",
      "> 3 audi a4 2 2008 4 manual~ f 20 31 p comp~\n",
      "> 4 audi a4 2 2008 4 auto(a~ f 21 30 p comp~\n",
      "> 5 audi a4 quat~ 1.8 1999 4 manual~ 4 18 26 p comp~\n",
      "> 6 audi a4 quat~ 1.8 1999 4 auto(l~ 4 16 25 p comp~\n",
      "> 7 audi a4 quat~ 2 2008 4 manual~ 4 20 28 p comp~\n",
      "> 8 audi a4 quat~ 2 2008 4 auto(s~ 4 19 27 p comp~\n",
      "> 9 chevrolet malibu 2.4 1999 4 auto(l~ f 19 27 r mids~\n",
      "> 10 chevrolet malibu 2.4 2008 4 auto(l~ f 22 30 r mids~\n",
      "> # ...\n",
      "with 71 more rows\n",
      "This extracts only 4 cylinder vehicles\n",
      "Other choices might be cyl != 4 ,cyl > 4,year == 1999 ,\n",
      "manufacturer==\"audi\"Brian Wright Fun with functions and dplyr 1/24/2020 19 / 28\n",
      "\n",
      "Practice Piping\n",
      "admit_df <- read_csv (\"~/git_3001/DS-4001/data/LogReg.csv\")\n",
      "str(admit_df)\n",
      "> tibble [400 x 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n",
      "> $ admit: num [1:400] 0 1 1 1 0 1 1 0 1 0 ...\n",
      "> $ gre : num [1:400] 380 660 800 640 520 760 560 400 540 700 ...\n",
      "> $ gpa : num [1:400] 3.61 3.67 4 3.19 2.93 3 2.98 3.08 3.39 3.92 ...\n",
      "> $ rank : num [1:400] 3 3 1 4 4 2 1 2 3 2 ...\n",
      "> - attr(*, \"spec\")=\n",
      "> ..\n",
      "cols(\n",
      "> ..\n",
      "admit = col_double(),\n",
      "> ..\n",
      "gre = col_double(),\n",
      "> ..\n",
      "gpa = col_double(),\n",
      "> ..\n",
      "rank = col_double()\n",
      "> ..\n",
      ")\n",
      "#Do we notice anything that seems a bit off.\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 20 / 28\n",
      "\n",
      "Coercion num to factor\n",
      "admit_df $rank <- as.factor (admit_df $rank)\n",
      "#changes rank to a factor\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 21 / 28\n",
      "\n",
      "Five Basic Classes in R\n",
      "character\n",
      "numeric (double precision floating point numbers, default)\n",
      "integer (subset of numeric)\n",
      "complex (j = 10 + 5i)\n",
      "logical (True/False)\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 22 / 28\n",
      "\n",
      "All have coercion calls (example from: R Nuts and\n",
      "Bolts)\n",
      "x <- 0 :6\n",
      "class (x)#why\n",
      "> [1] \"integer\"\n",
      "as.numeric (x)\n",
      "> [1] 0 1 2 3 4 5 6\n",
      "as.logical (x)\n",
      "> [1] FALSE TRUE TRUE TRUE TRUE TRUE TRUE\n",
      "as.character (x)\n",
      "> [1] \"0\" \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 23 / 28\n",
      "\n",
      "Functional Example: Pass a function get a vector\n",
      "We can also convert multiple columns using lapply(), great example of\n",
      "functional orientation of R.\n",
      "names <- c(\"admit\",\"rank\")\n",
      "#using names as a index on admit_df,\n",
      "admit_df[,names] <- lapply (admit_df[,names], factor)\n",
      "#Check class of those two variables\n",
      "(as.character (meta_fun <- lapply (subset (admit_df,\n",
      "select = names),\n",
      "class)))\n",
      "> [1] \"factor\" \"factor\"\n",
      "#using a functional with two functions inside that creates a object\n",
      "coerced to a character list...what fun.\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 24 / 28\n",
      "\n",
      "Using the code chunk below to “group_by” rank\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 25 / 28\n",
      "\n",
      "Using the code chunk below to filter by 1 in the admit\n",
      "column\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 26 / 28\n",
      "\n",
      "Ok now summarise by average GPA\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 27 / 28\n",
      "\n",
      "Now Pipe everything together\n",
      "Brian Wright Fun with functions and dplyr 1/24/2020 28 / 28\n"
     ]
    }
   ],
   "source": [
    "all_text = \"\\n\\n\".join([page.page_content for page in pages])\n",
    "all_text = re.split(r'(?<=[.!?])\\s+', all_text.replace(' \\n', ' '))\n",
    "print(\"\\n\".join(all_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting text into chunks...\n",
      "['Overview and Data ScienceBrian Wrightbrianwright@virginia.edu2Course Administration \\uf0d8Everybody Reads Even Computers: Text MiningFinal Projects\\uf0d8Work individually and use one of the areas below to answer a broad questions related to a given dataset.']\n",
      "['I’ll provide several datasets for you to potential use, but you are also welcome to chose your own.']\n",
      "['You can also use any dataset from the class if you choose.']\n",
      "['\\uf0d8Topics we will/have covered that can be a focus of the final project:\\uf076Data Visualization \\uf076Fairness/Bias\\uf076Text Mining\\uf076KNN\\uf076Tree based methods\\uf0d8Ensemble –Random Forrest – time permitting Final Projects\\uf0d8Generate a publishable Rmarkdown document with the following sections:1.Question and background information on the data and why you are asking this question(s).']\n",
      "['References to previous research/evidence generally would be nice to include.']\n",
      "['2.Exploratory Data Analysis –Initial summary statistics and graphs with an emphasis on variables you believe to be important for your analysis.']\n",
      "['3.Methods –Techniques you are using to address your question and the results of those methods.']\n",
      "['4.Conclusions –What can you say about the results of the methods section as it relates to your question.']\n",
      "['5.Future work –What additional analysis is needed or what limited your analysis on this project.']\n",
      "['“Text Mining” Broader field: What is Exploratory Text Analytics?']\n",
      "['(ETA)Much of the following content is from the Exploratory Text Analytics Class as part of UVA’s MSDS taught by Rafael Alvarado6Text Mining ETA refers to text analytics applied to long -form texts with the purpose of surfacing their latent cognitive, cultural, and social contentTexts: Novels, essays, newspaper articles, letters, blogs, journal articles, etc.']\n",
      "['Content : concepts, categories, themes, emotions, events ...']\n",
      "['7Text MiningIts called \"exploratory\" because it methods are primarily unsupervised and designed to support human -in-the-loop interpretation“interpretation support”8Text MiningFrom Kevin Murphy, 2012,  Machine Learning: A Probabilistic Perspective, p.']\n",
      "['9.']\n",
      "['Unsupervised learning is about knowledge discoverySome Unsupervised Methods:11Clustering —K-means, hierarchical, etc.']\n",
      "['Topic Modeling —PCA, LSI/A, NMF, LDA, etc.']\n",
      "['Word Embedding —SGNS (word2vec), etc.']\n",
      "['Sentiment Analysis --dictionary- based methods, etc.Text MiningExtracting features from unstructured data to support machine learningE.g.']\n",
      "['principal components are document featuresInformation retrieval tasks such as document summarization, grouping, classification, and knowledge discoveryProvide data to support language modeling, including grammar, syntax, and pragmatics for NLP and computational linguisticsExtraction and representation of cultural and social patternsfrom text —See cultural analytics and culturomics12Text Mining: Applications of ETA13Coined by Harvard researchers Jean -Baptiste Michel and Erez Lieberman Aiden , who helped create Google’s NGram ViewerMichel and Aiden (2010):Inferences about culture made from trends in n- gram usageAn n- gram is a sequence of n wordsBased on Google BooksTransformed the field of text analyticsBased on application of genomic sequencing techniques to text ( See recent book, Uncharted)Text Mining: Culturomics14Two examples of inferences drawn from n- gram trends(Hand 2011) Text Mininghttps://books.google.com/ngrams15Text MiningETA builds on the domain knowledge of textual theory and criticism from history, literary studies, anthropology, sociolinguistics, religious studies, etc.']\n",
      "['Text is regarded as a first -class object of study,not an incidental container of language data“We” study text as textText is not necessarily language16Text MiningText as Text: Langue and ParoleLanguage (langue)GrammarCompetence Finite rules (grammar)SystemCollectiveUnconsciousStructureLatentSpeech (parole)DiscoursePerformanceIndefinite patterns (discourse)UsageIndividualConsciousEventObserved17\"Language\" is divided into grammar and discourseDiscourse is expressed a speech and writingWriting is \"fixed discourse\"Writing is the direct entextualization of discourse in a documentDocuments have a material form (medium ) and an \"immaterial\" dimension --the text as structured sequence of symbolsText is not \"unstructured\"!']\n",
      "['20Text Mining: Some Substantive Properties of TextAbove all, texts contain cultural informationThey function as social genes that encode and express beliefs, opinions, ideas, symbolism, etc.']\n",
      "['--think of Homer, the Bible, etc.']\n",
      "['As discourse, distinctive of human beingsTexts may also represent eventsSocial media and newspapers are like social sensorsAs more and more social life becomes entextualized through social media and other conduits (e.g.']\n",
      "[\"Internet of Things)Texts contain granular representations of human behaviorIt is the principal means by which behavioral surplus is capturedSo, culture is a complex system of human thought and behaviorthat exhibits a consistent pattern in society It is expressed and communicatedby symbolic formsA  primary vehicle in our society for the expression and transmission of symbolic forms is the written word — textsA premise of ETA is that texts “contain” cultural patterns and these may be discovered  through unsupervised methods21ETA Related Fields (Antecedents)22Computational Linguistics (CL)Use of computers to represent and study human languageInformation Retrieval (IR)Document summarization, retrieval, indexing, classification based on contents and metadataNatural Language Processing (NLP)Get computers to understand and produce human languageText Mining (TM)Convert text -as-unstructured -data into features for data mining + MLDigital Humanities (DH) / Humanities ComputingCreate digital collections of primary textual sources and new forms of scholarship → 1949 Father Busa's Index Thomisticus23A genealogy of ETANote that text mining (TM) and natural language processing (NLP) are not the same thingAlthough often used as synonyms, they have different concerns, approaches, and methodsThey are, however, closely related24Text Mining25Areas of FocusNLPLanguage modelsTokenizationPart of speech labelingNamed entity recognitionDependency parsing Speech generationTMText as structured dataDocument classificationContent summarizationNetwork analysisKnowledge discoveryHypothesis discoveryText MiningThe functional relationship between NLP and TM26Text Mining27Text Mining: Implementation in R\\uf0d8Couple of packages in R that specialize in using text data:\\uf076tm –fairly popular (used often with the corpus package)\\uf076quanteda –developed by political scientist to analyze politically oriented text data\\uf076Tidytext –tidyverse of text analysis –this is what we will focus on this week.\"]\n",
      "['\\uf076textmineR –developed mostly for topic modelling28Text Mining: Implementation in R\\uf0d8The first step with conducting text analysis is getting the data loaded into R so we can tokenize the dataset \\uf0d8Text data comes in a wide variety of forms and can be difficult to wrangle into a data frame.']\n",
      "['We are going to use dataset that are in CSV but note this is often not the case.']\n",
      "['\\uf0d8Tokenization means that we take a block of text and separate it into separate observations for each\\uf076word,\\uf076combination of 2, 3, or 4 words,\\uf076sentence,\\uf076or paragraph.']\n",
      "['29Text Mining: Implementation in R\\uf0d8The first step with conducting text analysis is getting the data loaded into R so we can tokenize the dataset \\uf0d8Text data comes in a wide variety of forms and can be difficult to wrangle into a data frame.']\n",
      "['We are going to use dataset that are in CSV but note this is often not the case.']\n",
      "['\\uf0d8Tokenization means that we take a block of text and separate it into separate observations for each\\uf076word,\\uf076combination of 2, 3, or 4 words,\\uf076sentence,\\uf076or paragraph.']\n",
      "['30Text Mining: Implementation in RSwitch over to R31Text Mining: Sentiment AnalysisSource: Text Mining with R32Text Mining: Sentiment AnalysisSource: Text Mining with R\\uf0d8Sentiment analysis for our purposes considers text to composed of individual words that can have positive or negative meaning.']\n",
      "['\\uf0d8The tidytext package provides access to several lexicons that can work to classify words in our documents in a variety of ways according to sentiment.']\n",
      "['Examples:\\uf076AFINN provides a scale from -5 to 5 for included words\\uf076NRC – classifies words in categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.']\n",
      "['\\uf076Bing –Straight poss or neg\\uf0d8Let’s take a look….back to R  33Text Mining: Topic ModellingSource: Laten Dirichlet Allocation in R: Martin Ponweiser\\uf0d8Next Step in the text journal is Topic Modelling\\uf0d8Topic models are “[probabilistic] latent variable models of documents that exploit the correlations among the words and latent semantic themes” ( Blei and Lafferty, 2007).']\n",
      "['\\uf0d8The name “topics” signifies the hidden, to be estimated, variable relations (=distributions)that link words in a vocabulary and their occurrence in documents.']\n",
      "['\\uf0d8Essentially think of Topic Modelling as creating clusters of words that are associated with a set of similar documents in a corpus.']\n",
      "['\\uf076As an example if you were to gather newspaper articles from across the country from three sections: Politics, Sports and Entertainment.']\n",
      "['If we ran LDA it would like classify the individual stories into these three topics.']\n",
      "['Machine Learning Overview, EDA and ClusteringBrian Wrightbrianwright@virginia.edu23\\uf06eGiven  D :{2,4,10,12,3,11,20,25,30},  and  k=2 clusters\\uf06eRandomly assign the means:  m1=3, m2=4\\uf06eK1={2,3}, K2={4,10,12,11,20,25,30}, m1=2.5, m2=16\\uf06eK1={2,3,4}, K2={10,12,11,20,25,30}, m1=3, m2=18\\uf06eK1={2,3,4,10}, K2={12,11,20,25,30}, m1=4.75, m2=19.6\\uf06eK1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\\uf06eK1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\\uf06eStop, since the clusters and the means found in all subsequent iterations will be the same .Example of K -Means1.What is Machine Learning?']\n",
      "['2.What is exploratory data analysis?']\n",
      "['3.k-means clustering–Does Congress vote in patterns?']\n",
      "['4.Multi -dimensional k -means clustering–Are NBA players compensated according to performance?Outline: Intro to Unsupervised ML4•Exploratory data analysis or “EDA” is an approach where the intent is to see what the data can tell us beyond modeling or hypothesis testing–Data visualization is one of the most common forms of EDAWhat is exploratory data analysis?']\n",
      "['5When data is too big or complex to be analyzed just by visualizing it, these types of analysis can help:1.Clustering: compare pieces of data by measuring similarity among them2.Network analysis: analyze how people, places and entities are connected to evaluate the properties and structure of a network 3.Text mining: analyze what large bodies of unstructured or structured text sayTypes of exploratory data analysis6The data inputs have (x)no target outputs (y)Unsupervised machine learningInput x:VoterOutput y:Not given(To be discovered)?']\n",
      "['We want to impose structure on the inputs (x)to say something meaningful about the data71.Technique for finding similarity between groups2.Type of unsupervised machine learning•Not the only class of unsupervised learning        algorithms3.Similarity needs to be defined•Will depend on attributes of data•Usually a distance metricWhat is clustering?']\n",
      "['8Key assumption: data points that are “closer” together are related or similar•Haimowitz and Schwarz 1997 paper on clustering for credit line optimization–http://www.aaai.org/Papers/Workshops/1997/WS-97-07/WS97- 07-006.pdf•Cluster existing GE Capital customers based on similarity in use of their credit cards to pay bills and customers’ profitability to GE Capital•Resulted in five clusters of consumer credit behavior•Created classification model to predict customer type and offer tailored productsGE Capital case study: grouping clients9Example use case General question ConceptDoes Congress vote in patterns?']\n",
      "['Is there a pattern ?']\n",
      "['Is there structure in unstructured data?k-means clusteringAre basketball players \"priced\" efficiently (based on performance)?How to uncover trends with many variables that you    can\\'t easily visualize?k-means clustering in many dimensionsConcept summary121.Data set consists of 427 members (observations) 2.Members served a full year in 20133.Three vote types:•“Aye”•“Nay”•“Other”Goal: to understand how polarized the US Congress isPolitical clusteringThe joint session of Congress on Capitol Hill in Washington13•How do we identify swing votes?']\n",
      "['–Lobbying–Bridging party lines•Assumption:–Democrats and Republicans vote among partisan lines, which generates clustersEach data point represents a member of CongressFinding voting patterns14Intra vs.']\n",
      "['inter- cluster distanceIntra-Cluster DistanceInter-Cluster DistanceObjective: minimize intra -cluster dis tance, maximize inter -cluster distance15•The centroid is the average location of all points in the cluster•Another definition: the centroid minimizes the distance between a central location and all the data points in the clusterNote: Centroids are generally not existing data points, rather locations in spacek-means clustering is based on centroids161.Randomly choose k data points to be centroids k-means in 4 steps171.Randomly choose k data points to be centroids 2.Assign each point to closest centroidk-means in 4 steps181.Randomly choose k data points to be centroids 2.Assign each point to closest centroid3.Recalculate centroids based on current cluster membershipk-means in 4 steps191.Randomly choose k data points to be centroids 2.Assign each point to closest centroid3.Recalculate centroids based on current cluster membershipk-means in 4 steps204.Repeat steps 2 -3 with the new centroids until the centroids don’t change anymoreStep 1: load packages and data# Install packagesinstall.packages(\"e1071\") install.packages(\"ggplot2\" )# Load librarieslibrary(e1071)library(ggplot2)library(help = e1071)Learn about all the functionality of the package, be well informed about what you\\'re doing!']\n",
      "['21Step 1: load packages and data# Loading house datahouse_votes_Dem = read_csv (\"house_votes_Dem.csv\")# What does the data look like?']\n",
      "['View( house_votes_Dem )Script22Step 2: run k -means# Define the columns to be clustered by subsetting the dataclust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]# Run an algorithm with 2 centersset.seed(1 )kmeans_obj_Dem = kmeans(clust_data_Dem , centers = 2, algorithm = \"Lloyd\")# What does the new variable kmeans_obj contain?kmeans_obj_Dem# View the results of each output of the kmeans # functionhead( kmeans_obj_Dem)Script1.By placing the set of data we want     after the comma, we tell R we’re   looking for columns 2.kmeans uses a different starting data point each time it runs.']\n",
      "['To make the results reproducible make R start from the same point every time with set.seed()3.We’re not specifying the number of iterations so R defaults to 104.We’ll see that kmeans produces a list    of vectors of different lengths.']\n",
      "['As a result, we cannot use the View() function23Step 2: run k -means1.Number of points each cluster contains2.The “location” of each cluster center is specified by 3 coordinates, one for each column we’re clustering3.The list assigning either cluster 1 or 2 to each data point1.79.5% of the variance between the data points is explained by our clustering, we will discuss this in detail later2.List of other types of data included in kmeans_obj24•cluster: a vector indicating the cluster to which each point is allocated•centers: a matrix of cluster centers•totss: the total sum of squares (sum of distances between all points)•withinss: vector of within -cluster sum of distances, one number per cluster•tot.withinss: total within -cluster sum of distances, i.e.']\n",
      "['sum of withinss•betweenss: the between -cluster sum of squares, i.e.']\n",
      "['totss -tot.withinss•size: the number of points in each clusterTo learn more about the kmeans function run ?kmeanskmeans outputs26Intra vs.']\n",
      "['inter- cluster distanceIntra-Cluster DistanceInter-Cluster Distancewithinssbetweensstotss = withinss +betweenss27Step 3: visualize plot# Tell R to read the cluster labels as factors so that ggplot2 (the # graphing  package) can read them as category labels instead of # continuous variables (numeric variables).party_clusters_Dem = as.factor(kmeans_obj_Dem$cluster)# What does party_clusters look like?View( party_clusters_Dem )View(as.data.frame(party_clusters_Dem))# Set up labels for our data so that we can compare Democrats and # Republicans.party_labels_Dem = house_votes_Dem$partyScript28ggplot(house_votes_Dem, aes(x = aye, y = nay,shape = party_clusters_Dem)) + geom_point(size = 6) +ggtitle(\"Aye vs.']\n",
      "['Nay votes for Democrat -introduced bills\") +xlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", labels = c(\"Cluster 1\", \"Cluster 2\" ),values = c(\"1\" , \"2\")) +theme_light()Step 3: visualize plotCosmetics layerBase layerGeom LayerTitles and axisShapeTheme29ScriptStep 3: visualize plot30•Two groups exist•Algorithm identifies voting patternsWhat can we infer about the different clusters?Step 4: analyze results31ggplot(house_votes_Dem, aes(x = yea, y = nay,color= party_labels_Dem,shape = party_clusters_Dem)) + geom_point(size = 6) +ggtitle(\"Aye vs.']\n",
      "['Nay votes for Democrat -introduced bills\") +xlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", labels = c(\"Cluster 1\", \"Cluster 2\" ),values = c(\"1\" , \"2\")) +scale_color_manual(name = \"Party\", labels = c(\"Democratic\", \"Republican\"),values = c(\"blue\" , \"red\"))+theme_light()Step 5: validate resultsCosmetics layerScriptBase layerGeom LayerTitles and axisColor and shapeTheme32Step 5: validate results33•Diffuse among Democrats•Republicans more dense•Can gauge “outliers”•Can see the polarization between the two political parties Step 6: interpret results34•Clustering is more powerful than the human eye in3D•Clustering mathematically defines which cluster the peripheral points should be in when it’s not obvious to the human eye•Clustering is helpful when many dimensions / variables exist that you can’t visualize at once–Whiskey similarity example from classification lectureClustering vs.']\n",
      "['visualizingAye, Nay and Other Votesin House of Representatives35•Goals of clustering:–Maximize the separation between clusters •i.e.']\n",
      "['Maximize inter -cluster distance –Keep similar points in a cluster close together •i.e.']\n",
      "['Minimize intra -cluster distanceHow good is the clustering?']\n",
      "['36•Look at the variance explained by clusters–In particular, the ratio of inter -cluster variance to total variance•How much of the total variance is explained by the clustering?Assessing how well an algorithm performsHow good is the clustering?']\n",
      "['Variation explained by clusters= inter-cluster variance / total variance37•cluster: a vector indicating the cluster to which each point is allocated•centers: a matrix of cluster centers•totss: the total sum of squares (sum of distances between all points)•withinss: vector of within -cluster sum of distances, one number per cluster•tot.withinss: total within -cluster sum of distances, i.e.']\n",
      "['sum of withinss•betweenss: the between -cluster sum of squares, i.e.']\n",
      "['totss -tot.withinss•size: the number of points in each clusterTo learn more about the kmeans function run ?kmeanskmeans outputs38Intra vs.']\n",
      "['inter- cluster distanceIntra-Cluster DistanceInter-Cluster Distancewithinssbetweensstotss = withinss +betweenss39How good is the clustering?']\n",
      "['# Inter-cluster variance,# \"betweenss\" is the sum of the # distances between points from # different clustersnum_Dem= kmeans_obj_Dem$betweenss# Total variance# \"totss\" is the sum of the distances  # between all the points in # the data setdenom_Dem = kmeans_obj_Dem$totss# Variance accounted for by # clustersvar_exp_Dem = num_Dem/ denom_Demvar_exp_Dem[1] 0.7193405Script40•It’s easier when the number of clusters is known ahead of time, but what if we don\\'t know how many clusters we should have?']\n",
      "['•Since different starting points may generate different clusters, we need a way to assess cluster quality as well.How do we choose the number of clusters (i.e.']\n",
      "['k)?How good is the clustering?']\n",
      "['411.Elbow method–Computes the percentage of variance explained by clusters for a range of cluster numbers–Plots a graph so results are easier to see –Not guaranteed to work!']\n",
      "['It depends on the data in question2.NbClustHow to select k: two methods–Runs 30 different tests and provides “majority vote” for the best number of clusters (k’s) to use42Elbow method: measure variance# Run algorithm with 3 centersset.seed(1 )kmeans_obj_Dem = kmeans(clust_data_Dem,   centers = 3,algorithm = \"Lloyd\")# Inter- cluster variancenum_Dem = kmeans_obj_Dem$ betweenss# Total variancedenom_Dem = kmeans_obj_Dem $totss# Variance accounted for by clustersvar_exp_Dem = num_Dem / denom_Demvar_exp_Dem[1] 0.7949741Script43•We want to repeat the variance calculation from the previous slide for several numbers of clusters automatically•We can create a function that contains all the steps we want to automate Automating a step we want to repeatfunction(data, item to iterate through)44# The function explained_variance wraps our code from previous slides.']\n",
      "['explained_variance = function( data_in, k){# Running k- means algorithmset.seed(1 )  kmeans_obj = kmeans(data_in, centers = k,algorithm = \"Lloyd\" )# Variance accounted for by clustersvar_exp = kmeans_obj $betweenss / kmeans_obj$totssvar_exp}Automating a step we want to repeatScript1.A new variable is created and set equal to our function()2.The commands inside the function are wrapped in curly braces {}3.Inside the parentheses, we specify the variables that the user will input and that will then be used inside the function where they appear45# Recall the variable we are using for the # data that we\\'re clustering.']\n",
      "['clust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]View( clust_data_Dem)# The sapply() function plugs several values # into explained_variance.']\n",
      "['explained_var_Dem = sapply(1 :10, explained_variance, data_in = clust_data_Dem)View( explained_var_Dem)# Data for ggplot2elbow_data_Dem = data.frame(k = 1:10, explained_var_Dem)View( elbow_data_Dem)Automating a step we want to repeat1.sapply() applies a function to a vector2.We have to tell sapply() that the we want the explained_variance function to use the clust_data data3.Next, we create a data frame that contains both the new variance variable (explained_var_Dem) and the different numbers of k that we used in the previous function (1 through 10)Function we created Script46# Plotting dataggplot(elbow_data_Dem, aes(x = k,  y = explained_var_Dem)) + geom_point(size = 4) +geom_line(size = 1 ) +xlab(\"k\" ) + ylab(\"Intercluster Variance/Total Variance\" ) + theme_light()Elbow method: plotting the graphScript1.geom_point() sets the size of the data points2.geom_line() sets the thickness of the line47Looking for the kink in graph of  inter- cluster variance / total varianceElbow method: measure varianceOriginal data Elbow methodk = 248•Library: \"NbClust\"Functions:  \"NbClust\"Inputs : •data –data array or data frame•min.nc / max.nc –minimum/maximum number of clusters•method –\"kmeans\"•There are other, more advanced arguments that can be customized but are outside of the scope of this course and are note necessary to for NbClust to workThere are a number of ways to choose the right k.']\n",
      "['NbClust runs 30 tests and selects k based on majority voteNbClust: k by majority voteNbClust(data, max.nc, method = \"kmeans\")49# Install the package.']\n",
      "['install.packages(\"NbClust\" )library(NbClust)# Run NbClust.']\n",
      "['nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")# View the output of NbClust.nbclust_obj_Dem# View the output that shows the number of clusters each # method recommends.View( nbclust_obj_Dem $Best.nc)NbClust : k by majority voteScript50NbClust: k by majority vote> nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")...']\n",
      "['******************************************************************* * Among all indices:                                                * 14 proposed 2 as the best number of clusters * 3 proposed 3 as the best number of clusters * 1 proposed 4 as the best number of clusters * 3 proposed 6 as the best number of clusters * 1 proposed 9 as the best number of clusters * 1 proposed 10 as the best number of clusters * 1 proposed 15 as the best number of clusters ***** Conclusion *****                            * According to the majority rule, the best number of clusters is  2Note: additional information appears; the above information is most relevant to us for nowConsole51NbClust: k by majority vote> nbclust_obj_Dem Console•nbclust_obj_Dem shows the outputs of NbClust–One of the outputs is Best.nc, which shows the number of clusters                               recommended by each test 52NbClust: k by majority vote•We want to visualize a histogram to make it obvious how many votes there are for each number of clusters 53# Subset the 1st row from Best.nc and convert it  # to a data frame, so ggplot2 can plot it.']\n",
      "['freq_k_Dem = nbclust_obj_Dem $Best.nc[1 ,]freq_k_Dem = data.frame( freq_k_Dem)View(freq_k_Dem)# Check the maximum number of clusters.']\n",
      "['max(freq_k_Dem )# Plot as a histogram.']\n",
      "['ggplot(freq_k_Dem,aes(x = freq_k_Dem)) +geom_bar() +scale_x_continuous(breaks = seq(0 , 15, by = 1)) +scale_y_continuous(breaks = seq(0 , 12, by = 1)) +labs(x = \"Number of Clusters\",y = \"Number of Votes\" ,title = \"Cluster Analysis\")NbClust: k by majority voteScript2 clusters is the winner with 12 votes54•If you’re a lobbyist, which congressperson can you influence for swing votes?']\n",
      "['•If you’re managing a campaign and your competitor is always voting along party lines, how can you use that information?']\n",
      "['•If your congressperson is not an active voter, is she representing your interests?']\n",
      "['•What do the voting patterns look like for Republican -introduced bills?Application of results55•Could see differences between the patterns of Reb lead bills and Democrat lead bills•Could provide information on congressmen that might be see has swing votes.']\n",
      "[\"Implications of results56•We are assuming that the patterns correspond with the same bills being voted on –perhaps some Congressmen have the same number of 'aye' and 'nay' votes, but voted on different bills•Network analysis can help determine additional connections between Congressmen•We haven't taken extenuating factors into account – political initiatives, current events, etc.\"]\n",
      "[\"This is a preliminary analysis that gives us initial insights and can help us direct further researchLimitations of results57•The good and bad–+ cheap –NO LABELS , labels are expensive to create and maintain–+/-clustering always works–-Many methods to choose from and knowing the right one can be nontrivial and the differences between many are almost zero, so you need to understand what you're doing•The evil–Curse of dimensionality–Clusters may result from poor data quality–Non-deterministic (e.g.\"]\n",
      "['k -means) subject to local minimum.']\n",
      "['Since it works with averages, k-means does not get much better with Big Data (marginal improvements) –Non spherical data may result in poor clustering (depending on method used)–Unequal cluster sizes may result in poor clustering (depending on method used)The good, bad, and evil5859•Analysts need to ask the following questions–Do you want overlapping or non -overlapping clusters ?']\n",
      "['–Does your data satisfy the assumptions of the clustering algorithm?']\n",
      "['–How was the distance measure identified ?']\n",
      "['–How many clusters and why ?']\n",
      "['Identifying the number of clusters is a difficult task if the number of class labels is not known beforehand –Does your method scale to the size of the data?']\n",
      "['–Is the compute time congruent with the temporal budget of your business need (i.e.']\n",
      "['do you get answers back in time to make meaningful decisions)The good, bad, and evil60Machine Learning Overview, EDA and ClusteringBrian Wrightbrianwright@virginia.edu23\\uf06eGiven  D :{2,4,10,12,3,11,20,25,30},  and  k=2 clusters\\uf06eRandomly assign the means:  m1=3, m2=4\\uf06eK1={2,3}, K2={4,10,12,11,20,25,30}, m1=2.5, m2=16\\uf06eK1={2,3,4}, K2={10,12,11,20,25,30}, m1=3, m2=18\\uf06eK1={2,3,4,10}, K2={12,11,20,25,30}, m1=4.75, m2=19.6\\uf06eK1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\\uf06eK1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\\uf06eStop, since the clusters and the means found in all subsequent iterations will be the same .Example of K -Means1.What is Machine Learning?']\n",
      "['2.What is exploratory data analysis?']\n",
      "['3.k-means clustering–Does Congress vote in patterns?']\n",
      "['4.Multi -dimensional k -means clustering–Are NBA players compensated according to performance?Outline: Intro to Unsupervised ML4•Exploratory data analysis or “EDA” is an approach where the intent is to see what the data can tell us beyond modeling or hypothesis testing–Data visualization is one of the most common forms of EDAWhat is exploratory data analysis?']\n",
      "['5When data is too big or complex to be analyzed just by visualizing it, these types of analysis can help:1.Clustering: compare pieces of data by measuring similarity among them2.Network analysis: analyze how people, places and entities are connected to evaluate the properties and structure of a network 3.Text mining: analyze what large bodies of unstructured or structured text sayTypes of exploratory data analysis6The data inputs have (x)no target outputs (y)Unsupervised machine learningInput x:VoterOutput y:Not given(To be discovered)?']\n",
      "['We want to impose structure on the inputs (x)to say something meaningful about the data71.Technique for finding similarity between groups2.Type of unsupervised machine learning•Not the only class of unsupervised learning        algorithms3.Similarity needs to be defined•Will depend on attributes of data•Usually a distance metricWhat is clustering?']\n",
      "['8Key assumption: data points that are “closer” together are related or similar•Haimowitz and Schwarz 1997 paper on clustering for credit line optimization–http://www.aaai.org/Papers/Workshops/1997/WS-97-07/WS97- 07-006.pdf•Cluster existing GE Capital customers based on similarity in use of their credit cards to pay bills and customers’ profitability to GE Capital•Resulted in five clusters of consumer credit behavior•Created classification model to predict customer type and offer tailored productsGE Capital case study: grouping clients9Example use case General question ConceptDoes Congress vote in patterns?']\n",
      "['Is there a pattern ?']\n",
      "['Is there structure in unstructured data?k-means clusteringAre basketball players \"priced\" efficiently (based on performance)?How to uncover trends with many variables that you    can\\'t easily visualize?k-means clustering in many dimensionsConcept summary121.Data set consists of 427 members (observations) 2.Members served a full year in 20133.Three vote types:•“Aye”•“Nay”•“Other”Goal: to understand how polarized the US Congress isPolitical clusteringThe joint session of Congress on Capitol Hill in Washington13•How do we identify swing votes?']\n",
      "['–Lobbying–Bridging party lines•Assumption:–Democrats and Republicans vote among partisan lines, which generates clustersEach data point represents a member of CongressFinding voting patterns14Intra vs.']\n",
      "['inter- cluster distanceIntra-Cluster DistanceInter-Cluster DistanceObjective: minimize intra -cluster dis tance, maximize inter -cluster distance15•The centroid is the average location of all points in the cluster•Another definition: the centroid minimizes the distance between a central location and all the data points in the clusterNote: Centroids are generally not existing data points, rather locations in spacek-means clustering is based on centroids161.Randomly choose k data points to be centroids k-means in 4 steps171.Randomly choose k data points to be centroids 2.Assign each point to closest centroidk-means in 4 steps181.Randomly choose k data points to be centroids 2.Assign each point to closest centroid3.Recalculate centroids based on current cluster membershipk-means in 4 steps191.Randomly choose k data points to be centroids 2.Assign each point to closest centroid3.Recalculate centroids based on current cluster membershipk-means in 4 steps204.Repeat steps 2 -3 with the new centroids until the centroids don’t change anymoreStep 1: load packages and data# Install packagesinstall.packages(\"e1071\") install.packages(\"ggplot2\" )# Load librarieslibrary(e1071)library(ggplot2)library(help = e1071)Learn about all the functionality of the package, be well informed about what you\\'re doing!']\n",
      "['21Step 1: load packages and data# Loading house datahouse_votes_Dem = read_csv (\"house_votes_Dem.csv\")# What does the data look like?']\n",
      "['View( house_votes_Dem )Script22Step 2: run k -means# Define the columns to be clustered by subsetting the dataclust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]# Run an algorithm with 2 centersset.seed(1 )kmeans_obj_Dem = kmeans(clust_data_Dem , centers = 2, algorithm = \"Lloyd\")# What does the new variable kmeans_obj contain?kmeans_obj_Dem# View the results of each output of the kmeans # functionhead( kmeans_obj_Dem)Script1.By placing the set of data we want     after the comma, we tell R we’re   looking for columns 2.kmeans uses a different starting data point each time it runs.']\n",
      "['To make the results reproducible make R start from the same point every time with set.seed()3.We’re not specifying the number of iterations so R defaults to 104.We’ll see that kmeans produces a list    of vectors of different lengths.']\n",
      "['As a result, we cannot use the View() function23Step 2: run k -means1.Number of points each cluster contains2.The “location” of each cluster center is specified by 3 coordinates, one for each column we’re clustering3.The list assigning either cluster 1 or 2 to each data point1.79.5% of the variance between the data points is explained by our clustering, we will discuss this in detail later2.List of other types of data included in kmeans_obj24•cluster: a vector indicating the cluster to which each point is allocated•centers: a matrix of cluster centers•totss: the total sum of squares (sum of distances between all points)•withinss: vector of within -cluster sum of distances, one number per cluster•tot.withinss: total within -cluster sum of distances, i.e.']\n",
      "['sum of withinss•betweenss: the between -cluster sum of squares, i.e.']\n",
      "['totss -tot.withinss•size: the number of points in each clusterTo learn more about the kmeans function run ?kmeanskmeans outputs26Intra vs.']\n",
      "['inter- cluster distanceIntra-Cluster DistanceInter-Cluster Distancewithinssbetweensstotss = withinss +betweenss27Step 3: visualize plot# Tell R to read the cluster labels as factors so that ggplot2 (the # graphing  package) can read them as category labels instead of # continuous variables (numeric variables).party_clusters_Dem = as.factor(kmeans_obj_Dem$cluster)# What does party_clusters look like?View( party_clusters_Dem )View(as.data.frame(party_clusters_Dem))# Set up labels for our data so that we can compare Democrats and # Republicans.party_labels_Dem = house_votes_Dem$partyScript28ggplot(house_votes_Dem, aes(x = aye, y = nay,shape = party_clusters_Dem)) + geom_point(size = 6) +ggtitle(\"Aye vs.']\n",
      "['Nay votes for Democrat -introduced bills\") +xlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", labels = c(\"Cluster 1\", \"Cluster 2\" ),values = c(\"1\" , \"2\")) +theme_light()Step 3: visualize plotCosmetics layerBase layerGeom LayerTitles and axisShapeTheme29ScriptStep 3: visualize plot30•Two groups exist•Algorithm identifies voting patternsWhat can we infer about the different clusters?Step 4: analyze results31ggplot(house_votes_Dem, aes(x = yea, y = nay,color= party_labels_Dem,shape = party_clusters_Dem)) + geom_point(size = 6) +ggtitle(\"Aye vs.']\n",
      "['Nay votes for Democrat -introduced bills\") +xlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", labels = c(\"Cluster 1\", \"Cluster 2\" ),values = c(\"1\" , \"2\")) +scale_color_manual(name = \"Party\", labels = c(\"Democratic\", \"Republican\"),values = c(\"blue\" , \"red\"))+theme_light()Step 5: validate resultsCosmetics layerScriptBase layerGeom LayerTitles and axisColor and shapeTheme32Step 5: validate results33•Diffuse among Democrats•Republicans more dense•Can gauge “outliers”•Can see the polarization between the two political parties Step 6: interpret results34•Clustering is more powerful than the human eye in3D•Clustering mathematically defines which cluster the peripheral points should be in when it’s not obvious to the human eye•Clustering is helpful when many dimensions / variables exist that you can’t visualize at once–Whiskey similarity example from classification lectureClustering vs.']\n",
      "['visualizingAye, Nay and Other Votesin House of Representatives35•Goals of clustering:–Maximize the separation between clusters •i.e.']\n",
      "['Maximize inter -cluster distance –Keep similar points in a cluster close together •i.e.']\n",
      "['Minimize intra -cluster distanceHow good is the clustering?']\n",
      "['36•Look at the variance explained by clusters–In particular, the ratio of inter -cluster variance to total variance•How much of the total variance is explained by the clustering?Assessing how well an algorithm performsHow good is the clustering?']\n",
      "['Variation explained by clusters= inter-cluster variance / total variance37•cluster: a vector indicating the cluster to which each point is allocated•centers: a matrix of cluster centers•totss: the total sum of squares (sum of distances between all points)•withinss: vector of within -cluster sum of distances, one number per cluster•tot.withinss: total within -cluster sum of distances, i.e.']\n",
      "['sum of withinss•betweenss: the between -cluster sum of squares, i.e.']\n",
      "['totss -tot.withinss•size: the number of points in each clusterTo learn more about the kmeans function run ?kmeanskmeans outputs38Intra vs.']\n",
      "['inter- cluster distanceIntra-Cluster DistanceInter-Cluster Distancewithinssbetweensstotss = withinss +betweenss39How good is the clustering?']\n",
      "['# Inter-cluster variance,# \"betweenss\" is the sum of the # distances between points from # different clustersnum_Dem= kmeans_obj_Dem$betweenss# Total variance# \"totss\" is the sum of the distances  # between all the points in # the data setdenom_Dem = kmeans_obj_Dem$totss# Variance accounted for by # clustersvar_exp_Dem = num_Dem/ denom_Demvar_exp_Dem[1] 0.7193405Script40•It’s easier when the number of clusters is known ahead of time, but what if we don\\'t know how many clusters we should have?']\n",
      "['•Since different starting points may generate different clusters, we need a way to assess cluster quality as well.How do we choose the number of clusters (i.e.']\n",
      "['k)?How good is the clustering?']\n",
      "['411.Elbow method–Computes the percentage of variance explained by clusters for a range of cluster numbers–Plots a graph so results are easier to see –Not guaranteed to work!']\n",
      "['It depends on the data in question2.NbClustHow to select k: two methods–Runs 30 different tests and provides “majority vote” for the best number of clusters (k’s) to use42Elbow method: measure variance# Run algorithm with 3 centersset.seed(1 )kmeans_obj_Dem = kmeans(clust_data_Dem,   centers = 3,algorithm = \"Lloyd\")# Inter- cluster variancenum_Dem = kmeans_obj_Dem$ betweenss# Total variancedenom_Dem = kmeans_obj_Dem $totss# Variance accounted for by clustersvar_exp_Dem = num_Dem / denom_Demvar_exp_Dem[1] 0.7949741Script43•We want to repeat the variance calculation from the previous slide for several numbers of clusters automatically•We can create a function that contains all the steps we want to automate Automating a step we want to repeatfunction(data, item to iterate through)44# The function explained_variance wraps our code from previous slides.']\n",
      "['explained_variance = function( data_in, k){# Running k- means algorithmset.seed(1 )  kmeans_obj = kmeans(data_in, centers = k,algorithm = \"Lloyd\" )# Variance accounted for by clustersvar_exp = kmeans_obj $betweenss / kmeans_obj$totssvar_exp}Automating a step we want to repeatScript1.A new variable is created and set equal to our function()2.The commands inside the function are wrapped in curly braces {}3.Inside the parentheses, we specify the variables that the user will input and that will then be used inside the function where they appear45# Recall the variable we are using for the # data that we\\'re clustering.']\n",
      "['clust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]View( clust_data_Dem)# The sapply() function plugs several values # into explained_variance.']\n",
      "['explained_var_Dem = sapply(1 :10, explained_variance, data_in = clust_data_Dem)View( explained_var_Dem)# Data for ggplot2elbow_data_Dem = data.frame(k = 1:10, explained_var_Dem)View( elbow_data_Dem)Automating a step we want to repeat1.sapply() applies a function to a vector2.We have to tell sapply() that the we want the explained_variance function to use the clust_data data3.Next, we create a data frame that contains both the new variance variable (explained_var_Dem) and the different numbers of k that we used in the previous function (1 through 10)Function we created Script46# Plotting dataggplot(elbow_data_Dem, aes(x = k,  y = explained_var_Dem)) + geom_point(size = 4) +geom_line(size = 1 ) +xlab(\"k\" ) + ylab(\"Intercluster Variance/Total Variance\" ) + theme_light()Elbow method: plotting the graphScript1.geom_point() sets the size of the data points2.geom_line() sets the thickness of the line47Looking for the kink in graph of  inter- cluster variance / total varianceElbow method: measure varianceOriginal data Elbow methodk = 248•Library: \"NbClust\"Functions:  \"NbClust\"Inputs : •data –data array or data frame•min.nc / max.nc –minimum/maximum number of clusters•method –\"kmeans\"•There are other, more advanced arguments that can be customized but are outside of the scope of this course and are note necessary to for NbClust to workThere are a number of ways to choose the right k.']\n",
      "['NbClust runs 30 tests and selects k based on majority voteNbClust: k by majority voteNbClust(data, max.nc, method = \"kmeans\")49# Install the package.']\n",
      "['install.packages(\"NbClust\" )library(NbClust)# Run NbClust.']\n",
      "['nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")# View the output of NbClust.nbclust_obj_Dem# View the output that shows the number of clusters each # method recommends.View( nbclust_obj_Dem $Best.nc)NbClust : k by majority voteScript50NbClust: k by majority vote> nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")...']\n",
      "['******************************************************************* * Among all indices:                                                * 14 proposed 2 as the best number of clusters * 3 proposed 3 as the best number of clusters * 1 proposed 4 as the best number of clusters * 3 proposed 6 as the best number of clusters * 1 proposed 9 as the best number of clusters * 1 proposed 10 as the best number of clusters * 1 proposed 15 as the best number of clusters ***** Conclusion *****                            * According to the majority rule, the best number of clusters is  2Note: additional information appears; the above information is most relevant to us for nowConsole51NbClust: k by majority vote> nbclust_obj_Dem Console•nbclust_obj_Dem shows the outputs of NbClust–One of the outputs is Best.nc, which shows the number of clusters                               recommended by each test 52NbClust: k by majority vote•We want to visualize a histogram to make it obvious how many votes there are for each number of clusters 53# Subset the 1st row from Best.nc and convert it  # to a data frame, so ggplot2 can plot it.']\n",
      "['freq_k_Dem = nbclust_obj_Dem $Best.nc[1 ,]freq_k_Dem = data.frame( freq_k_Dem)View(freq_k_Dem)# Check the maximum number of clusters.']\n",
      "['max(freq_k_Dem )# Plot as a histogram.']\n",
      "['ggplot(freq_k_Dem,aes(x = freq_k_Dem)) +geom_bar() +scale_x_continuous(breaks = seq(0 , 15, by = 1)) +scale_y_continuous(breaks = seq(0 , 12, by = 1)) +labs(x = \"Number of Clusters\",y = \"Number of Votes\" ,title = \"Cluster Analysis\")NbClust: k by majority voteScript2 clusters is the winner with 12 votes54•If you’re a lobbyist, which congressperson can you influence for swing votes?']\n",
      "['•If you’re managing a campaign and your competitor is always voting along party lines, how can you use that information?']\n",
      "['•If your congressperson is not an active voter, is she representing your interests?']\n",
      "['•What do the voting patterns look like for Republican -introduced bills?Application of results55•Could see differences between the patterns of Reb lead bills and Democrat lead bills•Could provide information on congressmen that might be see has swing votes.']\n",
      "[\"Implications of results56•We are assuming that the patterns correspond with the same bills being voted on –perhaps some Congressmen have the same number of 'aye' and 'nay' votes, but voted on different bills•Network analysis can help determine additional connections between Congressmen•We haven't taken extenuating factors into account – political initiatives, current events, etc.\"]\n",
      "[\"This is a preliminary analysis that gives us initial insights and can help us direct further researchLimitations of results57•The good and bad–+ cheap –NO LABELS , labels are expensive to create and maintain–+/-clustering always works–-Many methods to choose from and knowing the right one can be nontrivial and the differences between many are almost zero, so you need to understand what you're doing•The evil–Curse of dimensionality–Clusters may result from poor data quality–Non-deterministic (e.g.\"]\n",
      "['k -means) subject to local minimum.']\n",
      "['Since it works with averages, k-means does not get much better with Big Data (marginal improvements) –Non spherical data may result in poor clustering (depending on method used)–Unequal cluster sizes may result in poor clustering (depending on method used)The good, bad, and evil5859•Analysts need to ask the following questions–Do you want overlapping or non -overlapping clusters ?']\n",
      "['–Does your data satisfy the assumptions of the clustering algorithm?']\n",
      "['–How was the distance measure identified ?']\n",
      "['–How many clusters and why ?']\n",
      "['Identifying the number of clusters is a difficult task if the number of class labels is not known beforehand –Does your method scale to the size of the data?']\n",
      "['–Is the compute time congruent with the temporal budget of your business need (i.e.']\n",
      "['do you get answers back in time to make meaningful decisions)The good, bad, and evil60Machine Learning Overview, EDA and ClusteringBrian Wrightbrianwright@virginia.edu23\\uf06eGiven  D :{2,4,10,12,3,11,20,25,30},  and  k=2 clusters\\uf06eRandomly assign the means:  m1=3, m2=4\\uf06eK1={2,3}, K2={4,10,12,11,20,25,30}, m1=2.5, m2=16\\uf06eK1={2,3,4}, K2={10,12,11,20,25,30}, m1=3, m2=18\\uf06eK1={2,3,4,10}, K2={12,11,20,25,30}, m1=4.75, m2=19.6\\uf06eK1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\\uf06eK1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\\uf06eStop, since the clusters and the means found in all subsequent iterations will be the same .Example of K -Means1.What is Machine Learning?']\n",
      "['2.What is exploratory data analysis?']\n",
      "['3.k-means clustering–Does Congress vote in patterns?']\n",
      "['4.Multi -dimensional k -means clustering–Are NBA players compensated according to performance?Outline: Intro to Unsupervised ML4•Exploratory data analysis or “EDA” is an approach where the intent is to see what the data can tell us beyond modeling or hypothesis testing–Data visualization is one of the most common forms of EDAWhat is exploratory data analysis?']\n",
      "['5When data is too big or complex to be analyzed just by visualizing it, these types of analysis can help:1.Clustering: compare pieces of data by measuring similarity among them2.Network analysis: analyze how people, places and entities are connected to evaluate the properties and structure of a network 3.Text mining: analyze what large bodies of unstructured or structured text sayTypes of exploratory data analysis6The data inputs have (x)no target outputs (y)Unsupervised machine learningInput x:VoterOutput y:Not given(To be discovered)?']\n",
      "['We want to impose structure on the inputs (x)to say something meaningful about the data71.Technique for finding similarity between groups2.Type of unsupervised machine learning•Not the only class of unsupervised learning        algorithms3.Similarity needs to be defined•Will depend on attributes of data•Usually a distance metricWhat is clustering?']\n",
      "['8Key assumption: data points that are “closer” together are related or similar•Haimowitz and Schwarz 1997 paper on clustering for credit line optimization–http://www.aaai.org/Papers/Workshops/1997/WS-97-07/WS97- 07-006.pdf•Cluster existing GE Capital customers based on similarity in use of their credit cards to pay bills and customers’ profitability to GE Capital•Resulted in five clusters of consumer credit behavior•Created classification model to predict customer type and offer tailored productsGE Capital case study: grouping clients9•Between 2001 and 2004 most European countries passed legislation that allowed customers to keep their cell phone number if they switched carriers•Telenor, one of the largest telecommunications companies in Norway wanted to ensure it kept its customers–Problem: the promotions the company sent to its clients reminded them that they could leave and resulted in greater defections!']\n",
      "['–Solution: predict which customers, if contacted, are more likely to stay with the company •Results:–Marketing campaign ROI increased 11x–Customer churn decreased 36%–Marketing campaign costs decreased 40%Telenor case study: predicting behavior101.What is Machine Learning?']\n",
      "['2.What is exploratory data analysis?']\n",
      "['3.k-means clustering–Does Congress vote in patterns?']\n",
      "['4.Multi -dimensional k -means clustering –Lab –Are NBA players compensated according to performance?Outline: Intro to Unsupervised ML11Example use case General question ConceptDoes Congress vote in patterns?']\n",
      "['Is there a pattern ?']\n",
      "['Is there structure in unstructured data?k-means clusteringAre basketball players \"priced\" efficiently (based on performance)?How to uncover trends with many variables that you    can\\'t easily visualize?k-means clustering in many dimensionsConcept summary121.Data set consists of 427 members (observations) 2.Members served a full year in 20133.Three vote types:•“Aye”•“Nay”•“Other”Goal: to understand how polarized the US Congress isPolitical clusteringThe joint session of Congress on Capitol Hill in Washington13•How do we identify swing votes?']\n",
      "['–Lobbying–Bridging party lines•Assumption:–Democrats and Republicans vote among partisan lines, which generates clustersEach data point represents a member of CongressFinding voting patterns14Intra vs.']\n",
      "['inter- cluster distanceIntra-Cluster DistanceInter-Cluster DistanceObjective: minimize intra -cluster dis tance, maximize inter -cluster distance15•The centroid is the average location of all points in the cluster•Another definition: the centroid minimizes the distance between a central location and all the data points in the clusterNote: Centroids are generally not existing data points, rather locations in spacek-means clustering is based on centroids161.Randomly choose k data points to be centroids k-means in 4 steps171.Randomly choose k data points to be centroids 2.Assign each point to closest centroidk-means in 4 steps181.Randomly choose k data points to be centroids 2.Assign each point to closest centroid3.Recalculate centroids based on current cluster membershipk-means in 4 steps191.Randomly choose k data points to be centroids 2.Assign each point to closest centroid3.Recalculate centroids based on current cluster membershipk-means in 4 steps204.Repeat steps 2 -3 with the new centroids until the centroids don’t change anymoreStep 1: load packages and data# Install packagesinstall.packages(\"e1071\") install.packages(\"ggplot2\" )# Load librarieslibrary(e1071)library(ggplot2)library(help = e1071)Learn about all the functionality of the package, be well informed about what you\\'re doing!']\n",
      "['21Step 1: load packages and data# Loading house datahouse_votes_Dem = read_csv (\"house_votes_Dem.csv\")# What does the data look like?']\n",
      "['View( house_votes_Dem )Script22Step 2: run k -means# Define the columns to be clustered by subsetting the dataclust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]# Run an algorithm with 2 centersset.seed(1 )kmeans_obj_Dem = kmeans(clust_data_Dem , centers = 2, algorithm = \"Lloyd\")# What does the new variable kmeans_obj contain?kmeans_obj_Dem# View the results of each output of the kmeans # functionhead( kmeans_obj_Dem)Script1.By placing the set of data we want     after the comma, we tell R we’re   looking for columns 2.kmeans uses a different starting data point each time it runs.']\n",
      "['To make the results reproducible make R start from the same point every time with set.seed()3.We’re not specifying the number of iterations so R defaults to 104.We’ll see that kmeans produces a list    of vectors of different lengths.']\n",
      "['As a result, we cannot use the View() function23Step 2: run k -means1.Number of points each cluster contains2.The “location” of each cluster center is specified by 3 coordinates, one for each column we’re clustering3.The list assigning either cluster 1 or 2 to each data point1.79.5% of the variance between the data points is explained by our clustering, we will discuss this in detail later2.List of other types of data included in kmeans_obj24Measuring distance(3,3)(1,2) 21Distance = √(22+12)x25•cluster: a vector indicating the cluster to which each point is allocated•centers: a matrix of cluster centers•totss: the total sum of squares (sum of distances between all points)•withinss: vector of within -cluster sum of distances, one number per cluster•tot.withinss: total within -cluster sum of distances, i.e.']\n",
      "['sum of withinss•betweenss: the between -cluster sum of squares, i.e.']\n",
      "['totss -tot.withinss•size: the number of points in each clusterTo learn more about the kmeans function run ?kmeanskmeans outputs26Intra vs.']\n",
      "['inter- cluster distanceIntra-Cluster DistanceInter-Cluster Distancewithinssbetweensstotss = withinss +betweenss27Step 3: visualize plot# Tell R to read the cluster labels as factors so that ggplot2 (the # graphing  package) can read them as category labels instead of # continuous variables (numeric variables).party_clusters_Dem = as.factor(kmeans_obj_Dem$cluster)# What does party_clusters look like?View( party_clusters_Dem )View(as.data.frame(party_clusters_Dem))# Set up labels for our data so that we can compare Democrats and # Republicans.party_labels_Dem = house_votes_Dem$partyScript28ggplot(house_votes_Dem, aes(x = aye, y = nay,shape = party_clusters_Dem)) + geom_point(size = 6) +ggtitle(\"Aye vs.']\n",
      "['Nay votes for Democrat -introduced bills\") +xlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", labels = c(\"Cluster 1\", \"Cluster 2\" ),values = c(\"1\" , \"2\")) +theme_light()Step 3: visualize plotCosmetics layerBase layerGeom LayerTitles and axisShapeTheme29ScriptStep 3: visualize plot30•Two groups exist•Algorithm identifies voting patternsWhat can we infer about the different clusters?Step 4: analyze results31ggplot(house_votes_Dem, aes(x = yea, y = nay,color= party_labels_Dem,shape = party_clusters_Dem)) + geom_point(size = 6) +ggtitle(\"Aye vs.']\n",
      "['Nay votes for Democrat -introduced bills\") +xlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", labels = c(\"Cluster 1\", \"Cluster 2\" ),values = c(\"1\" , \"2\")) +scale_color_manual(name = \"Party\", labels = c(\"Democratic\", \"Republican\"),values = c(\"blue\" , \"red\"))+theme_light()Step 5: validate resultsCosmetics layerScriptBase layerGeom LayerTitles and axisColor and shapeTheme32Step 5: validate results33•Diffuse among Democrats•Republicans more dense•Can gauge “outliers”•Can see the polarization between the two political parties Step 6: interpret results34•Clustering is more powerful than the human eye in3D•Clustering mathematically defines which cluster the peripheral points should be in when it’s not obvious to the human eye•Clustering is helpful when many dimensions / variables exist that you can’t visualize at once–Whiskey similarity example from classification lectureClustering vs.']\n",
      "['visualizingAye, Nay and Other Votesin House of Representatives35•Goals of clustering:–Maximize the separation between clusters •i.e.']\n",
      "['Maximize inter -cluster distance –Keep similar points in a cluster close together •i.e.']\n",
      "['Minimize intra -cluster distanceHow good is the clustering?']\n",
      "['36•Look at the variance explained by clusters–In particular, the ratio of inter -cluster variance to total variance•How much of the total variance is explained by the clustering?Assessing how well an algorithm performsHow good is the clustering?']\n",
      "['Variation explained by clusters= inter-cluster variance / total variance37•cluster: a vector indicating the cluster to which each point is allocated•centers: a matrix of cluster centers•totss: the total sum of squares (sum of distances between all points)•withinss: vector of within -cluster sum of distances, one number per cluster•tot.withinss: total within -cluster sum of distances, i.e.']\n",
      "['sum of withinss•betweenss: the between -cluster sum of squares, i.e.']\n",
      "['totss -tot.withinss•size: the number of points in each clusterTo learn more about the kmeans function run ?kmeanskmeans outputs38Intra vs.']\n",
      "['inter- cluster distanceIntra-Cluster DistanceInter-Cluster Distancewithinssbetweensstotss = withinss +betweenss39How good is the clustering?']\n",
      "['# Inter-cluster variance,# \"betweenss\" is the sum of the # distances between points from # different clustersnum_Dem= kmeans_obj_Dem$betweenss# Total variance# \"totss\" is the sum of the distances  # between all the points in # the data setdenom_Dem = kmeans_obj_Dem$totss# Variance accounted for by # clustersvar_exp_Dem = num_Dem/ denom_Demvar_exp_Dem[1] 0.7193405Script40•It’s easier when the number of clusters is known ahead of time, but what if we don\\'t know how many clusters we should have?']\n",
      "['•Since different starting points may generate different clusters, we need a way to assess cluster quality as well.How do we choose the number of clusters (i.e.']\n",
      "['k)?How good is the clustering?']\n",
      "['411.Elbow method–Computes the percentage of variance explained by clusters for a range of cluster numbers–Plots a graph so results are easier to see –Not guaranteed to work!']\n",
      "['It depends on the data in question2.NbClustHow to select k: two methods–Runs 30 different tests and provides “majority vote” for the best number of clusters (k’s) to use42Elbow method: measure variance# Run algorithm with 3 centersset.seed(1 )kmeans_obj_Dem = kmeans(clust_data_Dem,   centers = 3,algorithm = \"Lloyd\")# Inter- cluster variancenum_Dem = kmeans_obj_Dem$ betweenss# Total variancedenom_Dem = kmeans_obj_Dem $totss# Variance accounted for by clustersvar_exp_Dem = num_Dem / denom_Demvar_exp_Dem[1] 0.7949741Script43•We want to repeat the variance calculation from the previous slide for several numbers of clusters automatically•We can create a function that contains all the steps we want to automate Automating a step we want to repeatfunction(data, item to iterate through)44# The function explained_variance wraps our code from previous slides.']\n",
      "['explained_variance = function( data_in, k){# Running k- means algorithmset.seed(1 )  kmeans_obj = kmeans(data_in, centers = k,algorithm = \"Lloyd\" )# Variance accounted for by clustersvar_exp = kmeans_obj $betweenss / kmeans_obj$totssvar_exp}Automating a step we want to repeatScript1.A new variable is created and set equal to our function()2.The commands inside the function are wrapped in curly braces {}3.Inside the parentheses, we specify the variables that the user will input and that will then be used inside the function where they appear45# Recall the variable we are using for the # data that we\\'re clustering.']\n",
      "['clust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]View( clust_data_Dem)# The sapply() function plugs several values # into explained_variance.']\n",
      "['explained_var_Dem = sapply(1 :10, explained_variance, data_in = clust_data_Dem)View( explained_var_Dem)# Data for ggplot2elbow_data_Dem = data.frame(k = 1:10, explained_var_Dem)View( elbow_data_Dem)Automating a step we want to repeat1.sapply() applies a function to a vector2.We have to tell sapply() that the we want the explained_variance function to use the clust_data data3.Next, we create a data frame that contains both the new variance variable (explained_var_Dem) and the different numbers of k that we used in the previous function (1 through 10)Function we created Script46# Plotting dataggplot(elbow_data_Dem, aes(x = k,  y = explained_var_Dem)) + geom_point(size = 4) +geom_line(size = 1 ) +xlab(\"k\" ) + ylab(\"Intercluster Variance/Total Variance\" ) + theme_light()Elbow method: plotting the graphScript1.geom_point() sets the size of the data points2.geom_line() sets the thickness of the line47Looking for the kink in graph of  inter- cluster variance / total varianceElbow method: measure varianceOriginal data Elbow methodk = 248•Library: \"NbClust\"Functions:  \"NbClust\"Inputs : •data –data array or data frame•min.nc / max.nc –minimum/maximum number of clusters•method –\"kmeans\"•There are other, more advanced arguments that can be customized but are outside of the scope of this course and are note necessary to for NbClust to workThere are a number of ways to choose the right k.']\n",
      "['NbClust runs 30 tests and selects k based on majority voteNbClust: k by majority voteNbClust(data, max.nc, method = \"kmeans\")49# Install the package.']\n",
      "['install.packages(\"NbClust\" )library(NbClust)# Run NbClust.']\n",
      "['nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")# View the output of NbClust.nbclust_obj_Dem# View the output that shows the number of clusters each # method recommends.View( nbclust_obj_Dem $Best.nc)NbClust : k by majority voteScript50NbClust: k by majority vote> nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")...']\n",
      "['******************************************************************* * Among all indices:                                                * 14 proposed 2 as the best number of clusters * 3 proposed 3 as the best number of clusters * 1 proposed 4 as the best number of clusters * 3 proposed 6 as the best number of clusters * 1 proposed 9 as the best number of clusters * 1 proposed 10 as the best number of clusters * 1 proposed 15 as the best number of clusters ***** Conclusion *****                            * According to the majority rule, the best number of clusters is  2Note: additional information appears; the above information is most relevant to us for nowConsole51NbClust: k by majority vote> nbclust_obj_Dem Console•nbclust_obj_Dem shows the outputs of NbClust–One of the outputs is Best.nc, which shows the number of clusters                               recommended by each test 52NbClust: k by majority vote•We want to visualize a histogram to make it obvious how many votes there are for each number of clusters 53# Subset the 1st row from Best.nc and convert it  # to a data frame, so ggplot2 can plot it.']\n",
      "['freq_k_Dem = nbclust_obj_Dem $Best.nc[1 ,]freq_k_Dem = data.frame( freq_k_Dem)View(freq_k_Dem)# Check the maximum number of clusters.']\n",
      "['max(freq_k_Dem )# Plot as a histogram.']\n",
      "['ggplot(freq_k_Dem,aes(x = freq_k_Dem)) +geom_bar() +scale_x_continuous(breaks = seq(0 , 15, by = 1)) +scale_y_continuous(breaks = seq(0 , 12, by = 1)) +labs(x = \"Number of Clusters\",y = \"Number of Votes\" ,title = \"Cluster Analysis\")NbClust: k by majority voteScript2 clusters is the winner with 12 votes54•If you’re a lobbyist, which congressperson can you influence for swing votes?']\n",
      "['•If you’re managing a campaign and your competitor is always voting along party lines, how can you use that information?']\n",
      "['•If your congressperson is not an active voter, is she representing your interests?']\n",
      "['•What do the voting patterns look like for Republican -introduced bills?Application of results55•Could see differences between the patterns of Reb lead bills and Democrat lead bills•Could provide information on congressmen that might be see has swing votes.']\n",
      "[\"Implications of results56•We are assuming that the patterns correspond with the same bills being voted on –perhaps some Congressmen have the same number of 'aye' and 'nay' votes, but voted on different bills•Network analysis can help determine additional connections between Congressmen•We haven't taken extenuating factors into account – political initiatives, current events, etc.\"]\n",
      "[\"This is a preliminary analysis that gives us initial insights and can help us direct further researchLimitations of results57•The good and bad–+ cheap –NO LABELS , labels are expensive to create and maintain–+/-clustering always works–-Many methods to choose from and knowing the right one can be nontrivial and the differences between many are almost zero, so you need to understand what you're doing•The evil–Curse of dimensionality–Clusters may result from poor data quality–Non-deterministic (e.g.\"]\n",
      "['k -means) subject to local minimum.']\n",
      "['Since it works with averages, k-means does not get much better with Big Data (marginal improvements) –Non spherical data may result in poor clustering (depending on method used)–Unequal cluster sizes may result in poor clustering (depending on method used)The good, bad, and evil5859•Analysts need to ask the following questions–Do you want overlapping or non -overlapping clusters ?']\n",
      "['–Does your data satisfy the assumptions of the clustering algorithm?']\n",
      "['–How was the distance measure identified ?']\n",
      "['–How many clusters and why ?']\n",
      "['Identifying the number of clusters is a difficult task if the number of class labels is not known beforehand –Does your method scale to the size of the data?']\n",
      "['–Is the compute time congruent with the temporal budget of your business need (i.e.']\n",
      "['do you get answers back in time to make meaningful decisions)The good, bad, and evil60Introduction to Decision Trees, Background and Application….Ensemble OverviewBrian WrightOutline\\uf0d8Decision Trees\\uf076Basics\\uf076Background \\uf076Advantages and Limitations\\uf076Mathematical Approaches and Example\\uf076Example in R\\uf076Evaluation2Basics: Graph Elements\\uf0d8Tree begins with a Root Node that has no incoming edges and two or more out going edges\\uf0d8Internal Node –Has one incoming edge and two or more outgoing and represent test conditions at every given level \\uf0d8Leaf Node –One incoming edge and no outgoing edges\\uf0d8Edges –Connections between nodes3Basics: Graph Example4Root NodeInternal NodesEdgesLeaf Nodes1.What is the most important question to move on to a second date?']\n",
      "['The question with the most amount of relevant information.Basics: IntuitionAre you married?What music do you like?>52.How do you combine questions?']\n",
      "['Conditional on the first answer -select the next most important question for information gain.Basics: IntuitionBelief in a blue colored sky?Are you married?']\n",
      "['YESNOStop!Are you married?']\n",
      "['YES NO>Question 1Question 2 What music do you like?']\n",
      "['6Stop!']\n",
      "['3.When should you stop asking questions?']\n",
      "['When the answer no longer provides additional relevant information.Basics: Intuition50% WILL GO ON A SECOND DATE50% WILL GO ON A SECOND DATEWhat music do you like?']\n",
      "['7\\uf0d8Step 1: Ask the question with the most amount of information, where “most amount of information ”is based on some objective criteria.']\n",
      "['\\uf0d8Step 2: Conditional on the first answer, select the next most important question.']\n",
      "['\\uf0d8Step 3: When the answer no longer provides additional information (no information gain), stop growing the branch.']\n",
      "['\\uf0d8Step 4: Repeat steps 2 and 3 for each question branch.Basics: Building a tree in four steps8Basics\\uf0d8Decision trees are a hierarchical technique \\uf076Meaning that a series of decisions are made until a predetermined metric is met\\uf0d8Model is built such that a sequence of ordered decisions concerning values of data features results in assigning class labels\\uf0d8Nonparametric\\uf076Number of parameters is not pre- determined as is the case with linear models that have pre- determine parameters thus limiting their degrees of freedom\\uf076No assumptions need to be met concerning parameters or distributions\\uf0d8Best recognized through graphs produced\\uf076Type of Acyclic graph -are used to model probabilities, connectivity, and causality.']\n",
      "['A “graph” in this sense means a structure made from nodes and edges\\uf076Trees consist of nodes and edges defined by decisions rules applied to the data features9Background10 Source: https://www.thehindu.com/features/friday -review/where -sanskrit -meets -computer -science/article7061379.eceBackground \\uf0d8Uses recursive binary splitting - Considering every possible partition of space is computationally infeasible, a greedy approach is used to divide the space.']\n",
      "['\\uf0d8Greedy algorithm because at each step of the tree building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in the future.']\n",
      "['\\uf0d8Trees can be regression or classification based, but in both instances will use recursive binary splitting\\uf076The difference is that in regression based trees we are predicting the actual class whereas in classification we are generating the probability of class inclusion as the determinate of the splitting\\uf076This probability measure that drives the splitting for classification comes in two forms: Gini Index or Entropy 11Background: CART Algorithm and C4.512\\uf0d8Classification and Regression Tree (CART) –1984 Breiman , Friedman, Olshen and Stone –Binary Trees\\uf076Can be used on numerical or categorical data\\uf076First splits the training data in two subsets using a single feature k and a threshold tk\\uf076Searches through all possible pairs (k, tk) to identify the split that produces the purest subsets, based on weighted average of information gain.']\n",
      "['\\uf076Stops once it cannot find a split that reduces impurity or by a pre -determine node size (hyperparameter ).']\n",
      "['\\uf0d8C4.5 –Grew out of ID3 (early version) in the late 1980s early 90s both from J.']\n",
      "['Ross Quinlan, uses gain ratio, accepts cont.']\n",
      "['and discrete, introduced pruning and the application of different weights to variables \\uf0d8C5.0 –Next version of C4.5 –performance improvements, computationally more efficient and allows for boostingAdvantages and Limitations13 Source: https://socialmediamanager.tweetinggoddess.com/2018/04/20/advantages -and- disadvantages -of-twitter/Advantages \\uf0d8Simple to understand and to interpret through visualization.']\n",
      "['\\uf0d8Requires little data preparation.']\n",
      "['Other techniques often require data normalization, dummy variables need to be created and blank values to be removed.']\n",
      "['\\uf0d8Able to handle both numerical and categorical data.']\n",
      "['\\uf0d8Uses a white box model.']\n",
      "['If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic.']\n",
      "['By contrast, in a black box model (some neural network approaches), results may be more difficult to interpret.']\n",
      "['\\uf0d8Fairly straight forward to evaluate and understand reliability of the model.']\n",
      "['ROC/Hit Rate/Error Rate/14Limitations\\uf0d8Decision -tree learners can create over -complex trees that do not generalize the data well.']\n",
      "['This is called overfitting .']\n",
      "['\\uf076Compare terminal nodes to data points, use the depth of the tree to calculate terminal nodes, for example 6 levels = 26 or 64 terminal nodes, if you have 100 data points that’s a lot of single data terminal nodes.']\n",
      "['Leaf nodes roughly double with every additional level of the tree.']\n",
      "['\\uf076Mechanisms such setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree can be used to avoid this problem.']\n",
      "['\\uf0d8Decision trees can be unstable as small variations in the data might result in a completely different tree being generated.']\n",
      "['This problem is mitigated by using decision trees within an ensemble like Random Forest.']\n",
      "['15Limitations\\uf0d8Practical decision -tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node.']\n",
      "['\\uf076Such algorithms cannot guarantee to return the globally optimal decision tree.']\n",
      "['This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.']\n",
      "['\\uf0d8Decision tree learners create biased trees if some classes dominate.']\n",
      "['It is therefore recommended to balance the dataset prior to fitting if necessary 16Mathematical Approaches and Examples17Mathematical Approaches: Node split criterion 18\\uf0d8Decision Trees can use several different types of node split criteria depending on the data or data scientist’s preference \\uf076Regression/MSE –Continuous data\\uf076Entropy –Binary data splits \\uf076Gini Coefficient –Most common approach\\uf0d8Let’s take a look at each approachBoth Entropy and Gini Coefficient use Information Gain to determine variable split criteria Tree Based Methods\\uf0d8Several approaches to tree building \\uf076CART –Gini Index – Binary Trees\\uf076ID3 –Information Gain  \\uf076C4.5 –Gains Ratio –Introduced pruning \\uf076C5.0 –Improvement on C4.5 –Boosting, computationally efficient 19\\uf0d8The formula for entropy is below, where Pi is the probability that a random selection would have a state i(6/6log2 6/6)  = 0 Entropy = sum( -Pi* log2Pi)= 3  = 3  = 2  = 4  = 0= 6  (2/6log2 2/6)  -(4/6log2 4/6)  = 0.92 (3/6 log2 3/6) –(3/6log2 3/6) = 1 Mathematical Approaches: Classification, Entropy (C4.5)\\uf0d8Information gain helps us understand how important an attribute is in the data\\uf0d8We can use it to decide how to order the nodes of the decision treeInformation gain = entropy (parent) –average entropy (children)-(3/6 log2 3/6) –(3/6log2 3/6) = 1-(2/2log2 2/2)= 0-(3/4 log2 3/4) –(1/4log2 1/4)= 0.81entropy (parent) average entropy (children)Mathematical Approaches: Classification, Entropy\\uf0d8In order to calculate the average entropy for the split, we need to weigh the split by the number of data points in each node.']\n",
      "['So we create a weighted average of the entropy of the children nodes.']\n",
      "['Information gain (ratio) = entropy (parent) – average entropy (children)= (2/6 * 0) + (4/6 * 0.81)= 0.54entropy (parent) Weighted average entropy (children)= 1Mathematical Approaches: Classification, Entropy\\uf0d8In order to construct the tree, we need to follow three steps:1.Choose the attribute with the highest information gain2.Construct the child nodes3.Repeat steps 1 and 2 recursively until no more information can be gainedMathematical Approaches: Information gain + entropy: exampleOutlook Temp Humidity PlaySunny Hot High NoSunny Hot Low NoCloudy Cool High YesSunny Cool Low YesShould you play outside?']\n",
      "['231.Choose the attribute with the highest information gainMathematical Approaches: Information gain + entropy: example2 yes 2 noCloudy Sunny1 yes, 2 no 1 yes-(1/3 log2 1/3) –(2/3 log2 2/3)= 0.92-(1/1 log2 1/1) = 0-(2/4 log2 2/4) –(2/4 log2 2/4) = 1I.G.']\n",
      "['= 1 –((3/4 * 0.92) + (1/4 * 0 )) = 0.31weighted averageOutlook Temp Humidity PlaySunny Hot High NoSunny Hot Low NoCloudy Cool High YesSunny Cool Low YesShould you play outside?']\n",
      "['241.Choose the attribute with the highest information gainMathematical Approaches: Information gain + entropy: example2 yes 2 noCool Hot2 no 2 yes-(2/2 log2 2/2)= 0-(2/2 log2 2/2) = 0-(2/4 log2 2/4) –(2/4 log2 2/4) = 1I.G.']\n",
      "['= 1 –((2/4 * 0) + (2/4 * 0)) = 1weighted averageOutlook Temp Humidity PlaySunny Hot High NoSunny Hot Low NoCloudy Cool High YesSunny Cool Low YesShould you play outside?']\n",
      "['0.3125I.G.']\n",
      "['1.Choose the attribute with the highest information gainMathematical Approaches: Information gain + entropy: example2 yes 2 noLow High1 yes 1 no 1 yes 1 no-(2/4 log2 2/4) –(2/4log2 2/4) = 1I.G.']\n",
      "['= 1 –((2/4 * 1) + (2/4 * 1)) = 0weighted averageOutlook Temp Humidity PlaySunny Hot High NoSunny Hot Low NoCloudy Cool High YesSunny Cool Low YesShould you play outside?']\n",
      "['0.31 1-(1/2 log2 1/2) –(1/2log2 1/2)= 1-(1/2 log2 1/2) –(1/2log2 1/2) = 126I.G.']\n",
      "['\\uf0d8Temp has the highest information gain resulting in an entropy of 1, meaning that this attribute perfectly matches class predictionMathematical Approaches: Information gain + entropy: example2 yes 2 noCool Hot2 no 2 yesOutlook Temp Humidity PlaySunny Hot High NoSunny Hot Low NoCloudy Cool High YesSunny Cool Low YesShould you play outside?']\n",
      "['0.31 1 0Play27I.G.']\n",
      "['C4.5/C5.0 and Gains Ratio C4.5/C5.0 –Uses Gain Ratio that extends the previous example but dividing the information gain by the overall split ratio for the featureGr(S,A) = G(S,A)/ Split (S,A)28Information Gain Split Information 1.Choose the attribute with the highest info gain ratio Mathematical Approaches: Information gain + entropy + gains ratio (C4.5)2 yes 2 noCloudy Sunny1 yes, 2 no 1 yes-(1/3 log2 1/3) –(2/3 log2 2/3)= 0.92-(1/1 log2 1/1) = 0-(2/4 log2 2/4) –(2/4 log2 2/4) = 1I.G.']\n",
      "['= 1 –((3/4 * 0.92) + (1/4 * 0 )) = 0.31weighted averageOutlook Temp Humidity PlaySunny Hot High NoSunny Hot Low NoCloudy Cool High YesSunny Cool Low YesShould you play outside?']\n",
      "['29Split = -3/4log23/4 –1/4log21/4 = .811 G Ratio = .31/.811 = .38 Mathematical Approaches: Classification, Gini Coefficient (CART)\\uf0d8Gini\\uf076Gini Impurity = 1 – sum[(Pi)2]\\uf076Pi –Represents the probability that a random selection would have state i(kinda like a target)\\uf076Same mathematical process as entropy \\uf0d8Example:302 yes 2 no1 yes, 2 no 1 yes1 –[ (1/3)2+ (2/3)2] = 0.44 1 –[ (1/1)2]= 01 –[ (2/4)2–(2/4)2] = 0.5Gini Impurity  = 0.5 –((3/4 * 0.44) + (1/4 * 0)) = 0.19weighted averageMathematical Approaches: Regression/MSE31\\uf0d8Works to identify the split point in the data set that minimizes mean squared error (MSE) point\\uf0d8The average of each of the groups is the term that minimizes the mean squared error\\uf076MSE –is the average of the difference between the prediction and actual values\\uf0d8The decision tree algorithm searches through all variables and all possible split points to identify the point that minimizes errorPractice…Poll  32https://www.sli.do/Use # 68881Or https://app.sli.do/event/tyl2nmrkDecision Trees: Overfitting and Hyper -parameters\\uf0d8Decision trees are often prone to overfitting, one solution is to utilize the hyper -parameters to control how the tree grows\\uf0d8Another option is to use an ensemble method via bagging or what’s known as Random Forest33Decision Trees: Hyper -parameter tuning (Pruning)\\uf0d8Minimum samples for a node split Minimum number of samples (or observations) which are required in a node to be considered for splitting.']\n",
      "['Higher values prevent a model from learning relations which might be highly specific to the particular sample.']\n",
      "['It should be tuned using cross validation.']\n",
      "['\\uf0d8Minimum samples for a terminal node (leaf) The minimum number of samples (or observations) required in a terminal node or leaf.']\n",
      "['For imbalanced class problems, a lower value should be used since regions dominant with samples belonging to minority class will be much smaller in number.']\n",
      "['\\uf0d8Maximum depth of tree (vertical depth) The maximum depth of trees, lower values prevent a model from learning relations which might be highly specific to the particular sample.']\n",
      "['It should be tuned using cross validation.']\n",
      "['34Decision Trees: Hyper -parameter tuning (Pruning)\\uf0d8Maximum number of terminal nodes Also referred as number of leaves .']\n",
      "['Since binary trees are created, a depth of nwould produce a maximum of 2^n leaves.']\n",
      "['\\uf0d8Maximum features to consider for split The number of features to consider (selected randomly) while searching for a best split.']\n",
      "['A typical value is the square root of total number of available features.']\n",
      "['A higher number typically leads to over -fitting but is dependent on the problem as well.']\n",
      "['35Cross -Validation36Cross -Validation37Decision Trees: Definitions\\uf0d8Overfitting –model becomes overly complex and as a result is predicting noise or the space between features (random error) instead of the true relationship.']\n",
      "['It is in theory possible to create a leaf node for every data point.']\n",
      "['\\uf0d8Ensemble methods – Process of running numerous models and codifying them using a decision rule to choose the optimal model result –example is majority vote on feature inclusion\\uf0d8Heuristic algorithms –approaches designed for operational efficiency generating an approximation to the ideal result but does not guarantee the best model38Ensemble MethodsA standard error is by definition the standard deviation of the sampling distribution of a parameter estimate, generated by repeated sampling.']\n",
      "['xerror reflects the mean of the sample means (of the errors) from the ten folds; xstd reflects the standard deviation of the sample means (of the errors) from the ten folds.']\n",
      "['Thus, xstd is a standard deviation of sample means, which is also known as the standard error of the mean.']\n",
      "['39Example in R421Crash Trends in Commercial Vehicles in Virginia \\uf0d8Consultants: Students of Practice and Application 4001 \\uf0d8Client: Commonwealth of Virginia- Department of Motor Vehicles \\uf0d8Description: Crashes involving commercial motor vehicles have increased substantially, especially on certain roadways, in the past five years.']\n",
      "['The Virginia Department of Motor Vehicles is interested in learning about trends in those crashes and what they can tell us about potential driver training.']\n",
      "['\\uf0d8Objective: Determine factors involved in increasing commercial motor vehicle crashes.']\n",
      "['Identify trends that can alert us to additional training needs for Commercial Drivers License holders that could then be provided to Commercial Driver Training Schools.']\n",
      "['2Group Activity 1You have 20ish minutes for this exercise.']\n",
      "['Read through the project summary again, assign a timekeeper, a note taker, and a presenter then consider and answer the following:1.What is the current situation?']\n",
      "['Would you like to know more?']\n",
      "['What would you ask your client if you could?']\n",
      "['2.Define a solid, measurable goal that you think would satisfy your client.']\n",
      "['What is your metric of success?']\n",
      "['3.What data would you like to have (think big)?']\n",
      "['How would you get this data?']\n",
      "['How would it have been gathered (sensors, cameras, etc.)?']\n",
      "['Would retrospective data likely all have been in the same spot?']\n",
      "['4.What deliverable would you like to hand over to your client at the end of the project (you can think big here –be creative)?']\n",
      "['3Stop Here for Now…Answer the Four QuestionsAmerican Pharaoh won the Triple Crown.']\n",
      "['He won the Belmont, a 12 furlong track, with a time of 2:26.65, the Kentucky Derby, a 10 furlong track, with a time of 2:03.02, and the Preakness Stakes, a 9.5 furlong track, with a time of 1.58.46.']\n",
      "['What was his average speed in mph?']\n",
      "['1.What do I know?']\n",
      "['2.What am I looking for?']\n",
      "['3.What else do I need to find it?']\n",
      "['4.What do I expect?']\n",
      "[\"Now Solve the Problem\\uf0d8Work in your Lab Teams to code the solution with an output that reads “[Horse’s Name] averaged [ x ] mph across the three triple crown races.” \\uf0d8Psuedo Code the framework of the function using a google doc then code the answer together sharing one person's screen.\"]\n",
      "['American Pharaoh won the Triple Crown.']\n",
      "['He won the Belmont, a 12 furlong track, with a time of 2:26.65, the Kentucky Derby, a 10 furlong track, with a time of 2:03.02, and the Preakness Stakes, a 9.5 furlong track, with a time of 1.58.46.']\n",
      "['What was his average speed in mph?']\n",
      "['Fun with functions and dplyrBrian Wright1/24/2020Brian Wright Fun with functions and dplyr 1/24/2020 1 / 28Overview of Functions (Advanced R)Functions are at the core of R language, it’s really a function basedlanguage“R, at its heart, is a functional language.']\n",
      "['This means that it hascertain technical properties, but more importantly that it lendsitself to a style of problem solving centred on functions.” HadleyWickhamBrian Wright Fun with functions and dplyr 1/24/2020 2 / 28What is a functional based language?']\n",
      "['Recently functions have grown in popularity because they can produceeﬃcient and simple solutions to lots of problems.']\n",
      "['Many of theproblems with performance have been solved.']\n",
      "['Functional programming compliments object oriented programmingBrian Wright Fun with functions and dplyr 1/24/2020 3 / 28What makes a programming approach “functional”?']\n",
      "['Functions can behave like any other data structure▶Assign them to variables, store to lists, pass them as aurguments toother functions, create them inside functions and even produce afunction as a result of a funcionFunctions need to be “pure” meaning that if you call it again with thesame inputs you get the same results.']\n",
      "['sys.time() not a “pure”functionThe execution of the function shouldn’t change global variables, haveno side eﬀects.']\n",
      "['Brian Wright Fun with functions and dplyr 1/24/2020 4 / 28FunctionsFunction don’t have to be “pure” but it can help to ensure your codeis doing what you intend it to do.']\n",
      "['Functional programming helps to break a problem down into it’spieces.']\n",
      "['When working to solve a problem it helps to divide the codeinto individually operating functions that solve parts of the problem.']\n",
      "['Brian Wright Fun with functions and dplyr 1/24/2020 5 / 28Types of FunctionsFigure 1: Function TypesBrian Wright Fun with functions and dplyr 1/24/2020 6 / 28Let’s Build a FunctionBasically recipes composed of series of R statementsname <- funtion (variables){#In here goes the series of R statements}Brian Wright Fun with functions and dplyr 1/24/2020 7 / 28Example, talk out the stepsmy_mean <- function (x){Sum <- sum(x)#Here we are using a function#inside a function!']\n",
      "['N <- length (x)return (Sum /N)#return is optional but helps with#clarity on some level.']\n",
      "['}Create a little list and pass it to the function and see if it works.']\n",
      "['Also call the Sum and N variables...does this work?']\n",
      "['Brian Wright Fun with functions and dplyr 1/24/2020 8 / 28Functional - Will show later, Function Factory(Advanced R)power1 <- function (exp) {function (x) {x^exp}}#Assigning the exponentialssquare <- power1 (2)cube <- power1 (3)Brian Wright Fun with functions and dplyr 1/24/2020 9 / 28Run the Created Functionssquare (3)> [1] 9cube (3)> [1] 27Brian Wright Fun with functions and dplyr 1/24/2020 10 / 28Quick ExerciseCreate a function that computes the range of a variable and thenfornogoodreasonadds100anddividesby10.']\n",
      "['Writeoutthestepsyou would need ﬁrst in Pseudocode, then develop the function.']\n",
      "['Brian Wright Fun with functions and dplyr 1/24/2020 11 / 28dplyrverbs in the tidyverseThe dplyrpackage gives us a few verbs for data manipulationFunction Purposeselect Select columns based on name or positionmutate Create or change a columnﬁlter Extract rows based on some criteriaarrange Re-order rows based on values of variable(s)group_by Split a dataset by unique values of a variablesummarize Create summary statistics based on columnsBrian Wright Fun with functions and dplyr 1/24/2020 12 / 28selectYou can select columns by name or position, of course.']\n",
      "['You can also select columns based on some criteria, which areencapsulated in functions.']\n",
      "['starts_with(“ \"), ends_with(\" ”), contains(“____”)one_of(“____”,“_____”,“______”)There are others; see help(starts_with) .']\n",
      "['Brian Wright Fun with functions and dplyr 1/24/2020 13 / 28ExampleLoad the weather.csv .']\n",
      "['This contains daily temperature data in 2010 forsome location.']\n",
      "['head (weather, 2)> # A tibble: 2 x 35> id year month element d1 d2 d3 d4 d5 d6 d7 d8> <chr> <int> <int> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>> 1 MX17~ 2010 1 tmax NA NA NA NA NA NA NA NA> 2 MX17~ 2010 1 tmin NA NA NA NA NA NA NA NA> # ...']\n",
      "['with 23 more variables: d9 <lgl>, d10 <dbl>, d11 <dbl>, d12 <lgl>,> # d13 <dbl>, d14 <dbl>, d15 <dbl>, d16 <dbl>, d17 <dbl>, d18 <lgl>,> # d19 <lgl>, d20 <lgl>, d21 <lgl>, d22 <lgl>, d23 <dbl>, d24 <lgl>,> # d25 <dbl>, d26 <dbl>, d27 <dbl>, d28 <dbl>, d29 <dbl>, d30 <dbl>,> # d31 <dbl>Brian Wright Fun with functions and dplyr 1/24/2020 14 / 28How would you just select the columns with the dailydata?']\n",
      "['select (weather, starts_with (\"d\"))Brian Wright Fun with functions and dplyr 1/24/2020 15 / 28mutatemutatecan either transform a column in place or create a new column ina datasetWe’ll use the in-built mpgdataset for this example, We’ll select only thecity and highway mileages.']\n",
      "['To use this selection later, we will need toassign it to a new namempg1 <- select (mpg, cty, hwy)Brian Wright Fun with functions and dplyr 1/24/2020 16 / 28mutateWe’ll change the city and highway mileage to km/l from mpg.']\n",
      "['This willinvolve multiplying it by 1.6 and dividing by 3.8head (mutate (mpg1, cty = cty *1.6 /3.8,hwy = hwy *1.6/3.8), 5)> # A tibble: 5 x 2> cty hwy> <dbl> <dbl>> 1 7.58 12.2> 2 8.84 12.2> 3 8.42 13.1> 4 8.84 12.6> 5 6.74 10.9This is in-place replacementBrian Wright Fun with functions and dplyr 1/24/2020 17 / 28New Variable Deﬁnedmutate (mpg1, cty1 = cty *1.6/3.8, hwy1 = hwy *1.6/3.8)> # A tibble: 234 x 4> cty hwy cty1 hwy1> <int> <int> <dbl> <dbl>> 1 18 29 7.58 12.2> 2 21 29 8.84 12.2> 3 20 31 8.42 13.1> 4 21 30 8.84 12.6> 5 16 26 6.74 10.9> 6 18 26 7.58 10.9> 7 18 27 7.58 11.4> 8 18 26 7.58 10.9> 9 16 25 6.74 10.5> 10 20 28 8.42 11.8> # ...']\n",
      "['with 224 more rowsThis creates new variablesBrian Wright Fun with functions and dplyr 1/24/2020 18 / 28ﬁlterfilterextracts rows based on criteriafilter (mpg, cyl ==4)> # A tibble: 81 x 11> manufacturer model displ year cyl trans drv cty hwy fl class> <chr> <chr> <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>> 1 audi a4 1.8 1999 4 auto~ f 18 29 p comp~> 2 audi a4 1.8 1999 4 manu~ f 21 29 p comp~> 3 audi a4 2 2008 4 manu~ f 20 31 p comp~> 4 audi a4 2 2008 4 auto~ f 21 30 p comp~> 5 audi a4 q~ 1.8 1999 4 manu~ 4 18 26 p comp~> 6 audi a4 q~ 1.8 1999 4 auto~ 4 16 25 p comp~> 7 audi a4 q~ 2 2008 4 manu~ 4 20 28 p comp~> 8 audi a4 q~ 2 2008 4 auto~ 4 19 27 p comp~> 9 chevrolet mali~ 2.4 1999 4 auto~ f 19 27 r mids~> 10 chevrolet mali~ 2.4 2008 4 auto~ f 22 30 r mids~> # ...']\n",
      "['with 71 more rowsThis extracts only 4 cylinder vehiclesOther choices might be cyl != 4 ,cyl > 4,year == 1999 ,manufacturer==\"audi\"Brian Wright Fun with functions and dplyr 1/24/2020 19 / 28Practice Pipingadmit_df <- read_csv (\"LogReg.csv\")str(admit_df)> Classes /quotesingle.ts1spec_tbl_df /quotesingle.ts1,/quotesingle.ts1tbl_df /quotesingle.ts1,/quotesingle.ts1tbl/quotesingle.ts1and /quotesingle.ts1data.frame /quotesingle.ts1: 400 obs.']\n",
      "['of 4 variables:> $ admit: num 0 1 1 1 0 1 1 0 1 0 ...']\n",
      "['> $ gre : num 380 660 800 640 520 760 560 400 540 700 ...']\n",
      "['> $ gpa : num 3.61 3.67 4 3.19 2.93 3 2.98 3.08 3.39 3.92 ...']\n",
      "['> $ rank : num 3 3 1 4 4 2 1 2 3 2 ...']\n",
      "['> - attr(*, \"spec\")=> ..']\n",
      "['cols(> ..']\n",
      "['admit = col_double(),> ..']\n",
      "['gre = col_double(),> ..']\n",
      "['gpa = col_double(),> ..']\n",
      "['rank = col_double()> ..']\n",
      "[')#Do we notice anything that seems a bit off.']\n",
      "['Brian Wright Fun with functions and dplyr 1/24/2020 20 / 28Coercion num to factoradmit_df $rank <- as.factor (admit_df $rank)#changes rank to a factorBrian Wright Fun with functions and dplyr 1/24/2020 21 / 28Five Basic Classes in Rcharacternumeric (double precision ﬂoating point numbers, default)integer (subset of numeric)complex (j = 10 + 5i)logical (True/False)Brian Wright Fun with functions and dplyr 1/24/2020 22 / 28All have coercion calls (example from: R Nuts andBolts)x <- 0 :6class (x)#why> [1] \"integer\"as.numeric (x)> [1] 0 1 2 3 4 5 6as.logical (x)> [1] FALSE TRUE TRUE TRUE TRUE TRUE TRUEas.character (x)> [1] \"0\" \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"Brian Wright Fun with functions and dplyr 1/24/2020 23 / 28Functional Example: Pass a function get a vectorWe can also convert multiple columns using lapply(), great example offunctional orientation of R.']\n",
      "['names <- c(\"admit\",\"rank\")#using names as a index on admit_df,admit_df[,names] <- lapply (admit_df[,names], factor)#Check class of those two variables(as.character (meta_fun <- lapply (subset (admit_df,select = names),class)))> [1] \"factor\" \"factor\"#using a functional with two functions inside that creates a objectcoerced to a character list...what fun.']\n",
      "['Brian Wright Fun with functions and dplyr 1/24/2020 24 / 28Using the code chunk below to “group_by” rankBrian Wright Fun with functions and dplyr 1/24/2020 25 / 28Using the code chunk below to ﬁlter by 1 in the admitcolumnBrian Wright Fun with functions and dplyr 1/24/2020 26 / 28Ok now summarise by average GPABrian Wright Fun with functions and dplyr 1/24/2020 27 / 28Now Pipe everything togetherBrian Wright Fun with functions and dplyr 1/24/2020 28 / 28R Data Vis: ggplotBrian Wright, PhDFeb, 2020Brian Wright, PhD R Data Vis: ggplot Feb, 2020 1 / 137Why visualize data?']\n",
      "['Anscombe’s data exampledataset3 dataset4dataset1 dataset25 10 15 5 10 155.07.510.012.55.07.510.012.5xyBrian Wright, PhD R Data Vis: ggplot Feb, 2020 2 / 137All Have the Summary StatsStatistic Valuemean(x) 9mean(y) 7.5var(x) 11var(y) 4.13cor(x,y) 0.82Brian Wright, PhD R Data Vis: ggplot Feb, 2020 3 / 137The DataSaurus dozenx_shapestar v_lines wide_lineshigh_lines slant_down slant_updino dots h_linesaway bullseye circleBrian Wright, PhD R Data Vis: ggplot Feb, 2020 4 / 137Same StatsStatistic Valuemean(x) 54.3mean(y) 47.8var(x) 281var(y) 725cor(x,y) -0.07Brian Wright, PhD R Data Vis: ggplot Feb, 2020 5 / 137Bottom lineSummary statistics cannot always distinguish datasetsTake advantage of humans’ ability to visually recognize and rememberpatternsFind discrepancies in the data more easilyBrian Wright, PhD R Data Vis: ggplot Feb, 2020 6 / 137What is ggplot2?']\n",
      "['A second (and ﬁnal) iteration of the ggplotImplementation of Wilkerson’s Grammar of Graphics in RConceptually, a way to layer diﬀerent elements onto a canvas to createa data visualizationStarted as Dr.']\n",
      "['Hadley Wickham’s PhD thesis (with Dr.']\n",
      "['Dianne Cook)Won the John M.']\n",
      "['Chambers Statistical Software Award in 2006Mimicked in other software platforms▶ggplotandseaborn in Python▶Translated in plotlyBrian Wright, PhD R Data Vis: ggplot Feb, 2020 7 / 137ggplot2 uses the grammar ofgraphicsA grammar .']\n",
      "['.']\n",
      "['.']\n",
      "['compose and re-use small partsbuild complex structures from simpler unitsof graphicsthink of yourself as a painterbuild a visualization using layers on a canvasdraw layers on top of each otherBrian Wright, PhD R Data Vis: ggplot Feb, 2020 8 / 137A datasetlibrary (tidyverse)library (rio)beaches <- import (/quotesingle.ts1beaches.csv /quotesingle.ts1)#> # A tibble: 6 x 12#> date year month day season rainfall temperature enterococci day_num#> <date> <int> <int> <int> <int> <dbl> <dbl> <dbl> <int>#> 1 2013-01-02 2013 1 2 1 0 23.4 6.7 2#> 2 2013-01-06 2013 1 6 1 0 30.3 2 6#> 3 2013-01-12 2013 1 12 1 0 31.4 69.1 12#> 4 2013-01-18 2013 1 18 1 0 46.4 9 18#> 5 2013-01-24 2013 1 24 1 0 27.5 33.9 24#> 6 2013-01-30 2013 1 30 1 0.6 26.6 26.5 30#> # ...']\n",
      "['with 3 more variables: month_num <int>, month_name <chr>,#> # season_name <chr>Brian Wright, PhD R Data Vis: ggplot Feb, 2020 9 / 137Building a graph: Start with a blank canvasggplot ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 10 / 137Add a data setggplot (data = beaches #<< loaded earlier)Brian Wright, PhD R Data Vis: ggplot Feb, 2020 11 / 137Add a mapping from data to elementsggplot (data = beaches,mapping = aes(# everytime we add a \"data element\"#we add a aestheticx = temperature,y = rainfall))What goes inthe x and y axesthe color of markersthe shape of markersBrian Wright, PhD R Data Vis: ggplot Feb, 2020 12 / 137020406020 30 40temperaturerainfallBrian Wright, PhD R Data Vis: ggplot Feb, 2020 13 / 137Add a geometry to drawggplot (data = beaches,mapping = aes(x = temperature,y = rainfall))+geom_point ()#?']\n",
      "['What to draw:Points, linesHistogram, bars, pies, etc.']\n",
      "['Brian Wright, PhD R Data Vis: ggplot Feb, 2020 14 / 137020406020 30 40temperaturerainfallBrian Wright, PhD R Data Vis: ggplot Feb, 2020 15 / 137Add options for the geomggplot (data = beaches,mapping = aes(x = temperature,y = rainfall))+geom_point (size = 4) #?']\n",
      "['Brian Wright, PhD R Data Vis: ggplot Feb, 2020 16 / 137020406020 30 40temperaturerainfallBrian Wright, PhD R Data Vis: ggplot Feb, 2020 17 / 137Add a mapping to modify the geomggplot (data = beaches,mapping = aes(x = temperature,y = rainfall))+geom_point (mapping = aes(color = season_name),size = 4 #Why do we need the mapping?']\n",
      "[')Brian Wright, PhD R Data Vis: ggplot Feb, 2020 18 / 137020406020 30 40temperaturerainfallseason_nameAutumnSpringSummerWinterBrian Wright, PhD R Data Vis: ggplot Feb, 2020 19 / 137Split into facetsggplot (data = beaches,mapping = aes(x = temperature,y = rainfall))+geom_point (mapping = aes(color = season_name),size = 4)+facet_wrap (~season_name) ###Brian Wright, PhD R Data Vis: ggplot Feb, 2020 20 / 137Summer WinterAutumn Spring20 30 40 20 30 4002040600204060temperaturerainfallseason_nameAutumnSpringSummerWinterBrian Wright, PhD R Data Vis: ggplot Feb, 2020 21 / 137Remove the legendggplot (data = beaches,mapping = aes(x = temperature,y = rainfall))+geom_point (mapping = aes(color = season_name),size = 4,show.legend = FALSE ###)+facet_wrap (~season_name)Brian Wright, PhD R Data Vis: ggplot Feb, 2020 22 / 137Summer WinterAutumn Spring20 30 40 20 30 4002040600204060temperaturerainfallBrian Wright, PhD R Data Vis: ggplot Feb, 2020 23 / 137Change the general themeggplot (data = beaches,mapping = aes(x = temperature,y = rainfall))+geom_point (mapping = aes(color = season_name),size = 4,show.legend = FALSE)+facet_wrap (~season_name) +theme_bw ()###Brian Wright, PhD R Data Vis: ggplot Feb, 2020 24 / 137Summer WinterAutumn Spring20 30 40 20 30 4002040600204060temperaturerainfall##Update the labelsggplot (data = beaches,mapping = aes(x = temperature,y = rainfall))+geom_point (mapping = aes(color = season_name),size = 4,show.legend = FALSE)+facet_wrap (~season_name) +theme_bw ()+labs (x = /quotesingle.ts1Temperature (C) /quotesingle.ts1, y = /quotesingle.ts1Rainfall (mm) /quotesingle.ts1)###Brian Wright, PhD R Data Vis: ggplot Feb, 2020 25 / 137Summer WinterAutumn Spring20 30 40 20 30 4002040600204060Temperature (C)Rainfall (mm)Brian Wright, PhD R Data Vis: ggplot Feb, 2020 26 / 137Add titlesggplot (data = beaches,mapping = aes(x = temperature,y = rainfall))+geom_point (mapping = aes(color = season_name),size = 4,show.legend = FALSE)+facet_wrap (~season_name) +theme_bw ()+labs (x = /quotesingle.ts1Temperature (C) /quotesingle.ts1,y = /quotesingle.ts1Rainfall (mm) /quotesingle.ts1,title = /quotesingle.ts1Sydney weather by season /quotesingle.ts1,###subtitle = \"Data from 2013 to 2018\") ###Brian Wright, PhD R Data Vis: ggplot Feb, 2020 27 / 137Summer WinterAutumn Spring20 30 40 20 30 4002040600204060Temperature (C)Rainfall (mm)Data from 2013 to 2018Sydney weather by seasonBrian Wright, PhD R Data Vis: ggplot Feb, 2020 28 / 137Customizeggplot (data = beaches,mapping = aes(x = temperature,y = rainfall))+geom_point (mapping = aes(color = season_name),size = 4,show.legend = FALSE)+facet_wrap (~season_name) +theme_bw ()+labs (x = /quotesingle.ts1Temperature (C) /quotesingle.ts1,y = /quotesingle.ts1Rainfall (mm) /quotesingle.ts1,title = /quotesingle.ts1Sydney weather by season /quotesingle.ts1,subtitle = \"Data from 2013 to 2018\") +theme (axis.title = element_text (size = 14), ###axis.text = element_text (size = 12), ###strip.text = element_text (size = 12)) ###Brian Wright, PhD R Data Vis: ggplot Feb, 2020 29 / 137Summer WinterAutumn Spring20 30 40 20 30 4002040600204060Temperature (C)Rainfall (mm)Data from 2013 to 2018Sydney weather by seasonBrian Wright, PhD R Data Vis: ggplot Feb, 2020 30 / 137The grammarDataAesthetics (or aesthetic mappings)Geometries (as layers) or Statistics (as computed layers)FacetsThemes(Coordinates)(Scales)Brian Wright, PhD R Data Vis: ggplot Feb, 2020 31 / 137Peeking under the hood ...']\n",
      "['We input the below itemsggplot (data = beaches,aes(x = temperature,y = rainfall))+geom_point ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 32 / 137What’s really run is ...']\n",
      "['ggplot (data = beaches,mapping = aes(x = temperature, y = rainfall))+layer (geom = \"point\",stat = \"identity\",position = \"identity\") +facet_null ()+theme_grey ()+coord_cartesian ()+scale_x_continuous ()+scale_y_continuous ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 33 / 137Exploring aesthetics: Mapping colorggplot (data=beaches,aes(x = date,y = log10 (enterococci),color=season_name))+geom_line ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 34 / 137−101232014 2016 2018datelog10(enterococci)season_nameAutumnSpringSummerWinterBrian Wright, PhD R Data Vis: ggplot Feb, 2020 35 / 137Adding groups to the mappingggplot (data=beaches,aes(x = date,y = log10 (enterococci),color = season_name,group = 1) ###)+geom_line ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 36 / 137−101232014 2016 2018datelog10(enterococci)season_nameAutumnSpringSummerWinterBrian Wright, PhD R Data Vis: ggplot Feb, 2020 37 / 137Fixing the legend orderingggplot (data=beaches,aes(x = date,y = log10 (enterococci),color = fct_relevel (season_name,c(/quotesingle.ts1Spring /quotesingle.ts1,/quotesingle.ts1Summer /quotesingle.ts1,/quotesingle.ts1Autumn /quotesingle.ts1,/quotesingle.ts1Winter /quotesingle.ts1)),group = 1))+geom_line ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 38 / 137Yikes!']\n",
      "['−101232014 2016 2018datelog10(enterococci)fct_relevel(season_name, c(\"Spring\", \"Summer\", \"Autumn\", \"Winter\"))SpringSummerAutumnWinterBrian Wright, PhD R Data Vis: ggplot Feb, 2020 39 / 137Fixing the legend orderingggplot (data=beaches,aes(x = date,y = log10 (enterococci),color = fct_relevel (season_name,c(/quotesingle.ts1Spring /quotesingle.ts1,/quotesingle.ts1Summer /quotesingle.ts1,/quotesingle.ts1Autumn /quotesingle.ts1,/quotesingle.ts1Winter /quotesingle.ts1)),group = 1))+geom_line ()+labs (color = /quotesingle.ts1Seasons /quotesingle.ts1)###Brian Wright, PhD R Data Vis: ggplot Feb, 2020 40 / 137−101232014 2016 2018datelog10(enterococci)SeasonsSpringSummerAutumnWinterBrian Wright, PhD R Data Vis: ggplot Feb, 2020 41 / 137You can also ﬁll based on dataggplot (data=beaches,aes(x = log10 (enterococci),fill = season_name))+geom_histogram ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 42 / 137Works a little better0102030−1 0 1 2 3log10(enterococci)countseason_nameAutumnSpringSummerWinterBrian Wright, PhD R Data Vis: ggplot Feb, 2020 43 / 137Exploring geometries: Univariate plotslibrary (tidyverse)library (rio)dat_spine <- import (/quotesingle.ts1Dataset_spine.csv /quotesingle.ts1,check.names = T)ggplot (data=dat_spine,aes(x = Degree.spondylolisthesis))+geom_histogram ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 44 / 137#> /grave.ts1stat_bin() /grave.ts1using /grave.ts1bins = 30 /grave.ts1.']\n",
      "['Pick better value with /grave.ts1binwidth /grave.ts1.']\n",
      "['0501000 100 200 300 400Degree.spondylolisthesiscountBrian Wright, PhD R Data Vis: ggplot Feb, 2020 45 / 137Histogramsggplot (data=dat_spine,aes(x = Degree.spondylolisthesis))+geom_histogram (bins = 100)Brian Wright, PhD R Data Vis: ggplot Feb, 2020 46 / 137Very diﬀerent view of the data02040600 100 200 300 400Degree.spondylolisthesiscountBrian Wright, PhD R Data Vis: ggplot Feb, 2020 47 / 137Density plotsggplot (data=dat_spine,aes(x = Degree.spondylolisthesis))+geom_density ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 48 / 1370.0000.0050.0100.0150.0200 100 200 300 400Degree.spondylolisthesisdensityBrian Wright, PhD R Data Vis: ggplot Feb, 2020 49 / 137Density plotsggplot (data=dat_spine,aes(x = Degree.spondylolisthesis))+geom_density (adjust = 1 /5)# Use 1/5 the bandwidthBrian Wright, PhD R Data Vis: ggplot Feb, 2020 50 / 1370.000.010.020.030.040 100 200 300 400Degree.spondylolisthesisdensityBrian Wright, PhD R Data Vis: ggplot Feb, 2020 51 / 137Layering geometriesggplot (data=dat_spine,aes(x = Degree.spondylolisthesis,y = stat (density)))+# Re-scales histogramgeom_histogram (bins = 100, fill= /quotesingle.ts1yellow /quotesingle.ts1)+geom_density (adjust = 1 /5, color = /quotesingle.ts1orange /quotesingle.ts1, size = 2)Brian Wright, PhD R Data Vis: ggplot Feb, 2020 52 / 1370.000.010.020.030.040 100 200 300 400Degree.spondylolisthesisdensityBrian Wright, PhD R Data Vis: ggplot Feb, 2020 53 / 137Bar plots (categorical variable)dat_brca <-rio::import (/quotesingle.ts1clinical_data_breast_cancer_modified.csv /quotesingle.ts1,check.names = T)ggplot (data=dat_brca,aes(x = Tumor))+geom_bar ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 54 / 1370204060T1 T2 T3 T4TumorcountBrian Wright, PhD R Data Vis: ggplot Feb, 2020 55 / 137Bar plots (categorical variable)ggplot (data=dat_brca,aes(x = Tumor,fill = ER.Status))+#<<geom_bar ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 56 / 137Add additional information via mapping0204060T1 T2 T3 T4TumorcountER.StatusIndeterminateNegativePositiveBrian Wright, PhD R Data Vis: ggplot Feb, 2020 57 / 137Bar plots (categorical variable)ggplot (data=dat_brca,aes(x = Tumor,fill = ER.Status))+geom_bar (position = /quotesingle.ts1dodge /quotesingle.ts1)# Default is position = \"stack\"Brian Wright, PhD R Data Vis: ggplot Feb, 2020 58 / 137Change the nature of the geometry010203040T1 T2 T3 T4TumorcountER.StatusIndeterminateNegativePositiveBrian Wright, PhD R Data Vis: ggplot Feb, 2020 59 / 137Graphing tabulated data(tabulated <- dat_brca %>% count (Tumor))#> # A tibble: 4 x 2#> Tumor n#> <chr> <int>#> 1 T1 15#> 2 T2 65#> 3 T3 19#> 4 T4 6ggplot (data = tabulated,aes(x = Tumor, y = n))+geom_bar ()#> Error: stat_count() can only have an x or y aesthetic.']\n",
      "['Brian Wright, PhD R Data Vis: ggplot Feb, 2020 60 / 137Graphing tabulated datatabulated <- dat_brca %>% count (Tumor)tabulatedggplot (data = tabulated,aes(x = Tumor, y = n))+geom_bar (stat = /quotesingle.ts1identity /quotesingle.ts1)###Here we need to change the default computationThe barplot usually computes the counts ( stat_count )We suppress that here since we have alreadydone the computationBrian Wright, PhD R Data Vis: ggplot Feb, 2020 61 / 1370204060T1 T2 T3 T4TumornBrian Wright, PhD R Data Vis: ggplot Feb, 2020 62 / 137Peeking under the hoodplt <- ggplot (data = tabulated,aes(x = Tumor, y = n))+geom_bar ()plt$layers#> [[1]]#> geom_bar: width = NULL, na.rm = FALSE, orientation = NA#> stat_count: width = NULL, na.rm = FALSE, orientation = NA#> position_stackBrian Wright, PhD R Data Vis: ggplot Feb, 2020 63 / 137Peeking under the hoodplt <- ggplot (data = tabulated,aes(x = Tumor, y = n)) +geom_bar (stat = /quotesingle.ts1identity /quotesingle.ts1)plt$layers#> [[1]]#> geom_bar: width = NULL, na.rm = FALSE, orientation = NA#> stat_identity: na.rm = FALSE#> position_stackEach layer has a geometry, statistic and position associated with itBrian Wright, PhD R Data Vis: ggplot Feb, 2020 64 / 137Bivariate plots: Scatter plotsggplot (data = beaches,aes(x = date, y = temperature))+geom_point ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 65 / 1372030402014 2016 2018datetemperatureBrian Wright, PhD R Data Vis: ggplot Feb, 2020 66 / 137Scatter plotsggplot (data = beaches,aes(x = date, y = temperature, group=1) #Add the group argu.']\n",
      "[')+geom_point (color= /quotesingle.ts1black /quotesingle.ts1, size = 3) +geom_line (color= /quotesingle.ts1red/quotesingle.ts1,size=2)Brian Wright, PhD R Data Vis: ggplot Feb, 2020 67 / 137Layer points and lines2030402014 2016 2018datetemperatureBrian Wright, PhD R Data Vis: ggplot Feb, 2020 68 / 137Scatter plotsggplot (data = beaches,aes(x = date, y = temperature,group=1))+geom_line (color= /quotesingle.ts1red/quotesingle.ts1,size=2) +###geom_point (color= /quotesingle.ts1black /quotesingle.ts1, size = 3) ###Brian Wright, PhD R Data Vis: ggplot Feb, 2020 69 / 137Order of laying down geometries matters2030402014 2016 2018datetemperatureBrian Wright, PhD R Data Vis: ggplot Feb, 2020 70 / 137Doing some computationsggplot (data = beaches,aes(x = date, y = temperature, group=1))+geom_point ()+geom_smooth (method= /quotesingle.ts1loess /quotesingle.ts1)Brian Wright, PhD R Data Vis: ggplot Feb, 2020 71 / 137Averages over 75% of the data#> /grave.ts1geom_smooth() /grave.ts1using formula /quotesingle.ts1y ~ x /quotesingle.ts12030402014 2016 2018datetemperatureBrian Wright, PhD R Data Vis: ggplot Feb, 2020 72 / 137Doing some computationsggplot (data = beaches,aes(x = date, y = temperature, group=1))+geom_point ()+geom_smooth (method=\"loess\",span = 0.1) ####?geom_smooth, kinda funny...?']\n",
      "['Brian Wright, PhD R Data Vis: ggplot Feb, 2020 73 / 137Big O!']\n",
      "['Brian Wright, PhD R Data Vis: ggplot Feb, 2020 74 / 137Averages over 10% of the data#> /grave.ts1geom_smooth() /grave.ts1using formula /quotesingle.ts1y ~ x /quotesingle.ts12030402014 2016 2018datetemperatureBrian Wright, PhD R Data Vis: ggplot Feb, 2020 75 / 137Computations over groupsggplot (data = dat_spine,aes(x = Sacral.slope,y = Degree.spondylolisthesis))+geom_point ()+geom_smooth ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 76 / 137010020030040025 50 75 100 125Sacral.slopeDegree.spondylolisthesisBrian Wright, PhD R Data Vis: ggplot Feb, 2020 77 / 137Computations over groupsggplot (data = dat_spine,aes(x = Sacral.slope,y = Degree.spondylolisthesis,color = Class.attribute) ##)+geom_point ()+geom_smooth ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 78 / 137Computation is done by groups010020030040025 50 75 100 125Sacral.slopeDegree.spondylolisthesisClass.attributeAbnormalNormalBrian Wright, PhD R Data Vis: ggplot Feb, 2020 79 / 137Computations over groupsggplot (data = dat_spine,aes(x = Sacral.slope,y = Degree.spondylolisthesis,color = Class.attribute))+geom_point ()+geom_smooth ()+xlim (0, 100) #Changing the demonsions of the graphicBrian Wright, PhD R Data Vis: ggplot Feb, 2020 80 / 137Ignore the outlier for now01002003004000 25 50 75 100Sacral.slopeDegree.spondylolisthesisClass.attributeAbnormalNormalBrian Wright, PhD R Data Vis: ggplot Feb, 2020 81 / 137Computations over groupsggplot (data = dat_spine,aes(x = Sacral.slope,y = Degree.spondylolisthesis))+geom_point ()+geom_smooth (aes(color = Class.attribute)) +#xlim (0, 100)Brian Wright, PhD R Data Vis: ggplot Feb, 2020 82 / 137Only color-code the smoothersYou can change the plot based on where you map the aesthetic01002003004000 25 50 75 100Sacral.slopeDegree.spondylolisthesisClass.attributeAbnormalNormalBrian Wright, PhD R Data Vis: ggplot Feb, 2020 83 / 137Computations over groupsggplot (data = dat_spine,aes(x = Sacral.slope,y = Degree.spondylolisthesis))+geom_point ()+geom_smooth (aes(color = Class.attribute),se = F) +#Turning off the confidence intervalxlim (0, 100)Brian Wright, PhD R Data Vis: ggplot Feb, 2020 84 / 137Looks a little cleaner01002003004000 25 50 75 100Sacral.slopeDegree.spondylolisthesisClass.attributeAbnormalNormalBrian Wright, PhD R Data Vis: ggplot Feb, 2020 85 / 137Box Plotsggplot (data = beaches,aes(x = season_name,y = temperature))+geom_boxplot ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 86 / 137203040Autumn Spring Summer Winterseason_nametemperature>Whatare the components of a boxplot?']\n",
      "['Brian Wright, PhD R Data Vis: ggplot Feb, 2020 87 / 137Box Plotsggplot (data = beaches,aes(x = season_name,y = temperature))+geom_boxplot ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 88 / 137MinimumMaximum (within fence)3rd quartile1st quartileMedian\"Outliers\"203040Autumn Spring Summer Winterseason_nametemperatureBrian Wright, PhD R Data Vis: ggplot Feb, 2020 89 / 137Layers, againggplot (data = beaches,aes(x = season_name,y = temperature))+geom_boxplot ()+geom_jitter (width = 0.2)Brian Wright, PhD R Data Vis: ggplot Feb, 2020 90 / 137Layers, again203040Autumn Spring Summer Winterseason_nametemperature##Layers, againggplot (data = beaches,aes(x = season_name,y = temperature))+geom_violin ()+geom_jitter (width = 0.2)Brian Wright, PhD R Data Vis: ggplot Feb, 2020 91 / 137203040Autumn Spring Summer Winterseason_nametemperatureBrian Wright, PhD R Data Vis: ggplot Feb, 2020 92 / 137Exploring grouped dataggplot (data = beaches,aes(x = temperature,fill = season_name))+geom_density ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 93 / 137Not very useful0.000.050.100.1520 30 40temperaturedensityseason_nameAutumnSpringSummerWinterBrian Wright, PhD R Data Vis: ggplot Feb, 2020 94 / 137Overlaying graphsggplot (data = beaches,aes(x = temperature,fill = season_name))+geom_density (alpha = 0.4) # Changes the transparencyBrian Wright, PhD R Data Vis: ggplot Feb, 2020 95 / 137Make graphs more transparent0.000.050.100.1520 30 40temperaturedensityseason_nameAutumnSpringSummerWinterBrian Wright, PhD R Data Vis: ggplot Feb, 2020 96 / 137Exploding graphsggplot (data = beaches,aes(x = temperature,fill = season_name))+geom_density ()+facet_wrap (~season_name, ncol = 1) ###Brian Wright, PhD R Data Vis: ggplot Feb, 2020 97 / 137This is called “small multiples” (Tufte)WinterSummerSpringAutumn20 30 400.000.050.100.150.000.050.100.150.000.050.100.150.000.050.100.15temperaturedensityseason_nameAutumnSpringSummerWinter>Notice that all the graphs have the same x-axis.']\n",
      "['This is a good thingBrian Wright, PhD R Data Vis: ggplot Feb, 2020 98 / 137Exploding graphsggplot (data = beaches,aes(x = temperature,fill = season_name))+geom_density ()+facet_wrap (~season_name, nrow = 1) ###Brian Wright, PhD R Data Vis: ggplot Feb, 2020 99 / 137We can arrange them the other way tooAutumn Spring Summer Winter20 30 40 20 30 40 20 30 40 20 30 400.000.050.100.15temperaturedensityseason_nameAutumnSpringSummerWinterBrian Wright, PhD R Data Vis: ggplot Feb, 2020 100 / 137Order and orientation: Arrests in the USA in 1973arrests <- import (/quotesingle.ts1USArrests.csv /quotesingle.ts1)ggplot (data = arrests,aes(x = State,y = Murder))+geom_bar (stat = /quotesingle.ts1identity /quotesingle.ts1)Brian Wright, PhD R Data Vis: ggplot Feb, 2020 101 / 137051015AlabamaAlaskaArizonaArkansasCaliforniaColoradoConnecticutDelawareFloridaGeorgiaHawaiiIdahoIllinoisIndianaIowaKansasKentuckyLouisianaMaineMarylandMassachusettsMichiganMinnesotaMississippiMissouriMontanaNebraskaNevadaNew HampshireNew JerseyNew MexicoNew YorkNorth CarolinaNorth DakotaOhioOklahomaOregonPennsylvaniaRhode IslandSouth CarolinaSouth DakotaTennesseeTexasUtahVermontVirginiaWashingtonWest VirginiaWisconsinWyomingStateMurder>Hardto read, there is no ordering, and x-labels can’t be seenBrian Wright, PhD R Data Vis: ggplot Feb, 2020 102 / 137Arrests in the USA in 1973ggplot (data = arrests,aes(x = fct_reorder (State, Murder), #Order by murder ratey = Murder))+geom_bar (stat = /quotesingle.ts1identity /quotesingle.ts1)Brian Wright, PhD R Data Vis: ggplot Feb, 2020 103 / 137Arrest in the USA in 1973051015North DakotaMaineNew HampshireIowaVermontIdahoWisconsinMinnesotaUtahConnecticutRhode IslandSouth DakotaWashingtonNebraskaMassachusettsOregonHawaiiWest VirginiaDelawareKansasMontanaPennsylvaniaOklahomaWyomingIndianaOhioNew JerseyColoradoArizonaVirginiaArkansasCaliforniaMissouriKentuckyAlaskaIllinoisNew YorkMarylandNew MexicoMichiganNevadaTexasNorth CarolinaAlabamaTennesseeSouth CarolinaFloridaLouisianaMississippiGeorgiafct_reorder(State, Murder)MurderBrian Wright, PhD R Data Vis: ggplot Feb, 2020 104 / 137Arrests in the USA in 1973ggplot (data = arrests,aes(x = fct_reorder (State, Murder), # Order by murder ratey = Murder))+geom_bar (stat = /quotesingle.ts1identity /quotesingle.ts1)+coord_flip ()#Flipping the coordinatesBrian Wright, PhD R Data Vis: ggplot Feb, 2020 105 / 137Flipping the axes makes the states readableNorth DakotaMaineNew HampshireIowaVermontIdahoWisconsinMinnesotaUtahConnecticutRhode IslandSouth DakotaWashingtonNebraskaMassachusettsOregonHawaiiWest VirginiaDelawareKansasMontanaPennsylvaniaOklahomaWyomingIndianaOhioNew JerseyColoradoArizonaVirginiaArkansasCaliforniaMissouriKentuckyAlaskaIllinoisNew YorkMarylandNew MexicoMichiganNevadaTexasNorth CarolinaAlabamaTennesseeSouth CarolinaFloridaLouisianaMississippiGeorgia0 5 10 15Murderfct_reorder(State, Murder)Brian Wright, PhD R Data Vis: ggplot Feb, 2020 106 / 137Arrests in the USA in 1973ggplot (data = arrests,aes(x = fct_reorder (State, Murder), # Order by murder ratey = Murder))+geom_bar (stat = /quotesingle.ts1identity /quotesingle.ts1, fill=\"red\") +labs (x = /quotesingle.ts1State /quotesingle.ts1, y = /quotesingle.ts1Murder rate /quotesingle.ts1)+# Adding labelstheme_bw ()+# Themetheme (panel.grid.major.y = element_blank (),#panel.grid.minor.x = element_blank ()) +coord_flip ()# Flip lastBrian Wright, PhD R Data Vis: ggplot Feb, 2020 107 / 137Cleaning it up a littleNorth DakotaMaineNew HampshireIowaVermontIdahoWisconsinMinnesotaUtahConnecticutRhode IslandSouth DakotaWashingtonNebraskaMassachusettsOregonHawaiiWest VirginiaDelawareKansasMontanaPennsylvaniaOklahomaWyomingIndianaOhioNew JerseyColoradoArizonaVirginiaArkansasCaliforniaMissouriKentuckyAlaskaIllinoisNew YorkMarylandNew MexicoMichiganNevadaTexasNorth CarolinaAlabamaTennesseeSouth CarolinaFloridaLouisianaMississippiGeorgia0 5 10 15Murder rateStateBrian Wright, PhD R Data Vis: ggplot Feb, 2020 108 / 137Themesggplot comes with a default color scheme.']\n",
      "['There are several other schemesavailablescale_*_brewer uses the ColorBrewer palettesscale_*_gradient uses gradientsscale_*_distill uses the ColorBrewer palettes, for continuousoutcomesHere * can be color orfill , depending on what you want tocolorNote color refers to the outline, and fill refers to the insideBrian Wright, PhD R Data Vis: ggplot Feb, 2020 109 / 137No Themeggplot (data = beaches,aes(x = date, y = enterococci,color = temperature))+geom_point ()+scale_y_log10 (name = /quotesingle.ts1Enterococci /quotesingle.ts1,label = scales ::number_format (digits=3))Brian Wright, PhD R Data Vis: ggplot Feb, 2020 110 / 137No Theme0.101.0010.00100.001 000.002014 2016 2018dateEnterococci203040temperature##Darkggplot (data = beaches,aes(x = date, y = enterococci,color = temperature))+geom_point ()+scale_y_log10 (name = /quotesingle.ts1Enterococci /quotesingle.ts1,label = scales ::number_format (digits=3)) +scale_color_gradient (low = /quotesingle.ts1white /quotesingle.ts1, high= /quotesingle.ts1red/quotesingle.ts1)+theme_dark ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 111 / 137Dark0.101.0010.00100.001 000.002014 2016 2018dateEnterococci203040temperature##Black and Whiteggplot (data = beaches,aes(x = date, y = enterococci,color = temperature))+geom_point ()+scale_y_log10 (name = /quotesingle.ts1Enterococci /quotesingle.ts1,label = scales ::number_format (digits=3)) +scale_color_gradient (low = /quotesingle.ts1blue /quotesingle.ts1, high= /quotesingle.ts1red/quotesingle.ts1)+theme_bw ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 112 / 137Black and White0.101.0010.00100.001 000.002014 2016 2018dateEnterococci203040temperatureBrian Wright, PhD R Data Vis: ggplot Feb, 2020 113 / 137ThemesYou can create your own custom themes to keep a uniﬁed look to yourgraphsggplot comes withtheme_classictheme_bwtheme_voidtheme_darktheme_graytheme_lighttheme_minimalBrian Wright, PhD R Data Vis: ggplot Feb, 2020 114 / 137Create your ownggplot (data = dat_spine,aes(x = Sacral.slope, y = Degree.spondylolisthesis,color = Class.attribute))+geom_point ()+geom_smooth (se = F) +coord_cartesian (xlim = c(0, 100),ylim = c(0,200))Brian Wright, PhD R Data Vis: ggplot Feb, 2020 115 / 1370501001502000 25 50 75 100Sacral.slopeDegree.spondylolisthesisClass.attributeAbnormalNormalBrian Wright, PhD R Data Vis: ggplot Feb, 2020 116 / 137Create your ownmy_theme <- function (){theme_bw ()}ggplot (data = dat_spine,aes(x = Sacral.slope, y = Degree.spondylolisthesis,color = Class.attribute))+geom_point ()+geom_smooth (se = F) +coord_cartesian (xlim = c(0, 100),ylim = c(0,200)) +my_theme ()# Just Black and WhiteBrian Wright, PhD R Data Vis: ggplot Feb, 2020 117 / 1370501001502000 25 50 75 100Sacral.slopeDegree.spondylolisthesisClass.attributeAbnormalNormalBrian Wright, PhD R Data Vis: ggplot Feb, 2020 118 / 137Create your ownmy_theme <- function (){theme_bw ()+theme (axis.text = element_text (size = 14),axis.title = element_text (size = 16),panel.grid.minor = element_blank (),strip.text = element_text (size=14),strip.background = element_blank ())}ggplot (data = dat_brca,aes(x = Tumor)) +geom_bar ()+facet_grid (rows = vars (ER.Status),cols = vars (PR.Status))Brian Wright, PhD R Data Vis: ggplot Feb, 2020 119 / 137Negative PositiveIndeterminate Negative PositiveT1 T2 T3 T4 T1 T2 T3 T4010203001020300102030TumorcountBrian Wright, PhD R Data Vis: ggplot Feb, 2020 120 / 137Create your ownggplot (data = dat_brca,aes(x = Tumor))+geom_bar ()+facet_grid (rows = vars (ER.Status),cols = vars (PR.Status)) +my_theme ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 121 / 137Negative PositiveIndeterminate Negative PositiveT1 T2 T3 T4 T1 T2 T3 T4010203001020300102030TumorcountBrian Wright, PhD R Data Vis: ggplot Feb, 2020 122 / 137Annotations: Stand-alone storiesData visualization to stand on its ownRelevant information should be placed on the graphHowever, you need to balance the information content with real estate▶Don’t clutter the graph and make it not readableBrian Wright, PhD R Data Vis: ggplot Feb, 2020 123 / 137Adding derived statistics to a plot: Group meansggplot (iris,aes(x = Sepal.Length,y = Sepal.Width,color = Species))+geom_point ()+theme_bw ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 124 / 137Adding derived statistics to a plot: Group means2.02.53.03.54.04.55 6 7 8Sepal.LengthSepal.WidthSpeciessetosaversicolorvirginicaBrian Wright, PhD R Data Vis: ggplot Feb, 2020 125 / 137Adding derived statistics to a plot: Group meansmeans <- iris %>% group_by (Species) %>%summarize_at (vars (starts_with (/quotesingle.ts1Sepal /quotesingle.ts1)),mean)means#> # A tibble: 3 x 3#> Species Sepal.Length Sepal.Width#> <fct> <dbl> <dbl>#> 1 setosa 5.01 3.43#> 2 versicolor 5.94 2.77#> 3 virginica 6.59 2.97Brian Wright, PhD R Data Vis: ggplot Feb, 2020 126 / 137Adding derived statistics to a plot: Group meansggplot (iris,aes(x = Sepal.Length,y = Sepal.Width,color = Species))+geom_point ()+geom_point (data = means,size=5) +theme_bw ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 127 / 137Adding derived statistics to a plot: Group means2.02.53.03.54.04.55 6 7 8Sepal.LengthSepal.WidthSpeciessetosaversicolorvirginicaBrian Wright, PhD R Data Vis: ggplot Feb, 2020 128 / 137Adding regression metricsRegress highway mileage on city mileage (data: mpg)mod1 <- lm(hwy ~cty, data = mpg)r2 <- broom ::glance (mod1) %>% pull (r.squared)ggplot (mpg,aes(x = cty, y = hwy))+geom_point ()+geom_smooth (method = /quotesingle.ts1lm/quotesingle.ts1, se=F) +theme_bw ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 129 / 137Adding regression metrics#> /grave.ts1geom_smooth() /grave.ts1using formula /quotesingle.ts1y ~ x /quotesingle.ts120304010 15 20 25 30 35ctyhwyBrian Wright, PhD R Data Vis: ggplot Feb, 2020 130 / 137Regress highway mileage on city mileagemod1 <- lm(hwy ~cty, data = mpg)r2 <- broom ::glance (mod1) %>% pull (r.squared) %>%#pull is part of dplyrround (., 2) #part of base R, rounding behind the .']\n",
      "['by 2ggplot (mpg,aes(x = cty, y = hwy)) +geom_point ()+geom_smooth (method = /quotesingle.ts1lm/quotesingle.ts1, se=F) +annotate (geom= /quotesingle.ts1text /quotesingle.ts1,x = 15, y = 40,label=glue ::glue (\"R^2 == {r}\",r=r2),size=6,parse=T) +theme_bw ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 131 / 137Glance creates a quick summary of the modelbroom ::glance (mod1)#> # A tibble: 1 x 12#> r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC#> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>#> 1 0.914 0.913 1.75 2459.']\n",
      "['1.87e-125 1 -462.']\n",
      "['931.']\n",
      "['941.']\n",
      "['#> # ...']\n",
      "['with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>Brian Wright, PhD R Data Vis: ggplot Feb, 2020 132 / 137Nice Addition#> /grave.ts1geom_smooth() /grave.ts1using formula /quotesingle.ts1y ~ x /quotesingle.ts1R2=0.9120304010 15 20 25 30 35ctyhwyBrian Wright, PhD R Data Vis: ggplot Feb, 2020 133 / 137Highlighting regionsmpg %>%mutate (cyl = as.factor (cyl)) %>%ggplot (aes(x = cyl, y = hwy))+geom_boxplot ()+theme_bw ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 134 / 137Highlight regions2030404 5 6 8cylhwyBrian Wright, PhD R Data Vis: ggplot Feb, 2020 135 / 137Highlighting regionsmpg %>%mutate (cyl = as.factor (cyl)) %>%ggplot (aes(x = cyl, y = hwy))+geom_boxplot ()+theme_bw ()+annotate (geom = /quotesingle.ts1rect /quotesingle.ts1,xmin=3.75,xmax=4.25,ymin = 22, ymax = 28,fill = /quotesingle.ts1red/quotesingle.ts1,alpha = 0.2) +annotate (/quotesingle.ts1text /quotesingle.ts1,x = 4.5, y = 25,label = /quotesingle.ts1Outliers?']\n",
      "['/quotesingle.ts1,hjust = 0) +coord_cartesian (xlim = c(0,5)) +theme_bw ()Brian Wright, PhD R Data Vis: ggplot Feb, 2020 136 / 137Highlighting regionsOutliers?']\n",
      "['2030404 5 6 8cylhwy################EXAMPLES for inCLASS##################Brian Wright, PhD R Data Vis: ggplot Feb, 2020 137 / 137Markdown and the knitr Package in RPractice of Data ScienceCommunicating with CodeReproducible ResearchBaggerly and Coombes’ Replication WorkA Strategy for Reproducible ResearchThe Markdown LanguageChoosing an Output and CompilingThe HeaderPublishing the HTML OutputSyntaxBold, Italics, etc.']\n",
      "['SectioningTabbingEquationsListsTables (by hand)Graphics (from external ﬁles)Embedding CodeIn-line Code for Display OnlyIn-line Code that gets EvaluatedCode ChunksCode Chunk OptionsCachingTables and Figures (from code)Baggerly and CoombesThere’s a really infamous talk from 2010, calledThe Importance of Reproducible Research in High-ThroughputBiology: Case Studies in Forensic Bioinformatics :https://www.youtube.com/watch?v=7gYIs7uYbMoby Keith Baggerly and Kevin R.']\n",
      "['Coombes.']\n",
      "['Their talk illustrates what can go seriously wrong when peopleworking with data are not transparent about how they get theirresults.']\n",
      "['Baggerly and CoombesBaggerly and Coombes are bioinformaticians who study cancer.']\n",
      "['They attempted to replicate this study:This paper claims to show how treatments for childhood leukemiashould be tailored to patients based on the speciﬁc genomicinformation in the patient’s DNA.']\n",
      "['This was an important ﬁnding and was published in a prominentjournal.']\n",
      "['Baggerly and CoombesIn order to replicate the study, Baggerly and Coombes started withthe same raw dataset that the study’s authors used.']\n",
      "['Their goalwas to use the data to reproduce the study’s results.']\n",
      "['But the problem was that the authors did not provide anyscripts or discussion of what they did to the data prior to runningthe tests.']\n",
      "['As a result, Baggerly and Coombes had to reproduce the results ina “forensic” way: ﬁguring out after-the-fact what the authors musthave done to get these results.']\n",
      "['Baggerly and CoombesEventually, they were able to reproduce the results, but found theauthors had made two errors:1.']\n",
      "['The rows in the data refer to particular genes.']\n",
      "['Someonecopy-and-pasted the cells in a way that left oﬀ the column headers.']\n",
      "['That created an oﬀ-by-one error where the data for a gene werelisted one row above the gene name.']\n",
      "['2.']\n",
      "['The treatment was coded as 1 or 2, but someone along the wayconfused what 1 and 2 meant.']\n",
      "['So the treated patients werereported as control, and vice versa.']\n",
      "['That means that the reported positive eﬀect for the treatmentgroup is actually a positive eﬀect for the control group.']\n",
      "['In otherwords, the treatment harms people .']\n",
      "['Baggerly and CoombesBy the time Baggerly and Coombes made this discovery, theresearch had moved forward to the stage of clinical drug trials.']\n",
      "['Children with leukemia were being given harmful treatmentsbased on mistaken research.']\n",
      "['In his talk, Baggerly describes all the ways that he and Coombestried to sound the alarm on this research.']\n",
      "['But the stakeholderswere reluctant to retract and end the research because of theimplications for reputation and grant money.']\n",
      "['The study continued for many months.']\n",
      "['It was only stopped whenit was revealed that the principal investigator on the original studyhad lied on his CV about being a Rhodes Scholar.']\n",
      "['Baggerly and CoombesSo, what exactly went so wrong here?']\n",
      "['1.']\n",
      "['Dumb, typical mistakes that people make with spreadsheets allthe time (the oﬀ-by-one error, confusing the 1s and 2s).']\n",
      "['2.']\n",
      "['Laziness: no eﬀort to document the steps that were taken toprepare the data for analysis.']\n",
      "['3.']\n",
      "['Self-interest and ego: covering up mistakes instead of risking thepenalties of correcting them, thereby making the mistakes worse.']\n",
      "['4.']\n",
      "['Magical thinking: because the work involves data, there’s atendency by most people to simply believe that the work is correctwithout digging in to it (not Baggerly and Coombes though!)Baggerly and CoombesIf we are going to be working with data, how can we avoid themistakes that Baggerly and Coombes discovered?']\n",
      "['1.']\n",
      "['Mistakes are inevitable.']\n",
      "['But, if we use code instead ofpoint-and-clicking, it’s easier to see mistakes and to go back andcorrect them.']\n",
      "['2.']\n",
      "['If we document our steps as we go along, we’ll be transparentand able to show anyone exactly what we did with the data.']\n",
      "['3.']\n",
      "['We can feed our egos in a diﬀerent way: clear and professionaldocumentation looks impressive to others.']\n",
      "['4.']\n",
      "['Working with code and explaining what each part of the codedoes goes a long way towards dispelling the anxiety people haveabout data, and overcomes magical thinking.']\n",
      "['Reproducible ResearchOur goal : to give you the skills and practice you need to work withdata in a way that▶Is easy to document▶Allows you to combine code, results, and text to betterconvey the context of what you are doing▶Looks really goodThis morning we will discuss R markdown, one of the best tools wehave for conducting transparent research.']\n",
      "['This afternoon we will walk through an entire research pipelineusing R markdown, documenting everything we need to do to rawdata to prepare it for analysis, and including the ﬁnal results in thedocument.']\n",
      "['Reproducible ResearchReproducibility — the ability to get the same research results as anoriginal study by using the raw data and computer code providedby researchers.']\n",
      "['There have been serious crises in many ﬁelds, including medicine,economics, and political science, because researchers failed tomake their code and data available.']\n",
      "['We are going to learn how to make a document that shows ALLthe steps involved in a project, going from raw data to ﬁnalresults .']\n",
      "['We also want this document to be as easy to read andunderstand as possible.']\n",
      "['Practice on your own computer as we discuss the steps forcreating a markdown document.']\n",
      "['There are two ways to save R codeThe ﬁrst way is to save code in an R script .']\n",
      "['▶Pros : Scripts are easy to work with, run, save, and share.']\n",
      "['▶Cons : Scripts can be hard for even an advanced R user toread and understand.']\n",
      "['The second way is with an R markdown document compiled withtheknitr package.']\n",
      "['(If you’ve never used knitr before, install itby typing install.packages(\"knitr\") into the console.)▶Pros : Creates a beautiful, readable document by placing text,code, and the output of the code all in the same document(this is also called weaving: hence the name knitr ).']\n",
      "['Able tocreate HTML, PDF, or Word ﬁles.']\n",
      "['▶Cons : More syntax to learn in addition to R code.']\n",
      "['Might takea while to compile documents.']\n",
      "['The Markdown LanguageYou need to become experienced using both methods to savecode.']\n",
      "['They are each useful in diﬀerent situations.']\n",
      "['Markdown is a programming language for formatting text, usingminimal programming syntax.']\n",
      "['It’s basically a lightwight version ofHTML code.']\n",
      "['It’s not just for R — it’s for anything that involvestext together with some other kind of code.']\n",
      "['To start a markdown ﬁle : open R Studio, click File, then New File,then R Markdown.']\n",
      "['Give the document a title and author, andchoose whether you want this code to produce an HTML, PDF, orWord ﬁle.']\n",
      "['This will call up an example page with some text and code alreadyin it.']\n",
      "['(You will end up deleting this example text and code andwriting your own.)Choosing an Output and CompilingWhile R scripts are saved as “ .R” ﬁles, these markdown ﬁles aresaved as “ .Rmd ” ﬁles.']\n",
      "['This ﬁle is separate from the HTML, PDF,or Word document you will create with this code.']\n",
      "['First notice the “ Knit ” button.']\n",
      "['This button compiles thedocument – produces the desired output.']\n",
      "['To change the type ofoutput, click the arrow to the right of Knit.']\n",
      "['At the top of the .Rmd ﬁle, there’s code separated on the top andbottom by three dashes.']\n",
      "['This is the header.']\n",
      "['You can change thetitle, author, etc.']\n",
      "['here.']\n",
      "['You can also change the output here:▶For HTML output, output: html document▶For PDF output, output: pdf document▶For Word output, output: word documentUsing the Header to Set OptionsTo create a table of contents that lists up to 5 levels of sectioning:output:html_document:toc: truetoc_depth: 5By default the table of contents appears at the top of thedocument, just under the title.']\n",
      "['But, you can also use a ﬂoatingand collapsable table of contents window like this:output:html_document:toc: truetoc_depth: 5toc_float: truetoc_collapsed: trueUsing the Header to Set OptionsImportant : You will see three dashes --- at the beginning andend of the header.']\n",
      "['Do not delete these!']\n",
      "['The document will notcompile if you do.']\n",
      "['(It won’t even give you an intelligible errormessage)To change the overall theme (colors, fonts, etc.) of thedocument, add the theme argument, like this:output:html_document:theme: ceruleanDiﬀerent themes are illustrated here:https://www.datadreaming.org/post/r-markdown-theme-gallery/Try a few right now.']\n",
      "['Other options for the header are listed here:http://rmarkdown.rstudio.com/html document format.htmlPublishing the HTML OutputIf you choose to compile your markdown code as an HTML ﬁle, itdisplays in R Studio’s makeshift browser.']\n",
      "['That’s good enough forchecking your work.']\n",
      "['But remember, HTML code creates webpages.']\n",
      "['So you can clickthe “ Open in Browser ” button to see your code displayed in aweb browser.']\n",
      "['If you have your own webpage, you can post this HTML output toyour webpage.']\n",
      "['If you need space on the web to host this page, click on“Publish ”, then click “ RPubs ”.']\n",
      "['RPubs is a free service, run by RStudio, that provides server space for your markdown documents.']\n",
      "['If you post online using RPubs, you can use a URL to share yourwork with your audience.']\n",
      "['Markdown syntaxTo insert text into the output, just type into the .Rmd ﬁle andcompile.']\n",
      "['The goal of markdown is to give you an easy way to create stylizedtext, sections, equations, lists, tables, etc.']\n",
      "['quickly by typing .']\n",
      "['Foritalicized text , place ONE star * before and after the text.']\n",
      "['Forbold text , place TWO stars ** before and after the text.']\n",
      "['Forstruck through text, place TWO tildes ∼∼ before and after thetext.']\n",
      "['*this will be italicized***this will be bold**~~this will be struck out~~Markdown syntaxTo start a new paragraph, push Enter/Return TWICE, so thatthere is a blank line separating the paragraphs.']\n",
      "['For a hyperlink , either type the address itself (it will automaticallybecome a link), or use syntax like this to place the link on top ofother text:[The best web comic](https://smbc-comics.com/)For block quotes, push Enter/Return TWICE then start every lineof the quote with >and a space.']\n",
      "[\"Here's a profound quote:> I'd rather have this bottle in front of me> than a frontal lobotomySectioningOne of the most important ways to make a document readable isto use sectioning to organize the document.\"]\n",
      "['Two hashtags (pound signs) ##followed by a name denote asection.']\n",
      "['Three hashtags ### followed by a name denote a subsection.']\n",
      "['Four hashtags #### followed by a name denote a sub-subsection,and so on.']\n",
      "['One hashtag is used for titles, but generally, these titles appear toolarge and look weird, so most people start at two hashtags.']\n",
      "['If thetoc: true option is speciﬁed, the section titles willappear in the table of contents automatically.']\n",
      "['TabbingSometimes it makes sense for only one of several sub-sections toappear at a time.']\n",
      "['For example, suppose I wrote a paragraph inEnglish and Spanish.']\n",
      "['I could have the two sections represented bytabs, in which the user can switch between the two tabs.']\n",
      "['To create tabs, type {.tabset}immediately after the sectiontitle.']\n",
      "['Then all sub-sections within this section will exist in tabs.']\n",
      "['{.tabset .tabset-fade }does the same thing, but include anice fade-in animation when switching between tabs.']\n",
      "['{.tabset .tabset-fade .tabset-pills }places the tabs intosquares with rounded-edges.']\n",
      "['EquationsIn statistics and data science, sometimes it makes sense to includemathematical equations in the document.']\n",
      "['In Microsoft Word, youhave to point-and-click every symbol, but in Markdown, you canquickly type out equations .']\n",
      "['To include an equation in-line, place a dollar sign $before andafter the equation.']\n",
      "['To place the equation on its own line, placeTWO dollar signs and a space before and after the equation.']\n",
      "['Some special math characters:▶^exponentiation▶ subscripts▶\\\\sqrt{5}√5▶\\\\frac{1}{2}12A list of the code for many other math symbols is here:http://reu.dimacs.rutgers.edu/Symbols.pdfEquationsFor example, to include the quadratic formula in your document,x=−b±√b2−4ac2atype$$ x = \\\\frac{-b \\\\pm \\\\sqrt{b^2 - 4ac}}{2a} $$(Here the\\\\pmrefers to the “plus or minus” symbol)ListsTo create an unordered list (just bullets), type:* Item 1* Item 2* Item 2a* Item 2bTo create an ordered list (numbered), type:1.']\n",
      "['Item 12.']\n",
      "['Item 23.']\n",
      "['Item 3a.']\n",
      "['Item 3ab.']\n",
      "['Item 3bListsSome weird things about lists in R Markdown:1.']\n",
      "['It won’t compile as a list unless there’s an entire blank line(push enter twice) separating the list from the preceding text2.']\n",
      "['To create a sublist, you need to tab twice .']\n",
      "['Once won’t registerdiﬀerently from the other items3.']\n",
      "['For numbered lists, the top level must be regular (Arabic)numbers (1,2,3,...).']\n",
      "['The next levels can be lowercase letters orlowercase Roman numerals4.']\n",
      "['You can change the ﬁrst number to anything you want.']\n",
      "['But thefollowing numbers will always count up by 1 from the ﬁrstnumber, no matter what you type thereCreating tables by handThere are two kinds of tables: ones you enter by hand and onesyou can create with R.']\n",
      "['To create a nice-looking table by hand,markdown has a peculiar notation.']\n",
      "['In general, I don’t like memorizing the speciﬁcs of this notation.']\n",
      "['Instead I use https://tableconvert.com/ (or another website likeit).']\n",
      "['Then:1.']\n",
      "['In the top toolbar, click the “Table” button to choose thedesired dimensions2.']\n",
      "['Fill in the table however you want3.']\n",
      "['Select “Copy”4.']\n",
      "['And paste the syntax into your R markdown documentThe table will appear in a neatly formatted way when you compile.']\n",
      "['Graphics (from external ﬁles)Sometimes you might want to display a graphic in your documentthat you aren’t using R to create.']\n",
      "['For example, what kind ofmonster wouldn’t want to include this graphic?']\n",
      "['You can include graphics if you have the ﬁle on your local machine:![A caption, if wanted, goes here](duck.jpg)Or pull them directly oﬀ the web by entering the image’s URL:![](http://1funny.com/wp-content/uploads/2010/08/Baby-Animals-24.jpg)Embedding CodeRemember that the purpose of an R markdown ﬁle is to weavetext, code, and the results of code together in one, readabledocument.']\n",
      "['There are three ways to include code in a document:1.']\n",
      "['in-line for display only,2.']\n",
      "['in-line and evaluated,3.']\n",
      "['and evaluated inside a “code chunk.”In-line code for display only is just for referring to speciﬁc Rcommands as you are writing.']\n",
      "['Markdown will place these pieces ofcode in a diﬀerent, computery font with a grey background.']\n",
      "['Butthis code WILL NOT be evaluated or run.']\n",
      "['To write in-line code, use single, forward-sloping quotes ‘(on thesame key as the tilde).']\n",
      "['Then if you write about the lm()function, or the ggplot2 package, it will appear in this diﬀerentfont and have a grey background.']\n",
      "['In-line Code that is EvaluatedIf you use the single, forward-sloping quotes, the code is displayedbut not run.']\n",
      "['Alternatively, if you type rprior to any code withinthe quotes, markdown will evaluate the code and display theoutput in the text.']\n",
      "['For example, try typing the following into your markdowndocument:The current date and time are `r Sys.time()`Sys.time() is R’s internal clock, so this syntax should displaythe current date and time as regular text, not as computer code.']\n",
      "['This feature is great for ﬁlling in details about the data into yourtext automatically.']\n",
      "['Code ChunksA code chuck contains several lines of code, on separate lines fromthe text.']\n",
      "['It gets run by R , and the output appears in thedocument as well.']\n",
      "['Start a code chunk like this:```{r nameofthischunk}First, type three forward single-quotes.']\n",
      "['Then within curly braces,the letter r— this tells markdown that this is R code, then adistinct name you give this code chunk.']\n",
      "['Naming the chunks isoptional, but will help you isolate errors if there are any.']\n",
      "['End a code chunk by typing three more forward single-quoteson a new line:```Code ChunksYou can type as many lines of R code as you want inside one codechunk.']\n",
      "['But, best practice is to only write a few lines at a time inone code chunk.']\n",
      "['The reason is that you are trying to bring a reader along andexplain your code.']\n",
      "['It’s easier to explain a few lines at a time.']\n",
      "['Also, If there’s multiple outputs from one chunk, they will all bedisplayed after the chunk.']\n",
      "['Keeping the chunk small helps us keeptrack of which output goes with which code.']\n",
      "['Take a moment to try out some code chunks in your markdowndocument.']\n",
      "['Code Chunk OptionsYou can write options inside the curly braces, separated bycommas, to change the behavior of a code chunk.']\n",
      "['Here are some options you can use:echo=FALSE — don’t display the codeeval=FALSE — don’t display the resultswarning=FALSE, message=FALSE, error=FALSE — don’tdisplay warnings, messages, or errors (I always use these options forthe code chunk that loads the packages I need)Other options are listed here: https://yihui.name/knitr/options/Code Chunk OptionsA new .Rmd ﬁle has this code chunk at the top of the document:```{r setup, include=FALSE}knitr::opts_chunk$set(echo = TRUE)```This code chunk contains global options to be applied to allsubsequent chunks.']\n",
      "['By default, it sets echo=TRUE , which tells all the chunks todisplay the code in the document in addition to the output.']\n",
      "['If thiswere instead echo=FALSE , the output would get displayed but theR code that generates the output would not be displayed.']\n",
      "['You can set other global options here if you want them applied toall code chunks.']\n",
      "['Just write knitr::opts chunk$set( option)inthis chunk.']\n",
      "['CachingOne extremely useful code chunk option is caching the output ofthe code.']\n",
      "['Caching means that R saves the output of the chunk so thatthe next time the document compiles, it can load the saved resultsinstead of running the code again.']\n",
      "['To cache the output of a code chunk, use the optioncache=TRUE .']\n",
      "['This option saves A LOT OF TIME when using commands thattake a while to run, such as loading a big dataset or running acomplicated model.']\n",
      "['But don’t use this option for every chunk, as itcan cause problems with the keeping results accurate as codechanges.']\n",
      "['Tables and Figures (from code)A big reason why markdown makes sense for R is the ability tocleanly display tables and ﬁgures you produce with R code.']\n",
      "['To convert tables from R output to nice looking HTML, use thekable() function.']\n",
      "['Just place the code that creates the tableinside of kable() .']\n",
      "['For ﬁgures, such as ggplot graphics, no extra function is needed todisplay.']\n",
      "['But use the fig.width andfig.height options onthe code chunk to control the size of the ﬁgure in the output.']\n",
      "['For example:```{r plot, fig.width=6, fig.height=8}ggplot(mtcars, aes(x=wgt, y=mpg)) + geom_point()```KNN and Probability Classification\\uf0d8Definition\\uf076Classification can take two distinct meanings in Machine Learning\\uf0d8Unsupervised Learning \\uf076We may be given a set of observations with the aim of establishing the existence of classes or clusters in the data\\uf0d8Supervised Learning \\uf076We may know for certain that there are so many classes, and the aim is to establish a rule that we can use to classify a new observation into one of the existing classes\\uf076k-NN is a supervised method for classification 2Classification\\uf0d8A few classification examples:\\uf076A Person arrives at the emergency room with a set of symptoms that could possibly be attributed to one of three medical conditions.']\n",
      "['Which of the three conditions does the individual have?']\n",
      "['\\uf076An online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user’s IP address, past transaction history, and so forth.']\n",
      "['\\uf076On the basis of DN sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are disease -causing and which are not.']\n",
      "['3Classification\\uf0d8There are many possible techniques that a classifier might use to predict a qualitative response.']\n",
      "['Today we will discuss k -NN\\uf076k-Nearest Neighbors\\uf076Naïve Bayes \\uf076Logistic Regression\\uf076Tree –based methods4ClassificationA few issues to keep in mind when building a classifier\\uf0d8Accuracy.']\n",
      "['There is the reliability of the rule, usually represented by the proportion of correct classifications, although it may be that some errors are more serious than others, and it may be important to control the error rate for some key class.']\n",
      "['\\uf0d8Speed.']\n",
      "['In some circumstances, the speed of the classifier is a major issue.']\n",
      "['A classifier that is 85% accurate may be preferred over one that is 95% accurate if it is 100 times faster in testing (and such differences in time -scales are not uncommon in neural networks for example).']\n",
      "['\\uf076Such considerations would be important for the automatic reading of postal codes, or automatic fault detection of items on a production line for example.']\n",
      "['5ClassificationA few issues to keep in mind when building a classifier\\uf0d8Comprehensibility.']\n",
      "['If it is a human operator that must apply the classification procedure, the procedure must be easily understood else mistakes will be made in applying the rule.']\n",
      "['It is important also, that human operators believe the system.']\n",
      "['\\uf0d8Training Time.']\n",
      "['Especially in a rapidly changing environment, it may be necessary to learn a classification rule quickly, or make adjustments to an existing rule in real time.']\n",
      "['“Quickly” might imply also that we need only a small number of observations to establish our rule.']\n",
      "['6K-Nearest Neighbors7Simple approach for k -NNSimple goal:\\uf0d8Predict the label of a data point by:\\uf076Looking at the ‘k’ closest labeled data points (neighbors)\\uf076Uses a majority vote\\uf0d8One of the easiest algorithms to interpret, oftentimes used as a baseline for measuring model performance\\uf0d8Memory -Based Learning\\uf076Also known as “case -based” or “example -based” learning\\uf0d8Intuition behind memory -based learning\\uf076Similar inputs map to similar outputs\\uf0d8If true, we just have to define “similar”\\uf0d8Not all similarities created equal…8Memory -Based Learning\\uf0d8How do we determine “similar”?']\n",
      "['\\uf0d8For instance, if we wanted to:\\uf0d8Predict Brian’s weight\\uf076Who are the similar people?']\n",
      "['\\uf076Similar age, diet, height, waistline, activity level …\\uf0d8Predict Brian’s IQ\\uf076Similar occupation, writing style, undergraduate degree, SAT score, …\\uf0d8How do we calculate variously ranges in similarity?']\n",
      "['\\uf076Need some metric…\\uf0d8Distance9k-NN Approach\\uf0d8Define a distance 𝑑𝑑(𝑥𝑥1,𝑥𝑥2)between any 2 examples\\uf076Examples are essentially rows\\uf076Sowe could just use Euclidean distance … \\uf0d8Training\\uf076Index the training examples for fast lookup (build a “database”)\\uf0d8Test\\uf076Given a new 𝑥𝑥, find the closest neighbor (k=1) from training index\\uf076Classify x the same as its closest neighbor10k-NN Approach\\uf0d8Euclidean Distance Equation:\\uf0d8q is the current vector and p is new, think of q as the training and p as the test.']\n",
      "['Smaller values mean lines or more similar in value or closer together on a graph.']\n",
      "['11x1 x2 x3 x4 x5 x6 x7 x8 Resultq 3 2 2 5 6 2 1 5p 4 3 3 6 4 4 5 6Euclidean 1 1 1 1 4 4 16 1 5.39z 3 2 2 5 6 2 1 5Euclidean0 0 0 0 0 0 0 0 0Subtracting then squaring row p with q.']\n",
      "['Then summing and taking the sqrt.']\n",
      "['Do the same with z and comparekNN Decision Boundaries\\uf0d8kNN can learn complex decision boundaries12\\uf0d8Instead of picking the (1) nearest neighbor, what if we picked the k-Nearest Neighbors and have them vote?']\n",
      "['\\uf0d8Choosing k points is more reliable in the following cases:\\uf076Noise in training vectors x\\uf076Noise in training labels y\\uf076Overlapping classeskNN13x_1x_2+++++++++ooooooooooo+++ oook=1kNN\\uf0d8Instead of picking the (1) nearest neighbor, what if we picked the k-Nearest Neighbors and have them vote?']\n",
      "['\\uf0d8Choosing k points is more reliable in the following cases:\\uf076Noise in training vectors x\\uf076Noise in training labels y\\uf076Overlapping classes\\uf0d8Why?']\n",
      "['14x_1x_2+++++++++ooooooooooo+++ oook=1XkNN Decision Boundaries\\uf0d8Consider this example with R,G,B classes with significant overlap15kNN Decision Boundaries\\uf0d8Consider this example with R,G,B classes with significant overlap\\uf0d8k=1 Decision Boundary\\uf076Looks complex\\uf076Overfitting?']\n",
      "['16kNN Decision Boundaries\\uf0d8k=1 Decision Boundary\\uf076Looks complex\\uf076Overfitting?']\n",
      "['\\uf0d8What if we were to increase k?']\n",
      "['17kNN Decision Boundaries\\uf0d8k=1 Decision Boundary\\uf076Looks complex\\uf076Overfitting?']\n",
      "['\\uf0d8What if we were to increase k?']\n",
      "['\\uf076K=15 Decision boundary18kNN Decision Boundaries\\uf0d8k=1 Decision Boundary\\uf076Looks complex\\uf076Overfitting?']\n",
      "['\\uf0d8What if we were to increase k?']\n",
      "['\\uf076K=15 Decision boundary\\uf076Smoother boundaries19kNN Decision Boundaries\\uf0d8k=1 Decision Boundary\\uf076Looks complex\\uf076Overfitting?']\n",
      "['\\uf0d8What if we were to increase k?']\n",
      "['\\uf076K=15 Decision boundary\\uf076Smoother boundaries\\uf076Generalizes better on unseen data20kNN Decision Boundaries\\uf0d8k=1 Decision Boundary\\uf076Looks complex\\uf076Overfitting?']\n",
      "['\\uf0d8What if we were to increase k?']\n",
      "['\\uf076K=15 Decision boundary\\uf076Smoother boundaries\\uf076Generalizes better on unseen data\\uf0d8What makes the boundaries smoother?']\n",
      "['21kNN Decision Boundaries\\uf0d8k=1 Decision Boundary\\uf076Looks complex\\uf076Overfitting?']\n",
      "['\\uf0d8What if we were to increase k?']\n",
      "['\\uf076K=15 Decision boundary\\uf076Smoother boundaries\\uf076Generalizes better on unseen data\\uf0d8What makes the boundaries smoother?']\n",
      "['\\uf0d8Let’s look at a two -class (binary) example22k-NN Graphical Example23\\uf0d8Consider this two -dimensional dataset with points classified as Red or Green k-NN Graphical Example24\\uf0d8Consider this two -dimensional dataset with points classified as Red or Green \\uf0d8We want to Classify this pointk-NN Graphical Example25\\uf0d8Consider this two -dimensional dataset with points classified as Red or Green \\uf0d8We want to Classify this point\\uf0d8If we consider k=3 neighbors k-NN Graphical Example26\\uf0d8Consider this two -dimensional dataset with points classified as Red or Green \\uf0d8We want to Classify this point\\uf0d8If we consider k=3 neighbors \\uf076Measured by some distancedk-NN Graphical Example27\\uf0d8Consider this two -dimensional dataset with points classified as Red or Green \\uf0d8We want to Classify this point\\uf0d8If we consider k=3 neighbors \\uf076Measured by some distance\\uf076The point is classified as Redk-NN Graphical Example28\\uf0d8Consider this two -dimensional dataset with points classified as Red or Green \\uf0d8We want to Classify this point\\uf0d8If we consider k=3 neighbors \\uf076Measured by some distance\\uf076The point is classified as Red\\uf0d8If we consider k=5 neighborsk-NN Graphical Example29\\uf0d8Consider this two -dimensional dataset with points classified as Red or Green \\uf0d8We want to Classify this point\\uf0d8If we consider k=3 neighbors \\uf076Measured by some distance\\uf076The point is classified as Red\\uf0d8If we consider k=5 neighbors\\uf076Measured by some distancedk-NN Graphical Example30\\uf0d8Consider this two -dimensional dataset with points classified as Red or Green \\uf0d8We want to Classify this point\\uf0d8If we consider k=3 neighbors \\uf076Measured by some distance\\uf076The point is classified as Red\\uf0d8If we consider k=5 neighbors\\uf076Measured by some distance\\uf076The point is classified as Greenk-NN Graphical Example31\\uf0d8Consider this two -dimensional dataset with points classified as Red or Green \\uf0d8We want to Classify this point\\uf0d8If we consider k=3 neighbors \\uf076Measured by some distance\\uf076The point is classified as Red\\uf0d8If we consider k=5 neighbors\\uf076Measured by some distance\\uf076The point is classified as Green\\uf0d8So, how do we know what k to choose?']\n",
      "['How to choose “k”\\uf0d8Odd k (often 1, 3, or 5):\\uf076Avoids problem of breaking ties (in a binary classifier)\\uf0d8Large k:\\uf076Less sensitive to noise (particularly class noise)\\uf076Better probability estimates for discrete classes\\uf076Larger training sets allow larger values of k\\uf0d8Small k:\\uf076Captures fine structure of problem space better\\uf076May be necessary with small training sets\\uf0d8Balance between large and small k32kNN distance problem\\uf0d8Problem:\\uf076What if the input represents weight in milligrams?']\n",
      "['\\uf076Then small differences in physical weight dimension have a huge effect on distances, overwhelming other features\\uf076Should really correct for these arbitrary “scaling” issues\\uf0d8This leads to Standard Scaling\\uf0d8Rescale weights so that standard deviation = 133+oo+weight (lb)attribute_2+++++++oooooooooweight (mg)attribute_2++++++ +++ooooooooo oobadMore kNN Details\\uf0d8Nonparametric -makes no explicit assumptions about the underlying distribution of the input\\uf0d8Instance/memory -based learning means that this algorithm doesn’t explicitly learn a model.']\n",
      "['Instead, it chooses to memorize the training instances which are subsequently used as “knowledge” for the prediction phase\\uf0d8Learns arbitrarily complicated decision boundaries\\uf0d8Lazy learner -method that generalizes data in the testing (deployment) phase, rather than during the training phase –designed to be continuously updating as new data comes in\\uf076A benefit of lazy learning is that it can quickly adapt to changes, \\uf0d8Think Netflix recommendations, new options are appearing constantly so have a static training set it’s really valuable .']\n",
      "['\\uf076Very fast training time, but very slow prediction ( has to search for the nearest neighbors)34Advantages of k -NN\\uf0d8Simple and fast to deploy\\uf076Little to no training time\\uf0d8Easy to interpret/explain\\uf0d8Naturally handles multiclass datasets\\uf0d8Non-parametric \\uf076Does not assume any probability distributions on the input data35Disadvantages of k -NN\\uf0d8Storage of model takes a lot of disk space (contains entire training dataset)\\uf0d8Curse of Dimensionality -often works best with 25 or fewer dimensions\\uf076There is little difference between the nearest and farthest neighbor in high dimensional data (starts to normalize to 1)\\uf0d8Computationally expensive predictions (large search problem to find nearest neighbors)\\uf076Might be impractical in industry settings\\uf0d8Need to normalize -suffers from skewed class distributions\\uf076If one type of category occurs much more than another, classifying an input will be more biased towards that one category (dominates the majority vote since it is more likely to be neighbors with the input) 36kNN Exercise37Switch to RUnderstanding Probability Classifiers\\uf0d8To be able to unpack the probability classifiers, we need a good grasp on the following topics\\uf076Random variables\\uf076Distributions\\uf0d8Continuous \\uf0d8Discrete\\uf076Statistical Independence\\uf076Probability\\uf0d8Conditional Probability\\uf0d8Joint Probability\\uf0d8Marginal Probability38Random Variables\\uf0d8A random variable is a random number determined by chance, or more formally, drawn according to a probability distribution which specifies the probability that its value falls in any given interval.']\n",
      "[\"\\uf0d8Discrete Random Variable\\uf076Taking any of a specified finite or countable list of values, endowed with aprobability mass function characteristic of the random variable's probability distribution\\uf0d8Continuous\\uf076Taking any numerical value in an interval or collection of intervals, via aprobability density function that is characteristic of the random variable's probability distribution39Random Variables\\uf0d8Why do we care about Random Variables?\"]\n",
      "['\\uf0d8Our goal is to predict the target/class\\uf0d8We are not given the true (presumably deterministic) function\\uf0d8We are only given observations\\uf0d8Uncertainty arises through:\\uf076Noisy measurements\\uf076Finite size of data sets\\uf076Ambiguity: The word “bank” can mean (1) a financial institution, (2) the side of a river,or (3) tilting an airplane.']\n",
      "['Which meaning was intended, based on the words that appear nearby?']\n",
      "['\\uf0d8Probability theory provides a consistent framework for the quantification and manipulation of uncertainty\\uf0d8Allows us to make optimal predictions given all the information available to us, even though that information may be incomplete or ambiguous40Probability\\uf0d8Probabilities assign numbers to possibilities\\uf0d8A probability needs to satisfy three properties (Kolmogorov, 1956):\\uf076A probability must be nonnegative\\uf076The sum of the probabilities across all events in the entire sample space must be 1\\uf076For any two mutually exclusive events, the probability that one or the other occurs is the sum of their individual probabilities\\uf0d8For example, the probability that a fair six -sided die comes up 3 OR 4 is 1/6 + 1/6 = 2/6.']\n",
      "['41Probability Distributions\\uf0d8A probability distribution is simply a list of all possible events and their corresponding probabilities\\uf0d8There are two kinds of probability distributions\\uf076Discrete Distribution:\\uf0d8Probability of heads or tails\\uf076Continuous Distribution:\\uf0d8Probabilities of people’s heights42Discrete Probability Distribution\\uf0d8When the sample space consists of discrete outcomes (e.g., heads or tails), the probability distribution is a list of probabilities of the outcomes\\uf0d8The probability of a discrete outcome is called a probability mass\\uf0d8The sum of the probability masses across the sample space must be 143Discrete Probability Example\\uf0d8Example\\uf076Consider the simple experiment of tossing a coin three times.']\n",
      "['Let X = number of times the coin comes up heads.']\n",
      "['The 8 possible elementary events and the corresponding values for X are:44Elementary Event Count of Heads (X)TTT 0TTH 1THT 1HTT 1THH 2HTH 2HHT 2HHH 3Discrete Probability Example\\uf0d8Example\\uf076Therefore, the probability distribution for the number of heads occurring in three coin tosses is45Count of Heads (X) p(x) F(x)0 1/8 1/81 3/8 4/82 3/8 7/83 1/8 1Discrete Probability Example46Count of Heads (X) p(x) F(x)0 1/8 1/81 3/8 4/82 3/8 7/83 1/8 1Continuous Probability Distribution\\uf0d8When the sample space consists of continuous outcomes (ex: people’s heights) we cannot use probability mass for a specific outcome.']\n",
      "['\\uf0d8Why not?']\n",
      "['47Continuous Probability Distribution –Probability Density\\uf0d8When the sample space consists of continuous outcomes (ex: people’s heights) we cannot use probability mass for a specific outcome.']\n",
      "['\\uf0d8Why not?']\n",
      "['\\uf076Because the probability mass for a specific outcome will be zero\\uf076In other words, the probability of someone’s height being exactly 67.2141390842076153…\\uf0d8Instead, we can:\\uf076Discretize the space into a finite set of mutually exclusive and exhaustive intervals\\uf076Calculate the probability mass in each interval\\uf076Use the ratio of probability mass to interval width\\uf076This ratio is called the Probability Density48Probability Density\\uf0d8The top panel of this figure shows the discretized intervals and probability mass in each interval\\uf0d8The second panel shows the probability density\\uf0d8The third panel shows the narrower intervals and probability mass in each interval\\uf0d8The bottom panel shows the probability density corresponding to the more narrow intervals\\uf0d8Generally, the skinnier the intervals are, the more accurate the probability density is49Probability Density\\uf0d8While probability mass cannot exceed 1, probability densities can\\uf0d8The upper panel of this figure shows that most of the probability mass is concentrated around 84\\uf0d8Consequently, the probability density near 84 exceeds 1.0, as shown in the lower panel\\uf0d8This simply means that there is a high concentration of probability mass relative to the width of the interval50Properties of Probability Density Functions\\uf0d8We need to define some notations first\\uf0d8Let:\\uf076𝑥𝑥be the continuous variable\\uf076Δ𝑥𝑥be the width of an interval on 𝑥𝑥\\uf076𝑖𝑖be an index for the intervals\\uf076[𝑥𝑥𝑖𝑖,𝑥𝑥𝑖𝑖+∆𝑥𝑥]be the interval between 𝑥𝑥𝑖𝑖and 𝑥𝑥𝑖𝑖+∆𝑥𝑥\\uf076𝑃𝑃([𝑥𝑥𝑖𝑖,𝑥𝑥𝑖𝑖+∆𝑥𝑥])be the probability mass of the 𝑖𝑖th interval\\uf0d8Then the sum of those probability masses must be 1:�𝑖𝑖𝑃𝑃𝑥𝑥𝑖𝑖,𝑥𝑥𝑖𝑖+∆𝑥𝑥=1\\uf0d8We can rewrite the equation above in terms of the density of each interval, by  dividing and multiplying by x:�𝑖𝑖∆𝑥𝑥∗𝑃𝑃𝑥𝑥𝑖𝑖,𝑥𝑥𝑖𝑖+∆𝑥𝑥∆𝑥𝑥=151Properties of Probability Density Functions\\uf0d8In the limit, as the interval width becomes infinitesimal, we denote:\\uf076Summation as ∫ instead of ∑\\uf0d8Then, the previous equation (in terms of density) can be rewritten as:�𝑖𝑖∆𝑥𝑥∗𝑃𝑃𝑥𝑥𝑖𝑖,𝑥𝑥𝑖𝑖+∆𝑥𝑥∆𝑥𝑥=1⇒�𝑑𝑑𝑥𝑥𝑝𝑝𝑥𝑥=1\\uf0d8We use𝑝𝑝(𝑥𝑥)to represent the probability mass when 𝑥𝑥 is discrete\\uf0d8Thus, what 𝑝𝑝(𝑥𝑥)represents depends on the context52The Normal Probability Density Functions\\uf0d8Perhaps the most famous probability density function is the normal distribution, also known as the Gaussian distribution\\uf0d8The probability density function of normal distribution isp𝑥𝑥=1𝜎𝜎√2𝜋𝜋𝑒𝑒−12𝑥𝑥−𝜇𝜇𝜎𝜎2\\uf0d8Recall, what are 𝜎𝜎and 𝜇𝜇?']\n",
      "['what do they control?']\n",
      "['\\uf0d8An example of the probability density is shown in the figure where the x axis is divided into a dense comb of small intervals\\uf0d8The figure also shows that the area under the curve is, in fact, 153Example -Continuous Normal Distribution\\uf0d8Example of the continuous distribution of weights\\uf076The continuous normal distribution can describe the distribution of weight of adult males.']\n",
      "['\\uf076For example, you can calculate the probability that a man weighs between 160 and 170 pounds.']\n",
      "['\\uf076The area of this range is 0.136; therefore, the probability that a randomly selected man weighs between 160 and 170 pounds is 13.6%.']\n",
      "['\\uf076The entire area under the curve equals 1.0�−∞+∞𝑝𝑝𝑥𝑥𝑑𝑑𝑥𝑥=154Joint Probability\\uf0d8Joint Probability\\uf076Knowing that y occurred reduces the sample space to y\\uf0d8The part of y where x also occurred, or the probability of x and y occurring, is:\\uf0d8𝑃𝑃𝑥𝑥,𝑦𝑦=𝑃𝑃(𝑥𝑥∩𝑦𝑦)\\uf076Order does not matter:\\uf0d8𝑃𝑃(𝑥𝑥,𝑦𝑦)=𝑃𝑃(𝑦𝑦,𝑥𝑥)55\\uf0d8Disjoint Sets \\uf0d8Mutually Exclusive Events\\uf0d8𝑥𝑥∩𝑦𝑦=∅\\uf0d8Intersecting setsJoint Probability and Marginal Probability56\\uf0d8This table shows the probabilities of various combinations of people’s eye/hair color\\uf0d8Each entry indicates the joint probability of particular combinations of eye color (𝑒𝑒)and hair color (ℎ), denoted by 𝑝𝑝(𝑒𝑒,ℎ)\\uf0d8The right margin of the table shows the probabilities of the eye colors overall, collapsed across hair colors\\uf0d8Such probabilities are called marginal probability , denoted by 𝑝𝑝(𝑒𝑒):𝑝𝑝𝑒𝑒=�ℎ𝑝𝑝(𝑒𝑒,ℎ)\\uf0d8The marginal probabilities of the hair colors, 𝑝𝑝(ℎ), are indicated on the lower margin of the table:𝑝𝑝ℎ=�𝑒𝑒𝑝𝑝(𝑒𝑒,ℎ)Conditional Probability\\uf0d8Conditional Probability\\uf076P𝑥𝑥𝑦𝑦is the probability of the occurrence of event 𝑥𝑥, given that 𝑦𝑦occurred is given as:\\uf0d8P𝑥𝑥𝑦𝑦=𝑃𝑃(𝑥𝑥∩𝑦𝑦)𝑃𝑃(𝑦𝑦)=𝑃𝑃(𝑥𝑥,𝑦𝑦)𝑃𝑃(𝑦𝑦)\\uf076Answers the question: \\uf0d8How does the probability of an event change if we have extra information?']\n",
      "['57Conditional Probability Example\\uf0d8Coin Toss Example:\\uf076Toss a fair coin 3 times\\uf076What is the probability of 3 heads?']\n",
      "['\\uf0d8Answer: \\uf076𝑆𝑆𝑆𝑆𝑆𝑆𝑝𝑝𝑆𝑆𝑒𝑒𝑆𝑆𝑝𝑝𝑆𝑆𝑆𝑆𝑒𝑒={𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 }\\uf076All outcomes are equally likely (if the coin is fair)\\uf076𝑃𝑃(𝐻𝐻𝐻𝐻𝐻𝐻 )=18\\uf076Suppose we are told that the first toss was heads\\uf076Given this information, how should we compute the probability of {HHH}?']\n",
      "['\\uf0d8Answer: \\uf076We have a new (reduced) 𝑆𝑆𝑆𝑆𝑆𝑆𝑝𝑝𝑆𝑆𝑒𝑒𝑆𝑆𝑝𝑝𝑆𝑆𝑆𝑆𝑒𝑒={𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 ,𝐻𝐻𝐻𝐻𝐻𝐻 }\\uf076All outcomes are still equally likely (the coin is still fair)\\uf076𝑃𝑃(𝐻𝐻𝐻𝐻𝐻𝐻 )=1458Conditional Probability Example\\uf0d8We can visualize the conditional probability as follows\\uf076Think of 𝑃𝑃(𝐴𝐴)as the proportion of the area of the whole sample space taken up by A\\uf076For 𝑃𝑃(𝐴𝐴|𝐵𝐵)we restrict our attention to B\\uf076𝑃𝑃(𝐴𝐴|𝐵𝐵)is the proportion of B taken up by A\\uf0d8𝑃𝑃𝐴𝐴𝐵𝐵=𝑃𝑃(𝐴𝐴∩𝐵𝐵)𝑃𝑃(𝐵𝐵)59Statistical Independence\\uf0d8Independent Events\\uf076If x and y are independent then they are unconnected and not related to each other\\uf076We have:P𝑥𝑥𝑦𝑦=𝑃𝑃𝑥𝑥\\uf076From there it follows that𝑃𝑃𝑥𝑥,𝑦𝑦=𝑃𝑃𝑥𝑥∗𝑃𝑃(𝑦𝑦)\\uf076In other words, knowing that y occurred does not change the probability that x occurs (and vice versa)\\uf076Examples of absolute independence include:\\uf0d8Eye color and height\\uf0d8Hair color and weight60Statistical Independence Example\\uf0d8Independent Events\\uf076If we want to calculate the joint probability of two independent events, we can simply multiply each probability together to get the joint probability\\uf076“Joint Distribution” = “Product Distribution”\\uf076P𝑥𝑥,𝑦𝑦=𝑃𝑃𝑥𝑥∗𝑃𝑃(𝑦𝑦)\\uf0d8For Example:\\uf076Probability of tossing a coin and getting “Heads”:𝑃𝑃𝐻𝐻𝑒𝑒𝑆𝑆𝑑𝑑𝐻𝐻=𝑃𝑃𝑥𝑥=12\\uf076Probability of rolling a dice and getting “3”:𝑃𝑃𝑅𝑅𝑅𝑅𝑆𝑆𝑆𝑆“3” =𝑃𝑃𝑦𝑦=16𝑃𝑃𝐻𝐻𝑒𝑒𝑆𝑆𝑑𝑑𝐻𝐻∗𝑃𝑃𝑅𝑅𝑅𝑅𝑆𝑆𝑆𝑆“3” =P𝑥𝑥,𝑦𝑦=12∗16=11261Linear regression finds the straight line, called the least squares regression line or LSRL, that best represents observations in a data set.']\n",
      "['Suppose Yis a dependent variable, and Xis an independent variable.']\n",
      "['Then, the equation for the regression line would be: ŷ = b0+ b1xRegression Review63George Washington University, Intro to Data ScienceAssumptions\\uf0d8Linear relationship between dependent and independent -Plot\\uf0d8Multicollinearity –occurs when independent variables are not independent –checked with VIF \\uf0d8Auto -correlation –Occurs when residuals are not independent from each other – stock prices example -Durbin- Watson d tests\\uf0d8Homoscedasticity –Error term along the regression line are equal –Scatter Plot –can convert the dependent variable.']\n",
      "['64George Washington University, Intro to Data ScienceRegression\\uf0d8Works best with numeric/continuous data to increase the inferential power\\uf0d8We are trying to model or explain the relationship between a single variable Y (response, output, dependent) and one or more X1…..']\n",
      "['(predictor, input, independent, explanatory, regessors..etc.)\\uf0d8Y must be a continuous variable but X categorical, continuous, \\uf0d8Centering (scale/ Zscores )variables so that the predictors have mean 0, is often recommended.']\n",
      "['The intercept term is then interpreted as the expected value of Yiwhen the predictor values are set to their means .']\n",
      "['Otherwise, the intercept is interpreted as the expected value of Yi when the predictors are set to 0, which may not be a realistic or interpretable situation (e.g.']\n",
      "['what if the predictors were height and weight?).']\n",
      "['65George Washington University, Intro to Data ScienceWhat Does r2(Coefficient of Determination) Mean?']\n",
      "['This statistic quantifies the proportion of the variance of one variable “explained” (in a statistical sense, not a causal sense) by the other.']\n",
      "['As an example at data set in R (.2697) = 27% of the variability in trunk Height is explained by Girth66George Washington University, Intro to Data ScienceR Regression Output: Coefficients\\uf0d8Estimate?']\n",
      "['Coefficients for the regression equation\\uf0d8Standard Error?']\n",
      "['Measures the average amount that the coefficient estimates vary from the actual average value of our response variable, can be used to generate confidence intervals.']\n",
      "['\\uf0d8P value?']\n",
      "['A small p -value indicates that it is unlikely we will observe a relationship between the predictor (Girth) and response (Height) variables due to chance, a p-value of .05 or less is considered statistical significant \\uf0d8Formula Call?']\n",
      "['Regression equation67George Washington University, Intro to Data ScienceR Regression Output\\uf0d8Residual Standard ErrorThe average amount that the response will deviate from the true regression line.']\n",
      "['Used to evaluate our model.']\n",
      "['In our example we have a residual error of 5.538 in the predication of tree height.']\n",
      "['The average tree height is 76 meaning we could be off by roughly 7%.']\n",
      "['\\uf0d8Multiple R -squared?']\n",
      "['Provides a measure of how well the model is fitting the actual data, percentage of variance explained by the predictors, ours example is 27%68George Washington University, Intro to Data ScienceVisual 69George Washington University, Intro to Data ScienceBike Share OutputAnother Definition: Functional Approximation\\uf0d8What is a functional approximation problem?']\n",
      "['\\uf076Target variable: Dependent: What we are trying to predict\\uf076Other Variables: Independent: Using to Predict\\uf0d8Functional approximation is a approach that uses the other variables we have access to approximate the dependent and does so through the function development\\uf0d8We will use regression and which assumes that we have a numeric target variable, for classification it’s often a bi- variate or class level variable 70Assessment Measures \\uf0d8Assessing Regression Models: MSE, RMSE and MAE\\uf076MSE –The difference between the predicted values and the actual values squared\\uf076RSME –Same as above only the square root is taken to put the error back in terms of the dependent variable\\uf0d8Can also normalize the RSME to the range of the data in order to be able to compare RSME outputs that include different data ranges \\uf076MAE –The same approach only taking the absolute value instead of squaring 71Equations 72nX XRMSEniidelmo iobs∑=−=12, , ) (min, max, obs obs X XRMSENRMSE−=Linear Regression\\uf0d8Best used in situations where the data being deploy lacks a high level of complexity \\uf0d8Trains very fast but isn’t able to create complex decision boundaries when data is heterogeneous.']\n",
      "['\\uf0d8Also as compared to most ML approaches OLS does not have a built in function that can control for overfitting \\uf076However not all hope is lost, we can use forward stepwise regression –which we discussed in Intro to DS or\\uf076Ridge/Penalized Regression 73Regression \\uf0d8Said another way basic linear regression has a Prediction Accuracy Problem:\\uf076Has a low bias (overfitting) but a high variance\\uf076This can be improved by injecting some level of bias into the equation by reducing the impact of certain coefficients \\uf076This can improve overall accuracy by reducing variance \\uf0d8Draw Dart Board –\\uf0d8Another issue is interpretation –with a large number of predictor variables and large data sets it often hard to identify variable importance and explain model outcomes74Regression and Sparsity\\uf0d8Often we have more features than observations in the world of big data\\uf076What type of problem is this?']\n",
      "['\\uf0d8So we strive to have Sparse models\\uf076What do we mean by Sparse?']\n",
      "['75Bias Versus Variance 76Ridge Regression \\uf0d8Injecting bias can be done through a regulator or utilization of a penalizing attribute.']\n",
      "['two common examples are Ridge and Lasso, let’s start with Ridge77Basic Regression Equation78Ridge Penalty \\uf0d8In this example if 𝛼𝛼 is 0 we simply have normal regression equations\\uf0d8If 𝛼𝛼is large or ∞ then the 𝛽𝛽approaches zero and only the constant term is available to predict y\\uf0d8Essentially provides a weight on the squared residuals during the normal OLS process \\uf0d8We want to train the regulator to fall between 0 and 1 using cross -validation in such a way that it minimize mean square error79Essentially the square of the Euclidean norm (magnitude) of 𝛽𝛽which is the vector of coefficientsor𝛼𝛼Gradient Decent \\uf0d8Penalized regression methods use a very common algorithm called gradient decent\\uf0d8It is essentially a step function that works to minimize some cost function through a iterative process80Gradient Decent 8182L2 Norm or Euclidean Norm or Euclidean Length\\uf0d8Square of all the elements in matrix \\uf0d8Sum the values together\\uf0d8Take the square root \\uf0d8Ridge also squares the final result of this, so we are taking the square of the Euclidean norm and multiply this number by the penalty to weight the coefficients 83In words, the L2 norm is defined as, 1) square all the elements in the vector together; 2) sum these squared values; and, 3) take the square root o f this sum.']\n",
      "['Ridge Visualization84Measuring the Magnitude of Vectors, L2 and L1\\uf0d8𝛽𝛽=𝛽𝛽0𝛽𝛽1\\uf0d8L2 –Norm : II𝛽𝛽II2 = 𝛽𝛽02+ 𝛽𝛽12 : What we used for ridge, essentially calculates the magnitude of the coefficient vector of the regression equation, gives us a bias minimum \\uf0d8L1 85Measuring the Magnitude of Vectors, L2 and L1\\uf0d8𝛽𝛽=𝛽𝛽0𝛽𝛽1\\uf0d8L1 –Norm : II𝛽𝛽II1 = I𝛽𝛽0I + I𝛽𝛽1I : What we used for ridge, essentially calculates the magnitude of the coefficient vector of the regression equation, gives us a bias minimum.']\n",
      "['Cartesian Distance or Euclidean Distance of our vector86Measuring the Magnitude of Vectors, L2 and L187Lasso Equation88Machine Learning OverviewBrian Wright, PhD2ThemesMachine Learning LifecycleAre you ready for Machine Learning?']\n",
      "['Terms and Phases Engineering of Machine Learning Algos versus Software Development Source: https://towardsdatascience.com/stoend -to-end-data -science -life-cycle -6387523b5afcBrian’s Version of Data Science Lifecycle4Question IDBusiness UnderstandingData Acquisition -ETLInitial ModelEvaluationData Understanding -EDAInitial Model(s) BuildingEvaluation Criteria Value Metric Model Creation & Training Feature Engineering and EvaluationOptimization –Hyperpara and EvaluationModel DeploymentData Drift AnalysisModel Performance –Evaluation Value MetricModel Drift Analysis –Model EvaluationReports –Dashboards -Products G1 G2G3Gate Reviews5Machine Learning Time6“A field of Computer Science that gives computers the ability to learnwithout being explicitly programmed.”-Arthur Samuel (Coined the term in 1959 at IBM)“The ability [for systems] to acquire their own knowledge, byextracting patterns from raw data.”-Deep Learning, Goodfellow et al“A computer program is said to learn from experience E with respectto some set of tasks T and performance measure P if its performancetasks in T, as measured by P, improves with experience E.”-Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs.']\n",
      "['humanMachine HumanUnderstanding context ✔Thinking through the problem ✔Asking the right questions ✔Selecting the right tools ✔Performing calculations quickly ✔Performing repetitive tasks ✔Following pre-defined rules ✔Interpreting results ✔8Pattern discovery when inputs (x) and outputs (y) are knownSupervised machine learningInput x:VoterOutput y:Political affiliationExamples: Classification and regression are supervised machine learning 9The data inputs (x)have no target outputs (y)Unsupervised machine learningInput x:VoterOutput y:Not given(to be discovered)?']\n",
      "['We want to impose structure on the inputs (x)to say something meaningful about the data101112Machine Learning is a general use technology what does that mean?']\n",
      "['Machine Learning Overview \\uf0d8Ageneral -purpose technology orGPT is a term coined to describe a new method of producing and inventing that is important enough to have a protracted aggregate impact.']\n",
      "['\\uf0d8Similar to electricity or the internet, in that it can be applied across domains and work to improve market outcomes.']\n",
      "['13Machine Learning Overview \\uf0d8Twitter Data Usage\\uf0d8Error rates on ImageNet (10,000 labelled images) have been driven down from 30% in 2010 to less than 3% today.']\n",
      "['\\uf076Below 5% is important why?']\n",
      "['\\uf0d8Chess: Deep Blue (IBM AI) searched some 200 million positions per second, Kasparov was searching not more than 5– 10 positions probably, per second.']\n",
      "['Yet he played almost at the same level….why?']\n",
      "['14Machine Learning Overview \\uf0d8However, before we all turn into robots consider two important facts: 1.We remain remarkably far away from what would be consider a similar general intelligence that can be compared to humans2.Machines cannot do the full range of tasks that humans can doWe can then refer to jobs or activities that might be good cases for Machine Learning as SML or Suitable for Machine Learning15What are examples of tasks that might be SML and how do we know if our organizations are ready?']\n",
      "['Machine Learning Overview \\uf0d8Successful implementation of ML requires very detailed specifications on what is to be learned and data to support that learning activity.']\n",
      "['\\uf0d8Including the development of engineering features through a series of trial-and- error and..']\n",
      "['\\uf0d8Then most importantly embedding these products into normal business operations in such a way that efficiencies can be realized.']\n",
      "['16Machine Learning Overview \\uf0d8What tasks are most suitable for ML to take over: \\uf076Most recent successes are predicated on supervised learning \\uf076Competency is narrow as compared to the complexity of human decision making  1.Learning a function that maps well -defined inputs to well- defined outputsoIf can predict Y given any value of X –still might not produce the actual causal effect2.Large Data is present or can be created containing input -output pairsoThe more training data available the more arcuate the model3.Task provides clear feedback with well definable goals and metrics oIf we know what to achieve –(optimize flight patterns not a single flight)4.Where reasoning and diverse background knowledge is not necessaryoGood at empirical associations but terrible at decision making that requires common sense of historical knowledge5.No need for why the decision was made to be clearoNN could use millions numerical weights17Machine Learning Overview 6.A tolerance for error or sub- optimal solutionsoML use probabilistic outputs which means some error is always assumed7.Function of item being learned should not change rapidly over timeoWork best when the distribution of future test examples is the same roughly as the training set over timeoIf not the case systems need to be in place to refresh algorithms 18How do machines learn?']\n",
      "['\\uf0d8The basic machine learning process can be divided into three parts.']\n",
      "['\\uf076Data Input: Past data or information is utilized as a basis for future decision-making\\uf076Abstraction: The input data is represented in a broader way through the underlying algorithm\\uf076Generalization: The abstracted representation is generalized to form a framework for making decisions19(reference Introduction to ML by Subramanian Chandramouli , Saikat Dutt , Amit Kumar Das(https://learning.oreilly.com/library/view/machine -learning/9789389588132/xhtml/chapter001.xhtml#ch1_1)20Brian’s Version of Data Science Lifecycle21Question IDBusiness UnderstandingData Acquisition/ RepresentationInitial ModelEvaluationData Understanding -EDAInitial Model(s) BuildingEvaluation Criteria/Value Metric Model Creation & Training Feature Engineering and EvaluationFinal Model DevelopmentModel DeploymentData Drift AnalysisModel Performance –Evaluation Value MetricModel Drift Analysis –Model EvaluationReports – Dashboards -Products G1 G2G3Gate ReviewsOverview of SOME Key ML Methods/Terms \\uf0d8Phase –1 Idea Development \\uf076Prediction versus Inference \\uf076Independent Metric for Business Value\\uf076Target Variable and features \\uf076Classification versus Regression\\uf076Probabilistic Interpretation\\uf076Data acquisition/gathering   \\uf0d8Phase –2 Data Prep and Problem Exploration\\uf076Variable types and data types\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\uf076Missing Data \\uf076Baseline –prevalence\\uf076Data Partitioning/Sampling \\uf076EDA and (Summary Stats and Visuals)\\uf076Cross Validation \\uf0d8Phase –3 –Solution Model Development\\uf076Parameters versus Hyperparameters\\uf076Thresholding \\uf076Feature Engineering\\uf076Bias versus Variance Tradeoff \\uf076Model Evaluation \\uf076Non -parametric modelling (random state)2223Phase I\\uf0d8# Prediction versus Inference 24\\uf0d8# Prediction versus Inference\\uf076Goals of prediction are not centered on how the features are interacting or resulting in an event but are instead focused on the ability of the model to predicted an event.']\n",
      "['\\uf076Almost all ML methods are focused on predication not causation or inference.']\n",
      "['\\uf076This is why model performance is based largely on how well a model predicts not necessarily how much individual variables are contributing to error reduction.']\n",
      "['25Overview of Key ML Methods/Terms \\uf0d8Independent Metric for Business Value \\uf076A key part of building a solution using Machine Learning Techniques is having a metric that is independent of the model that can be used to determine if the model is providing value.']\n",
      "['\\uf076Examples\\uf0d8Recommender Engine for Netflix: Number of user clicks\\uf0d8Spam Block Predictor: Number of viruses in the network \\uf0d8Market Clustering: Did sales increase\\uf0d8Others?']\n",
      "['26Overview of Key ML Methods/Terms: Target Variable versus Features \\uf076Target variable –Is the variable that includes the patterns the machine learning algorithm is trying to learn.']\n",
      "['It is the variable of interest and key to evaluating the model output.']\n",
      "['\\uf0d8More simply it is the variable we are trying to predict.']\n",
      "['\\uf076Feature variables – Are the variables the model will use to learn the patterns of the target variable.']\n",
      "['The process of feature engineering can result in additional features.']\n",
      "['\\uf0d8More simply these are the variables used for predicting the target27Overview of Key ML Methods/Terms: Classification versus Regression \\uf0d8Classification is the process of developing a model to predict whether a target variable is in defined categories.']\n",
      "['This is driven by having either a binary or multi- level categorical variable as the target variable.']\n",
      "['\\uf0d8Examples: \\uf076Predicting whether someone is male, or female based on 1,000s of pictures.']\n",
      "['\\uf076Predicting whether a team will have a winning season or not based on player performance \\uf076Predicting whether a person will default on a loan or not\\uf0d8Key point: The predications of the model are not binary (1s or 0s) but are given as percentages indicating the likelihood that any one row of data belongs to any one category.']\n",
      "['In the case of target variables with multiple categories each row will get the same number of percent predictions as categories.']\n",
      "['28Overview of Key ML Methods/Terms: Classification versus Regression \\uf0d8Regression is the process of developing a model to predict a specific number or range of numbers.']\n",
      "['This is driven by having a continuous variable as the target variable for the model\\uf0d8Examples: \\uf076Predicting the score given the players playing a game.']\n",
      "['\\uf076Predicting an amount of rain given weather conditions \\uf076Predicting a persons weight based on various personal statistics29Overview of Key ML Methods/Terms: Probabilistic Interpretation \\uf0d8A significant portion of this class will focus on building models for classification.']\n",
      "['Classification is a much more common machine learning goal versus regression.']\n",
      "['\\uf0d8We all know the range of values for probabilities, 0 to 100, the key to understanding these outputs is to think of them as risk measures, with 100 being no risk and 0 being all the risk!']\n",
      "['\\uf0d8How the outputs are used will depend on your question.']\n",
      "['\\uf076Example: How certain do you want to be that a drug is effective as compared to whether a customer will open a marketing email?']\n",
      "['The results could both yield 75% probabilities but is that high enough?']\n",
      "['\\uf0d8Could also think of the outputs as a quantification of uncertainty, the question becomes given your problem how much uncertainty are you willing to accept?']\n",
      "['30Overview of Key ML Methods/Terms: Data Brainstorming \\uf076Data to Concept –Does the data available support the algo target and goal\\uf0d8How difficult is the data to gather?']\n",
      "['\\uf0d8Is the data large enough?']\n",
      "['\\uf0d8What is the rate of change of the data?']\n",
      "['\\uf0d8Do we believe this is the correct source and data content to address the problem?']\n",
      "['\\uf076Learning Difficulty –How complex or vague is the target variable?']\n",
      "['\\uf0d8Are there imbalances in the classes?']\n",
      "['\\uf0d8Does the data clearly link to the problem?']\n",
      "['\\uf0d8Has this data been used in the past, to what success?']\n",
      "['\\uf0d8Is the target difficult to measure or break into smaller components?']\n",
      "['\\uf0d8What risk level are you willing to accept given the question?3132Phase IIBrian’s Version of Data Science Lifecycle33Question IDBusiness UnderstandingData Acquisition/ RepresentationInitial ModelEvaluationData Understanding -EDAInitial Model(s) BuildingEvaluation Criteria/Value Metric Model Creation & Training Feature Engineering and EvaluationFinal Model DevelopmentModel DeploymentData Drift AnalysisModel Performance –Evaluation Value MetricModel Drift Analysis –Model EvaluationReports – Dashboards -Products G1 G2G3Gate ReviewsOverview of SOME Key ML Methods/Terms \\uf0d8Phase –1 Idea Development \\uf076Prediction versus Inference \\uf076Independent Metric for Business Value\\uf076Target Variable and features \\uf076Classification versus Regression\\uf076Probabilistic Interpretation \\uf076Data acquisition/gathering     \\uf0d8Phase –2 Data Prep and Problem Exploration\\uf076Variable types and data types\\uf076Baseline –prevalence\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\uf076Missing Data \\uf076Data Partitioning/Sampling \\uf076EDA (Summary Stats and Visuals)\\uf076Cross Validation \\uf0d8Phase –3 –Solution Model Development\\uf076Parameters versus Hyperparameters\\uf076Thresholding \\uf076Feature Engineering\\uf076Bias versus Variance Tradeoff \\uf076Model Evaluation\\uf076Non -parametric modelling (random state)34Overview of Key ML Methods/Terms \\uf0d8Variable Types and Data Types\\uf076Five Atomic Variable Types in R\\uf0d8Numeric –number unlimited size\\uf0d8Integer –number with constraints on size  \\uf0d8Complex –numbers and characters\\uf0d8Character –words\\uf0d8Factor –unique character class that is limited in the number of categories\\uf0d8Logical –True or False \\uf0d8Data Types \\uf076List -A list is an R -object containing different types of elements inside it like vectors, functions, and even another list inside it.']\n",
      "['\\uf076Vector -Avector in R is a series of data items of the same basic type (from above)\\uf076Array -is alistorvector with two or more dimensions\\uf076Matrix -A matrix is a two -dimensional rectangular data structure, created through the use of matrix function.']\n",
      "['Usually numeric, can’t have different data types, think of it as many vectors \\uf076Dataframe –A two dimensional object that can contain multiple variable types 35Overview of Key ML Methods/Terms \\uf0d8Some useful variable and data type  \\uf076str()\\uf076class()\\uf076names()\\uf076length()\\uf076dim()Open up Rstudio and try these functions out on the mtcars dataset.']\n",
      "['See if you agree with the output.']\n",
      "['36Overview of Key ML Methods/Terms \\uf0d8Baseline –prevalence \\uf076The proportion of a particular population found to be in the positive class at a specific time.']\n",
      "['“Positive class” in this example is the class to which we are trying to learn.']\n",
      "['Percentage split across classes of our target variable.']\n",
      "['\\uf076Using mtcars again, what is the prevalence of vs variable?']\n",
      "['37Overview of Key ML Methods/Terms \\uf0d8Scaling and/or Normalizing Data/One -Hot Encoding\\uf076Many DS approaches require the data to be normalized or placed into a standard format so comparison between variables is possible.']\n",
      "['\\uf076For factor variables this measure creating individual columns for each level that are logical or boolien 1s and 0s.']\n",
      "['\\uf076We will mostly use a min max scaler that will maintain the variance of the values but re -calculate them to be between 1 and 0.']\n",
      "['\\uf0d8Use the minmax scaling function in the gradDescent package and scale the mtcars dataset setting the results to a new object.']\n",
      "['What happens?']\n",
      "['What class is the object?']\n",
      "['Can you view the data.frame?']\n",
      "['38Overview of Key ML Methods/Terms \\uf0d8Missing Data \\uf076Large area of study concerning missing data.']\n",
      "['Here we just need to be aware of how to check for missing data and quick solutions \\uf076R comes with several functions/packages that handle missing data we are going to focus on the MICE package.']\n",
      "['\\uf0d8First you need to try to detect if there are patterns of missing data, is it random or not.']\n",
      "['If you detect patterns than you have to develop a strategy to deal with that issue.']\n",
      "['\\uf076MCAR –missing completely at random\\uf076MNAR  -missing not at random\\uf0d8Start with the summary() function on a data frame \\uf076Load in the beaches dataframe from the data file and find the columns that have missing data using the summary function\\uf076Generally variables with more the about 5% missing values should be deleted or imputation needs to occur \\uf0d8Dig a little deeper and use the md.pattern () function in the Mice package.']\n",
      "['\\uf0d8Since there doesn’t appear to be a pattern we will use complete cases to remove the NAs.']\n",
      "['39Overview of Key ML Methods/Terms \\uf0d8Missing Data \\uf076Complete.cases () function creates aindex to remove missing values \\uf0d8remove missing values from a vector x <-x[complete.cases(x)]\\uf0d8remove from a data.framedf <-df[complete.cases(df), ]\\uf0d8remove from individual rows df <-df[complete.cases(df[ , c(row1, row2, ….)]), ]Try the dataframe version on the beaches dataset, then use summary() to see if the missing datapoints are gone.']\n",
      "['MICE package can also do imputation (NA replacement) very easily, lots of examples online on how to do these in very robust ways.']\n",
      "['40Overview of Key ML Methods/Terms \\uf0d8Partitioning and Sampling\\uf076We need to split our data into three sections (in most cases) to build machine learning models \\uf076Training –What we use to build the original model \\uf076Tune –Data used to evaluate initial outputs of a model after it’s been modified (example: changing the k in kNN ) (Feature Engineering)\\uf076Test –Very last step to evaluate the quality of the model after training and tune\\uf0d8The function we will be using throughout the course will be the createDataPartition() function in the caret package.']\n",
      "['\\uf076The problem is that it’s not great at creating multiple partition, so we essentially use it twice to create a sample, then a sample of a sample.']\n",
      "['\\uf076Need to make sure to use the target variable to do stratified sampling, otherwise we could create imbalances in our samples.']\n",
      "['41Cross -Validation42Cross -Validation4344Phase IIIOverview of SOME Key ML Methods/Terms 45\\uf0d8Phase –1 Idea Development \\uf076Prediction versus Inference \\uf076Independent Metric for Business Value\\uf076Target Variable and features \\uf076Classification versus Regression\\uf076Probabilistic Interpretation \\uf076Data acquisition/gathering     \\uf0d8Phase –2 Data Prep and Problem Exploration\\uf076Variable types and data types\\uf076Baseline –prevalence\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\uf076Missing Data \\uf076Data Partitioning/Sampling \\uf076EDA (Summary Stats and Visuals)\\uf076Cross Validation \\uf0d8Phase –3 –Solution Model Development\\uf076Parameters versus Hyperparameters\\uf076Thresholding \\uf076Feature Engineering\\uf076Bias versus Variance Tradeoff \\uf076Model Evaluation\\uf076Non- parametric modelling (random state)Overview of Key ML Methods/Terms \\uf0d8# Feature Engineering – Combining or exploring different levels of variable that best work in your model.']\n",
      "['Likely going to dedicate a week to just this topic.']\n",
      "['46Overview of Key ML Methods/Terms \\uf0d8Thresholding –The percentage point where our models will predict the result to be either a 0 or 1, in the typical binary case.']\n",
      "['\\uf0d8Adjust the threshold associated with indication of a positive class.']\n",
      "['The default is 50%, could be that we want to be extra careful and instead adjust that measure up to 75% or 90%.']\n",
      "['47Overview of Key ML Methods/Terms \\uf0d8Evaluation –The metrics you use to assess model quality.']\n",
      "['There are a ton of this measures, and we are dedicating an entire week to the exploring these further.']\n",
      "['I’ll show some examples in the code for this week.']\n",
      "['48Bias Versus Variance 4950Extra Material 51Bookings.com  52Lesson Learned: Booking.comBookings.com\\uf0d8Swiss Army Knife –Their approach to ML is highly adoptable , meaning it can be used in a variety of settings –generate specific results or more generalizable depending on the inputs (data)\\uf0d8Offline Health Check– Use Randomized Control Trails (RCT) to test model outputs aligned with normative business metrics to assess quality (customer conversion)\\uf076Increase model performance doesn’t necessary translate to better gain in value53Bookings.com\\uf0d8Make a Target Before you Shoot –Develop a clear understanding of the business case and target variable (what is date flexibility) \\uf076Learning Difficulty –How complex or vague is the target\\uf076Data to Concept –Does the data available support the algo target and goal\\uf076Selection Bias –Does the model perform better for a subset of the target \\uf0d8Speed Kills –ML algos, even simple ones, take a lot of computing power –to reduce user weight time (latency) measures should be taken\\uf076See page 1748 (sparsity, model redundancy, caching…etc.)54Machine Learning Overview \\uf0d8Keep a watchful eye –Used specialized monitoring tools to understand how the models are performing in practice (even when the result was unclear)\\uf0d8Traditional Research Methods (Experimental Design) is a Best Practice Approach to ML –\\uf076“Experimentation through Randomized Controlled Trials is ingrained into Booking.com culture”55Machine Learning OverviewBrian Wright, PhD2ThemesMachine Learning LifecycleAre you ready for Machine Learning?']\n",
      "['Terms and Phases Engineering of Machine Learning Algos versus Software Development Source: https://towardsdatascience.com/stoend -to-end-data -science -life-cycle -6387523b5afcBrian’s Version of Data Science Lifecycle4Question IDBusiness UnderstandingData Acquisition -ETLInitial ModelEvaluationData Understanding -EDAInitial Model(s) BuildingEvaluation Criteria Value Metric Model Creation & Training Feature Engineering and EvaluationOptimization –Hyperpara and EvaluationModel DeploymentData Drift AnalysisModel Performance –Evaluation Value MetricModel Drift Analysis –Model EvaluationReports –Dashboards -Products G1 G2G3Gate Reviews5Machine Learning Time6“A field of Computer Science that gives computers the ability to learnwithout being explicitly programmed.”-Arthur Samuel (Coined the term in 1959 at IBM)“The ability [for systems] to acquire their own knowledge, byextracting patterns from raw data.”-Deep Learning, Goodfellow et al“A computer program is said to learn from experience E with respectto some set of tasks T and performance measure P if its performancetasks in T, as measured by P, improves with experience E.”-Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs.']\n",
      "['humanMachine HumanUnderstanding context ✔Thinking through the problem ✔Asking the right questions ✔Selecting the right tools ✔Performing calculations quickly ✔Performing repetitive tasks ✔Following pre-defined rules ✔Interpreting results ✔8Pattern discovery when inputs (x) and outputs (y) are knownSupervised machine learningInput x:VoterOutput y:Political affiliationExamples: Classification and regression are supervised machine learning 9The data inputs (x)have no target outputs (y)Unsupervised machine learningInput x:VoterOutput y:Not given(to be discovered)?']\n",
      "['We want to impose structure on the inputs (x)to say something meaningful about the data101112Machine Learning is a general use technology what does that mean?']\n",
      "['Machine Learning Overview \\uf0d8Ageneral -purpose technology orGPT is a term coined to describe a new method of producing and inventing that is important enough to have a protracted aggregate impact.']\n",
      "['\\uf0d8Similar to electricity or the internet, in that it can be applied across domains and work to improve market outcomes.']\n",
      "['13Machine Learning Overview \\uf0d8Twitter Data Usage\\uf0d8Error rates on ImageNet (10,000 labelled images) have been driven down from 30% in 2010 to less than 3% today.']\n",
      "['\\uf076Below 5% is important why?']\n",
      "['\\uf0d8Chess: Deep Blue (IBM AI) searched some 200 million positions per second, Kasparov was searching not more than 5– 10 positions probably, per second.']\n",
      "['Yet he played almost at the same level….why?']\n",
      "['14Machine Learning Overview \\uf0d8However, before we all turn into robots consider two important facts: 1.We remain remarkably far away from what would be consider a similar general intelligence that can be compared to humans2.Machines cannot do the full range of tasks that humans can doWe can then refer to jobs or activities that might be good cases for Machine Learning as SML or Suitable for Machine Learning15What are examples of tasks that might be SML and how do we know if our organizations are ready?']\n",
      "['Machine Learning Overview \\uf0d8Successful implementation of ML requires very detailed specifications on what is to be learned and data to support that learning activity.']\n",
      "['\\uf0d8Including the development of engineering features through a series of trial-and- error and..']\n",
      "['\\uf0d8Then most importantly embedding these products into normal business operations in such a way that efficiencies can be realized.']\n",
      "['16Machine Learning Overview \\uf0d8What tasks are most suitable for ML to take over: \\uf076Most recent successes are predicated on supervised learning \\uf076Competency is narrow as compared to the complexity of human decision making  1.Learning a function that maps well -defined inputs to well- defined outputsoIf can predict Y given any value of X –still might not produce the actual causal effect2.Large Data is present or can be created containing input -output pairsoThe more training data available the more arcuate the model3.Task provides clear feedback with well definable goals and metrics oIf we know what to achieve –(optimize flight patterns not a single flight)4.Where reasoning and diverse background knowledge is not necessaryoGood at empirical associations but terrible at decision making that requires common sense of historical knowledge5.No need for why the decision was made to be clearoNN could use millions numerical weights17Machine Learning Overview 6.A tolerance for error or sub- optimal solutionsoML use probabilistic outputs which means some error is always assumed7.Function of item being learned should not change rapidly over timeoWork best when the distribution of future test examples is the same roughly as the training set over timeoIf not the case systems need to be in place to refresh algorithms 18How do machines learn?']\n",
      "['\\uf0d8The basic machine learning process can be divided into three parts.']\n",
      "['\\uf076Data Input: Past data or information is utilized as a basis for future decision-making\\uf076Abstraction: The input data is represented in a broader way through the underlying algorithm\\uf076Generalization: The abstracted representation is generalized to form a framework for making decisions19(reference Introduction to ML by Subramanian Chandramouli , Saikat Dutt , Amit Kumar Das(https://learning.oreilly.com/library/view/machine -learning/9789389588132/xhtml/chapter001.xhtml#ch1_1)20Brian’s Version of Data Science Lifecycle21Question IDBusiness UnderstandingData Acquisition/ RepresentationInitial ModelEvaluationData Understanding -EDAInitial Model(s) BuildingEvaluation Criteria/Value Metric Model Creation & Training Feature Engineering and EvaluationFinal Model DevelopmentModel DeploymentData Drift AnalysisModel Performance –Evaluation Value MetricModel Drift Analysis –Model EvaluationReports – Dashboards -Products G1 G2G3Gate ReviewsOverview of SOME Key ML Methods/Terms \\uf0d8Phase –1 Idea Development \\uf076Prediction versus Inference \\uf076Independent Metric for Business Value\\uf076Target Variable and features \\uf076Classification versus Regression\\uf076Probabilistic Interpretation\\uf076Data acquisition/gathering   \\uf0d8Phase –2 Data Prep and Problem Exploration\\uf076Variable types and data types\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\uf076Missing Data \\uf076Baseline –prevalence\\uf076Data Partitioning/Sampling \\uf076EDA and (Summary Stats and Visuals)\\uf076Cross Validation \\uf0d8Phase –3 –Solution Model Development\\uf076Parameters versus Hyperparameters\\uf076Thresholding \\uf076Feature Engineering\\uf076Bias versus Variance Tradeoff \\uf076Model Evaluation \\uf076Non -parametric modelling (random state)2223Phase I\\uf0d8# Prediction versus Inference 24\\uf0d8# Prediction versus Inference\\uf076Goals of prediction are not centered on how the features are interacting or resulting in an event but are instead focused on the ability of the model to predicted an event.']\n",
      "['\\uf076Almost all ML methods are focused on predication not causation or inference.']\n",
      "['\\uf076This is why model performance is based largely on how well a model predicts not necessarily how much individual variables are contributing to error reduction.']\n",
      "['25Overview of Key ML Methods/Terms \\uf0d8Independent Metric for Business Value \\uf076A key part of building a solution using Machine Learning Techniques is having a metric that is independent of the model that can be used to determine if the model is providing value.']\n",
      "['\\uf076Examples\\uf0d8Recommender Engine for Netflix: Number of user clicks\\uf0d8Spam Block Predictor: Number of viruses in the network \\uf0d8Market Clustering: Did sales increase\\uf0d8Others?']\n",
      "['26Overview of Key ML Methods/Terms: Target Variable versus Features \\uf076Target variable –Is the variable that includes the patterns the machine learning algorithm is trying to learn.']\n",
      "['It is the variable of interest and key to evaluating the model output.']\n",
      "['\\uf0d8More simply it is the variable we are trying to predict.']\n",
      "['\\uf076Feature variables – Are the variables the model will use to learn the patterns of the target variable.']\n",
      "['The process of feature engineering can result in additional features.']\n",
      "['\\uf0d8More simply these are the variables used for predicting the target27Overview of Key ML Methods/Terms: Classification versus Regression \\uf0d8Classification is the process of developing a model to predict whether a target variable is in defined categories.']\n",
      "['This is driven by having either a binary or multi- level categorical variable as the target variable.']\n",
      "['\\uf0d8Examples: \\uf076Predicting whether someone is male, or female based on 1,000s of pictures.']\n",
      "['\\uf076Predicting whether a team will have a winning season or not based on player performance \\uf076Predicting whether a person will default on a loan or not\\uf0d8Key point: The predications of the model are not binary (1s or 0s) but are given as percentages indicating the likelihood that any one row of data belongs to any one category.']\n",
      "['In the case of target variables with multiple categories each row will get the same number of percent predictions as categories.']\n",
      "['28Overview of Key ML Methods/Terms: Classification versus Regression \\uf0d8Regression is the process of developing a model to predict a specific number or range of numbers.']\n",
      "['This is driven by having a continuous variable as the target variable for the model\\uf0d8Examples: \\uf076Predicting the score given the players playing a game.']\n",
      "['\\uf076Predicting an amount of rain given weather conditions \\uf076Predicting a persons weight based on various personal statistics29Overview of Key ML Methods/Terms: Probabilistic Interpretation \\uf0d8A significant portion of this class will focus on building models for classification.']\n",
      "['Classification is a much more common machine learning goal versus regression.']\n",
      "['\\uf0d8We all know the range of values for probabilities, 0 to 100, the key to understanding these outputs is to think of them as risk measures, with 100 being no risk and 0 being all the risk!']\n",
      "['\\uf0d8How the outputs are used will depend on your question.']\n",
      "['\\uf076Example: How certain do you want to be that a drug is effective as compared to whether a customer will open a marketing email?']\n",
      "['The results could both yield 75% probabilities but is that high enough?']\n",
      "['\\uf0d8Could also think of the outputs as a quantification of uncertainty, the question becomes given your problem how much uncertainty are you willing to accept?']\n",
      "['30Overview of Key ML Methods/Terms: Data Brainstorming \\uf076Data to Concept –Does the data available support the algo target and goal\\uf0d8How difficult is the data to gather?']\n",
      "['\\uf0d8Is the data large enough?']\n",
      "['\\uf0d8What is the rate of change of the data?']\n",
      "['\\uf0d8Do we believe this is the correct source and data content to address the problem?']\n",
      "['\\uf076Learning Difficulty –How complex or vague is the target variable?']\n",
      "['\\uf0d8Are there imbalances in the classes?']\n",
      "['\\uf0d8Does the data clearly link to the problem?']\n",
      "['\\uf0d8Has this data been used in the past, to what success?']\n",
      "['\\uf0d8Is the target difficult to measure or break into smaller components?']\n",
      "['\\uf0d8What risk level are you willing to accept given the question?3132Phase IIBrian’s Version of Data Science Lifecycle33Question IDBusiness UnderstandingData Acquisition/ RepresentationInitial ModelEvaluationData Understanding -EDAInitial Model(s) BuildingEvaluation Criteria/Value Metric Model Creation & Training Feature Engineering and EvaluationFinal Model DevelopmentModel DeploymentData Drift AnalysisModel Performance –Evaluation Value MetricModel Drift Analysis –Model EvaluationReports – Dashboards -Products G1 G2G3Gate ReviewsOverview of SOME Key ML Methods/Terms \\uf0d8Phase –1 Idea Development \\uf076Prediction versus Inference \\uf076Independent Metric for Business Value\\uf076Target Variable and features \\uf076Classification versus Regression\\uf076Probabilistic Interpretation \\uf076Data acquisition/gathering     \\uf0d8Phase –2 Data Prep and Problem Exploration\\uf076Variable types and data types\\uf076Baseline –prevalence\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\uf076Missing Data \\uf076Data Partitioning/Sampling \\uf076EDA (Summary Stats and Visuals)\\uf076Cross Validation \\uf0d8Phase –3 –Solution Model Development\\uf076Parameters versus Hyperparameters\\uf076Thresholding \\uf076Feature Engineering\\uf076Bias versus Variance Tradeoff \\uf076Model Evaluation\\uf076Non -parametric modelling (random state)34Overview of Key ML Methods/Terms \\uf0d8Variable Types and Data Types\\uf076Five Atomic Variable Types in R\\uf0d8Numeric –number unlimited size\\uf0d8Integer –number with constraints on size  \\uf0d8Complex –numbers and characters\\uf0d8Character –words\\uf0d8Factor –unique character class that is limited in the number of categories\\uf0d8Logical –True or False \\uf0d8Data Types \\uf076List -A list is an R -object containing different types of elements inside it like vectors, functions, and even another list inside it.']\n",
      "['\\uf076Vector -Avector in R is a series of data items of the same basic type (from above)\\uf076Array -is alistorvector with two or more dimensions\\uf076Matrix -A matrix is a two -dimensional rectangular data structure, created through the use of matrix function.']\n",
      "['Usually numeric, can’t have different data types, think of it as many vectors \\uf076Dataframe –A two dimensional object that can contain multiple variable types 35Overview of Key ML Methods/Terms \\uf0d8Some useful variable and data type  \\uf076str()\\uf076class()\\uf076names()\\uf076length()\\uf076dim()Open up Rstudio and try these functions out on the mtcars dataset.']\n",
      "['See if you agree with the output.']\n",
      "['36Overview of Key ML Methods/Terms \\uf0d8Baseline –prevalence \\uf076The proportion of a particular population found to be in the positive class at a specific time.']\n",
      "['“Positive class” in this example is the class to which we are trying to learn.']\n",
      "['Percentage split across classes of our target variable.']\n",
      "['\\uf076Using mtcars again, what is the prevalence of vs variable?']\n",
      "['37Overview of Key ML Methods/Terms \\uf0d8Scaling and/or Normalizing Data/One -Hot Encoding\\uf076Many DS approaches require the data to be normalized or placed into a standard format so comparison between variables is possible.']\n",
      "['\\uf076For factor variables this measure creating individual columns for each level that are logical or boolien 1s and 0s.']\n",
      "['\\uf076We will mostly use a min max scaler that will maintain the variance of the values but re -calculate them to be between 1 and 0.']\n",
      "['\\uf0d8Use the minmax scaling function in the gradDescent package and scale the mtcars dataset setting the results to a new object.']\n",
      "['What happens?']\n",
      "['What class is the object?']\n",
      "['Can you view the data.frame?']\n",
      "['38Overview of Key ML Methods/Terms \\uf0d8Missing Data \\uf076Large area of study concerning missing data.']\n",
      "['Here we just need to be aware of how to check for missing data and quick solutions \\uf076R comes with several functions/packages that handle missing data we are going to focus on the MICE package.']\n",
      "['\\uf0d8First you need to try to detect if there are patterns of missing data, is it random or not.']\n",
      "['If you detect patterns than you have to develop a strategy to deal with that issue.']\n",
      "['\\uf076MCAR –missing completely at random\\uf076MNAR  -missing not at random\\uf0d8Start with the summary() function on a data frame \\uf076Load in the beaches dataframe from the data file and find the columns that have missing data using the summary function\\uf076Generally variables with more the about 5% missing values should be deleted or imputation needs to occur \\uf0d8Dig a little deeper and use the md.pattern () function in the Mice package.']\n",
      "['\\uf0d8Since there doesn’t appear to be a pattern we will use complete cases to remove the NAs.']\n",
      "['39Overview of Key ML Methods/Terms \\uf0d8Missing Data \\uf076Complete.cases () function creates aindex to remove missing values \\uf0d8remove missing values from a vector x <-x[complete.cases(x)]\\uf0d8remove from a data.framedf <-df[complete.cases(df), ]\\uf0d8remove from individual rows df <-df[complete.cases(df[ , c(row1, row2, ….)]), ]Try the dataframe version on the beaches dataset, then use summary() to see if the missing datapoints are gone.']\n",
      "['MICE package can also do imputation (NA replacement) very easily, lots of examples online on how to do these in very robust ways.']\n",
      "['40Overview of Key ML Methods/Terms \\uf0d8Partitioning and Sampling\\uf076We need to split our data into three sections (in most cases) to build machine learning models \\uf076Training –What we use to build the original model \\uf076Tune –Data used to evaluate initial outputs of a model after it’s been modified (example: changing the k in kNN ) (Feature Engineering)\\uf076Test –Very last step to evaluate the quality of the model after training and tune\\uf0d8The function we will be using throughout the course will be the createDataPartition() function in the caret package.']\n",
      "['\\uf076The problem is that it’s not great at creating multiple partition, so we essentially use it twice to create a sample, then a sample of a sample.']\n",
      "['\\uf076Need to make sure to use the target variable to do stratified sampling, otherwise we could create imbalances in our samples.']\n",
      "['41Cross -Validation42Cross -Validation4344Phase IIIOverview of SOME Key ML Methods/Terms 45\\uf0d8Phase –1 Idea Development \\uf076Prediction versus Inference \\uf076Independent Metric for Business Value\\uf076Target Variable and features \\uf076Classification versus Regression\\uf076Probabilistic Interpretation \\uf076Data acquisition/gathering     \\uf0d8Phase –2 Data Prep and Problem Exploration\\uf076Variable types and data types\\uf076Baseline –prevalence\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\uf076Missing Data \\uf076Data Partitioning/Sampling \\uf076EDA (Summary Stats and Visuals)\\uf076Cross Validation \\uf0d8Phase –3 –Solution Model Development\\uf076Parameters versus Hyperparameters\\uf076Thresholding \\uf076Feature Engineering\\uf076Bias versus Variance Tradeoff \\uf076Model Evaluation\\uf076Non- parametric modelling (random state)Overview of Key ML Methods/Terms \\uf0d8# Feature Engineering – Combining or exploring different levels of variable that best work in your model.']\n",
      "['Likely going to dedicate a week to just this topic.']\n",
      "['46Overview of Key ML Methods/Terms \\uf0d8Thresholding –The percentage point where our models will predict the result to be either a 0 or 1, in the typical binary case.']\n",
      "['\\uf0d8Adjust the threshold associated with indication of a positive class.']\n",
      "['The default is 50%, could be that we want to be extra careful and instead adjust that measure up to 75% or 90%.']\n",
      "['47Overview of Key ML Methods/Terms \\uf0d8Evaluation –The metrics you use to assess model quality.']\n",
      "['There are a ton of this measures, and we are dedicating an entire week to the exploring these further.']\n",
      "['I’ll show some examples in the code for this week.']\n",
      "['48Bias Versus Variance 4950Extra Material 51Bookings.com  52Lesson Learned: Booking.comBookings.com\\uf0d8Swiss Army Knife –Their approach to ML is highly adoptable , meaning it can be used in a variety of settings –generate specific results or more generalizable depending on the inputs (data)\\uf0d8Offline Health Check– Use Randomized Control Trails (RCT) to test model outputs aligned with normative business metrics to assess quality (customer conversion)\\uf076Increase model performance doesn’t necessary translate to better gain in value53Bookings.com\\uf0d8Make a Target Before you Shoot –Develop a clear understanding of the business case and target variable (what is date flexibility) \\uf076Learning Difficulty –How complex or vague is the target\\uf076Data to Concept –Does the data available support the algo target and goal\\uf076Selection Bias –Does the model perform better for a subset of the target \\uf0d8Speed Kills –ML algos, even simple ones, take a lot of computing power –to reduce user weight time (latency) measures should be taken\\uf076See page 1748 (sparsity, model redundancy, caching…etc.)54Machine Learning Overview \\uf0d8Keep a watchful eye –Used specialized monitoring tools to understand how the models are performing in practice (even when the result was unclear)\\uf0d8Traditional Research Methods (Experimental Design) is a Best Practice Approach to ML –\\uf076“Experimentation through Randomized Controlled Trials is ingrained into Booking.com culture”55Machine Learning OverviewBrian Wright, PhD2ThemesMachine Learning LifecycleAre you ready for Machine Learning?']\n",
      "['Terms and Phases Engineering of Machine Learning Algos versus Software Development Source: https://towardsdatascience.com/stoend -to-end-data -science -life-cycle -6387523b5afcBrian’s Version of Data Science Lifecycle4Question IDBusiness UnderstandingData Acquisition -ETLInitial ModelEvaluationData Understanding -EDAInitial Model(s) BuildingEvaluation Criteria Value Metric Model Creation & Training Feature Engineering and EvaluationOptimization –Hyperpara and EvaluationModel DeploymentData Drift AnalysisModel Performance –Evaluation Value MetricModel Drift Analysis –Model EvaluationReports –Dashboards -Products G1 G2G3Gate Reviews5Machine Learning Time6“A field of Computer Science that gives computers the ability to learnwithout being explicitly programmed.”-Arthur Samuel (Coined the term in 1959 at IBM)“The ability [for systems] to acquire their own knowledge, byextracting patterns from raw data.”-Deep Learning, Goodfellow et al“A computer program is said to learn from experience E with respectto some set of tasks T and performance measure P if its performancetasks in T, as measured by P, improves with experience E.”-Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs.']\n",
      "['humanMachine HumanUnderstanding context ✔Thinking through the problem ✔Asking the right questions ✔Selecting the right tools ✔Performing calculations quickly ✔Performing repetitive tasks ✔Following pre-defined rules ✔Interpreting results ✔8Pattern discovery when inputs (x) and outputs (y) are knownSupervised machine learningInput x:VoterOutput y:Political affiliationExamples: Classification and regression are supervised machine learning 9The data inputs (x)have no target outputs (y)Unsupervised machine learningInput x:VoterOutput y:Not given(to be discovered)?']\n",
      "['We want to impose structure on the inputs (x)to say something meaningful about the data101112Machine Learning is a general use technology what does that mean?']\n",
      "['Machine Learning Overview \\uf0d8Ageneral -purpose technology orGPT is a term coined to describe a new method of producing and inventing that is important enough to have a protracted aggregate impact.']\n",
      "['\\uf0d8Similar to electricity or the internet, in that it can be applied across domains and work to improve market outcomes.']\n",
      "['13Machine Learning Overview \\uf0d8Twitter Data Usage\\uf0d8Error rates on ImageNet (10,000 labelled images) have been driven down from 30% in 2010 to less than 3% today.']\n",
      "['\\uf076Below 5% is important why?']\n",
      "['\\uf0d8Chess: Deep Blue (IBM AI) searched some 200 million positions per second, Kasparov was searching not more than 5– 10 positions probably, per second.']\n",
      "['Yet he played almost at the same level….why?']\n",
      "['14Machine Learning Overview \\uf0d8However, before we all turn into robots consider two important facts: 1.We remain remarkably far away from what would be consider a similar general intelligence that can be compared to humans2.Machines cannot do the full range of tasks that humans can doWe can then refer to jobs or activities that might be good cases for Machine Learning as SML or Suitable for Machine Learning15What are examples of tasks that might be SML and how do we know if our organizations are ready?']\n",
      "['Machine Learning Overview \\uf0d8Successful implementation of ML requires very detailed specifications on what is to be learned and data to support that learning activity.']\n",
      "['\\uf0d8Including the development of engineering features through a series of trial-and- error and..']\n",
      "['\\uf0d8Then most importantly embedding these products into normal business operations in such a way that efficiencies can be realized.']\n",
      "['16Machine Learning Overview \\uf0d8What tasks are most suitable for ML to take over: \\uf076Most recent successes are predicated on supervised learning \\uf076Competency is narrow as compared to the complexity of human decision making  1.Learning a function that maps well -defined inputs to well- defined outputsoIf can predict Y given any value of X –still might not produce the actual causal effect2.Large Data is present or can be created containing input -output pairsoThe more training data available the more arcuate the model3.Task provides clear feedback with well definable goals and metrics oIf we know what to achieve –(optimize flight patterns not a single flight)4.Where reasoning and diverse background knowledge is not necessaryoGood at empirical associations but terrible at decision making that requires common sense of historical knowledge5.No need for why the decision was made to be clearoNN could use millions numerical weights17Machine Learning Overview 6.A tolerance for error or sub- optimal solutionsoML use probabilistic outputs which means some error is always assumed7.Function of item being learned should not change rapidly over timeoWork best when the distribution of future test examples is the same roughly as the training set over timeoIf not the case systems need to be in place to refresh algorithms 18How do machines learn?']\n",
      "['\\uf0d8The basic machine learning process can be divided into three parts.']\n",
      "['\\uf076Data Input: Past data or information is utilized as a basis for future decision-making\\uf076Abstraction: The input data is represented in a broader way through the underlying algorithm\\uf076Generalization: The abstracted representation is generalized to form a framework for making decisions19(reference Introduction to ML by Subramanian Chandramouli , Saikat Dutt , Amit Kumar Das(https://learning.oreilly.com/library/view/machine -learning/9789389588132/xhtml/chapter001.xhtml#ch1_1)20Brian’s Version of Data Science Lifecycle21Question IDBusiness UnderstandingData Acquisition/ RepresentationInitial ModelEvaluationData Understanding -EDAInitial Model(s) BuildingEvaluation Criteria/Value Metric Model Creation & Training Feature Engineering and EvaluationFinal Model DevelopmentModel DeploymentData Drift AnalysisModel Performance –Evaluation Value MetricModel Drift Analysis –Model EvaluationReports – Dashboards -Products G1 G2G3Gate ReviewsOverview of SOME Key ML Methods/Terms \\uf0d8Phase –1 Idea Development \\uf076Prediction versus Inference \\uf076Independent Metric for Business Value\\uf076Target Variable and features \\uf076Classification versus Regression\\uf076Probabilistic Interpretation\\uf076Data acquisition/gathering   \\uf0d8Phase –2 Data Prep and Problem Exploration\\uf076Variable types and data types\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\uf076Missing Data \\uf076Baseline –prevalence\\uf076Data Partitioning/Sampling \\uf076EDA and (Summary Stats and Visuals)\\uf076Cross Validation \\uf0d8Phase –3 –Solution Model Development\\uf076Parameters versus Hyperparameters\\uf076Thresholding \\uf076Feature Engineering\\uf076Bias versus Variance Tradeoff \\uf076Model Evaluation \\uf076Non -parametric modelling (random state)2223Phase I\\uf0d8# Prediction versus Inference 24\\uf0d8# Prediction versus Inference\\uf076Goals of prediction are not centered on how the features are interacting or resulting in an event but are instead focused on the ability of the model to predicted an event.']\n",
      "['\\uf076Almost all ML methods are focused on predication not causation or inference.']\n",
      "['\\uf076This is why model performance is based largely on how well a model predicts not necessarily how much individual variables are contributing to error reduction.']\n",
      "['25Overview of Key ML Methods/Terms \\uf0d8Independent Metric for Business Value \\uf076A key part of building a solution using Machine Learning Techniques is having a metric that is independent of the model that can be used to determine if the model is providing value.']\n",
      "['\\uf076Examples\\uf0d8Recommender Engine for Netflix: Number of user clicks\\uf0d8Spam Block Predictor: Number of viruses in the network \\uf0d8Market Clustering: Did sales increase\\uf0d8Others?']\n",
      "['26Overview of Key ML Methods/Terms: Target Variable versus Features \\uf076Target variable –Is the variable that includes the patterns the machine learning algorithm is trying to learn.']\n",
      "['It is the variable of interest and key to evaluating the model output.']\n",
      "['\\uf0d8More simply it is the variable we are trying to predict.']\n",
      "['\\uf076Feature variables – Are the variables the model will use to learn the patterns of the target variable.']\n",
      "['The process of feature engineering can result in additional features.']\n",
      "['\\uf0d8More simply these are the variables used for predicting the target27Overview of Key ML Methods/Terms: Classification versus Regression \\uf0d8Classification is the process of developing a model to predict whether a target variable is in defined categories.']\n",
      "['This is driven by having either a binary or multi- level categorical variable as the target variable.']\n",
      "['\\uf0d8Examples: \\uf076Predicting whether someone is male, or female based on 1,000s of pictures.']\n",
      "['\\uf076Predicting whether a team will have a winning season or not based on player performance \\uf076Predicting whether a person will default on a loan or not\\uf0d8Key point: The predications of the model are not binary (1s or 0s) but are given as percentages indicating the likelihood that any one row of data belongs to any one category.']\n",
      "['In the case of target variables with multiple categories each row will get the same number of percent predictions as categories.']\n",
      "['28Overview of Key ML Methods/Terms: Classification versus Regression \\uf0d8Regression is the process of developing a model to predict a specific number or range of numbers.']\n",
      "['This is driven by having a continuous variable as the target variable for the model\\uf0d8Examples: \\uf076Predicting the score given the players playing a game.']\n",
      "['\\uf076Predicting an amount of rain given weather conditions \\uf076Predicting a persons weight based on various personal statistics29Overview of Key ML Methods/Terms: Probabilistic Interpretation \\uf0d8A significant portion of this class will focus on building models for classification.']\n",
      "['Classification is a much more common machine learning goal versus regression.']\n",
      "['\\uf0d8We all know the range of values for probabilities, 0 to 100, the key to understanding these outputs is to think of them as risk measures, with 100 being no risk and 0 being all the risk!']\n",
      "['\\uf0d8How the outputs are used will depend on your question.']\n",
      "['\\uf076Example: How certain do you want to be that a drug is effective as compared to whether a customer will open a marketing email?']\n",
      "['The results could both yield 75% probabilities but is that high enough?']\n",
      "['\\uf0d8Could also think of the outputs as a quantification of uncertainty, the question becomes given your problem how much uncertainty are you willing to accept?']\n",
      "['30Overview of Key ML Methods/Terms: Data Brainstorming \\uf076Data to Concept –Does the data available support the algo target and goal\\uf0d8How difficult is the data to gather?']\n",
      "['\\uf0d8Is the data large enough?']\n",
      "['\\uf0d8What is the rate of change of the data?']\n",
      "['\\uf0d8Do we believe this is the correct source and data content to address the problem?']\n",
      "['\\uf076Learning Difficulty –How complex or vague is the target variable?']\n",
      "['\\uf0d8Are there imbalances in the classes?']\n",
      "['\\uf0d8Does the data clearly link to the problem?']\n",
      "['\\uf0d8Has this data been used in the past, to what success?']\n",
      "['\\uf0d8Is the target difficult to measure or break into smaller components?']\n",
      "['\\uf0d8What risk level are you willing to accept given the question?3132Phase IIBrian’s Version of Data Science Lifecycle33Question IDBusiness UnderstandingData Acquisition/ RepresentationInitial ModelEvaluationData Understanding -EDAInitial Model(s) BuildingEvaluation Criteria/Value Metric Model Creation & Training Feature Engineering and EvaluationFinal Model DevelopmentModel DeploymentData Drift AnalysisModel Performance –Evaluation Value MetricModel Drift Analysis –Model EvaluationReports – Dashboards -Products G1 G2G3Gate ReviewsOverview of SOME Key ML Methods/Terms \\uf0d8Phase –1 Idea Development \\uf076Prediction versus Inference \\uf076Independent Metric for Business Value\\uf076Target Variable and features \\uf076Classification versus Regression\\uf076Probabilistic Interpretation \\uf076Data acquisition/gathering     \\uf0d8Phase –2 Data Prep and Problem Exploration\\uf076Variable types and data types\\uf076Baseline –prevalence\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\uf076Missing Data \\uf076Data Partitioning/Sampling \\uf076EDA (Summary Stats and Visuals)\\uf076Cross Validation \\uf0d8Phase –3 –Solution Model Development\\uf076Parameters versus Hyperparameters\\uf076Thresholding \\uf076Feature Engineering\\uf076Bias versus Variance Tradeoff \\uf076Model Evaluation\\uf076Non -parametric modelling (random state)34Overview of Key ML Methods/Terms \\uf0d8Variable Types and Data Types\\uf076Five Atomic Variable Types in R\\uf0d8Numeric –number unlimited size\\uf0d8Integer –number with constraints on size  \\uf0d8Complex –numbers and characters\\uf0d8Character –words\\uf0d8Factor –unique character class that is limited in the number of categories\\uf0d8Logical –True or False \\uf0d8Data Types \\uf076List -A list is an R -object containing different types of elements inside it like vectors, functions, and even another list inside it.']\n",
      "['\\uf076Vector -Avector in R is a series of data items of the same basic type (from above)\\uf076Array -is alistorvector with two or more dimensions\\uf076Matrix -A matrix is a two -dimensional rectangular data structure, created through the use of matrix function.']\n",
      "['Usually numeric, can’t have different data types, think of it as many vectors \\uf076Dataframe –A two dimensional object that can contain multiple variable types 35Overview of Key ML Methods/Terms \\uf0d8Some useful variable and data type  \\uf076str()\\uf076class()\\uf076names()\\uf076length()\\uf076dim()Open up Rstudio and try these functions out on the mtcars dataset.']\n",
      "['See if you agree with the output.']\n",
      "['36Overview of Key ML Methods/Terms \\uf0d8Baseline –prevalence \\uf076The proportion of a particular population found to be in the positive class at a specific time.']\n",
      "['“Positive class” in this example is the class to which we are trying to learn.']\n",
      "['Percentage split across classes of our target variable.']\n",
      "['\\uf076Using mtcars again, what is the prevalence of vs variable?']\n",
      "['37Overview of Key ML Methods/Terms \\uf0d8Scaling and/or Normalizing Data/One -Hot Encoding\\uf076Many DS approaches require the data to be normalized or placed into a standard format so comparison between variables is possible.']\n",
      "['\\uf076For factor variables this measure creating individual columns for each level that are logical or boolien 1s and 0s.']\n",
      "['\\uf076We will mostly use a min max scaler that will maintain the variance of the values but re -calculate them to be between 1 and 0.']\n",
      "['\\uf0d8Use the minmax scaling function in the gradDescent package and scale the mtcars dataset setting the results to a new object.']\n",
      "['What happens?']\n",
      "['What class is the object?']\n",
      "['Can you view the data.frame?']\n",
      "['38Overview of Key ML Methods/Terms \\uf0d8Missing Data \\uf076Large area of study concerning missing data.']\n",
      "['Here we just need to be aware of how to check for missing data and quick solutions \\uf076R comes with several functions/packages that handle missing data we are going to focus on the MICE package.']\n",
      "['\\uf0d8First you need to try to detect if there are patterns of missing data, is it random or not.']\n",
      "['If you detect patterns than you have to develop a strategy to deal with that issue.']\n",
      "['\\uf076MCAR –missing completely at random\\uf076MNAR  -missing not at random\\uf0d8Start with the summary() function on a data frame \\uf076Load in the beaches dataframe from the data file and find the columns that have missing data using the summary function\\uf076Generally variables with more the about 5% missing values should be deleted or imputation needs to occur \\uf0d8Dig a little deeper and use the md.pattern () function in the Mice package.']\n",
      "['\\uf0d8Since there doesn’t appear to be a pattern we will use complete cases to remove the NAs.']\n",
      "['39Overview of Key ML Methods/Terms \\uf0d8Missing Data \\uf076Complete.cases () function creates aindex to remove missing values \\uf0d8remove missing values from a vector x <-x[complete.cases(x)]\\uf0d8remove from a data.framedf <-df[complete.cases(df), ]\\uf0d8remove from individual rows df <-df[complete.cases(df[ , c(row1, row2, ….)]), ]Try the dataframe version on the beaches dataset, then use summary() to see if the missing datapoints are gone.']\n",
      "['MICE package can also do imputation (NA replacement) very easily, lots of examples online on how to do these in very robust ways.']\n",
      "['40Overview of Key ML Methods/Terms \\uf0d8Partitioning and Sampling\\uf076We need to split our data into three sections (in most cases) to build machine learning models \\uf076Training –What we use to build the original model \\uf076Tune –Data used to evaluate initial outputs of a model after it’s been modified (example: changing the k in kNN ) (Feature Engineering)\\uf076Test –Very last step to evaluate the quality of the model after training and tune\\uf0d8The function we will be using throughout the course will be the createDataPartition() function in the caret package.']\n",
      "['\\uf076The problem is that it’s not great at creating multiple partition, so we essentially use it twice to create a sample, then a sample of a sample.']\n",
      "['\\uf076Need to make sure to use the target variable to do stratified sampling, otherwise we could create imbalances in our samples.']\n",
      "['41Cross -Validation42Cross -Validation4344Phase IIIOverview of SOME Key ML Methods/Terms 45\\uf0d8Phase –1 Idea Development \\uf076Prediction versus Inference \\uf076Independent Metric for Business Value\\uf076Target Variable and features \\uf076Classification versus Regression\\uf076Probabilistic Interpretation \\uf076Data acquisition/gathering     \\uf0d8Phase –2 Data Prep and Problem Exploration\\uf076Variable types and data types\\uf076Baseline –prevalence\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\uf076Missing Data \\uf076Data Partitioning/Sampling \\uf076EDA (Summary Stats and Visuals)\\uf076Cross Validation \\uf0d8Phase –3 –Solution Model Development\\uf076Parameters versus Hyperparameters\\uf076Thresholding \\uf076Feature Engineering\\uf076Bias versus Variance Tradeoff \\uf076Model Evaluation\\uf076Non- parametric modelling (random state)Overview of Key ML Methods/Terms \\uf0d8# Feature Engineering – Combining or exploring different levels of variable that best work in your model.']\n",
      "['Likely going to dedicate a week to just this topic.']\n",
      "['46Overview of Key ML Methods/Terms \\uf0d8Thresholding –The percentage point where our models will predict the result to be either a 0 or 1, in the typical binary case.']\n",
      "['\\uf0d8Adjust the threshold associated with indication of a positive class.']\n",
      "['The default is 50%, could be that we want to be extra careful and instead adjust that measure up to 75% or 90%.']\n",
      "['47Overview of Key ML Methods/Terms \\uf0d8Evaluation –The metrics you use to assess model quality.']\n",
      "['There are a ton of this measures, and we are dedicating an entire week to the exploring these further.']\n",
      "['I’ll show some examples in the code for this week.']\n",
      "['48Bias Versus Variance 4950Extra Material 51Bookings.com  52Lesson Learned: Booking.comBookings.com\\uf0d8Swiss Army Knife –Their approach to ML is highly adoptable , meaning it can be used in a variety of settings –generate specific results or more generalizable depending on the inputs (data)\\uf0d8Offline Health Check– Use Randomized Control Trails (RCT) to test model outputs aligned with normative business metrics to assess quality (customer conversion)\\uf076Increase model performance doesn’t necessary translate to better gain in value53Bookings.com\\uf0d8Make a Target Before you Shoot –Develop a clear understanding of the business case and target variable (what is date flexibility) \\uf076Learning Difficulty –How complex or vague is the target\\uf076Data to Concept –Does the data available support the algo target and goal\\uf076Selection Bias –Does the model perform better for a subset of the target \\uf0d8Speed Kills –ML algos, even simple ones, take a lot of computing power –to reduce user weight time (latency) measures should be taken\\uf076See page 1748 (sparsity, model redundancy, caching…etc.)54Machine Learning Overview \\uf0d8Keep a watchful eye –Used specialized monitoring tools to understand how the models are performing in practice (even when the result was unclear)\\uf0d8Traditional Research Methods (Experimental Design) is a Best Practice Approach to ML –\\uf076“Experimentation through Randomized Controlled Trials is ingrained into Booking.com culture”55Machine Learning OverviewBrian Wright, PhD2ThemesMachine Learning LifecycleAre you ready for Machine Learning?']\n",
      "['Terms and Phases Engineering of Machine Learning Algos versus Software Development Source: https://towardsdatascience.com/stoend -to-end-data -science -life-cycle -6387523b5afcBrian’s Version of Data Science Lifecycle4Question IDBusiness UnderstandingData Acquisition -ETLInitial ModelEvaluationData Understanding -EDAInitial Model(s) BuildingEvaluation Criteria Value Metric Model Creation & Training Feature Engineering and EvaluationOptimization –Hyperpara and EvaluationModel DeploymentData Drift AnalysisModel Performance –Evaluation Value MetricModel Drift Analysis –Model EvaluationReports –Dashboards -Products G1 G2G3Gate Reviews5Machine Learning Time6“A field of Computer Science that gives computers the ability to learnwithout being explicitly programmed.”-Arthur Samuel (Coined the term in 1959 at IBM)“The ability [for systems] to acquire their own knowledge, byextracting patterns from raw data.”-Deep Learning, Goodfellow et al“A computer program is said to learn from experience E with respectto some set of tasks T and performance measure P if its performancetasks in T, as measured by P, improves with experience E.”-Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs.']\n",
      "['humanMachine HumanUnderstanding context ✔Thinking through the problem ✔Asking the right questions ✔Selecting the right tools ✔Performing calculations quickly ✔Performing repetitive tasks ✔Following pre-defined rules ✔Interpreting results ✔8Pattern discovery when inputs (x) and outputs (y) are knownSupervised machine learningInput x:VoterOutput y:Political affiliationExamples: Classification and regression are supervised machine learning 9The data inputs (x)have no target outputs (y)Unsupervised machine learningInput x:VoterOutput y:Not given(to be discovered)?']\n",
      "['We want to impose structure on the inputs (x)to say something meaningful about the data101112Machine Learning is a general use technology what does that mean?']\n",
      "['Machine Learning Overview \\uf0d8Ageneral -purpose technology orGPT is a term coined to describe a new method of producing and inventing that is important enough to have a protracted aggregate impact.']\n",
      "['\\uf0d8Similar to electricity or the internet, in that it can be applied across domains and work to improve market outcomes.']\n",
      "['13Machine Learning Overview \\uf0d8Twitter Data Usage\\uf0d8Error rates on ImageNet (10,000 labelled images) have been driven down from 30% in 2010 to less than 3% today.']\n",
      "['\\uf076Below 5% is important why?']\n",
      "['\\uf0d8Chess: Deep Blue (IBM AI) searched some 200 million positions per second, Kasparov was searching not more than 5– 10 positions probably, per second.']\n",
      "['Yet he played almost at the same level….why?']\n",
      "['14Machine Learning Overview \\uf0d8However, before we all turn into robots consider two important facts: 1.We remain remarkably far away from what would be consider a similar general intelligence that can be compared to humans2.Machines cannot do the full range of tasks that humans can doWe can then refer to jobs or activities that might be good cases for Machine Learning as SML or Suitable for Machine Learning15What are examples of tasks that might be SML and how do we know if our organizations are ready?']\n",
      "['Machine Learning Overview \\uf0d8Successful implementation of ML requires very detailed specifications on what is to be learned and data to support that learning activity.']\n",
      "['\\uf0d8Including the development of engineering features through a series of trial-and- error and..']\n",
      "['\\uf0d8Then most importantly embedding these products into normal business operations in such a way that efficiencies can be realized.']\n",
      "['16Machine Learning Overview \\uf0d8What tasks are most suitable for ML to take over: \\uf076Most recent successes are predicated on supervised learning \\uf076Competency is narrow as compared to the complexity of human decision making  1.Learning a function that maps well -defined inputs to well- defined outputsoIf can predict Y given any value of X –still might not produce the actual causal effect2.Large Data is present or can be created containing input -output pairsoThe more training data available the more arcuate the model3.Task provides clear feedback with well definable goals and metrics oIf we know what to achieve –(optimize flight patterns not a single flight)4.Where reasoning and diverse background knowledge is not necessaryoGood at empirical associations but terrible at decision making that requires common sense of historical knowledge5.No need for why the decision was made to be clearoNN could use millions numerical weights17Machine Learning Overview 6.A tolerance for error or sub- optimal solutionsoML use probabilistic outputs which means some error is always assumed7.Function of item being learned should not change rapidly over timeoWork best when the distribution of future test examples is the same roughly as the training set over timeoIf not the case systems need to be in place to refresh algorithms 18How do machines learn?']\n",
      "['\\uf0d8The basic machine learning process can be divided into three parts .']\n",
      "['\\uf076Data Input: Past data or information is utilized as a basis for future decision-making\\uf076Abstraction : The input data is represented in a broader way through the underlying algorithm\\uf076Generalization : The abstracted representation is generalized to form a framework for making decisions19(reference Introduction to ML by Subramanian Chandramouli , Saikat Dutt , Amit Kumar Das(https://learning.oreilly.com/library/view/machine -learning/9789389588132/xhtml/chapter001.xhtml#ch1_1)20Brian’s Version of Data Science Lifecycle21Question IDBusiness UnderstandingData Acquisition/ RepresentationInitial ModelEvaluationData Understanding -EDAInitial Model(s) BuildingEvaluation Criteria/Value Metric Model Creation & Training Feature Engineering and EvaluationFinal Model DevelopmentModel DeploymentData Drift AnalysisModel Performance –Evaluation Value MetricModel Drift Analysis –Model EvaluationReports – Dashboards -Products G1 G2G3Gate ReviewsOverview of SOME Key ML Methods/Terms \\uf0d8Phase –1 Idea Development \\uf076Prediction versus Inference \\uf076Independent Metric for Business Value\\uf076Target Variable and features \\uf076Classification versus Regression\\uf076Probabilistic Interpretation\\uf076Data acquisition/gathering   \\uf0d8Phase –2 Data Prep and Problem Exploration\\uf076Variable classes/types\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\uf076Missing Data \\uf076Baseline –prevalence\\uf076Data Partitioning/Sampling \\uf076EDA and (Summary Stats and Visuals)\\uf0d8Phase –3 –Solution Model Development\\uf076Parameters versus Hyperparameters\\uf076Thresholding \\uf076Feature Engineering\\uf076Bias versus Variance Tradeoff \\uf076Model Evaluation \\uf076Non -parametric modelling (random state) 2223Phase I\\uf0d8# Prediction versus Inference 24\\uf0d8# Prediction versus Inference\\uf076Goals of prediction are not centered on how the features are interacting or resulting in an event but are instead focused on the ability of the model to predicted an event.']\n",
      "['\\uf076Almost all ML methods are focused on predication not causation or inference.']\n",
      "['\\uf076This is why model performance is based largely on how well a model predicts not necessarily how much individual variables are contributing to error reduction.']\n",
      "['25Overview of Key ML Methods/Terms \\uf0d8Independent Metric for Business Value \\uf076A key part of building a solution using Machine Learning Techniques is having a metric that is independent of the model that can be used to determine if the model is providing value.']\n",
      "['\\uf076Examples\\uf0d8Recommender Engine for Netflix: Number of user clicks\\uf0d8Spam Block Predictor: Number of viruses in the network \\uf0d8Market Clustering: Did sales increase\\uf0d8Others?']\n",
      "['26Overview of Key ML Methods/Terms: Target Variable versus Features \\uf076Target variable –Is the variable that includes the patterns the machine learning algorithm is trying to learn.']\n",
      "['It is the variable of interest and key to evaluating the model output.']\n",
      "['\\uf0d8More simply it is the variable we are trying to predict.']\n",
      "['\\uf076Feature variables – Are the variables the model will use to learn the patterns of the target variable.']\n",
      "['The process of feature engineering can result in additional features.']\n",
      "['\\uf0d8More simply these are the variables used for predicting the target27Overview of Key ML Methods/Terms: Classification versus Regression \\uf0d8Classification is the process of developing a model to predict whether a target variable is in defined categories.']\n",
      "['This is driven by having either a binary or multi- level categorical variable as the target variable.']\n",
      "['\\uf0d8Examples: \\uf076Predicting whether someone is male, or female based on 1,000s of pictures.']\n",
      "['\\uf076Predicting whether a team will have a winning season or not based on player performance \\uf076Predicting whether a person will default on a loan or not\\uf0d8Key point: The predications of the model are not binary (1s or 0s) but are given as percentages indicating the likelihood that any one row of data belongs to any one category.']\n",
      "['In the case of target variables with multiple categories each row will get the same number of percent predictions as categories.']\n",
      "['28Overview of Key ML Methods/Terms: Classification versus Regression \\uf0d8Regression is the process of developing a model to predict a specific number or range of numbers.']\n",
      "['This is driven by having a continuous variable as the target variable for the model\\uf0d8Examples: \\uf076Predicting the score given the players playing a game.']\n",
      "['\\uf076Predicting an amount of rain given weather conditions \\uf076Predicting a persons weight based on various personal statistics29Overview of Key ML Methods/Terms: Probabilistic Interpretation \\uf0d8A significant portion of this class will focus on building models for classification.']\n",
      "['Classification is a much more common machine learning goal versus regression.']\n",
      "['\\uf0d8We all know the range of values for probabilities, 0 to 100, the key to understanding these outputs is to think of them as risk measures, with 100 being no risk and 0 being all the risk!']\n",
      "['\\uf0d8How the outputs are used will depend on your question.']\n",
      "['\\uf076Example: How certain do you want to be that a drug is effective as compared to whether a customer will open a marketing email?']\n",
      "['The results could both yield 75% probabilities but is that high enough?']\n",
      "['\\uf0d8Could also think of the outputs as a quantification of uncertainty, the question becomes given your problem how much uncertainty are you willing to accept?']\n",
      "['30Overview of Key ML Methods/Terms: Data Brainstorming \\uf076Data to Concept –Does the data available support the algo target and goal\\uf0d8How difficult is the data to gather?']\n",
      "['\\uf0d8Is the data large enough?']\n",
      "['\\uf0d8What is the rate of change of the data?']\n",
      "['\\uf0d8Do we believe this is the correct source and data content to address the problem?']\n",
      "['\\uf076Learning Difficulty –How complex or vague is the target variable?']\n",
      "['\\uf0d8Are there imbalances in the classes?']\n",
      "['\\uf0d8Does the data clearly link to the problem?']\n",
      "['\\uf0d8Has this data been used in the past, to what success?']\n",
      "['\\uf0d8Is the target difficult to measure or break into smaller components?']\n",
      "['\\uf0d8What risklevel areyou willing toaccept given thequestion?3132Phase IIOverview of SOME Key ML Methods/Terms \\uf0d8Phase –1 Idea Development \\uf076Prediction versus Inference \\uf076Independent Metric for Business Value\\uf076Target Variable and features \\uf076Classification versus Regression\\uf076Probabilistic Interpretation \\uf076Data acquisition/gathering     \\uf0d8Phase –2 Data Prep and Problem Exploration\\uf076Variable classes/types\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\uf076Missing Data \\uf076Baseline –prevalence\\uf076Data Partitioning/Sampling \\uf076EDA (Summary Stats and Visuals)\\uf0d8Phase –3 –Solution Model Development\\uf076Parameters versus Hyperparameters\\uf076Thresholding \\uf076Feature Engineering\\uf076Bias versus Variance Tradeoff \\uf076Model Evaluation\\uf076Non -parametric modelling (random state) 33Overview of Key ML Methods/Terms \\uf0d8# Baseline – prevalence 3435Phase IIIOverview of SOME Key ML Methods/Terms \\uf0d8Phase –1 Idea Development \\uf076Prediction versus Inference \\uf076Independent Metric for Business Value\\uf076Target Variable and features \\uf076Classification versus Regression\\uf076Probabilistic Interpretation\\uf076Data acquisition/gathering      \\uf0d8Phase –2 Data Prep and Problem Exploration\\uf076Variable classes/types\\uf076Scaling and/or Normalizing Data/One -Hot Encoding\\uf076Missing Data \\uf076Baseline –prevalence\\uf076Data Partitioning/Sampling \\uf076EDA (Summary Stats and Visuals)\\uf0d8Phase –3 –Solution Model Development\\uf076Parameters versus Hyperparameters\\uf076Thresholding \\uf076Feature Engineering\\uf076Bias versus Variance Tradeoff \\uf076Model Evaluation \\uf076Non -parametric modelling (random state) 36Overview of Key ML Methods/Terms \\uf0d8# Feature Engineering – Combining or exploring different levels of variable that best work in your model.']\n",
      "['Likely going to dedicate a week to just this topic.']\n",
      "['37Overview of Key ML Methods/Terms \\uf0d8Thresholding –The percentage point where our models will predict the result to be either a 0 or 1, in the typical binary case.']\n",
      "['\\uf0d8Adjust the threshold associated with indication of a positive class.']\n",
      "['The default is 50%, could be that we want to be extra careful and instead adjust that measure up to 75% or 90%.']\n",
      "['38Overview of Key ML Methods/Terms \\uf0d8Evaluation –The metrics you use to assess model quality.']\n",
      "['There are a ton of this measures, and we are dedicating an entire week to the exploring these further.']\n",
      "['I’ll show some examples in the code for this week.']\n",
      "['39Bias Versus Variance 4041Extra Material 42Bookings.com  43Lesson Learned: Booking.comBookings.com\\uf0d8Swiss Army Knife –Their approach to ML is highly adoptable , meaning it can be used in a variety of settings –generate specific results or more generalizable depending on the inputs (data)\\uf0d8Offline Health Check– Use Randomized Control Trails (RCT) to test model outputs aligned with normative business metrics to assess quality (customer conversion)\\uf076Increase model performance doesn’t necessary translate to better gain in value44Bookings.com\\uf0d8Make a Target Before you Shoot –Develop a clear understanding of the business case and target variable (what is date flexibility) \\uf076Learning Difficulty –How complex or vague is the target\\uf076Data to Concept –Does the data available support the algo target and goal\\uf076Selection Bias –Does the model perform better for a subset of the target \\uf0d8Speed Kills –ML algos, even simple ones, take a lot of computing power –to reduce user weight time (latency) measures should be taken\\uf076See page 1748 (sparsity, model redundancy, caching…etc.)45Machine Learning Overview \\uf0d8Keep a watchful eye –Used specialized monitoring tools to understand how the models are performing in practice (even when the result was unclear)\\uf0d8Traditional Research Methods (Experimental Design) is a Best Practice Approach to ML –\\uf076“Experimentation through Randomized Controlled Trials is ingrained into Booking.com culture”46Machine Learning Overview, EDA and ClusteringBrian Wrightbrianwright@virginia.edu1.What is Machine Learning ?']\n",
      "['2.What is exploratory data analysis?']\n",
      "['3.k-means clustering–Does Congress vote in patterns?']\n",
      "['4.Multi -dimensional k -means clustering–Are NBA players compensated according to performance?Outline: Intro to Unsupervised ML2“A field of Computer Science that gives computers the ability to learnwithout being explicitly programmed.”-Arthur Samuel (Coined the term in 1959 at IBM)“The ability [for systems] to acquire their own knowledge, byextracting patterns from raw data.”-Deep Learning , Goodfellow et alMachine vs.']\n",
      "['humanMachine HumanUnderstanding context ✔Thinking through the problem ✔Asking the right questions ✔Selecting the right tools ✔Performing calculations quickly ✔Performing repetitive tasks ✔Following pre-defined rules ✔Interpreting results ✔5Pattern discovery when inputs (x) and outputs (y) are knownSupervised machine learningInput x:VoterOutput y:Political affiliationExamples: Classification and regression are supervised machine learning 6The data inputs (x)have no target outputs (y)Unsupervised machine learningInput x:VoterOutput y:Not given(to be discovered)?']\n",
      "['We want to impose structure on the inputs (x) to say something meaningful about the data789\\uf06eGiven  D :{2,4,10,12,3,11,20,25,30},  and  k=2 clusters\\uf06eRandomly assign the means:  m1=3, m2=4\\uf06eK1={2,3}, K2={4,10,12,11,20,25,30}, m1=2.5, m2=16\\uf06eK1={2,3,4}, K2={10,12,11,20,25,30}, m1=3, m2=18\\uf06eK1={2,3,4,10}, K2={12,11,20,25,30}, m1=4.75, m2=19.6\\uf06eK1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\\uf06eK1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\\uf06eStop, since the clusters and the means found in all subsequent iterations will be the same .Example of K -Means•Value of more accurate information = incremental improvement of results above status quo•What is the value of being 5% more accurate ?']\n",
      "['–For Wal -Mart: on ~$470B of revenues, $23B of potential revenue increase by more accurately predicting demand or recommending the right products–For the IRS: on ~$21B of annual tax fraud, $1B of potential incremental tax collections•~30% of Amazon.com sales come from product recommendationsMaking $ with Machine Learnin gSource: Predictive Analytics by Eric Siegel; NBC News10•The value of data mining and predictive analytics is in how you use these tools•Insights on their own can have little value without intelligent applicationHow does data mining make $?']\n",
      "['11Engineering of Machine Learning Algos versus Software Development Source: https://towardsdatascience.com/stoend -to-end-data -science -life-cycle -6387523b5afcData Science Life Cycle (Everything includes Evaluation13TrainFeature EngineerTest DeployEvaluate Evaluate Evaluate EvaluateMonitor1.What is Machine Learning?']\n",
      "['2.What is exploratory data analysis?']\n",
      "['3.k-means clustering–Does Congress vote in patterns?']\n",
      "['4.Multi -dimensional k -means clustering–Are NBA players compensated according to performance?Outline: Intro to Unsupervised ML14•Exploratory data analysis or “EDA” is an approach where the intent is to see what the data can tell us beyond modeling or hypothesis testing–Data visualization is one of the most common forms of EDAWhat is exploratory data analysis?']\n",
      "['15When data is too big or complex to be analyzed just by visualizing it, these types of analysis can help:1.Clustering: compare pieces of data by measuring similarity among them2.Network analysis: analyze how people, places and entities are connected to evaluate the properties and structure of a network 3.Text mining: analyze what large bodies of unstructured or structured text sayTypes of exploratory data analysis16The data inputs have (x)no target outputs (y)Unsupervised machine learningInput x:VoterOutput y:Not given(To be discovered)?']\n",
      "['We want to impose structure on the inputs (x)to say something meaningful about the data171.Technique for finding similarity between groups2.Type of unsupervised machine learning•Not the only class of unsupervised learning        algorithms3.Similarity needs to be defined•Will depend on attributes of data•Usually a distance metricWhat is clustering?']\n",
      "['18Key assumption: data points that are “closer” together are related or similar•Haimowitz and Schwarz 1997 paper on clustering for credit line optimization–http://www.aaai.org/Papers/Workshops/1997/WS-97-07/WS97- 07-006.pdf•Cluster existing GE Capital customers based on similarity in use of their credit cards to pay bills and customers’ profitability to GE Capital•Resulted in five clusters of consumer credit behavior•Created classification model to predict customer type and offer tailored productsGE Capital case study: grouping clients19•Between 2001 and 2004 most European countries passed legislation that allowed customers to keep their cell phone number if they switched carriers•Telenor, one of the largest telecommunications companies in Norway wanted to ensure it kept its customers–Problem: the promotions the company sent to its clients reminded them that they could leave and resulted in greater defections !']\n",
      "['–Solution: predict which customers, if contacted, are more likely to stay with the company •Results:–Marketing campaign ROI increased 11x–Customer churn decreased 36%–Marketing campaign costs decreased 40%Telenor case study: predicting behavior201.What is Machine Learning?']\n",
      "['2.What is exploratory data analysis?']\n",
      "['3.k-means clustering–Does Congress vote in patterns?']\n",
      "['4.Multi -dimensional k -means clustering –Lab –Are NBA players compensated according to performance?Outline: Intro to Unsupervised ML21Example use case General question ConceptDoes Congress vote in patterns?']\n",
      "['Is there a pattern ?']\n",
      "['Is there structure in unstructured data?k-means clusteringAre basketball players \"priced\" efficiently (based on performance)?How to uncover trends with many variables that you    can\\'t easily visualize?k-means clustering in many dimensionsConcept summary221.Data set consists of 427 members (observations) 2.Members served a full year in 20133.Three vote types:•“Aye”•“Nay”•“Other”Goal: to understand how polarized the US Congress isPolitical clusteringThe joint session of Congress on Capitol Hill in Washington23•How do we identify swing votes?']\n",
      "['–Lobbying–Bridging party lines•Assumption:–Democrats and Republicans vote among partisan lines, which generates clustersEach data point represents a member of CongressFinding voting patterns24Intra vs.']\n",
      "['inter- cluster distanceIntra-Cluster DistanceInter-Cluster DistanceObjective: minimize intra -cluster distan ce, maximize inter -cluster distance25•The centroid is the average location of all points in the cluster•Another definition: the centroid minimizes the distance between a central location and all the data points in the clusterNote: Centroids are generally not existing data points, rather locations in spacek-means clustering is based on centroids261.Randomly choose k data points to be centroids k-means in 4 steps271.Randomly choose k data points to be centroids 2.Assign each point to closest centroidk-means in 4 steps281.Randomly choose k data points to be centroids 2.Assign each point to closest centroid3.Recalculate centroids based on current cluster membershipk-means in 4 steps291.Randomly choose k data points to be centroids 2.Assign each point to closest centroid3.Recalculate centroids based on current cluster membershipk-means in 4 steps304.Repeat steps 2 -3 with the new centroids until the centroids don’t change anymoreStep 1: load packages and data# Install packagesinstall.packages(\"e1071\") install.packages(\"ggplot2\" )# Load librarieslibrary(e1071)library(ggplot2)library(help = e1071)Learn about all the functionality of the package, be well informed about what you\\'re doing!']\n",
      "['31Step 1: load packages and data# Loading house datahouse_votes_Dem = read_csv (\"house_votes_Dem.csv\")# What does the data look like?']\n",
      "['View( house_votes_Dem )Script32Step 2: run k -means# Define the columns to be clustered by subsetting the dataclust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]# Run an algorithm with 2 centersset.seed(1 )kmeans_obj_Dem = kmeans(clust_data_Dem , centers = 2, algorithm = \"Lloyd\")# What does the new variable kmeans_obj contain?kmeans_obj_Dem# View the results of each output of the kmeans # functionhead( kmeans_obj_Dem)Script1.By placing the set of data we want     after the comma, we tell R we’re   looking for columns 2.kmeans uses a different starting data point each time it runs.']\n",
      "['To make the results reproducible make R start from the same point every time with set.seed()3.We’re not specifying the number of iterations so R defaults to 104.We’ll see that kmeans produces a list    of vectors of different lengths.']\n",
      "['As a result, we cannot use the View() function33Step 2: run k -means1.Number of points each cluster contains2.The “location” of each cluster center is specified by 3 coordinates, one for each column we’re clustering3.The list assigning either cluster 1 or 2 to each data point1.79.5% of the variance between the data points is explained by our clustering, we will discuss this in detail later2.List of other types of data included in kmeans_obj34Measuring distance(3,3)(1,2) 21Distance = √(22+12)x35•cluster: a vector indicating the cluster to which each point is allocated•centers: a matrix of cluster centers•totss: the total sum of squares (sum of distances between all points)•withinss: vector of within -cluster sum of distances, one number per cluster•tot.withinss: total within -cluster sum of distances, i.e.sum of withinss•betweenss: the between -cluster sum of squares, i.e.']\n",
      "['totss -tot.withinss•size: the number of points in each clusterTo learn more about the kmeans function run ?kmeanskmeans outputs36Intra vs.']\n",
      "['inter- cluster distanceIntra-Cluster DistanceInter-Cluster Distancewithinssbetweensstotss = withinss +betweenss37Step 3: visualize plot# Tell R to read the cluster labels as factors so that ggplot2 (the # graphing  package) can read them as category labels instead of # continuous variables (numeric variables).party_clusters_Dem = as.factor(kmeans_obj_Dem$cluster)# What does party_clusters look like?View( party_clusters_Dem )View(as.data.frame(party_clusters_Dem))# Set up labels for our data so that we can compare Democrats and # Republicans.party_labels_Dem = house_votes_Dem$partyScript38ggplot(house_votes_Dem, aes(x = aye, y = nay,shape = party_clusters_Dem)) + geom_point(size = 6) +ggtitle(\"Aye vs.']\n",
      "['Nay votes for Democrat -introduced bills\") +xlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", labels = c(\"Cluster 1\", \"Cluster 2\" ),values = c(\"1\" , \"2\")) +theme_light()Step 3: visualize plotCosmetics layerBase layerGeom LayerTitles and axisShapeTheme39ScriptStep 3: visualize plot40•Two groups exist•Algorithm identifies voting patternsWhat can we infer about the different clusters?Step 4: analyze results41ggplot(house_votes_Dem, aes(x = yea, y = nay,color= party_labels_Dem,shape = party_clusters_Dem)) + geom_point(size = 6) +ggtitle(\"Aye vs.']\n",
      "['Nay votes for Democrat -introduced bills\") +xlab(\"Number of Aye Votes\") +ylab(\"Number of Nay Votes\") +scale_shape_manual(name = \"Cluster\", labels = c(\"Cluster 1\", \"Cluster 2\" ),values = c(\"1\" , \"2\")) +scale_color_manual(name = \"Party\", labels = c(\"Democratic\", \"Republican\"),values = c(\"blue\" , \"red\"))+theme_light()Step 5: validate resultsCosmetics layerScriptBase layerGeom LayerTitles and axisColor and shapeTheme42Step 5: validate results43•Diffuse among Democrats•Republicans more dense•Can gauge “outliers”•Can see the polarization between the two political parties Step 6: interpret resultsoutlieroutlier44•Clustering is more powerful than the human eye in3D•Clustering mathematically defines which cluster the peripheral points should be in when it’s not obvious to the human eye•Clustering is helpful when many dimensions / variables exist that you can’t visualize at once–Whiskey similarity example from classification lectureClustering vs.']\n",
      "['visualizingAye, Nay and Other Votesin House of Representatives45•Goals of clustering:–Maximize the separation between clusters •i.e.Maximize inter -cluster distance –Keep similar points in a cluster close together •i.e.Minimize intra-cluster distanceHow good is the clustering?']\n",
      "['46•Look at the variance explained by clusters–In particular, the ratio of inter -cluster variance to total variance•How much of the total variance is explained by the clustering?Assessing how well an algorithm performsHow good is the clustering?']\n",
      "['Variation explained by clusters= inter-cluster variance / total variance47•cluster: a vector indicating the cluster to which each point is allocated•centers: a matrix of cluster centers•totss: the total sum of squares (sum of distances between all points)•withinss: vector of within -cluster sum of distances, one number per cluster•tot.withinss: total within -cluster sum of distances, i.e.sum of withinss•betweenss: the between -cluster sum of squares, i.e.']\n",
      "['totss -tot.withinss•size: the number of points in each clusterTo learn more about the kmeans function run ?kmeanskmeans outputs48Intra vs.']\n",
      "['inter- cluster distanceIntra-Cluster DistanceInter-Cluster Distancewithinssbetweensstotss = withinss +betweenss49How good is the clustering?']\n",
      "['# Inter-cluster variance,# \"betweenss\" is the sum of the # distances between points from # different clustersnum_Dem= kmeans_obj_Dem$betweenss# Total variance# \"totss\" is the sum of the distances  # between all the points in # the data setdenom_Dem = kmeans_obj_Dem$totss# Variance accounted for by # clustersvar_exp_Dem = num_Dem/ denom_Demvar_exp_Dem[1] 0.7952692Script50•It’s easier when the number of clusters is known ahead of time, but what if we don\\'t know how many clusters we should have?']\n",
      "['•Since different starting points may generate different clusters, we need a way to assess cluster quality as well.How do we choose the number of clusters ( i.e.k)?How good is the clustering?']\n",
      "['511.Elbow method–Computes the percentage of variance explained by clusters for a range of cluster numbers–Plots a graph so results are easier to see –Not guaranteed to work!']\n",
      "['It depends on the data in question2.NbClustHow to select k: two methods–Runs 30 different tests and provides “majority vote” for the best number of clusters (k’s) to use52Elbow method: measure variance# Run algorithm with 3 centersset.seed(1 )kmeans_obj_Dem = kmeans(clust_data_Dem,   centers = 3,algorithm = \"Lloyd\")# Inter- cluster variancenum_Dem = kmeans_obj_Dem$ betweenss# Total variancedenom_Dem = kmeans_obj_Dem $totss# Variance accounted for by clustersvar_exp_Dem = num_Dem / denom_Demvar_exp_Dem[1] 0.8463623Script53•We want to repeat the variance calculation from the previous slide for several numbers of clusters automatically•We can create a function that contains all the steps we want to automate Automating a step we want to repeatfunction(data, item to iterate through)54# The function explained_variance wraps our code from previous slides.']\n",
      "['explained_variance = function( data_in, k){# Running k- means algorithmset.seed(1 )  kmeans_obj = kmeans(data_in, centers = k,algorithm = \"Lloyd\" )# Variance accounted for by clustersvar_exp = kmeans_obj $betweenss / kmeans_obj$totssvar_exp}Automating a step we want to repeatScript1.A new variable is created and set equal to our function()2.The commands inside the function are wrapped in curly braces {}3.Inside the parentheses, we specify the variables that the user will input and that will then be used inside the function where they appear55# Recall the variable we are using for the # data that we\\'re clustering.']\n",
      "['clust_data_Dem = house_votes_Dem[, c(\"aye\" , \"nay\", \"other\")]View( clust_data_Dem)# The sapply() function plugs several values # into explained_variance.']\n",
      "['explained_var_Dem = sapply(1 :10, explained_variance, data_in = clust_data_Dem)View( explained_var_Dem)# Data for ggplot2elbow_data_Dem = data.frame(k = 1:10, explained_var_Dem)View( elbow_data_Dem)Automating a step we want to repeat1.sapply() applies a function to a vector2.We have to tell sapply() that the we want the explained_variance function to use the clust_data data3.Next, we create a data frame that contains both the new variance variable (explained_var_Dem) and the different numbers of k that we used in the previous function (1 through 10)Function we created Script56# Plotting dataggplot(elbow_data_Dem, aes(x = k,  y = explained_var_Dem)) + geom_point(size = 4) +geom_line(size = 1 ) +xlab(\"k\" ) + ylab(\"Intercluster Variance/Total Variance\" ) + theme_light()Elbow method: plotting the graphScript1.geom_point() sets the size of the data points2.geom_line() sets the thickness of the line57Looking for the kink in graph of  inter- cluster variance / total varianceElbow method: measure varianceOriginal data Elbow methodk = 258Elbow method: measure variancek 1 2 3Inter-cluster variance/ total variance~0% 79.5% 86.4%k =1 k =2 k =359•Library: \"NbClust\"Functions:  \"NbClust\"Inputs : •data –data array or data frame•min.nc / max.nc –minimum/maximum number of clusters•method –\"kmeans\"•There are other, more advanced arguments that can be customized but are outside of the scope of this course and are note necessary to for NbClust to workThere are a number of ways to choose the right k.']\n",
      "['NbClust runs 30 tests and selects k based on majority voteNbClust: k by majority voteNbClust(data, max.nc, method = \"kmeans\")60# Install the package.']\n",
      "['install.packages(\"NbClust\" )library(NbClust)# Run NbClust.']\n",
      "['nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")# View the output of NbClust.nbclust_obj_Dem# View the output that shows the number of clusters each # method recommends.View( nbclust_obj_Dem $Best.nc)NbClust : k by majority voteScript61NbClust: k by majority vote> nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")...']\n",
      "['******************************************************************* * Among all indices:                                                * 12 proposed 2 as the best number of clusters * 4 proposed 3 as the best number of clusters ...']\n",
      "['***** Conclusion *****                            * According to the majority rule, the best number of clusters is  2 *******************************************************************Note: additional information appears; the above information is most relevant to us for nowConsole62NbClust: k by majority vote> nbclust_obj_Dem Console•nbclust_obj_Dem shows the outputs of NbClust–One of the outputs is Best.nc, which shows the number of clusters                               recommended by each test 63NbClust: k by majority vote•We want to visualize a histogram to make it obvious how many votes there are for each number of clusters 64# Subset the 1st row from Best.nc and convert it  # to a data frame, so ggplot2 can plot it.']\n",
      "['freq_k_Dem = nbclust_obj_Dem $Best.nc[1 ,]freq_k_Dem = data.frame( freq_k_Dem)View(freq_k_Dem)# Check the maximum number of clusters.']\n",
      "['max(freq_k_Dem )# Plot as a histogram.']\n",
      "['ggplot(freq_k_Dem,aes(x = freq_k_Dem)) +geom_bar() +scale_x_continuous(breaks = seq(0 , 15, by = 1)) +scale_y_continuous(breaks = seq(0 , 12, by = 1)) +labs(x = \"Number of Clusters\",y = \"Number of Votes\" ,title = \"Cluster Analysis\")NbClust: k by majority voteScript2 clusters is the winner with 12 votes65•If you’re a lobbyist, which congressperson can you influence for swing votes?']\n",
      "['•If you’re managing a campaign and your competitor is always voting along party lines, how can you use that information?']\n",
      "['•If your congressperson is not an active voter, is she representing your interests?']\n",
      "['•What do the voting patterns look like for Republican-introduced bills?Application of results66•Could see differences between the patterns of Reb lead bills and Democrat lead bills•Could provide information on congressmen that might be see has swing votes.']\n",
      "[\"Implications of results67•We are assuming that the patterns correspond with the same bills being voted on –perhaps some Congressmen have the same number of 'aye' and 'nay' votes, but voted on different bills•Network analysis can help determine additional connections between Congressmen•We haven't taken extenuating factors into account –political initiatives, current events, etc.\"]\n",
      "['This is a preliminary analysis that gives us initial insights and can help us direct further researchLimitations of results68Problem 1: clusters overlap when data points are unequally distributedCommon pitfalls with clusteringOriginal data k-means clustering69Problem 2: clusters should have roughly the same density or else they may split!Common pitfalls with clusteringOriginal data k-means clustering70Problem 3: clusters should be circular / elliptical.']\n",
      "['We can’t use this method for particular shapesCommon pitfalls with clusteringOriginal data k-means clustering71Problem 4: a bad starting point can lead to bad clustering!Common problems with clusteringOriginal data k-means clustering72Lab, Part 1 73•Go through the same process only using bills introduced by republican congressmen.']\n",
      "['–What is the ideal number of clusters’ (You can probably guess this answer)–Are the patterns the same between the parties or do they vary?']\n",
      "['–Evaluate the model, is it better or worse than the model using the democratic introduced bills.']\n",
      "['Which clustering algorithm to use?']\n",
      "['74Which clustering algorithm to you use?']\n",
      "['Method Parameters Scalability Use case Geometry (metric used)K-means Number of clusters Very large n_samples, medium n_clusters with MiniBatch codeGeneral purpose, even cluster size, flat geometry, not too  many clustersDistances between pointsAffinity propagation Damping, sample preferenceNot scalable with n_samplesMany clusters, uneven cluster size, non- flat geometryGraph distance ( e.g.']\n",
      "['nearest -neighbor graph)Mean -shift Bandwidth Not scalable with n_samplesMany clusters, uneven cluster size, non- flat geometryDistances between pointsSpectral clustering Number of clusters Medium n_samples, small n_clustersFew clusters, even cluster size, non- flat geometryGraph distances ( e.g.']\n",
      "[\"nearest -neighbor graph)Ward hierarchicalclusteringNumber of clusters Large n_samples and n_clustersMany clusters, possibly connectivity constraintsDistances between pointsAgglomerative clusteringNumber of clusters, linkage type, distanceLarge n_samples and n_clustersMany clusters, possiblyconnectivity constraints, non -Euclidean distancesAny pairwise distanceDBSCAN Neighborhood size Very large n_samples,   medium n_clustersNon-flatgeometry, uneven cluster sizesDistances between nearest pointsGaussian mixtures Many Not scalable Flatgeometry, good for density estimationMahalanobis distances to centersBirch Branching factor, threshold, optional global clusterLarge n_samples and n_clustersLarge dataset, outlier removal, data reductionEuclidean distance between points75•The good and bad–+ cheap –NO LABELS , labels are expensive to create and maintain–+/-clustering always works–-Many methods to choose from and knowing the right one can be nontrivial and the differences between many are almost zero, so you need to understand what you're doing•The evil–Curse of dimensionality–Clusters may result from poor data quality–Non-deterministic ( e.g.k-means) subject to local minimum.\"]\n",
      "['Since it works with averages, k -means does not get much better with Big Data (marginal improvements) but luckily is naïve to parallelize –Non spherical data may result in poor clustering (depending on method used)–Unequal cluster sizes may result in poor clustering (depending on method used)The good, bad, and evil76•Analysts need to ask the following questions–Do you want overlapping or non- overlapping clusters ?']\n",
      "['–Does your data satisfy the assumptions of the clustering algorithm?']\n",
      "['–How was the distance measure identified ?']\n",
      "['–How many clusters and why ?']\n",
      "['Identifying the number of clusters is a difficult task if the number of class labels is not known beforehand –Does your method scale to the size of the data?']\n",
      "['–Is the compute time congruent with the temporal budget of your business need ( i.e.']\n",
      "['do you get answers back in time to make meaningful decisions)The good, bad, and evil77Supervised Machine LearningComputer learns through examples how to classify events/objects/instances.']\n",
      "['Supervised Machine LearningComputer learns through examples how to classify events/objects/instances.']\n",
      "['Supervised Machine LearningComputer learns through examples how to classify events/objects/instances.']\n",
      "['Performance MetricsSupervised Machine LearningComputer learns through examples how to classify events/objects/instances.']\n",
      "['•What metrics should we use?']\n",
      "['•Accuracy may not be enough•How reliable are the predicted values from your model?']\n",
      "['•Are errors on the training data a good indication of errors on future data?']\n",
      "['•optimisticEvaluation of Performance8Source: https://developers.google.com/machine -learning/crash- course/classification/true -false -positive -negativeMachine Learning Bias and Social Science Methods\\uf0d8Confusion Matrix, ROC (receiver operating curve) and AUC (area under the curve) are very common approaches for measuring the performance of classification models\\uf0d8Classification models output percentages that an individual input will belong to a specific class, usually a 1 or 0, with one being a positive attribute.']\n",
      "['\\uf076Likelihood of email spam/fraud is an example.']\n",
      "['The higher the model percentage prediction on any one email the higher chance it is fraud.']\n",
      "[\"\\uf0d8Essentially both measure the misclassification error rate associated with your model\\uf0d8A Confusion Matrix is a good tool for understanding how accurate you model is classifying and is used to build ROCMachine Learning Bias and Social Science Methods\\uf0d8Let's use intruder/fraud detection as an example\\uf0d8Say we have 135 emails entering our system and we are trying to detect whether they are fraudulent or not \\uf076We use lots of criteria –source, subject, if they came from a prince…\\uf0d8Generate probability measures as a result for a tree -based classifier to determine the likelihood that any one of these emails is fraudulent \\uf0d8The cutoff point that is predetermined in the tree (and is a universal standard) is 50% but can be modified as an input if neededMachine Learning Bias and Social Science Methods\\uf0d8Below is are the results of our model in a Confusion Matrix.\"]\n",
      "['They center on the positive and negative classifications in sub-categories of true and false positive.']\n",
      "['\\uf0d8Keep in mind we know because of the labels, what is fraud and not, so we can measure how good the model is classifying.']\n",
      "['\\uf0d8Both true negative and true positive are good, false negative and false positive are errors.']\n",
      "['1 = Fraud/Spam0 = Not Fraud/SpamPredicted ClassPositive Fraud Pred (1)NegativeNot Fraud Pred (0)Actual ClassPositive Fraud Actual (1)True Positive10False Negatives22Negative Not Fraud Actual (0)False Positives7True Negative96Machine Learning Bias and Social Science Methods\\uf0d8Let’s consider the extremes: what if we set the threshold to 0?']\n",
      "['\\uf076Means that everything is captured as Fraud and no ever gets an email again!']\n",
      "['1 = Fraud/Spam0 = Not Fraud/SpamPredicted ClassPositive Fraud Pred (1)NegativeNot Fraud Pred (0)Actual ClassPositive Fraud Actual (1)True Positive32False Negatives0Negative Not Fraud Actual (0)False Positives103True Negative0Machine Learning Bias and Social Science Methods \\uf0d8Let’s consider the other extreme: what if we set the threshold to 100?']\n",
      "['\\uf076Means nothing is fraud and now everyone is getting rich off of Arabian princes1 = Fraud/Spam0 = Not Fraud/SpamPredicted ClassPositive Fraud Pred (1)NegativeNot Fraud Pred (0)Actual ClassPositive Fraud Actual (1)True Positive0False Negatives32Negative Not Fraud Actual (0)False Positives0True Negative103Machine Learning Bias and Social Science Methods\\uf0d8We can further assess our model by generating classification rates:\\uf076True Positive Rate (TPR) (Sensitivity) = TP/(TP+FN) = 10/(10+22) = .31\\uf0d8% of fraud correctly labeled as fraud\\uf076False Positive Rate (FPR) (1- Specificity) = FP/(FP+TN) = 7/(7+96) = .06\\uf0d8% of emails labelled not fraud that were false positives1 = Fraud/Spam0 = Not Fraud/SpamPredicted ClassPositive Fraud Pred (1)NegativeNot Fraud Pred (0)Actual ClassPositive Fraud Actual (1)True Positive10False Negatives22Negative Not Fraud Actual (0)False Positives7True Negative96Machine Learning Bias and Social Science Methods\\uf0d8These two data points can used to begin to develop a Receiver Operating Characteristic Curve or ROC curve\\uf076True Positive Rate (TPR) = 10/(10+22) = .31 = y-axis\\uf076False Positive Rate (FPR) = 7/(7+96) = .06 = x-axis1 = Fraud/Spam0 = Not Fraud/SpamPredicted ClassPositive Fraud Pred (1)NegativeNot Fraud Pred (0)Actual ClassPositive Fraud Actual (1)True Positive10False Negatives22Negative Not Fraud Actual (0)False Positives7True Negative96Machine Learning Bias and Social Science Methods\\uf0d8ROC curve is essentially a graphical representation of the adjusted threshold values of the confusion matrix, below are two examplesAUCMachine Learning Bias and Social Science Methods\\uf0d8ROC curve generates the area under the curve as a percentage of the total graph under the curve.']\n",
      "['The Area Under the Curve (AUC) is indicative of performance.']\n",
      "['AUC:0.9 –1.0 = Excellent0.8 –0.9 = Good0.7 –0.8 = Fair0.6 –0.7 = Poor0.5 –0.6 = Fail19Source: https://developers.google.com/machine -learning/crash- course/classification/true -false -positive -negativeMachine Learning Bias and Social Science MethodsAdditional Performance MeasuresAccuracy –(TP+TN )/(TP+FP+FN+TN)Prevalance –The percentage of the positive class in thetest data set Detection Rate -The rate of true events also predicted to be eventsBalanced Accuracy -(sensitivity+specificity )/2Precision -TP/TP+FP –When predicting True Positives , what percentage is correct?']\n",
      "['(no FP, precision = 1)Recall –TP/TP+FN (same as sensitivity) –What proportion of Actual Positives where identified correctly?']\n",
      "['F1 Score –Harmonic mean of Precision and Recall, where accuracy is used when True Positives and True Negatives are important, F1 is used when False Negatives and False Positives are more of a concern.']\n",
      "['Also really bestused on unbalanced datasets Machine Learning Bias and Social Science MethodsAdditional Performance MeasuresLog Loss - log loss measures the UNCERTAINTY of the probabilities of your model by comparing them to the true labels –CLASSIFICATION.']\n",
      "['It heavily penalizes classifications that are highly confident in the wrong direction.']\n",
      "['So, seeing a log loss of 1 can be expected in the case when our model only gives less than a ~ 40 % probability estimate for selecting the actual class.']\n",
      "['Knowing the baseline rate (prevalence) here is important!']\n",
      "['Machine Learning Bias and Social Science MethodsAdditional Performance MeasuresKappa - Landis and Koch (1977) provide a way to characterize values.']\n",
      "['According to their scheme a value < 0 is indicating no agreement , 0–0.20 as slight, 0.21–0.40 as fair, 0.41–0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1 as almost perfect agreement.']\n",
      "['Indicates how much better our classifier is performing over the performance of a classifier that would just guess at random according to the frequency of each class.']\n",
      "['Its especially useful for multi -class models as many of the metrics we have reviewed are better suited to binary examples.']\n",
      "['Machine Learning Bias and Fairness \\uf0d8These metrics can be used to assess the fairness of the machine learning models\\uf0d8We will review these topics again later in the semester, as there’s much more to be learning but having a basic understand will help when we walk through the fairness formulas.']\n",
      "['Another Definition: Functional Approximation•What is a functional approximation problem?']\n",
      "['•Target variable: Dependent: What we are trying to predict•Other Variables: Independent: Using to Predict•Functional approximation is an approach that uses the other variables we have access to approximate the dependent and does so through the function development•We will use regression, which assumes that we have a numeric target variable, for classification it’s often a bi- variate or class level variable 24Assessment Measures •Assessing Regression Models: MSE, RMSE and MAE and Log Loss •MSE –The difference between the predicted values and the actual values squared•RSME –Same as above only the square root is taken to put the error back in terms of the dependent variable•Can also normalize the RSME to the range of the data in order to be able to compare RSME outputs that include different data ranges •MAE –The same approach only taking the absolute value instead of squaring25Equations 26nX XRMSEniidelmo iobs∑=−=12, , ) (min, max, obs obs X XRMSENRMSE−=Regression •Said another way basic linear regression has a Prediction Accuracy Problem:•Has a low bias (overfitting) but a high variance•This can be improved by injecting some level of bias into the equation by reducing the impact of certain coefficients •This can improve overall accuracy by reducing variance •Draw Dart Board –•Another issue is interpretation –with a large number of predictor variables and large data sets it often hard to identify variable importance and explain model outcomes27Regression and Sparsity•Often we have more features than observations in the world of big data•What type of problem is this?']\n",
      "['•So we strive to have Sparse models•What do we mean by Sparse?']\n",
      "['28Bias Versus Variance 29The Confusion Matrix and AccuracyMost widely used metricAccuracy•Accuracy is better measured using test data that was not used to build the classifier.']\n",
      "['•Referred to as the overall recognition rate of the classifier•Error rate or misclassification rate:1-Accuracy•When training data are used to compute the accuracy, the error rate is called resubstitution error .']\n",
      "['Accuracy may not be enough.Consider this two- class case.']\n",
      "['Consider the accuracy if a model predicts that the outcome is always class a.']\n",
      "['28/30 = 93% accuracyThis is misleading.']\n",
      "['Sensitivity and SpecificityReceiver Operating Characteristic (ROC) Curve1-SpecificityFalse Negative RateSensitivityTrue Positive Rate0 101The ROC CurveCleaning the Data and Documenting theProcess Using R MarkdownIntroduction to R ProgrammingDistrict Data LabsThe Data “Pipeline”Document Your Data Cleaning ProcessTidy DataData cleaning methodsOpening data ﬁles in R and viewing themWorking with data framesDropping variablesRenaming variablesLogical operatorsDropping observationsReshapingReshaping from wide to longReshaping from long to wideCreating new variablesReplacing existing variablesManaging string/character variablesMergingBinding columns or rowsMatching on ID variablesChecking the mergeSorting rowsAdditional data management techniquesManaging the workspaceDeleting objectsSaving/loading the workspaceOpening ASCII data ﬁlesSaving ASCII data ﬁlesArranging columnsMissing dataManaging categorical variablesRecoding/labeling valuesCombining categoriesReordering categoriesCombining categories across multiple variablesMore methods for managing string/character variablesManaging date/time variablesPipingCollapsing and within-group calculationsUsing group by()Using count()Document Your Data Cleaning ProcessRemember the work by Baggerly and Coombes.']\n",
      "['They had tospend months trying to replicate a study in a “forensic” waybecause the authors did not document their data cleaning steps.']\n",
      "['They had to deal with this:Instead, we will learn how to do this:Document Your Data Cleaning ProcessReal-world data are always messy: that is, original data are neverimmediately ready for analysis.']\n",
      "['Data must be cleaned in order tobe useful.']\n",
      "['But small mistakes in the data cleaning stage can have profoundeﬀects for the analysis.']\n",
      "['If your data are corrupted, no amount ofstatistical-machine-learning-artiﬁcial-intelligence-magic will savethe analysis.']\n",
      "['This afternoon, we will review the tidyverse packages, and all themethods we have for cleaning data.']\n",
      "['We will also introduce acouple new methods.']\n",
      "['We will work through how to do this work using R markdown, sothat we document our steps as we go along .']\n",
      "['We can catch errorsthis way, and make them easier to ﬁx.']\n",
      "['What is Tidy Data?']\n",
      "['“Happy families are all alike; every unhappy family is unhappy inits own way.']\n",
      "['” — Leo Tolstoy“Like families, tidy datasets are all alike but every messy dataset ismessy in its own way.']\n",
      "['” — Hadley WickhamData can come in many shapes and formats.']\n",
      "['The data can beassembled in one place or might be in many diﬀerent places.']\n",
      "['Tidy data is a philosophy about the best way to code a dataset tomake data easy to analyze in a reproducible way.']\n",
      "['What is Tidy Data?']\n",
      "['Atidy dataset meets the following minimum standards :1.']\n",
      "['The dataset exists as one table.']\n",
      "['2.']\n",
      "['It’s square: every row has an entry for every column (even if itis missing).']\n",
      "['3.']\n",
      "['The rows represent observations.']\n",
      "['4.']\n",
      "['The columns represent variables.']\n",
      "['5.']\n",
      "['The observations are comparable units (for example: people,countries, but not a mix of the two)What is Tidy Data?']\n",
      "['Even better tidy data also:6.']\n",
      "['Sorts the rows and columns into a logical order7.']\n",
      "['Has descriptive variable names8.']\n",
      "['Creates new variables to convey the important information9.']\n",
      "['Deletes irrelevant variables (and sometime observations)10.']\n",
      "['Reads date variables as dates11.']\n",
      "['Uses consistent codes for missing valuesThese are all things to do with the edited, working data.']\n",
      "['Neveroverwrite the original data ﬁle.']\n",
      "['We may need to return to theoriginal data many times.']\n",
      "['Opening data ﬁles in R and viewing themThe most common type of data ﬁle is CSV (comma separatedvalues).']\n",
      "['CSVs are space eﬃcient, portable, and can be transferredto any data/statistics software.']\n",
      "['To load a CSV ﬁle called data.csv , load the tidyverse packageby typinglibrary(tidyverse)Then typedata <- read_csv(\"data.csv\")Then to look at the data spreadsheet type View(data) in theconsole .']\n",
      "['(Please note the capital V.)Looking at the data is the most useful way to understand thechallenges ahead of you for preparing the data for analysis.']\n",
      "['Working with data framesAll of the following commands start by assigning the datamanagement step to a data frame.']\n",
      "['If you assign the step to a new name, you create a new object .']\n",
      "['The step is applied to the NEW data frame, but the step is NOTapplied to the original data frame object.']\n",
      "['Example : If the data frame is called data ,▶data <- command overwrites the data object,▶data2 <- command creates another object, data2 , inwhich the command was applied.']\n",
      "['It leaves the data objectalone.']\n",
      "['In general : assign to the same object over and over, UNLESS youdon’t want the step to be permanent.']\n",
      "['Dropping variablesSometimes datasets have hundreds or thousands of variables.']\n",
      "['Only a small portion of them might be relevant to your work.']\n",
      "['Youmay have to curate the data by deleting variables.']\n",
      "['Use the select() command to keep only the variables youspecify.']\n",
      "['To keep only the country name andyear variables, typevdem <- select(vdem, country_name, year)You can keep country name ,year , and every variable thatstarts with “v2x” by typingvdem <- select(vdem, country_name, year,starts_with(\"v2x\"))Dropping variablesYou can keep country name ,year , and every variable that endswith “ocracy” by typingvdem <- select(vdem, country_name, year,ends_with(\"ocracy\"))You can also specify which variables to drop instead of keep byusing a minus sign:vdem <- select(vdem, -ends_with(\"error\"))Renaming variablesTidy data should have descriptive variable names so you actuallyknow what each variable is.']\n",
      "['It’s crazy how often the data collectorsuse dumb names like XAB14G7B!']\n",
      "['To rename the variable v2xpolyarchy todemocracy , type:vdem <- rename(vdem, democracy=v2x_polyarchy)Note that the new name comes ﬁrst, and the old name comessecond.']\n",
      "['You can rename many variables with one command.']\n",
      "['Just separatethese statements with commas:vdem <- rename(vdem, democracy=v2x_polyarchy,corrupt=v2x_corr,civil_lib=v2x_civlib)Logical operatorsA logical statement is one that can only take on two values: trueor false.']\n",
      "['Logical operators are used to build logical statements.']\n",
      "['These operators are▶== is equal to?']\n",
      "['▶>is greater than?']\n",
      "['▶>=is greater than orequal to?']\n",
      "['▶<is less than?▶<=is less than or equalto?']\n",
      "['▶%in% is an element of theset?']\n",
      "['A!sign in front of >,>=,<, or<= means “not greater than”,“not greater than or equal to”, etc.']\n",
      "['Also, != means “not equal to”, and !(3 %in% c(4,5,6))means “3 is not an element of the set 4, 5, 6”Logical operatorsTwo symbols are used to combine logical operators:▶&“and”▶|“or” (shift + the button just above enter)Parentheses work here too to designate an order of evaluation.']\n",
      "['Examples:1==1 true 2==3 false(1==1)|(2==3) true (1==1)&(2==3) false(1!=1)|(2!=3) true !((1==1)&(2==3)) true(4*3-2) %in% c(10, 11, 12) true !(4*3-2) %in% c(10, 11, 12) falseIf variables are used in a logical statement, R puts the value TRUEon the rows for which the statement is true, and FALSE on therows for which the statement is false.']\n",
      "['If you change the class of a logical variable to numeric, the variablewill be binary (0,1).']\n",
      "['Dropping observationsThefilter() command works just like select() , except it keepsobservations instead of variables, and you use logical statementsto identify rows instead of variable names.']\n",
      "['To keep only the rows corresponding to the year 2014, typevdem <- filter(vdem, year==2014)To keep only the rows in 2014 when the democracy variable isgreater than .5, typevdem <- filter(vdem, year==2014 & democracy > .5)To keep only the rows for 1997, 2003, and 2011, typevdem <- filter(vdem, year %in% c(1997, 2003, 2011))ReshapingReshaping is a way to transform a dataset by turning columns intorows, or rows into columns.']\n",
      "['There are two kinds of reshapes :1.gather() : turning columns into rows.']\n",
      "['(In Stata, this is calledareshape long .)2.spread() : turning rows into columns.']\n",
      "['(In Stata, this is calledareshape wide .)Remember: tidy data requires that rows are observations andcolumns are variables .']\n",
      "['Years are observations, not variables.']\n",
      "['Reshaping is tricky.']\n",
      "['But any command can be broken down to itssimple logical elements.']\n",
      "['We just need to examine the rules onwhich the gather() andspread() commands operate.']\n",
      "['ReshapingHere’s an untidy dataset because observations are in the columns:country 1999 20001 Afghanistan 745 26662 Brazil 37737 804883 China 212258 213766The gather() command brings the observations to the rows.']\n",
      "['Thiscommand has the following syntax:gather(data, ...']\n",
      "[', key, value)Let’s clearly deﬁne what each argument does.']\n",
      "['Reshapinggather(data, ...']\n",
      "[', key, value)data is the name of the data frame you want to reshape.']\n",
      "['.']\n",
      "['.']\n",
      "['.represents the names of the variables that you will move to therows.']\n",
      "['▶You can type all the raw variable names here, separated bycommas▶If the variable names are numeric (as with years), placeforward-slanted single quotes around the names▶You can type something like var1:var10 to refer to allvariables in between between var1 andvar10 in the dataframe.']\n",
      "['Reshapinggather(data, ...']\n",
      "[', key, value)key is the name of the variable that will contain the names of theold variables you are moving.']\n",
      "['This variable doesn’t yet exist.']\n",
      "['If thecolumn names are numeric years, you should type key=\"year\" .']\n",
      "['value is the name of the variable that will contain the data insidethe old variables you are moving.']\n",
      "['This variable also doesn’t yetexist.']\n",
      "['ReshapingThis data is an object named table4a in R:country 1999 20001 Afghanistan 745 26662 Brazil 37737 804883 China 212258 213766The data inside the columns represent a variable named cases.']\n",
      "['ReshapingTo reshape this data, we type:gather(table4a, ‘1999‘, ‘2000‘, key=\"year\", value=\"cases\")The data now looks likecountry year casesAfghanistan 1999 745Brazil 1999 37737China 1999 212258Afghanistan 2000 2666Brazil 2000 80488China 2000 213766ReshapingHere’s an untidy dataset because variables are in the rows:country year type countAfghanistan 1999 cases 745Afghanistan 1999 population 19987071Afghanistan 2000 cases 2666Afghanistan 2000 population 20595360Brazil 1999 cases 37737Brazil 1999 population 172006362Brazil 2000 cases 80488Brazil 2000 population 174504898China 1999 cases 212258China 1999 population 1272915272China 2000 cases 213766China 2000 population 1280428583ReshapingTo clean this dataset, we use the spread() command, whichworks similarly (but not exactly like) the gather() command:spread(data, key, value)The notable diﬀerence here is the lack of an .']\n",
      "['.']\n",
      "['.argument.']\n",
      "['Wedon’t have to specify columns, because the variables are already inthe rows, and R knows what to place in the columnsautomatically by looking at the levels of the factor (named type inthis data).']\n",
      "['data is the name of the data frame you want to reshape.']\n",
      "['Reshapingspread(data, key, value)key is the name of the existing variable that contains the names ofthe variables you will move to the columns.']\n",
      "['value is the name of the existing variable that contains the datainside the variables you will move to the columns.']\n",
      "['To reshape the data (named table2 in R), we type:spread(table2, key=\"type\", value=\"count\")ReshapingThe data now looks like this:country year cases population1 Afghanistan 1999 745 199870712 Afghanistan 2000 2666 205953603 Brazil 1999 37737 1720063624 Brazil 2000 80488 1745048985 China 1999 212258 12729152726 China 2000 213766 1280428583Creating new variablesUse the mutate() command to create new variables.']\n",
      "['Write the new variable name, one equal sign, then what the newvariable equals.']\n",
      "['You can use math and logic operators and thenames of existing variables.']\n",
      "['For example:vdem <- mutate(vdem,after2001 = (year>=2001),adjusted_dem = democracy - corruption,avg = (democracy + civil_rights)/2)If you use logical operators , the new variable will be BINARY toexpress whether the logical statement is TRUE or FALSE.']\n",
      "['Replacing existing variablesYou can also use the mutate() command to replace old variables.']\n",
      "['The diﬀerence is that now you write old variable names instead ofnew variables.']\n",
      "['In this example, the variables democracy andcorruptionalready exist :vdem <- mutate(vdem,democracy = 100*democracy,adjusted_dem = democracy - corruption)The ﬁrst transformation says “replace each value of democracywith 100 times its old value.”The second transformation says “create adjusted dem to be thediﬀerence of democracy andcorruption .”But there’s a problem here.']\n",
      "['See it?']\n",
      "['Replacing existing variablesvdem <- mutate(vdem,democracy = 100*democracy,adjusted_dem = democracy - corruption)These transformations are done one at a time and in order.']\n",
      "['In the second transformation, our intention was to use the oldversion ofdemocracy .']\n",
      "['Instead it uses the new version (×100).']\n",
      "['This command instead does what we want:vdem <- mutate(vdem,adjusted_dem = democracy - corruptiondemocracy = 100*democracy)So pay attention to the order in which you create/replace variables.']\n",
      "['Managing string/character variablesSome variables are coded as character (words), but in reality, theyare categorical variables.']\n",
      "['We’ve covered how to deal with variableslike that.']\n",
      "['But other variables are character because they really areopen-ended responses .']\n",
      "['Open-ended questions contain a LOT of data, some of it novel andunexpected, since these responses are not constrained to follow anymultiple choices.']\n",
      "['But how to extract this data?']\n",
      "['Managing string/character variablesThe following commands are part of the stringr library, which ispart of the tidyverse packages.']\n",
      "['Here’s an example of a string:response <- \"I have the faintest idea.']\n",
      "['I really don’t watch the news\"String is just another word for character, text, words, etc.']\n",
      "['It hasthe character class in R:> class(response)[1] \"character\"Managing string/character variablesThe length of a string is the number of characters in the string,including spaces:> str_length(response)[1] 55To pull out a subsection of this string, use the strsub()command.']\n",
      "['Specify the string variable to work with, the NUMBERof the character that starts the substring, and the NUMBER of thelast character of the substring:> str_sub(response, 1, 10)[1] \"I have the\"> str_sub(response, 25, 37)[1] \".']\n",
      "['I really do\"Managing string/character variablesNegative numbers with the strsub() command tell R to countbackwards .']\n",
      "['To get a substring starting 15 characters from theend, until the end, type:> str_sub(response, -15, -1)[1] \" watch the news\"It might be useful to convert all letters to UPPERCASE, so wedon’t have to worry about case sensitivity when looking for speciﬁcpatterns and words:> str_to_upper(response)[1] \"I HAVE THE FAINTEST IDEA.']\n",
      "['I REALLY DON’T WATCHTHE NEWS\"MergingMerging is the technique of combining two data frames.']\n",
      "['There area few diﬀerent ways to perform a merge.']\n",
      "['(1) Adding columns, without trying to match observations .']\n",
      "['Thebind cols() (or the older cbind() ) function pastes two dataframes together, side by side, exactly as they are .']\n",
      "['So if you sortone or both data frames, it changes the result of the bind cols()command.']\n",
      "['For example, consider data1 anddata2 :country x yUSA -0.10 -0.90Canada 0.18 -0.88UK 0.50 0.92country x yChina -0.61 -1.23Japan -0.24 -1.05S.']\n",
      "['Korea -0.11 -1.10MergingUsing bind cols() to combine these data frames has thefollowing result:cbind(data1,data2)country x y country1 x1 y1USA -0.10 -0.90 China -0.61 -1.23Canada 0.18 -0.88 Japan -0.24 -1.05UK 0.50 0.92 S.']\n",
      "['Korea -0.11 -1.10To avoid duplicate column names, R placed an arbitrary “1” afterthe variable names for data2 .']\n",
      "['R pasted the two data frames together without any regard for thedata’s structure or meaning.']\n",
      "['In general, this is NOT what we wantto do.']\n",
      "['Merging(2) Adding rows (in Stata this is called “appending”)The bind rows() (or the older rbind() ) function is similar tobind cols() , but with two important diﬀerences:1.bind rows() pastes data frames together by pasting one ontop of the other, resulting in more rows but not more columns2.bind rows() rearranges the columns of each data frame totry to match corresponding variables.']\n",
      "['This is a good option to group observations that belong together,but are stored in separate places for whatever reason.']\n",
      "['MergingUsing bind rows() to combine these data frames has thefollowing result:rbind(data1,data2)country x y1 USA -0.10 -0.902 Canada 0.18 -0.883 UK 0.50 0.924 China -0.61 -1.235 Japan -0.24 -1.056 S.']\n",
      "['Korea -0.11 -1.10Merging(3) Adding columns, matching observations that share thesame ID variable(s) .']\n",
      "['This is the most important kind of merge.']\n",
      "['It’s also the trickiest todo correctly.']\n",
      "['Suppose you have two data frames that look like this:country x yUSA 0.97 0.76China -0.96 -2.12Russia 1.10 0.24country w zChina 0.99 0.33USA 0.37 -0.64Russia 0.10 -0.49How do you combine the two data frames?']\n",
      "['MergingYou can sort both data frames alphabetically by country, then usecbind() .']\n",
      "['But while that works here, that approach won’t work ingeneral.']\n",
      "['We need a way for R to recognize that country is the IDvariable , and that the two data frames share IDs.']\n",
      "['Then R needs tomatch rows that share the same ID .']\n",
      "['The most important merge command is full join() .']\n",
      "['Applyingthis command gives the following result:full_join(data1, data2)country x y w zUSA 0.97 0.76 0.37 -0.64China -0.96 -2.12 0.99 0.33Russia 1.10 0.24 0.10 -0.49MergingHere’s what full join() does :1.']\n",
      "['It looks at the variable names for each dataset, anddetermines whether there is any overlap in these names.']\n",
      "['2.']\n",
      "['It assumes that the shared variable names are the ID variablesyou want to use to match the observations in each data frame.']\n",
      "['3.']\n",
      "['It combines observations with the same IDs.']\n",
      "['full join() is very useful and easy to use compared to mergecommands in other packages and software.']\n",
      "['But it tends to mess up without displaying any warnings orerrors !']\n",
      "['You have to understand how full join() works, and youhave to visualize what you want the data to look like after usingfull join() .']\n",
      "['MergingIf an observation has no match in the other data frame, it stillappears in the merged data with NAvalues for the variables fromthe other data frame.']\n",
      "['There are related commands that treat unmatched observationsdiﬀerently:▶inner join() : drops all unmatched observations▶left join(data1, data2) : keeps all observations fromdata1 , but drops all unmatched observations in data2▶right join(data1, data2) : keeps all observations fromdata2 , but drops all unmatched observations in data1In general, it is safer to stick with full join() .']\n",
      "['These othercommands might delete observations you don’t want to delete.']\n",
      "['MergingSuppose for example we want to merge these two data frames:country x yUSA 0.43 -0.57China -0.70 -1.38Russia 1.64 0.60France -0.38 -0.30country w zUSA 0.99 -1.27China 1.06 0.13Russia 1.65 0.72Japan 0.28 -0.09Note that some, but not all, of the ID values have a match in theother data frame.']\n",
      "['Mergingfull join() keeps all observations that appear in either dataframe, but creates missing values for unmatched observations:data3 <- full_join(data1, data2)country x y w zUSA 0.43 -0.57 0.99 -1.27China -0.70 -1.38 1.06 0.13Russia 1.64 0.60 1.65 0.72France -0.38 -0.30 NA NAJapan NA NA 0.28 -0.09Merginginner join() keeps the observations that have a match in bothobservations, and deletes all other observations:data3 <- inner_join(data1, data2)country x y w zUSA 0.43 -0.57 0.99 -1.27China -0.70 -1.38 1.06 0.13Russia 1.64 0.60 1.65 0.72Mergingleft join() keeps all observations from data1 (the data frametyped FIRST), keeps all observations from data2 (the data frametyped SECOND) that have a match in data1 , and deletesobservations from data2 without a match in data1 :data3 <- left_join(data1, data2)country x y w zUSA 0.43 -0.57 0.99 -1.27China -0.70 -1.38 1.06 0.13Russia 1.64 0.60 1.65 0.72France -0.38 -0.30 NA NAMergingright join() keeps all observations from data2 (the data frametyped SECOND), keeps all observations from data1 (the dataframe typed FIRST) that have a match in data2 , and deletesobservations from data1 without a match in data2 :data3 <- left_join(data1, data2)country x y w zUSA 0.43 -0.57 0.99 -1.27China -0.70 -1.38 1.06 0.13Russia 1.64 0.60 1.65 0.72Japan NA NA 0.28 -0.09MergingSometimes there is more than one ID variable that identiﬁes anobservation.']\n",
      "['For example, we might have data on▶U.S States within years▶Political parties within countries within years▶Students within classes within schools within school districtswithin statesA set of ID variables are called unique identiﬁers of the rows if notwo rows share the same values of every ID variable.']\n",
      "['Two rowsmight represent China at diﬀerent years.']\n",
      "['So country alone is not aunique identiﬁer, but country and year TOGETHER are.']\n",
      "['The full join() command can match on multiple ID variables,as long as corresponding ID variables have the same name ineach data frame .']\n",
      "['MergingSuppose we want to merge the following two data frames:country year x yUSA 2014 0.53 0.26USA 2015 -1.12 -1.68USA 2016 0.75 -1.24China 2014 -0.01 0.17China 2015 -0.09 0.14China 2016 -0.32 0.07Russia 2014 1.49 0.83Russia 2015 -0.96 1.04Russia 2016 2.50 -0.43country year w zUSA 2014 0.27 -1.95USA 2015 0.88 0.14USA 2016 -0.06 0.49China 2014 -0.25 -1.02China 2015 0.72 0.22China 2016 0.89 1.20Russia 2014 0.59 0.45Russia 2015 0.06 -1.57Russia 2016 -0.93 -0.01There are two ID variables, country and year.']\n",
      "['Because the havethe same name in each data frame, we can use full join() likewe did before.']\n",
      "['MergingWe use the following command:data3 <- full_join(data1, data2)country year x y w zUSA 2014 0.53 0.26 0.27 -1.95USA 2015 -1.12 -1.68 0.88 0.14USA 2016 0.75 -1.24 -0.06 0.49China 2014 -0.01 0.17 -0.25 -1.02China 2015 -0.09 0.14 0.72 0.22China 2016 -0.32 0.07 0.89 1.20Russia 2014 1.49 0.83 0.59 0.45Russia 2015 -0.96 1.04 0.06 -1.57Russia 2016 2.50 -0.43 -0.93 -0.01MergingSometimes one data frame has more unique identifying variablesthan the other.']\n",
      "['For example:country year x yUSA 2014 0.53 0.26USA 2015 -1.12 -1.68USA 2016 0.75 -1.24China 2014 -0.01 0.17China 2015 -0.09 0.14China 2016 -0.32 0.07Russia 2014 1.49 0.83Russia 2015 -0.96 1.04Russia 2016 2.50 -0.43country w zUSA 0.99 -1.27China 1.06 0.13Russia 1.65 0.72Mergingfull join() handles this situation too.']\n",
      "['It recognizes the sharedID of country, and places the data for a country in data2 on everyrow for that country:data3 <- full_join(data1, data2)country year x y w zUSA 2014 0.53 0.26 0.99 -1.27USA 2015 -1.12 -1.68 0.99 -1.27USA 2016 0.75 -1.24 0.99 -1.27China 2014 -0.01 0.17 1.06 0.13China 2015 -0.09 0.14 1.06 0.13China 2016 -0.32 0.07 1.06 0.13Russia 2014 1.49 0.83 1.65 0.72Russia 2015 -0.96 1.04 1.65 0.72Russia 2016 2.50 -0.43 1.65 0.72What can go wrong while mergingfull join() doesn’t always know if something went wrong.']\n",
      "['Sothere’s no automated procedure to check errors.']\n",
      "['But there arecommon problems you can check for before merging the data.']\n",
      "['Prior to merging, we will perform three checks :1.ID name check : do the shared ID variables have the samename?']\n",
      "['Are these variables the ONLY ones that share thesame name in both data frames?']\n",
      "['2.Unique ID check : do we expect the ID variables to beunique identiﬁers in one or both data frames?']\n",
      "['If so, are they?']\n",
      "['3.ID value check : are there discrepancies in the values of theID variable?']\n",
      "['If all three checks pass, then the merge will work without problems.']\n",
      "['What can go wrong while mergingID name check : First, look at each data frame with the View() ,summary() , and head() commands.']\n",
      "['Decide on what the uniqueID variables are in each data frame, and which you will use tomatch in the merge.']\n",
      "['Use the names() command to display the names for each dataframe, and use the intersect() command to see which variablenames are shared by both data frames:> names(data1)[1] \"country\" \"year\" \"x\" \"y\"> names(data2)[1] \"country\" \"year\" \"w\" \"z\"> intersect(names(data1), names(data2))[1] \"country\" \"year\"There are two problems this check can reveal.']\n",
      "['What can go wrong while mergingProblem 1 : the ID variables don’t have the same name in each dataframe.']\n",
      "['In this case R won’t know which variable to match on.']\n",
      "['That can happen if one data frame has “state” while the other has“State” (case sensitive!).']\n",
      "['Or “year” vs.']\n",
      "['“yr”, “countrycode” vs.']\n",
      "['“ccode”, etc.']\n",
      "['Solution :prior to merging , rename the ID variable in one of thetwo data frames, so that they have the same name.']\n",
      "['Problem 2 : non-ID variables in each data frame unexpectedly havethe same name.']\n",
      "['In this case, R will mistakenly think this variable isan ID.']\n",
      "['Solution :prior to merging , rename the non-ID variable in one ofthe two data frames, so that they have they DON’T same nameanymore.']\n",
      "['What can go wrong while mergingUnique ID check : If the ID variables are not unique identiﬁers ineither data frame, then R places ALL combinations of matchingobservations in the merged data.']\n",
      "['Here’s a simple example:name xA 2A 7A 1name yA 6A 5A 8These two data frames share an ID variable named “name”.']\n",
      "['Butname is NOT a unique ID in either data frame since multiple rowshave the same value of “A”.']\n",
      "['There’s no second ID variable like year that we can use to identifythe rows.']\n",
      "['What can go wrong while mergingIf we just go ahead and merge, the result isdata3 <- full_join(data1, data2)name x yA 2 6A 2 5A 2 8A 7 6A 7 5A 7 8A 1 6A 1 5A 1 8What can go wrong while mergingR didn’t have enough information to match each row to a singlerow in the other data frame, so it matched each row to everypossible row.']\n",
      "['That’s not what we want.']\n",
      "['To check whether this is a problem :1.']\n",
      "['Create a second data frame with just the ID variables2.']\n",
      "['Use the unique() command to keep only the non-repeatedrows3.']\n",
      "['Use nrow() to compare the number of rows of the unique IDdata frame to the number of rows in the original data.']\n",
      "['Ifthese numbers are the same, then the ID variables are unique.']\n",
      "['What can go wrong while mergingFor example, to check whether country andyear are unique IDsin this data frame:country year x yUSA 2014 0.53 0.26USA 2015 -1.12 -1.68USA 2016 0.75 -1.24China 2014 -0.01 0.17China 2015 -0.09 0.14China 2016 -0.32 0.07Russia 2014 1.49 0.83Russia 2015 -0.96 1.04Russia 2016 2.50 -0.43What can go wrong while merging(1)Create a second data frame with just the ID variablesdata.temp <- select(data1, country, year)(2)Use the unique() command to keep only the non-repeatedrowsdata.temp <- unique(data.temp)(3)Usenrow() to compare the number of rows of the unique IDdata frame to the number of rows in the original data.']\n",
      "['If thesenumbers are the same, then the ID variables are unique.']\n",
      "['> nrow(data.temp)[1] 9> nrow(data1)[1] 9Because the numbers of rows are equal, country and year areunique IDs.']\n",
      "['What can go wrong while mergingIDs don’t always have to be unique.']\n",
      "['In this example:country year x yUSA 2014 0.53 0.26USA 2015 -1.12 -1.68USA 2016 0.75 -1.24China 2014 -0.01 0.17China 2015 -0.09 0.14China 2016 -0.32 0.07Russia 2014 1.49 0.83Russia 2015 -0.96 1.04Russia 2016 2.50 -0.43country w zUSA 0.99 -1.27China 1.06 0.13Russia 1.65 0.72We merge on country .']\n",
      "['We didn’t expect country to be unique inthe ﬁrst data frame, just the second.']\n",
      "['That’s okay!']\n",
      "['Make sure theresults of this test match your expectations.']\n",
      "['What can go wrong while mergingID value check : It’s common for two diﬀerent datasets to codethe same observations with slightly diﬀerent IDs.']\n",
      "['Some examples:▶“District of Columbia” vs.']\n",
      "['“DC”▶“South Korea” vs.']\n",
      "['“S.']\n",
      "['Korea” vs.']\n",
      "['“ROK”▶1998 vs 98If you don’t catch these discrepancies, the data won’t getmatched for this observation .']\n",
      "['Also, two datasets might not cover the exact same cases.']\n",
      "['Or theymight use diﬀerent time frames.']\n",
      "['You have to decide whether ornot that’s okay.']\n",
      "['Sometimes data contain thousands of unique IDs.']\n",
      "['That’s toomany to read through manually.']\n",
      "['We need a reliable way to identifythe unmatched IDs.']\n",
      "['What can go wrong while mergingIf we merge two data frames, data1 anddata2 , there are twoways for an ID value to be unmatched:▶An ID appears in data1 but not data2 , or▶An ID appears in data2 but not data1We can look at each type of unmatched ID separately.']\n",
      "['The anti join() command is a kind of “reverse merging”.']\n",
      "['Itdrops the matched observations and leaves the unmatched ones.']\n",
      "['Speciﬁcally:▶anti join(data1, data2) keeps only observations indata1 that have no match in data2▶anti join(data2, data1) keeps only observations indata2 that have no match in data1What can go wrong while mergingTo identify the unmatched observations, type:check1 <- anti_join(data1, data2, by=c(\"id1\", \"id2\"))check2 <- anti_join(data2, data1, by=c(\"id1\", \"id2\"))Unlike full join() , these commands require the byargument,which takes a character vector with the names of the ID variablesto match on.']\n",
      "['(If there’s only one ID, type its name in quoteswithout the c() command.)Now check1 has all the observations in data1 that have no matchindata2 , and check2 has all the observations in data2 that haveno match in data1 .']\n",
      "['You can use View() to see these observations, and nrow() tocount the number of observations in check1 andcheck2 .']\n",
      "['If bothhave 0 observations, then every observation was matched .']\n",
      "['Sorting rowsSorting means rearranging the rows/columns in a way that doesnot break any row or column apart.']\n",
      "['You can sort rows based on the values of one variable numerically(from smallest to largest, or from largest to smallest) oralphabetically .']\n",
      "['You can move the columns to appear in any order you like.']\n",
      "['These steps are mostly cosmetic (they won’t change statisticalresults) but they make the data much easier to look at .']\n",
      "['Sorting rowsTo sort observations, use the arrange() function.']\n",
      "['Example : The state legislature data lists estimates of left/rightideologies for U.S.']\n",
      "['state legislatures from 1993-2014.']\n",
      "['To sort therows from the most liberal (smallest value) to the mostconservative (largest value) state house, typestateleg <- arrange(stateleg, hou_chamber)To sort from most conservative (largest value) to most liberal(smallest value), use a minus sign in front of the sorting variable:stateleg <- arrange(stateleg, -hou_chamber)Sometimes I get an error when I use the minus sign.']\n",
      "['If thathappens, this should work instead:stateleg <- arrange(stateleg, desc(hou_chamber))Sorting rowsYou can specify more than one sorting variable:1.']\n",
      "['The data are sorted by the ﬁrst variable.']\n",
      "['2.']\n",
      "['If there are ties , they are broken by sorting the secondvariable within values of the ﬁrst.']\n",
      "['3.']\n",
      "['The third variable breaks ties with the ﬁrst two variables, andso on.']\n",
      "['To sort alphabetically by state name, then sort the observationsfrom the same state by year, typestateleg <- arrange(stateleg, st, year)Deleting ObjectsThe memory that R sets aside for all the objects you load andcreate is called the workspace.']\n",
      "['To see all of the objects that currently exist in the workspace, typels() .']\n",
      "['To delete an object, use rm() .']\n",
      "['To delete an object named data ,typerm(data)To delete three objects named data ,polity , and cow, typerm(list=c(\"data\", \"polity\", \"cow\"))Note : you need to put object names in quotes here.']\n",
      "['Deleting ObjectsWhen you start a new R session (by closing and restarting R),there are no objects in the workspace.']\n",
      "['Sometimes you switch from working on one project to another.']\n",
      "['When you do, it might be a good idea to delete ALL objects inthe workspace .']\n",
      "['That way, there’s less chance of confusing newobjects with old ones.']\n",
      "['To delete all objects, typerm(list=ls())Sometimes this command is compared to clear in Stata.']\n",
      "['It’ssimilar, but two big diﬀerences:1.clear closes datasets only, rm(list=ls()) removes anyobject.']\n",
      "['2.clear closes a ﬁle without saving it.']\n",
      "['rm(list=ls())DELETES objects.']\n",
      "['Saving and Loading the WorkspaceThe workspace (the set of all of the objects you’ve made) istemporary.']\n",
      "['But, when you close R and R Studio, they always ask you if youwant to save the “workspace image”, even when there’s nothingin it .']\n",
      "['You can also save the whole workspace without being prompted bytypingsave(list=ls(), file=\"filename.Rdata\")If you save the workspace, it gets saved as an .Rdata ﬁle in yourworking directory.']\n",
      "['This is NOT the same as a data ﬁle.']\n",
      "['It can only be read by Rand R Studio.']\n",
      "['It contains many objects, potentially, not just dataframes.']\n",
      "['Managing the workspaceYou can save just a few of the objects by typing something likesave(list=c(\"data\", \"polity\", \"cow\"),file=\"filename.Rdata\")Caution : if “ﬁlename.Rdata” already exists in the workingdirectory, this command OVERWRITES it.']\n",
      "['Be very careful if theraw data is stored in an .Rdata ﬁle.']\n",
      "['You can load a saved workspace when you start a new R session bytypingload(\"filename.Rdata\")Now all the objects you had saved are in the workspace again.']\n",
      "['Managing the workspaceI don’t recommend relying on the save() andload() commandsor.Rdata ﬁles.']\n",
      "['Here’s why :▶.Rdata ﬁles are speciﬁc to R and do not transfer to otherprograms.']\n",
      "['▶.Rdata ﬁles are too easy to overwrite .']\n",
      "['▶.Rdata ﬁles encourage saving data in disparate objectsinstead of in a clean dataset.']\n",
      "['Avoiding .Rdata ﬁles is counter-intuitive since it’s natural toequate .Rdata ﬁles to .dta ﬁles for Stata or .xls ﬁles for Excel.']\n",
      "['I use .Rdata ﬁles only when there is a very speciﬁc reason forsaving objects instead of a data frame.']\n",
      "['Much more often, I use commands to load and save data in ASCIIformat ﬁles .']\n",
      "['Opening and Saving ASCII Data FilesBy default, read csv() assumes row 1 contains the variablenames.']\n",
      "['But if the data does not have names in row 1, typedata <- read_csv(\"data.csv\", col_names=FALSE)To replace the given variable names with names you choose , typesomething likedata <- read_csv(\"data.csv\",col_names=c(\"id\", \"vote\", \"age\"))Sometimes data ﬁles have a few lines of text at the top withinformation like authors, title, grant number, etc.']\n",
      "['To skip 3 linesat the top, typedata <- read_csv(\"data.csv\", skip=3)Opening and Saving ASCII Data FilesSometimes certain rows in the ASCII ﬁle are commented out withsome symbol the data’s authors chose.']\n",
      "['To avoid reading rows thatare marked with % at the beginning as data, typedata <- read_csv(\"data.csv\", comment=\"%\")You might need to convert missing codes.']\n",
      "['For example, Statamarks missing values as .and R marks them as NA.']\n",
      "['To read the .']\n",
      "['markers as missing, typedata <- read_csv(\"data.csv\", na=\".\")Opening and Saving ASCII Data Filesread csv2() is for semi-colon separated ﬁles.']\n",
      "['read tsv() is for tab separated ﬁles.']\n",
      "['read table() is for ﬁles where datapoints are separated by whitespace, not necessary tab.']\n",
      "['read fwf() is for ﬁxed width ﬁles.']\n",
      "['You will have to also specifywhich characters (counting left to right) correspond to whichvariables.']\n",
      "['To read data that is delimited in any other way (by &, forexample), typedata <- read_delim(\"data.txt\", delim=\"&\")Saving CSV data ﬁlesRemember the workﬂow : First, load the raw data.']\n",
      "['Then do stuﬀ.']\n",
      "['End by saving the cleaned/edited data under a diﬀerentname , and NEVER overwrite the original data.']\n",
      "['To save a data frame object data as a new CSV ﬁle, typewrite_csv(data, \"different_name.csv\")To save a data frame object data as a new tab separated ﬁle, typewrite_tsv(data, \"different_name.txt\")These two commands will be enough the vast majority of the time.']\n",
      "['SeeR for Data Science for some commands for very specialcircumstances.']\n",
      "['Arranging columnsYou can arrange columns from left to right.']\n",
      "['Mostly, this task justmakes the data nicer to look at when you use View() .']\n",
      "['Note, however, that it will change the column numbers .']\n",
      "['Sostateleg[,15] no longer refers to the same variable as before.']\n",
      "['To arrange the stateleg data so that state name comes ﬁrst,then year, then everything else, type:stateleg <- select(stateleg, st, year, everything())Careful : The select() command is also used to delete variables.']\n",
      "['If you forget to include everything() in the above command:stateleg <- select(stateleg, st, year)then every variable EXCEPT for standyear gets deleted.']\n",
      "['Missing dataA data point is missing if for some reason we can’t know what itis.']\n",
      "['R denotes missing values as NA.']\n",
      "['“Missing values are contagious.']\n",
      "['” Since we don’t know what anNAis, we can’t know what operations with an NAare either:1 + NA=NAIf you want to see the mean of a variable, this might happen:> mean(stateleg$hou_dem)[1] NAThat happened because the variable houdem has at least onemissing value.']\n",
      "['To ignore missing values when using functions likethe mean, use the na.rm=TRUE argument:> mean(stateleg$hou_dem, na.rm=TRUE)[1] -0.7418481Missing dataThe summary() command for a data frame will show you thenumber of missing values for each variable.']\n",
      "['To see a giant matrix of TRUE/FALSE values, indicating whethereach data point is missing, typeis.na(stateleg)Toforcibly delete any row with at least one missing value, typestateleg <- na.omit(stateleg)In general, I don’t recommend doing that.']\n",
      "['It’s heavy handed andthere are better approaches to handling missing data.']\n",
      "['Some notes on missing dataData may be missing for a lot of reasons.']\n",
      "['On a survey, respondentsmight say “don’t know,” may refuse to answer, or the questionmight not be applicable.']\n",
      "['Sometimes surveys will place a marker value to indicate a missingvalue.(For example, using 998 for “don’t know”).']\n",
      "['If you do notrecode these values, then it messes up mathematical functions likeaverages.']\n",
      "['“Missing values are contagious.”Any mathematical function that includes even one missing valuewill be missing.']\n",
      "['1 + NA = NA , 1 + NA̸= 1In other words, don’t confuse missing with 0!']\n",
      "['0 is information,and missing is a lack of information.']\n",
      "['Missing values for continuous variablesWe can use logical statements to deal with situations in whichmissing values for continuous variables are given numeric codes.']\n",
      "['For example, in the data data, thermometer scores are continuous,coded 0 to 100 scales .']\n",
      "['But if a respondent says “don’t know”,the score is marked as 998.']\n",
      "['If we don’t replace these values, allresults with these variables are thrown oﬀ.']\n",
      "['The logical statement data$ft obama == 998 returns a vectorof logical values: TRUE if the ftobama variable is 998, FALSE ifnot.']\n",
      "['This command replaces these values with NAs:data$ft_obama[data$ft_obama == 998] <- NARecoding valuesRecoding values means replacing many values of a categoricalvariable with new values, simultaneously.']\n",
      "['There are a few reasons why you may want to recode :1.']\n",
      "['Change the labels of values.']\n",
      "['Make it easier to see andremember what each value means.']\n",
      "['Better to code as \"Male\",\"Female\" than 1, 2.']\n",
      "['2.']\n",
      "['Replace missing codes with NA.']\n",
      "['3.']\n",
      "['Change the order of categories for display in graphs, or in caseyou want to treat the variable as ordinal and categories areout of order.']\n",
      "['Labeling categorical valuesSuppose you have a categorical variable with values 1, 2, 3, 4, 8and 9 .']\n",
      "['You look in the data’s codebook (hopefully it has one) and see thatCategory Meaning1 I speak Spanish primarily2 I speak both Spanish and English equally3 I speak English primarily but can speak Spanish4 I can not speak Spanish8 refused9 skippedWe can replace the numeric values with their written meanings,without changing the way the categorical data is treated in R.']\n",
      "['Using fctrecode()There are many ways to replace numeric categories with theirwritten meanings.']\n",
      "['The easiest method uses the fctrecode()function from the forcats package (one of the tidyverse ).']\n",
      "['Here’s an example of how to use fctrecode() :Let’s break down the elements of this code:Using fctrecode()This function must be only ever used inside the mutate()function.']\n",
      "['Type mutate() , then the data frame, then the name of thecategorical variable you are editing, then an equal sign.']\n",
      "['Parentheses will appear automatically and will indent correctlywhen you push enter.']\n",
      "['Leave the closing parentheses alone.']\n",
      "['Using fctrecode()Then type fctrecode()The ﬁrst argument of fctrecode() is the categorical variable,which must be of the factor class.']\n",
      "['If it is not (here it is numeric),useas.factor() to coerce the variable:Using fctrecode()The press enter, and on each new line write the new categoricaltext label, in quotes, equal to the old categorical label, also inquotes.']\n",
      "['Remember, as with the rename() function: new ﬁrst, then old .']\n",
      "['This code works whether the old labels are numbers or text.']\n",
      "['Using fctrecode()Finally, for the categories you want to set to be missing , write thenew category labels as NULL , with no quotes:This code is more space-consuming than other approaches.']\n",
      "['Butthe advantage is that we can more easily keep track of the newand old categories, minimizing the risk of confusing which labelgoes with which number.']\n",
      "['Using fctrecode()One more nice thing that fctrecode() can do: combinecategories.']\n",
      "['To combine two old categories into the same new category, justuse the same new category label for multiple old categories.']\n",
      "['For example, suppose we want to group every category in which aperson knows at least some Spanish as Yes, and the category inwhich a person knows no Spanish as No.']\n",
      "['We can write:Reordering categoriesThe categories of a factor variable have a built-in order.']\n",
      "['Theorder controls a few things:1.']\n",
      "['The order of the categories appear in any table2.']\n",
      "['The order the categories appear left-to-right in any graph3.']\n",
      "['The meaning of the variable when used in a regression modelSometimes the categories have a natural ordering: for example, wecan arrange categories in order of how much Spanish a personspeaks.']\n",
      "['Sometimes the categories don’t have a natural ordering, but itmakes sense to choose a particular order because it makes a tableor graph looks better, or to change the base category in aregression.']\n",
      "['Reordering categoriesTo change the order, use the fctrelevel() function.']\n",
      "['It works alot like fctrecode() , only instead of writing old categories,simply write the existing categories in the order you want.']\n",
      "['For example:We’ll talk more about why it matters to change the order in moredetail over the next few weeks.']\n",
      "['Reordering categoriesAlso, note that both the fctrecode() and fctrelevel()functions can be called within the same call to mutate() :If you have multiple categorical variables to edit in this way, youcan place all the calls to fctrecode() and fctrelevel() inthe same mutate() command.']\n",
      "['Combining categories across multiple variablesSometimes a survey will store categorical data in multiplevariables.']\n",
      "['For example:▶Variable 1 : are you a Democrat, Republican, or neither?']\n",
      "['▶Variable 2 : (if Democrat/Republican) are you a strongDemocrat/Republican?']\n",
      "['▶Variable 3 : (if neither) do you lean towards one party or theother?']\n",
      "['To combine categorical variables, use the unite() command.']\n",
      "['data <- unite(data, pid, pid1d, pid1r, pidstr, pidlean)It pastes categories from diﬀerent variables together.']\n",
      "['THEN youcan recode these pasted categories.']\n",
      "['Managing string/character variablesTo split up a string into a list of fragments of the string, use thestrsplit() command.']\n",
      "['> str_split(response, pattern=\" \")[[1]][1] \"I\" \"have\" \"the\" \"faintest\" \"idea.\"[6] \"I\" \"really\" \"don’t\" \"watch\" \"the\"[11] \"news\"You can break the string on any kind of character you type withthepattern argument:> str_split(response, pattern=\"’\")[[1]][1] \"I have the faintest idea.']\n",
      "['I really don\"[2] \"t watch the news\"Managing string/character variablesOne annoying thing about how strings are coded sometimes isextraneous white space before and after the text.']\n",
      "['To removeleading and trailing spaces , use the strtrim() command:> hello <- c(\" whatsup?']\n",
      "['\")> hello[1] \" whatsup?']\n",
      "['\"> str_trim(hello)[1] \"whatsup?\"Managing string/character variablesOne of the most useful ways to pull data from a string is to searchfor particular words.']\n",
      "['The strdetect() command returns TRUEif the word is found in the string, FALSE if not.']\n",
      "['The match mustbe exact, and it is case sensitive.']\n",
      "['> str_detect(response, \"news\")[1] TRUE> str_detect(response, \"have\")[1] TRUE> str_detect(response, \"haven’t\")[1] FALSEThe strdetect() function can also take regular expressionsinstead of a single word.']\n",
      "['We’ll cover that in more detail later inthe semester.']\n",
      "['Managing string/character variablesYou can also search for multiple patterns at the same time withstrdetect() , but the code is a bit strange.']\n",
      "['There are two waysto proceed.']\n",
      "['First, you can separate diﬀerent words or terms with the |symbolinside the quotes.']\n",
      "['This only works if there are no spaces orcarriage returns next to any|symbol.']\n",
      "['So if you are looking forone of “larry”, “curly”, and “moe” in the text:This works: strdetect(response, \"larry|curly|moe\")You will get a variable that is TRUE if the text contains ”larry” or”curly” or ”moe”Doesn’t work:strdetect(response, \"larry | curly | moe\")You won’t get an error but the TRUE and FALSE values won’t beassigned correctly.']\n",
      "['Managing string/character variablesIf you have a long list of words, it’s better to use the followingtechnique:1.']\n",
      "['Put all of the words you want to search for in a character vectorobject2.']\n",
      "['Instead of a word in quotes, typepaste(object, collapse = \"|\")where object is the vector you made in step 1.']\n",
      "['Then the variable you create will be TRUE if any of the words inyour vector are present.']\n",
      "['Managing string/character variablesFor example, to search for the names of UVA men’s basketballplayers in a variable named text , type:players <- c(\"Marco Anthony\", \"Francesco Badocchi\",\"Francisco Caffaro\", \"Kihei Clark\",\"Mamadi Diakite\", \"Kyle Guy\", \"Jay Huff\",\"De’Andre Hunter\", \"Ty Jerome\",\"Austin Katstra\", \"Braxton Key\",\"Jayden Nixon\", \"Jack Salt\", \"Kody Stattmann\")data <- mutate(data, mention_player =str_detect(text, paste(players, collapse=\"|\")))The variable mention player will be TRUE if text contains any ofthese names, and FALSE otherwise.']\n",
      "['Managing date/time variablesThere are lots of data in political science that involve dates andtimes.']\n",
      "['Unfortunately, there is NOT a consistent way that datesand times are recorded in data.']\n",
      "['Problem 1 : Sometimes a single date/time is recorded in manyvariables: year, month, day, hour, minute might all be separatecolumns .']\n",
      "['Problem 2 : Sometimes a date/time is recorded in one variable as amessy and unusable block of text .']\n",
      "['How do we deal with each type of coding?']\n",
      "['How do we get R torecognize that the variable is a date or time?']\n",
      "['Managing date/time variablesAs with anything in R, there are many ways to do the same thing.']\n",
      "['Here we will use the lubridate package:library(lubridate)Problem 1 : the date and time are stored in many variables:year1 <- 1983; month1 <- 4; day1 <- 14year2 <- 2018; month2 <- 2; day2 <- 22We can use the make datetime() command to create singlevariables from multiple variables.']\n",
      "['We specify which variablescontain the year, month, day, etc.']\n",
      "['date1 <- make_datetime(year=year1, month=month1, day=day1)date2 <- make_datetime(year=year2, month=month2, day=day2)We can also add the hour ,minute , and second arguments if wehave that information.']\n",
      "['Managing date/time variablesNow date1 anddate2 are recognized as dates:> date1[1] \"1983-04-14 UTC\"> date2[1] \"2018-02-22 UTC\"The class of a date variable is called POSIXct andPOSIXt .']\n",
      "['It’s atechnical name, but it just means “date and time”.']\n",
      "['> class(date1)[1] \"POSIXct\" \"POSIXt\"Managing date/time variablesNow that we have two date variables, we can use basic arithmeticfunctions with them.']\n",
      "['We can for example measure the length oftime between the two dates:> duration <- date2 - date1> durationTime difference of 12733 days> as.numeric(duration)/365[1] 34.88493The make datetime() command can work within the mutate()command to create new variables.']\n",
      "['Managing date/time variablesProblem 2 : pulling date and time information from a single, messyvariable.']\n",
      "['time1 <- \"2018-02-22 10:05:28\"time2 <- \"02-22-2018 10:05:28\"time3 <- \"22-02-2018 10:05:28\"These variables contain the year, month, day, hour, minute, andsecond, all in one variable.']\n",
      "['They are also currently read ascharacter.']\n",
      "['How do we get R to read these as dates and times ,and to recognize the correct year, month, etc.?']\n",
      "['The ymdhms() command assumes that the data contains theyear, month, day, hour, minute, and second in that order:ymd_hms(time1)[1] \"2018-02-22 10:05:28 UTC\"Managing date/time variablesThe time2 variable is diﬀerent: it lists the MONTH ﬁrst, then theDAY, then the YEAR.']\n",
      "['If we use the ymdhms() command, weparse incorrectly:> ymd_hms(time2)[1] NAWarning message:All formats failed to parse.']\n",
      "['No formats found.']\n",
      "['The error occurs because it reads 2018 as the day, and that can’thappen.']\n",
      "['Instead we can use the mdyhms() command:> mdy_hms(time2)[1] \"2018-02-22 10:05:28 UTC\"Managing date/time variablesThere are many of these commands, and they diﬀer only in theorder you write the letters y,m, and d.']\n",
      "['These commands automatically recognize the diﬀerent characters(hyphens, slashes, etc.) separate elements of the date, so no needto worry about specifying that.']\n",
      "['PipingAdvanced R programmers know how to use a pipe.']\n",
      "['PipingA pipe is a way to connect consecutive R functions thatmanipulate the same data frame.']\n",
      "['The code for a pipe is %>%at the end of one command.']\n",
      "['It means“apply the next command to the data output by thiscommand .” Pipes are implemented in the magrittr package, oneof the packages included with tidyverse .']\n",
      "['Using the pipe is completely optional .']\n",
      "['It saves some time andspace, and it makes you look like an advanced programmer.']\n",
      "['PipingHere are some data cleaning commands you know:data <- read_csv(\"data.csv\")data <- select(data, vote, age, sex, starts_with(\"party\"))data <- arrange(data, age)data <- filter(data, party_ID==\"republican\")data <- mutate(data, vote = fct_recode(vote,\"Trump\" = \"1.']\n",
      "['D Trump\",\"Clinton\" = \"2.']\n",
      "['H Clinton\",NULL = \"3.']\n",
      "['G Johnson\",NULL = \"4.']\n",
      "['J Stein\",NULL = \"5.']\n",
      "['Other\"))Notice how every command begins by calling the data frame wewant to alter.']\n",
      "['PipingThe exact same commands can be run with the following code:data <- read_csv(\"data.csv\")data <- data %>%select(vote, age, sex, starts_with(\"party\")) %>%arrange(age) %>%filter(party_ID==\"republican\") %>%mutate(vote = fct_recode(vote,\"Trump\" = \"1.']\n",
      "['D Trump\",\"Clinton\" = \"2.']\n",
      "['H Clinton\",NULL = \"3.']\n",
      "['G Johnson\",NULL = \"4.']\n",
      "['J Stein\",NULL = \"5.']\n",
      "['Other\"))Notice that the select() ,arrange() ,filter() , and mutate()functions no longer need the data frame listed ﬁrst.']\n",
      "['Pipes can be used with any function that takes a data frame asthe ﬁrst argument .']\n",
      "['Using group by()Sometimes it is necessary to do calculations within groups ofobservations.']\n",
      "['Consider these example data (saved as objecttable1 in R once you load tidyverse ):country year cases populationAfghanistan 1999 745 19987071Afghanistan 2000 2666 20595360Brazil 1999 37737 172006362Brazil 2000 80488 174504898China 1999 212258 1272915272China 2000 213766 1280428583We might want to create a variable with the total number of casesper year, or the average population by country.']\n",
      "['Using group by()The group by() function doesn’t do anything by itself, but itallows you to use mutate() orsummarize() to performcalculations in groups.']\n",
      "['Within group by() , specify the variable or variables that denotethe groups.']\n",
      "['mutate() performs within-group calculations within the existingdata frame.']\n",
      "['summarize() deletes rows, and leaves you with one row pergroup .']\n",
      "['To turn oﬀ within-group calculations, use the ungroup function.']\n",
      "['Using group by()To calculate the total number of cases per year in table1 use thiscode:table2 <- table1 %>%group_by(year) %>%mutate(totalcases = sum(cases))The data now look like this:country year cases population totalcasesAfghanistan 1999 745 19987071 250740Afghanistan 2000 2666 20595360 296920Brazil 1999 37737 172006362 250740Brazil 2000 80488 174504898 296920China 1999 212258 1272915272 250740China 2000 213766 1280428583 296920Using group by()To do this calculation leaving only one row per year, usesummarize() instead of mutate() :table2 <- table1 %>%group_by(year) %>%summarize(totalcases = sum(cases))The data now look like this:year totalcases1999 2507402000 296920Using group by()To calculate the average population for each country in table1use this code:table2 <- table1 %>%group_by(country) %>%mutate(avg.pop = mean(population))The data now look like this:country year cases population avg.popAfghanistan 1999 745 19987071 20291216Afghanistan 2000 2666 20595360 20291216Brazil 1999 37737 172006362 173255630Brazil 2000 80488 174504898 173255630China 1999 212258 1272915272 1276671928China 2000 213766 1280428583 1276671928Using group by()To do this calculation leaving only one row per country, usesummarize() instead of mutate() :table2 <- table1 %>%group_by(country) %>%summarize(avg.pop = mean(population))The data now look like this:country avg.popAfghanistan 20291216Brazil 173255630China 1276671928Using group by()ungroup turns oﬀ within-group calculations.']\n",
      "['To calculate thepercent of cases that each country accounts for:table2 <- table1 %>%group_by(year) %>%mutate(totalcases = sum(cases)) %>%ungroup %>%mutate(percent_of_cases = 100*cases/totalcases)The data now look like this:country year cases population totalcases percentAfghanistan 1999 745 19987071 250740 0.297Afghanistan 2000 2666 20595360 296920 0.898Brazil 1999 37737 172006362 250740 15.05Brazil 2000 80488 174504898 296920 27.11China 1999 212258 1272915272 250740 84.65China 2000 213766 1280428583 296920 71.99Using count()The count() function counts the number of rows within eachgroup.']\n",
      "['It’s not necessary to use group by() when callingcount() .']\n",
      "['Suppose we have data with terrorist attacks, with variables yearandcountry .']\n",
      "['To create a dataset with a count of the totalattacks by country , sorted from most to least, typeattacks2 <- attacks %>%count(country, sort=TRUE)To create a dataset with a count of the total attacks by in eachcountry and year , sorted from most to least, typeattacks2 <- attacks %>%count(country, year, sort=TRUE)Fun with functions and dplyrBrian Wright1/24/2020Brian Wright Fun with functions and dplyr 1/24/2020 1 / 28Overview of Functions (Advanced R)Functions are at the core of R language, it’s really a function basedlanguage“R, at its heart, is a”functional\" language.']\n",
      "['This means that it hascertain technical properties, but more importantly that it lendsitself to a style of problem solving centred on functions.\" HadleyWickhamBrian Wright Fun with functions and dplyr 1/24/2020 2 / 28What is a functional based language?']\n",
      "['Recently functions have grown in popularity because they can produceefficient and simple solutions to lots of problems.']\n",
      "['Many of theproblems with performance have been solved.']\n",
      "['Functional programming compliments object oriented programmingBrian Wright Fun with functions and dplyr 1/24/2020 3 / 28What makes a programming approach “functional”?']\n",
      "['Functions can behave like any other data structure▶Assign them to variables, store to lists, pass them as aurguments toother functions, create them inside functions and even produce afunction as a result of a funcionFunctions need to be “pure” meaning that if you call it again with thesame inputs you get the same results.']\n",
      "['sys.time() not a “pure”functionThe execution of the function shouldn’t change global variables, haveno side effects.']\n",
      "['Brian Wright Fun with functions and dplyr 1/24/2020 4 / 28FunctionsFunction don’t have to be “pure” but it can help to ensure your codeis doing what you intend it to do.']\n",
      "['Functional programming helps to break a problem down into it’spieces.']\n",
      "['When working to solve a problem it helps to divide the codeinto individually operating functions that solve parts of the problem.']\n",
      "['Brian Wright Fun with functions and dplyr 1/24/2020 5 / 28Types of FunctionsFigure 1: Function TypesBrian Wright Fun with functions and dplyr 1/24/2020 6 / 28Let’s Build a FunctionBasically recipes composed of series of R statementsname <- funtion (variables){#In here goes the series of R statements}Brian Wright Fun with functions and dplyr 1/24/2020 7 / 28Example, talk out the stepsmy_mean <- function (x){Sum <- sum(x)#Here we are using a function#inside a function!']\n",
      "['N <- length (x)return (Sum /N)#return is optional but helps with#clarity on some level.']\n",
      "['}Create a little list and pass it to the function and see if it works.']\n",
      "['Also call the Sum and N variables...does this work?']\n",
      "['Brian Wright Fun with functions and dplyr 1/24/2020 8 / 28Functional - Will show later, Function Factory(Advanced R)power1 <- function (exp) {function (x) {x^exp}}#Assigning the exponentialssquare <- power1 (2)cube <- power1 (3)Brian Wright Fun with functions and dplyr 1/24/2020 9 / 28Run the Created Functionssquare (3)> [1] 9cube (3)> [1] 27Brian Wright Fun with functions and dplyr 1/24/2020 10 / 28Quick ExerciseCreate a function that computes the range of a variable and thenfornogoodreasonadds100anddividesby10.']\n",
      "['Writeoutthestepsyou would need first in Pseudocode, then develop the function.']\n",
      "['Brian Wright Fun with functions and dplyr 1/24/2020 11 / 28dplyrverbs in the tidyverseThe dplyrpackage gives us a few verbs for data manipulationFunction Purposeselect Select columns based on name or positionmutate Create or change a columnfilter Extract rows based on some criteriaarrange Re-order rows based on values of variable(s)group_by Split a dataset by unique values of a variablesummarize Create summary statistics based on columnsBrian Wright Fun with functions and dplyr 1/24/2020 12 / 28selectYou can select columns by name or position, of course.']\n",
      "['You can also select columns based on some criteria, which areencapsulated in functions.']\n",
      "['starts_with(“ \"), ends_with(\" ”), contains(“____”)one_of(“____”,“_____”,“______”)There are others; see help(starts_with) .']\n",
      "['Brian Wright Fun with functions and dplyr 1/24/2020 13 / 28ExampleLoad the weather.csv .']\n",
      "['This contains daily temperature data in 2010 forsome location.']\n",
      "['> [1] \"C:/Users/Brian Wright/Documents/git_3001/DS-4001/2_R_function_basics\"head (weather, 2)> # A tibble: 2 x 35> id year month element d1 d2 d3 d4 d5 d6 d7 d8> <chr> <int> <int> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>> 1 MX17~ 2010 1 tmax NA NA NA NA NA NA NA NA> 2 MX17~ 2010 1 tmin NA NA NA NA NA NA NA NA> # ...']\n",
      "['with 23 more variables: d9 <lgl>, d10 <dbl>, d11 <dbl>, d12 <lgl>,> # d13 <dbl>, d14 <dbl>, d15 <dbl>, d16 <dbl>, d17 <dbl>, d18 <lgl>,> # d19 <lgl>, d20 <lgl>, d21 <lgl>, d22 <lgl>, d23 <dbl>, d24 <lgl>,> # d25 <dbl>, d26 <dbl>, d27 <dbl>, d28 <dbl>, d29 <dbl>, d30 <dbl>, d31 <dbl>Brian Wright Fun with functions and dplyr 1/24/2020 14 / 28How would you just select the columns with the dailydata?']\n",
      "['select (weather, starts_with (\"d\"))Brian Wright Fun with functions and dplyr 1/24/2020 15 / 28mutatemutatecan either transform a column in place or create a new column ina datasetWe’ll use the in-built mpgdataset for this example, We’ll select only thecity and highway mileages.']\n",
      "['To use this selection later, we will need toassign it to a new namempg1 <- select (mpg, cty, hwy)Brian Wright Fun with functions and dplyr 1/24/2020 16 / 28mutateWe’ll change the city and highway mileage to km/l from mpg.']\n",
      "['This willinvolve multiplying it by 1.6 and dividing by 3.8head (mutate (mpg1, cty = cty *1.6 /3.8,hwy = hwy *1.6/3.8), 5)> # A tibble: 5 x 2> cty hwy> <dbl> <dbl>> 1 7.58 12.2> 2 8.84 12.2> 3 8.42 13.1> 4 8.84 12.6> 5 6.74 10.9This is in-place replacementBrian Wright Fun with functions and dplyr 1/24/2020 17 / 28New Variable Definedmutate (mpg1, cty1 = cty *1.6/3.8, hwy1 = hwy *1.6/3.8)> # A tibble: 234 x 4> cty hwy cty1 hwy1> <int> <int> <dbl> <dbl>> 1 18 29 7.58 12.2> 2 21 29 8.84 12.2> 3 20 31 8.42 13.1> 4 21 30 8.84 12.6> 5 16 26 6.74 10.9> 6 18 26 7.58 10.9> 7 18 27 7.58 11.4> 8 18 26 7.58 10.9> 9 16 25 6.74 10.5> 10 20 28 8.42 11.8> # ...']\n",
      "['with 224 more rowsThis creates new variablesBrian Wright Fun with functions and dplyr 1/24/2020 18 / 28filterfilterextracts rows based on criteriafilter (mpg, cyl ==4)> # A tibble: 81 x 11> manufacturer model displ year cyl trans drv cty hwy fl class> <chr> <chr> <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>> 1 audi a4 1.8 1999 4 auto(l~ f 18 29 p comp~> 2 audi a4 1.8 1999 4 manual~ f 21 29 p comp~> 3 audi a4 2 2008 4 manual~ f 20 31 p comp~> 4 audi a4 2 2008 4 auto(a~ f 21 30 p comp~> 5 audi a4 quat~ 1.8 1999 4 manual~ 4 18 26 p comp~> 6 audi a4 quat~ 1.8 1999 4 auto(l~ 4 16 25 p comp~> 7 audi a4 quat~ 2 2008 4 manual~ 4 20 28 p comp~> 8 audi a4 quat~ 2 2008 4 auto(s~ 4 19 27 p comp~> 9 chevrolet malibu 2.4 1999 4 auto(l~ f 19 27 r mids~> 10 chevrolet malibu 2.4 2008 4 auto(l~ f 22 30 r mids~> # ...']\n",
      "['with 71 more rowsThis extracts only 4 cylinder vehiclesOther choices might be cyl != 4 ,cyl > 4,year == 1999 ,manufacturer==\"audi\"Brian Wright Fun with functions and dplyr 1/24/2020 19 / 28Practice Pipingadmit_df <- read_csv (\"~/git_3001/DS-4001/data/LogReg.csv\")str(admit_df)> tibble [400 x 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)> $ admit: num [1:400] 0 1 1 1 0 1 1 0 1 0 ...']\n",
      "['> $ gre : num [1:400] 380 660 800 640 520 760 560 400 540 700 ...']\n",
      "['> $ gpa : num [1:400] 3.61 3.67 4 3.19 2.93 3 2.98 3.08 3.39 3.92 ...']\n",
      "['> $ rank : num [1:400] 3 3 1 4 4 2 1 2 3 2 ...']\n",
      "['> - attr(*, \"spec\")=> ..']\n",
      "['cols(> ..']\n",
      "['admit = col_double(),> ..']\n",
      "['gre = col_double(),> ..']\n",
      "['gpa = col_double(),> ..']\n",
      "['rank = col_double()> ..']\n",
      "[')#Do we notice anything that seems a bit off.']\n",
      "['Brian Wright Fun with functions and dplyr 1/24/2020 20 / 28Coercion num to factoradmit_df $rank <- as.factor (admit_df $rank)#changes rank to a factorBrian Wright Fun with functions and dplyr 1/24/2020 21 / 28Five Basic Classes in Rcharacternumeric (double precision floating point numbers, default)integer (subset of numeric)complex (j = 10 + 5i)logical (True/False)Brian Wright Fun with functions and dplyr 1/24/2020 22 / 28All have coercion calls (example from: R Nuts andBolts)x <- 0 :6class (x)#why> [1] \"integer\"as.numeric (x)> [1] 0 1 2 3 4 5 6as.logical (x)> [1] FALSE TRUE TRUE TRUE TRUE TRUE TRUEas.character (x)> [1] \"0\" \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"Brian Wright Fun with functions and dplyr 1/24/2020 23 / 28Functional Example: Pass a function get a vectorWe can also convert multiple columns using lapply(), great example offunctional orientation of R.']\n",
      "['names <- c(\"admit\",\"rank\")#using names as a index on admit_df,admit_df[,names] <- lapply (admit_df[,names], factor)#Check class of those two variables(as.character (meta_fun <- lapply (subset (admit_df,select = names),class)))> [1] \"factor\" \"factor\"#using a functional with two functions inside that creates a objectcoerced to a character list...what fun.']\n",
      "['Brian Wright Fun with functions and dplyr 1/24/2020 24 / 28Using the code chunk below to “group_by” rankBrian Wright Fun with functions and dplyr 1/24/2020 25 / 28Using the code chunk below to filter by 1 in the admitcolumnBrian Wright Fun with functions and dplyr 1/24/2020 26 / 28Ok now summarise by average GPABrian Wright Fun with functions and dplyr 1/24/2020 27 / 28Now Pipe everything togetherBrian Wright Fun with functions and dplyr 1/24/2020 28 / 28']\n",
      "Created 1526 chunks of text\n"
     ]
    }
   ],
   "source": [
    "# 2. Split text into chunks\n",
    "print(\"Splitting text into chunks...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,   # Number of characters per chunk\n",
    "    chunk_overlap=500, # Overlap to provide context\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \"] # Split by paragraphs, then lines, then words\n",
    ")\n",
    "\n",
    "texts = []\n",
    "for page in all_text:\n",
    "    cont = page.replace(\"\\xad\\n\",\"\").replace(\"\\n\",\"\")\n",
    "    chunks = text_splitter.split_text(cont)\n",
    "    print(chunks)\n",
    "    texts.extend(chunks)\n",
    "\n",
    "print(f\"Created {len(texts)} chunks of text\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishg\\AppData\\Local\\Temp\\ipykernel_36360\\685889271.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vishg\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishg\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3. Create embeddings\n",
    "print(\"Creating embeddings...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': device}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "if os.path.exists(persist_dir): os.system(f'sudo rmdir /s /q \"{persist_dir}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating and persisting vector store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishg\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "# 4. Create and persist the vector store\n",
    "print(\"Creating and persisting vector store...\")\n",
    "\n",
    "vector_store = Chroma.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_dir,\n",
    "    collection_name=\"pdf_collection\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# Load vector store\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': device}\n",
    ")\n",
    "\n",
    "\n",
    "vector_store = Chroma(\n",
    "    persist_directory=\"pdf_store\",\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"pdf_collection\"\n",
    ")\n",
    "\n",
    "# Load language model\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "def generate_with_context(query, k=5):\n",
    "    context_docs = vector_store.similarity_search(query, k=k)\n",
    "    \n",
    "    context = \" \".join([doc.page_content for doc in context_docs])\n",
    "\n",
    "    print(\"\\n\".join([str(doc) for doc in context_docs]))\n",
    "    \n",
    "    augmented_prompt = f\"\"\"Context: {context}\n",
    "    Question: {query}\n",
    "    Answer based on the provided context:\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = tokenizer(augmented_prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_length=1300, \n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='11Engineering of Machine Learning Algos versus Software Development Source: https://towardsdatascience.com/stoend -to-end-data -science -life-cycle -6387523b5afcData Science Life Cycle (Everything includes Evaluation13TrainFeature EngineerTest DeployEvaluate Evaluate Evaluate EvaluateMonitor1.What is Machine Learning?'\n",
      "page_content='Terms and Phases Engineering of Machine Learning Algos versus Software Development Source: https://towardsdatascience.com/stoend -to-end-data -science -life-cycle -6387523b5afcBrian’s Version of Data Science Lifecycle4Question IDBusiness UnderstandingData Acquisition -ETLInitial ModelEvaluationData Understanding -EDAInitial Model(s) BuildingEvaluation Criteria Value Metric Model Creation & Training Feature Engineering and EvaluationOptimization –Hyperpara and EvaluationModel DeploymentData Drift AnalysisModel Performance –Evaluation Value MetricModel Drift Analysis –Model EvaluationReports –Dashboards -Products G1 G2G3Gate Reviews5Machine Learning Time6“A field of Computer Science that gives computers the ability to learnwithout being explicitly programmed.”-Arthur Samuel (Coined the term in 1959 at IBM)“The ability [for systems] to acquire their own knowledge, byextracting patterns from raw data.”-Deep Learning, Goodfellow et al“A computer program is said to learn from experience E with respectto some set of tasks T and performance measure P if its performancetasks in T, as measured by P, improves with experience E.”-Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs.'\n",
      "page_content='Terms and Phases Engineering of Machine Learning Algos versus Software Development Source: https://towardsdatascience.com/stoend -to-end-data -science -life-cycle -6387523b5afcBrian’s Version of Data Science Lifecycle4Question IDBusiness UnderstandingData Acquisition -ETLInitial ModelEvaluationData Understanding -EDAInitial Model(s) BuildingEvaluation Criteria Value Metric Model Creation & Training Feature Engineering and EvaluationOptimization –Hyperpara and EvaluationModel DeploymentData Drift AnalysisModel Performance –Evaluation Value MetricModel Drift Analysis –Model EvaluationReports –Dashboards -Products G1 G2G3Gate Reviews5Machine Learning Time6“A field of Computer Science that gives computers the ability to learnwithout being explicitly programmed.”-Arthur Samuel (Coined the term in 1959 at IBM)“The ability [for systems] to acquire their own knowledge, byextracting patterns from raw data.”-Deep Learning, Goodfellow et al“A computer program is said to learn from experience E with respectto some set of tasks T and performance measure P if its performancetasks in T, as measured by P, improves with experience E.”-Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs.'\n",
      "page_content='Terms and Phases Engineering of Machine Learning Algos versus Software Development Source: https://towardsdatascience.com/stoend -to-end-data -science -life-cycle -6387523b5afcBrian’s Version of Data Science Lifecycle4Question IDBusiness UnderstandingData Acquisition -ETLInitial ModelEvaluationData Understanding -EDAInitial Model(s) BuildingEvaluation Criteria Value Metric Model Creation & Training Feature Engineering and EvaluationOptimization –Hyperpara and EvaluationModel DeploymentData Drift AnalysisModel Performance –Evaluation Value MetricModel Drift Analysis –Model EvaluationReports –Dashboards -Products G1 G2G3Gate Reviews5Machine Learning Time6“A field of Computer Science that gives computers the ability to learnwithout being explicitly programmed.”-Arthur Samuel (Coined the term in 1959 at IBM)“The ability [for systems] to acquire their own knowledge, byextracting patterns from raw data.”-Deep Learning, Goodfellow et al“A computer program is said to learn from experience E with respectto some set of tasks T and performance measure P if its performancetasks in T, as measured by P, improves with experience E.”-Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs.'\n",
      "page_content='Terms and Phases Engineering of Machine Learning Algos versus Software Development Source: https://towardsdatascience.com/stoend -to-end-data -science -life-cycle -6387523b5afcBrian’s Version of Data Science Lifecycle4Question IDBusiness UnderstandingData Acquisition -ETLInitial ModelEvaluationData Understanding -EDAInitial Model(s) BuildingEvaluation Criteria Value Metric Model Creation & Training Feature Engineering and EvaluationOptimization –Hyperpara and EvaluationModel DeploymentData Drift AnalysisModel Performance –Evaluation Value MetricModel Drift Analysis –Model EvaluationReports –Dashboards -Products G1 G2G3Gate Reviews5Machine Learning Time6“A field of Computer Science that gives computers the ability to learnwithout being explicitly programmed.”-Arthur Samuel (Coined the term in 1959 at IBM)“The ability [for systems] to acquire their own knowledge, byextracting patterns from raw data.”-Deep Learning, Goodfellow et al“A computer program is said to learn from experience E with respectto some set of tasks T and performance measure P if its performancetasks in T, as measured by P, improves with experience E.”-Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs.'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input length of input_ids is 1342, but `max_length` is set to 1300. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is machine learning?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_with_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[1;32mIn[15], line 37\u001b[0m, in \u001b[0;36mgenerate_with_context\u001b[1;34m(query, k)\u001b[0m\n\u001b[0;32m     31\u001b[0m augmented_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mContext: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124mAnswer based on the provided context:\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     36\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(augmented_prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\utils.py:1906\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_supports_num_logits_to_keep() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_logits_to_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[0;32m   1904\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_logits_to_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1906\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_generated_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_default_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1908\u001b[0m \u001b[38;5;66;03m# 7. Prepare the cache.\u001b[39;00m\n\u001b[0;32m   1909\u001b[0m \u001b[38;5;66;03m# - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\u001b[39;00m\n\u001b[0;32m   1910\u001b[0m \u001b[38;5;66;03m# - different models have a different cache name expected by the model (default = \"past_key_values\")\u001b[39;00m\n\u001b[0;32m   1911\u001b[0m \u001b[38;5;66;03m# - `max_length`, prepared above, is used to determine the maximum cache length\u001b[39;00m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;66;03m# TODO (joao): remove `user_defined_cache` after v4.47 (remove default conversion to legacy format)\u001b[39;00m\n\u001b[0;32m   1913\u001b[0m cache_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmamba\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_params\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\utils.py:1228\u001b[0m, in \u001b[0;36mGenerationMixin._validate_generated_length\u001b[1;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[0;32m   1227\u001b[0m     input_ids_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but `max_length` is set to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1230\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1231\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1232\u001b[0m     )\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n\u001b[0;32m   1235\u001b[0m min_length_error_suffix \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1236\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1237\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincrease the maximum length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1238\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Input length of input_ids is 1342, but `max_length` is set to 1300. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"what is machine learning?\"\n",
    "response = generate_with_context(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
