{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_vector_store(pdf_path: str, persist_directory: str = \"pdf_store\"):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 5. Persist the vector store\n",
    "    vector_store.persist()\n",
    "    print(f\"Vector store created and saved to {persist_directory}\")\n",
    "    \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = r\"..\\..\\data\\pdf_books\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf\"  \n",
    "persist_dir = \"pdf_store\"  # Directory where the vector store will be saved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF from ..\\..\\data\\pdf_books\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading PDF from {pdf_path}\")\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415 Pages in the PDF\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 0}, page_content=''),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 1}, page_content=''),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 2}, page_content='Pattern Recognition \\nand Neural Networks \\nB. D. RIPLEY \\nUniversity of Oxford '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 3}, page_content='PUBLISHED BY THE PRESS SYNDICATE OF THE UNIVERSITY OF CAMBRIDGE \\nThe Pitt Building, Trumpington Street, Cambridge, United Kingdom \\nCAMBRIDGE UNIVERSITY PRESS \\nThe Edinburgh Building, Cambridge CB2 2RU, UK \\n40 West 20th Street, New York NY 10011-4211, USA \\n477 Williamstown Road, Port Melbourne, VIC 3207, Australia \\nRuiz de Alarcon 13, 28014 Madrid, Spain \\nDock House, The Waterfront, Cape Town 8001, South Africa \\nhttp://www.cambridge.org \\n© B. D. Ripley 1996 \\nThis book is in copyright. Subject to statutory exception \\nand to the provisions of relevant collective licensing agreements, \\nno reproduction of any part may take place without \\nthe written permission of Cambridge University Press. \\nFirst published 1996 \\nEighth printing 2005 \\nPrinted in the United Kingdom at the University Press, Cambridge \\nA catalogue record for this book is available from the British Library \\nLibrary of Congress Cataloguing in Publication data \\nRipley, Brian D., 1952-\\nPattern recognition and neural networks I B.D. Ripley. \\np. em. \\nIncludes bibliographical references and index. \\nISBN 0 521 46086 7 \\n1. Neural networks Computer science). 2. Pattern Recognition systems \\nI. Title \\nQA 76.87.R56 1996 \\n006.4--dc20 95-25223 CIP \\nISBN 0 521 46086 7 hardback '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 4}, page_content='Contents \\nPreface ix \\nNotation xii \\n1 Introduction and Examples 1 \\n1.1 How do neural methods differ? 4 \\n1.2 The pattern recognition task . . 5 \\n1.3 Overview of the remaining chapters . 9 \\n1.4 Examples 10 \\n1.5 Literature 15 \\n2 Statistical Decision Theory 17 \\n2.1 Bayes rules for known distributions 18 \\n2~2 Parametric models ... 26 \\n2.3 Logistic discrimination 43 \\n2.4 Predictive classification 45 \\n2.5 Alternative estimation procedures 55 \\n2.6 How complex a model do we need? . 59 \\n2.7 Performance assessment . . . . . . . . 66 \\n2.8 Computational learning approaches . 77 \\n3 Linear Discriminant Analysis 91 \\n3.1 Classical linear discrimination 92 \\n3.2 Linear discriminants via regression 101 \\n3.3 Robustness ..... 105 \\n3.4 Shrinkage methods . 106 '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 5}, page_content='VI Contents \\n3.5 Logistic discrimination 0 0 .... 109 \\n3.6 Linear separation and perceptrons . 116 \\n4 Flexible Discriminants 121 \\n4.1 Fitting smooth parametric functions 122 \\n4.2 Radial basis functions 131 \\n4.3 Regularization • 0 • • 136 \\n5 Feed-forward Neural Networks 143 \\n5.1 Biological motivation 145 \\n5.2 Theory ........ 147 \\n5.3 Learning algorithms 148 \\n5.4 Examples ...... 160 \\n5.5 Bayesian perspectives 163 \\n5.6 Network complexity . 168 \\n5.7 Approximation results . 173 \\n6 Non-parametric Methods 181 \\n6.1 Non-parametric estimation of class densities . 181 \\n6.2 Nearest neighbour methods .. 191 \\n6.3 Learning vector quantization . 201 \\n6.4 Mixture representations 207 \\n7 Tree-structured Classifiers 213 \\n7.1 Splitting rules . 216 \\n7.2 Pruning rules 221 \\n7.3 Missing values 231 \\n7.4 Earlier approaches 235 \\n7.5 Refinements . . . . 237 \\n7.6 Relationships to neural networks 240 \\n7.7 Bayesian trees . •• 0 ••••• ••• 241 \\n8 Belief Networks 243 \\n8.1 Graphical models and networks 246 \\n8.2 Causal networks 0 ••• 0 ••• • 262 \\n8.3 Learning the network structure 275 '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 6}, page_content='Contents \\n8.4 Boltzmann machines . . . . . . \\n8.5 Hierarchical mixtures of experts \\n9 Unsupervised Methods \\n9.1 Projection methods . \\n9.2 Multidimensional scaling \\n9.3 Clustering algorithms \\n9.4 Self-organizing maps ... \\n10 Finding Good Pattern Features \\n10.1 Bounds for the Bayes error . \\n10.2 Normal class distributions . \\n10.3 Branch-and-bound techniques \\n10.4 Feature extraction \\nA Statistical Sidelines \\nA.1 Maximum likelihood and MAP estimation . \\nA.2 The EM algorithm . . . . . . . . . . . . \\nA.3 Markov chain Monte Carlo . . . . . . . \\nA.4 Axioms for conditional independence . \\nA.5 Optimization . . . . . . . . . . . . . . . \\nGlossary \\nReferences \\nAuthor Index \\nSubject Index Vll \\n279 \\n283 \\n287 \\n288 \\n305 \\n311 \\n322 \\n327 \\n328 \\n329 \\n330 \\n331 \\n333 \\n333 \\n334 \\n337 \\n339 \\n342 \\n347 \\n355 \\n391 \\n399 '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 7}, page_content=''),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 8}, page_content=\"Preface \\nPattern recognition has a long and respectable history within engineer\\xad\\ning, especially for military applications, but the cost of the hardware \\nboth to acquire the data (signals and images) and to compute the \\nanswers made it for many years a rather specialist subject. Hardware \\nadvances have made the concerns of pattern recognition of much wider \\napplicability. In essence it covers the following problem: \\n'Given some examples of complex signals and the correct \\ndecisions for them, make decisions automatically for a stream \\nof future examples.' \\nThere are many examples from everyday life: \\nN arne the species of a flowering plant. \\nGrade bacon rashers from a visual image. \\nClassify an X-ray image of a tumour as cancerous or benign. \\nDecide to buy or sell a stock option. \\nGive or refuse credit to a shopper. \\nMany of these are currently performed by human experts, but it is \\nincreasingly becoming feasible to design automated systems to replace \\nthe expert and either perform better (as in credit scoring) or 'clone' the \\nexpert (as in aids to medical diagnosis). \\nNeural networks have arisen from analogies with models of the way \\nthat humans might approach pattern recognition tasks, although they \\nhave developed a long way from the biological roots. Great claims have \\nbeen made for these procedures, and although few of these claims have \\nwithstood careful scrutiny, neural network methods have had great \\nimpact on pattern recognition practice. A theoretical understanding of \\nhow they work is still under construction, and is attempted here by \\nviewing neural networks within a statistical framework, together with \\nmethods developed in the field of machine learning. \\nOne of the aims of this book is to be a reference resource, so almost \\nall the results used are proved (and the remainder are given references \\nto complete proofs). The proofs are often original, short and I believe \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 9}, page_content=\"X Preface \\nshow insight into why the methods work. Another unusual feature of \\nthis book is that the methods are illustrated on examples, and those \\nexamples are either real ones or realistic abstractions. Unlike the proofs, \\nthe examples are not optional! \\nThe formal pre-requisites to follow this book are rather few, espe\\xad\\ncially if no attempt is made to follow the proofs. A background in \\nlinear algebra is needed, including eigendecompositions. (The singular \\nvalue decomposition is used, but explained.) A knowledge of calculus \\nand its use in finding extrema (such as local minima) is needed, as well \\nas the simplest notions of asymptotics (Taylor series expansions and \\nO(n) notation). Graph theory is used in Chapter 8, but developed from \\nscratch. Only a first course in probability and statistics is assumed, but \\nconsiderable experience in manipulations will be needed to follow the \\nderivations without writing out the intermediate steps. The glossary \\nshould help readers with non-technical backgrounds. \\nA graduate-course knowledge of statistical concepts will be needed \\nto appreciate fully the theoretical developments and proofs. The sections \\non examples need a much less mathematical background; indeed a good \\noverview of the state of the subject can be obtained by skimming the \\ntheoretical sections and concentrating on the examples. The theory and \\nthe insights it gives are important in understanding the relative merits \\nof the methods, and it is often very much harder to show that an idea \\nis unsound than to explain the idea. \\nSeveral chapters have been used in graduate courses to statisticians \\nand to engineers, computer scientists and physicists. A core of material \\nwould be Sections 2.1-2.3, 2.6, 2.7, 3.1, 3.5, 3.6, 4.1, 4.2, 5.1-5.4, 6.1-6.4, \\n7.1-7.3 and 9.1-9.4, supplemented by material of particular interest to \\nthe audience. For example, statisticians should cover 2.4, 2.5, 3.3, 3.4, \\n5.5, 5.6 and are likely to be interested in Chapter 8, and a fuller view \\nof neural networks in pattern recognition will be gained by adding 3.2, \\n4.3, 5.5-5.7, 7.6 and 8.4 to the core. \\nAcknowledgements \\nThis book was originally planned as a joint work with Nils Lid Hjort \\n(University of Oslo), and his influence will be immediately apparent \\nto those who have seen Hjort (1986), a limited circulation report. \\nMy own interest in neural networks was kindled by the invitation \\nfrom Ole Barndorff-Nielsen and David Cox to give a short course \\nat SemStat in 1992, which resulted in Ripley (1993). The book was \\nplanned and parts were written during a six-month period of leave at \\nthe programme on 'Computer Vision' at the Isaac Newton Institute Those hardy perennials, \\nthe 'exclusive or' and \\n'two spirals' problems, \\ndo not appear in this \\nbook. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 10}, page_content='Some of the software \\nused is supplied with \\nVenables & Ripley \\n(1994). Preface Xl \\nfor the Mathematical Sciences in Cambridge (England) ; discussions \\nwith the participants helped shape my impressions of leading-edge \\npattern recognition problems. Discussions with Lionel Tarassenko, \\nWray Buntine, John Moody and Chris Bishop have also helped to \\nshape my treatment. I was introduced to the machine-learning literature \\nand its distinctive goals by Donald Michie. Several people have read \\nand commented on chapters, notably Phil Dawid, Francis Marriott \\nand Ruth Ripley. I am grateful to Lionel Tarassenko and his co\\xad\\nauthors for the cover picture of outlier detection in a mammogram \\n(from Tarassenko et al., 1995). \\nParts of this book have been used as source material for graduate \\nlectures and seminar courses at Oxford, and I am grateful to my \\nstudents and colleagues for feedback ; present readers will appreciate \\nthe results of their insistence on more details in the mathematics. \\nThe examples were computed within the statistical system S-Pius \\nof MathSoft Inc., using software developed by the author and other \\ncontributors to the library of software for that system (notably Trevor \\nHastie and Rob Tibshirani). \\nIt has been a pleasure to work with CUP staff on the design and \\nproduction of this volume; especial thanks go to David Tranah, the \\neditor for this project who also contributed many aspects of the design. \\nB. D. Ripley \\nOxford, June 1995 '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 11}, page_content=\"Notation \\nThe notation used generally follows the standard conventions of math\\xad\\nematics and statistics. Random variables are usually denoted by capital \\nletters; if X is a random variable then x denotes its value. Often \\nbold letters denote vectors, so x = (xi) is a vector with components \\nxi, i = 1, ... , m, with m being deduced from the context. \\n~ \\nE \\n!(A) \\nNp{,u, :E} \\nO(g(n)) \\nOp(g(n)) \\n((j \\np(x) \\nPr{A} \\nPr{A I B} \\n.IRm \\nxT \\ne \\ne,e \\n[]+ \\nl J \\nr 1 is the 'doubt' report. \\ndenotes expectation. A suffix denotes the random variable \\nor distribution over which the averaging takes place. \\nis the indicator function of event A, one if A happens, \\notherwise zero. \\ndenotes a normal distribution in p dimensions. \\nf(n) = O(g(n)) means lf(n)/g(n)l is bounded as n-oo. \\nXn = Op(g(n)) means given E > 0 there is a constant B \\nsuch that Pr{IXn/g(n)l > B} < E for all n. \\nis the outlier report. \\ndenotes a probability density function. \\ndenotes the probability of an event A. \\ndenotes the conditional probability of A given B. \\nm -dimensional Euclidean space . \\ndenotes the transpose of a matrix X. \\na parameter or vector of parameters. \\na parameter estimate. \\nthe positive part, the maximum of the expression and \\nzero. \\nthe integer part (rounding down). The floor function. \\nthe nearest integer (rounding up). The ceiling function. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 12}, page_content=\"1 \\nIntroduction and Examples \\nThis book is primarily about pattern recognition, which covers a wide \\nrange of activities from many walks of life. It is something which we \\nhumans are particularly good at; we receive data from our senses and \\nare often able, immediately and without conscious effort, to identify the \\nsource of the data. For example, many of us can \\nrecognize faces we have not seen for many years, even in disguise, \\nrecognize voices over a poor telephone line, \\nas babies recognize our mothers by smell, \\ndistinguish the grapes used to make a wine, and sometimes \\neven recognize the vineyard and year, \\nidentify thousands of species of flowers and \\nspot an approaching storm. \\nScience, technology and business has brought to us many similar tasks, \\nincluding \\ndiagnosing diseases, \\ndetecting abnormal cells in cervical smears, \\nrecognizing dangerous driving conditions, \\nidentifying types of car, aeroplane, ... , \\nidentifying suspected criminals by fingerprints and DNA profiles, \\nreading Zip codes (US postal codes) on envelopes, \\nreading hand-written symbols (on a penpad computer), \\nreading maps and circuit diagrams, \\nclassifying galaxies by shape, \\npicking an optimal move or strategy in a game such as chess, \\nidentifying incoming missiles from radar or sonar signals, \\ndetecting shoals of fish by sonar, \\nchecking packets of frozen peas for 'foreign bodies', \\nspotting fake 'antique' furniture, \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 13}, page_content=\"2 1 Introduction and Examples \\ndeciding which customers will be good credit risks and \\nspotting good opportunities on the financial markets. \\nHumans can (and do) do some of the tasks quite well, but the techno\\xad\\nlogical pressure is to build machines which can perform such tasks more \\naccurately or faster or more cheaply than humans, or even to release \\nhumans from drudgery. There are also purely technological tasks such \\nas reading bar codes at which humans are poor. Pattern recognition is \\nthe discipline of building such machines: \\n'It is felt that the decision-making processes of a human being are \\nsomewhat related to the recognition of patterns; for example the next \\nmove in a chess game is based upon the present position on the \\nboard, and buying or selling stocks is decided by a complex pattern \\nof information. The goal of pattern recognition research is to clarify \\nthese complicated mechanisms of decision-making processes and to \\nautomate these functions using computers. However, because of the \\ncomplex nature of the problem, most pattern recognition research has \\nbeen concentrated on more realistic problems, such as the recognition \\nof Latin characters and the classification of waveforms.' \\n(Fukunaga, 1990, p. 1) \\nSince the best humans can perform many of these tasks very well, \\neven better than the best machines, it has been of great interest to \\nunderstand how we do so, and this is of independent scientific interest. \\nSo there has for many years been an interchange of ideas between \\nengineers building pattern recognition systems and psychologists and \\nphysiologists studying human and animal brains. Twice this has led to \\ngreat enthusiasm about machines influenced by ideas from psychology \\nand biology. The first was in the late 1950s with the perceptron, the \\nsecond in the mid 1980s over neural networks. Both rapidly left their \\nbiological roots, and were studied by mathematical techniques against \\nengineering performance goals as pattern recognizers. This book is \\nnot about the impact of the study of neural networks as models of \\nanimal brains, but discusses what are more accurately (but rarely) called \\nartificial neural networks which have been developed by a community \\nwhich was originally biologically motivated (although many 'neural \\nnetwork' methods were not). Thus for the purposes of this book, a \\nneural network is a method which arose or was popularized by the \\nneural network community and has been or could be used for pattern \\nrecognition. Many of the originators of the current wave of interest \\nwere more careful in their terminology; whereas Hopfield (1982) did \\ntalk about neural networks, Rumelhart & McClelland (1986) used the \\nterm 'parallel distributed processing', and 'connectionist' has also been \\npopular (for example, see Hinton, 1989a). Marginal notes such as \\nthis replace footnotes \\nand offer explanation, \\nsidelines, and opinion. \\nMany of the ideas had \\narisen earlier in the \\npattern recognition \\ncontext, but without the \\nseductive titles had \\nmade little impact. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 14}, page_content=\"Gooseberries are the \\nfruits of the species \\nRibes grossulari a. \\nWe should never \\nunderestimate the \\npower of simply \\nremembering some or \\nall of the examples and \\ncomparing test \\nexamples with our \\nmemory. 1 Introduction and Examples 3 \\nOne characteristic of human pattern recognition is that it is mainly \\nlearnt. We cannot describe the rules we use to recognize a particular \\nface, and will probably be unable to describe it well enough for anyone \\nelse to use the description for recognition. On the other hand, botanists \\ncan give the rules they use to identify flowering plants. \\nMost learning involves a teacher. If we try enough different wines \\nfrom unlabelled bottles, we may well discover that there are common \\ngroupings, and that one group has the aroma of gooseberries (if the \\nlatter have been experienced) . But we will need a teacher to tell us that \\nthe common factor is that they were made (in part) from the sauvignon \\nblanc grape. The discovery of new groupings is called unsupervised \\npattern recognition. A more common mode of learning both for us and \\nfor machines is to be given a collection of labelled examples, known \\nas the training set, and from these to distil the essence of the grouping. \\nThis is supervised pattern recognition and is used to classify future \\nexamples into one of the same set of classes (or say it is none of these). \\nThere is a subject known as machine learning which has emerged \\nfrom the artificial intelligence and computer science communities. It too \\nis concerned with distilling structure from labelled examples , although \\nthe labels are usually 'true' and 'false'. \\n'Machine Learning is generally taken to encompass automatic learning \\nprocedures based on logical or binary operations, that learn a task \\nfrom a series of examples.' \\n'Machine Learning aims to generate classifying expressions simple \\nenough to be understood easily by humans. They must mimic human \\nreasoning sufficiently well to provide insight into the decision process. \\nLike statistical approaches, background knowledge may be exploited \\nin development , but operation is assumed without human intervention.' \\n(Michie et al., 1994, p. 2) \\nThis stresses the need for a comprehensible explanation, which is needed \\nin some but not all pattern recognition tasks. We have already noted \\nthat we cannot explain our identification of faces, and to recognize Zip \\ncodes no explanation is needed, just speed and accuracy. \\nThis quotation mentions statistical approaches, and statistics is the \\noldest of the disciplines concerned with automatically finding structure \\nin examples. As in the quotation, statistics is often thought of as \\nbeing less automatic than the other disciplines , but this is largely an \\nartefact of its greater age; its current research frontiers are very much \\nconcerned with replacing the human choice of methods by computation . \\nFurthermore, statistics encompasses what the community of statisticians \\ndo, of whom your author is one! \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 15}, page_content=\"4 1 Introduction and Examples \\n1.1 How do neural methods differ? \\nAssertions are often made that neural networks provide a new approach \\nto computing, involving analog (real-valued) rather than digital signals \\nand massively parallel computation. For example, Haykin (1994, p. 2) \\noffers a definition of a neural network adapted from Aleksander & \\nMorton (1990): \\n'A neural network is a massively parallel distributed processor that has \\na natural propensity for storing experiential knowledge and making it \\navailable for use. It resembles the brain in two respects: \\n1. Knowledge is acquired by the network through a learning process. \\n2. Interneuron connection strengths known as synaptic weights are \\nused to store the knowledge.' \\nIn practice the vast majority of neural network applications are run on \\nsingle-processor digital computers, although specialist parallel hardware \\nis being developed (if not yet massively parallel). However, all the \\nother methods we consider use real signals and can be parallelized to a \\nconsiderable extent; it is far from clear that neural network methods will \\nhave an advantage as parallel computation becomes common, although \\nthey are frequently so slow that they need a speed-up. (Parallelization \\non real hardware has proved to be non-trivial; see Pitas, 1993 and \\nPrzytula & Prasanna, 1993.) We will argue that a large speed-up can \\nbe achieved by designing better learning algorithms using experience \\nborrowed from other fields. \\nThe traditional methods of statistics and pattern recognition are \\neither parametric based on a family of models with a small number \\nof parameters, or non-parametric in which the models used are totally \\nflexible. One of the impacts of neural network methods on pattern \\nrecognition has been to emphasize the need in large-scale practical \\nproblems for something in between, families of models with large but \\nnot unlimited flexibility given by a large number of parameters. The two \\nmost widely used neural network architectures, multi-layer perceptrons \\nand radial basis functions (RBFs ), provide two such families (and several \\nothers already existed in statistics). \\nAnother difference in emphasis is on 'on-line' methods, in which the \\ndata are not stored except through the changes the learning algorithm \\nhas made. The theory of such algorithms is studied for a very long \\nstream of examples, but the practical distinction is less clear, as this \\nstream is made up either by repeatedly cycling through the training set \\nor by sampling the training examples (with replacement). In contrast, \\nmethods which use all the examples together are called 'batch' methods. Many neural networks \\nare excluded by this \\ndefinition, including \\nthose of Kohonen. One \\ncould ask how a \\nmachine comes to have \\n'natural' properties. \\nThe name 'multi-layer \\nperceptrons' is \\nconfusing; they are not \\nmultiple layers of \\nperceptrons. We call \\nthem feed-forward \\nneural nets. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 16}, page_content=\"Someone else may have \\nmade the measurements \\nfor us. \\nIt may help to know \\nwhich classes are \\nplausible. \\nThis might be \\nunrealistic for \\nhand-written addresses, \\nand is well beyond \\ncurrent performance \\nlevels. 1.2 The pattern recognition task 5 \\nIt is often forgotten that there are intermediate positions, such as using \\nsmall batches chosen from the training set. \\n1.2 The pattern recognition task \\nExcept in Chapter 9 we will be exclusively concerned with supervised \\npattern recognition. Thus we are given a set of K pre-determined \\nclasses, and assume (in theory) the existence of an oracle that could \\ncorrectly label each example which might be presented to us. When we \\nreceive an example, some measurements are made, known as features, \\nand these data are fed into the pattern recognition machine, known as \\nthe classifier. This is allowed to report \\n'this example is from class t' or \\n'this example is from none of these classes' or \\n'this example is too hard for me'. \\nThe second category are called outliers and the third rejects or 'doubt' \\nreports. Both can have great importance in applications . Suppose we \\nhave a medical diagnosis aid. We would want it to report any patient \\nwho apparently had an unknown disease, and we would also want it \\nto ask the opinion of a senior doctor if there was real doubt. Often \\nrejects are referred to a more expensive second tier of classification, \\nperhaps a human or (as in Zip code recognition) a slower but more \\npowerful method or even (as in analytical chemistry) for more expensive \\nmeasurements to be made. Many pattern recognition systems always \\nmake a firm classification , but this seems to us more often to be bad \\ndesign than a conscious decision that a firm decision was necessary. \\nThe primary assessment of a system will be by its performance; a \\nZip code recognition system might be required to reject less than 2% of \\nthe examples and mis-read less than 0.5% of the remainder . In medical \\ndiagnosis we will be more interested in some errors than others, in \\nparticular in missing a disease, so the errors will need to be weighted. \\nThere may be a cost trade-off between rejection and error rate. \\nThe other aspect of performance stressed in the quote from Michie \\net al. (1994, p. 2) is the power of explanation. Users need to have \\nconfidence in the system before it will be adopted. No one really cares \\nif an odd letter is mis-routed , but patients do care if they mis-diagnosed , \\nand when a civilian airliner is mistaken for an enemy aircraft, questions \\nare raised. So for some tasks 'black boxes' are unacceptable whatever \\ntheir performance advantage (possibly even if they appear perfect on \\ntest). The methods of Chapters 7 and 8 are often found to be more \\nacceptable for such tasks. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 17}, page_content='6 1 Introduction and Examples \\nSome tasks are slightly different. We (and medics) often think of \\nmedical diagnosis as deciding which disease a patient has, but this \\nignores the possibility of two or more concurrent diseases; what we \\nshould really be asking is whether the patient has this disease for each \\nof a range of diseases. This can be thought of as a compound decision, \\nthe classes each being a subset of the diseases, but it is normally helpful \\nto make use of special structure within the classes. \\nDesign issues \\nAlthough most of this book is about designing the pattern recognition \\nmachine, often the most important aspect of design is to choose the right \\nfeatures. If the wrong things are measured (or, more often these days \\nwith digital data, if the data are condensed too much) the task may be \\nunachievable. Much of the enhanced success of Zip-code recognition \\nsystems has come from better features (for example, Simard et al., \\n1993) rather than through more complicated classifiers. Sometimes \\ngood features can be found by training a classifier on a large number \\nof features and extracting good ones (for example, by the methods of \\nChapters 9 and 10), but most often problem-specific insights are used. \\nIn a few problem domains very specific rules are known which \\ncan be used to design a classifier; as an extreme example compilers \\ncan classify C programs as correct or invalid without needing to see \\nany previous programs. Such information is often in the form of a \\nformal grammar, and systems based on specifying such grammars are \\noften called syntactic pattern recognition systems (Fu, 1982; Gonzalez \\n& Thomason, 1978), but are of very restricted application. Allowing \\nstochastic grammars in which the structure is given but the probabilities \\nare learnt allows a little more flexibility. Chou (1989) gives an exam\\xad\\nple of recognizing typeset mathematical expressions using a stochastic \\ngrammar. \\nIn the vast majority of applications no structural assumptions are \\nmade, all the structure in the classifier being learnt from data. In the \\npattern recognition literature this is known as statistical pattern recog\\xad\\nnition. The training set is regarded as a sample from a population of \\npossible examples, and the statistical similarities of each class extracted, \\nor more precisely the significant differences between classes are found. \\nA parametric or non-parametric model is constructed for the distribu\\xad\\ntion of features for examples from each class, and statistical decision \\ntheory used to find an optimal classification. This is sometimes known \\n(Dawid, 1976) as the sampling paradigm. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 18}, page_content=\"Note that this is not the \\nprocedure called \\ncross-validation, despite \\nthe misuse of that term \\nin the neural networks \\nliterature. 1.2 The pattern recognition task 7 \\nAnother view, the diagnostic paradigm, goes back in the statistical \\nliterature at least to Cox (1958), and was developed in medical ap\\xad\\nplications by Jerome Cornfield. This said that we were not interested \\nin what the classes looked like, but only given an example in what \\nthe distribution over classes is for similar examples. The main method \\nof this approach became known as logistic discrimination (Anderson, \\n1982), but was never widely known even in statistics and (as far as \\nwe could ascertain) appears in no pattern recognition text. This is the \\nmain approach of the neural network school. \\nWhen humans are learning concepts, we are often able to ask \\nquestions or to seek the classifications of examples which we synthesize \\n(this being a paradigm of experimental science). Alternatively, we \\nmay describe our understanding to an expert, who will then supply a \\ncounter-example. Can we allow our machines to do the same? The \\nidea has occurred in machine learning (Angluin, 1987, 1988, 1993), but \\napparently only for learning logical concepts. \\nWe will sometimes have qualitative knowledge about the task in \\nhand; we might know that only the sign of one of the features was \\nmaterial, or that the probability of a positive outcome was increasing \\nin some continuous feature. Of course we should design the classifier to \\nagree with such information, which Abu-Mostafa (1990, 1993, 1995a, b, \\nc) calls 'hints'. Sometimes this is easy (just use the sign of the feature) \\nbut it can be very difficult (as in monotonicity). Generally hints (if true) \\nhelp to avoid over-fitting to the training set, and this seems to be the \\nreal explanation of the gains in exchange-rate performance observed by \\nAbu-Mostafa (1995a). \\nMethod tuning and checking \\nAll methods have some knobs which can be tweaked. Sometimes \\ntaking the class of the nearest training-set example is regarded as a \\nfully automatic method, but we need to specify the metric used to find \\nthe nearest. (If the answer is 'use Euclidean distance' we still have to \\nspecify the units of measurement.) \\nHow should those knobs be set? The most obvious way is to \\nchoose them to maximize performance. One thing we should not do \\nis to evaluate the performance on a test set and choose the best\\xad\\nperforming classifier, since we will then have no way to measure the \\ntrue performance. We can keep back another test set, called a validation \\nset, and use the performance on that to set the knobs. However, to \\nobtain a sensitive measure of the performance, the validation set will \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 19}, page_content='8 1 Introduction and Examples \\nneed to be very large, and this is data which could otherwise be used \\nfor learning. \\nThis problem has been ignored for a long time, but now methods to \\nuse the training set for both learning and knob-setting are beginning to \\nbe used. These are discussed in Chapter 2 and illustrated on the quite \\nsmall running examples that we chose. \\nTo see why this is a real issue, consider Figure 1.1. Without knowing \\nthe true curve, it is hard to tell which of plots (b) and (c) is closer to \\nthe truth. \\n~ ~ .. .. ... . . .-·.·:., .. ... ... \\n~ ... .. . ··,: ... .. . . . ... . . . \\n~ \\n~ 0.0 0.5 1.0 1.5 2.0 2.S 3.0 0.0 0.5 1.0 1.5 2.0 \\n(a) (b) \\n. :\\' .-:, \\' .... ·. : \\' ~ \\' : \\n~ 0! . •, ·. :\\' . ···. :,· \\ni. \\n~1 \\n~1 . . \\n.·.\\' \\n~~\\'-------- ------ ------~ \";\\' 0.0 0.5 1.0 3.0 \\n(c) (d) \\nPerformance assessment \\nWe will often want to choose between different candidate classifiers, \\nand it will be usual to check that the performance targets are likely \\nto be met. This needs an experimental test of the classifiers on some \\nunseen examples. Such experiments are often (usually?) very poorly \\ndesigned, and slanted towards a favourite method. The reader is urged \\nto consult a good book on experimental design (such as Box et al., \\n1978) before conducting such experiments. \\nMany of the experiments reported in the literature are designed to \\ncompare methods, when there is even more scope for confusion. In Figure 1.1 : An \\nillustration of model \\nselection. Plot (a) shows \\n250 points generated by \\nfrom the curve shown \\nplus random noise, and \\nplots (b-d) show fits by \\na single-hidden-layer \\nneural network with 2, \\n4 and 8 hidden units. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 20}, page_content=\"This test does not \\nconsider experimental \\nbiases nor if an \\nevaluation of the \\nsignificance of the \\nresults was made. 1.3 Overview of the remaining chapters 9 \\nmedicine, methods (treatments) are compared in double-blind trials so \\nthere can be no preferential treatment , and in pure science experiments \\nmust be repeatable. (The large-scale trial of the StatLog project reported \\nin Michie et al., 1994, was designed to be run in these ways.) One source \\nof confusion is that such trials may confuse the merits of the methods \\nwith the expertise of the experimenter in using them; this is a particular \\ndifficulty when the experimenter's own invention is in the trial. Two \\ncases are of interest. One is where every method is used by a real expert \\nand so assesses the best attainable performance. The other is when all \\nmethods are used by typical (or even new) users, which might provide \\na basis for recommendations to such users. \\nPrechelt (1994) surveyed two le~1.ding neural network journals for \\n1993 and half of 1994. He deemed an evaluation of an algorithm ac\\xad\\nceptable if it used two or more realistic or real problems and compared \\nat least one alternative algorithm . Only 18% passed-in his words 'sad, \\nbut true'. Note that this book is not about evaluating algorithms , but \\nwe have used real examples to explore the merits and limitations of the \\nmethods. Amazingly, almost all books on pattern recognition or neural \\nnetworks include no real or realistic examples. \\n1.3 Overview of the remaining chapters \\nOur approach to building a classifier will be based on statistical decision \\ntheory. In Chapter 2 we consider the Bayes rule, the best possible \\nclassifier if we knew everything about the population of examples , and \\nthen various approximations we can make if we have to learn from a \\ntraining set. This includes several ways to use parametric models (which \\nwe assume to be false but perhaps convenient approximations) ; these \\nsections include the classic methods based on the multidimensional \\nnormal distribution but also some improvements which are much less \\nwell known. \\nThe next questions are: how complicated do our models need to \\nbe, and how well do they perform? These are discussed in Sections 2.6 \\nand 2.7. There is a trade-off between adapting well to complexity of the \\nreal structure in the examples and fitting the structure of our particular \\ntraining set (Figure 1.1). This explains why we are not interested in the \\nusual asymptotics of mathematical statistics; as we receive more data \\nwe will want to choose more complicated models, and only limit the \\nmodel complexity to avoid over-fitting the current training set. Another \\nview of the effect of model flexibility on over-fitting is the study of \\ngeneralization in Section 2.8. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 21}, page_content=\"10 1 Introduction and Examples \\nChapters 3 to 5 make weaker assumptions than standard parametric \\nmodels. In Chapter 3 we study how we could use linear methods. Both \\nChapters 4 and 5 discuss how to apply flexible families of functions from \\nthe feature space !![ to d-dimensional Euclidean space .JRd, building \\non the linear methods, and consider the commonest such families, \\nneural networks and radial basis functions, as well as splines and their \\ngeneralizations. \\nThe sixth chapter is on (nearly) non-parametric methods, where \\nminimal assumptions are made about the classes. Most of these methods \\nare based on looking at the classes of nearby examples, in some methods \\nafter designing a set of representative examples to replace the training \\nset. That chapter also includes the use of mixtures of densities to model \\nvery general distributions. \\nChapter 7 is about a rather different class of methods that partition \\nthe feature space !![ into regions and assign a class to each. This is \\ndone by splitting along a feature at a time, and then subdividing each \\nsubregion recursively. Classification trees have been considered in both \\nstatistics and machine learning; they are often easy to interpret but not \\namongst the highest performers. \\nBelief networks, also known as causal probability networks and \\nBayes networks, are not primarily designed for classification, but to \\nexplain the relationships between all of the observations. They are the \\nsubject of Chapter 8. They are very good for explanation, but may be \\nless good for classification (as the finite amount of training data has to \\nbe used to learn more structure than just the relationship of the class \\nto their features). Their strength is that they can incorporate qualitative \\nknowledge about causal relationships amongst the features (an earlier \\nand more sophisticated use of 'hints'). Also included in that chapter \\nare the methods of Boltzmann machines and hierarchical mixtures of \\nexperts which can be considered within the framework of belief nets. \\nChapters 9 and 10 are concerned with finding good features and \\nchoosing which features to use. \\nThe appendix discusses a number of complements; some are statis\\xad\\ntical background and some explore issues a little further than is needed \\nfor pattern recognition. \\n1.4 Examples \\nThe examples have been chosen to illustrate the properties of the \\nmethods we describe; not every method is used on each. . . . and many more \\nnames beside \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 22}, page_content=\"Figure 1.2: Results of \\ntwo diagnostic tests on \\npatients with Cushing's \\nsyndrome. Cushing's syndrome \\n0 ;:; 1.4 Examples \\nu b b b \\nb \\n10 \\nT etrahydrocortisone 11 \\n50 \\nThese data are taken from Aitchison & Dunsmore (1975, Tables 11.1-3) \\non diagnostic tests on patients with Cushing's syndrome , a hypersensi\\xad\\ntive disorder associated with over-secretion of cortisol by the adrenal \\ngland. This dataset has three recognized types of the syndrome rep\\xad\\nresented as a, b, c. (These encode 'adenoma' , 'bilateral hyperplasia ' \\nand 'carcinoma' , and represent the underlying cause of over-secretion. \\nThis can only be determined histopathologically.) The observations are \\nurinary excretion rates (mg/24h) of the steroid metabolites tetrahydro\\xad\\ncortisone and pregnanetriol , and are considered on log scale. \\nOne of the patients of unknown type (marked u) was later found \\nto be of a fourth type, and another was measured faultily. \\nTitterington (1976) discusses a different dataset which· had 87 pa\\xad\\ntients, five types, and fifteen measurements per patient, which suggests \\nthe current dataset is an abstraction of the full problem. \\nSynthetic two-class problem \\nThis is a 'realistic' problem from Ripley (1994a), used there (and here) \\nto illustrate how methods work. There are two features and two \\nclasses; each class has a bimodal distribution as should be clear from \\nFigure 1.3. The class distributions were chosen to allow a best-possible \\nerror rate of about 8%, and are in fact equal mixtures of two normal \\ndistributions. The component normal distributions have a common \\ncovariance matrix. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 23}, page_content='12 \\n., \\n\"\\' \\n1\\'l \\n~ \\n~ \\n§ \\n~ \\n0 \\n~ \"\\' ci \\n0 \\nci \\n120 .• \\n·1.0 1 Introduction and Examples \\n0 • 0 . . ./• .· .. .. . ,.. . .. . .. . ~ . ·. \\n-0.5 0.0 oo \\no:\\'o • Ba.,on , .~s \\n.. . .· co0o,c 0 \\n... .. \\n0.5 1.0 \\n--·---·· 140 160 160 200 220 \\ntotal residue \\n~ --\\n= ---\\nHerd Tobr Toba Furo \\nvirus group 240 \\nViruses \\nThis is a dataset on 61 viruses with rod-shaped particles affecting \\nvarious crops (tobacco, tomato, cucumber and others) described by \\nFauquet et al. (1988) and analysed by Eslava-G6mez (1989). There are \\n18 measurements on each virus, the number of amino acid residues per \\nmolecule of coat protein; the data come from a total of 26 sources. \\nThere is an existing classification by the number of RNA molecules \\nand mode of transmission, into \\n39 Tobamoviruses with monopartite genomes spread by contact, \\n6 Tobraviruses with bipartite genomes spread by nematodes, \\n3 Hordeiviruses with tripartite genomes, transmission mode \\nunknown and \\n13 \\'furoviruses\\', 12 of which are known to be spread fungally. Figure 1.3: Two-class \\nsynthetic data from \\nRipley (1994a). The two \\nclasses are shown by \\nsolid circles and open \\nsquares: there are 125 \\npoints in each class. \\nFigure 1.4: Histogram \\nand boxplot by group \\nof the viruses dataset. A \\nboxplot is a \\nrepresentation of the \\ndistribution; the central \\ngrey box shows the \\nmiddle 50% of the \\ndata, with median as a \\nwhite bar. \\'Whiskers\\' go \\nout to plausible \\nextremes, with outliers \\nmarked by bars. \\nNo experimental details \\nare provided in the \\nsource. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 24}, page_content='1.4 Examples 13 \\nThe question of interest to Fauquet et al. was whether the furoviruses \\nform a distinct group, and they performed various multivariate analyses. \\nOne initial question with this dataset is whether the numbers of \\nresidues are absolute or relative. The data are counts from 0 to 32, with \\nthe totals per virus varying from 122 to 231. The average numbers for \\neach amino acid range from 1.4 to 20.3. As a classification problem, this \\nis very easy as Figure 1.4 shows. The histogram shows a multimodal \\ndistribution, and the boxplots show an almost complete separation by \\nvirus type. The only exceptional value is one virus in the furovirus \\ngroup with a total of 170; this is the only virus in that group whose \\nmode of transmission is unknown and Fauquet et al. (1988) suggest it \\nhas been tentatively classified as a Tobamovirus. The other outlier in \\nthat group (with a total of 198) is the only beet virus. The conclusions \\nof Fauquet et al. may be drawn from the totals alone. \\nIt is interesting to see if there are subgroups within the groups, so \\nwe will only use this dataset in Chapter 9, principally to investigate \\nfurther the largest group (the Tobamoviruses ). There are two viruses \\nwith identical scores, of which only one is included in the analyses. (No \\nanalysis of these data could differentiate between the two.) \\nLeptograpsus crabs \\nCampbell & Mahon (1974) studied rock crabs of the genus Lepto\\xad\\ngrapsus. One species, L. variegatus, had been split into two new species, \\npreviously grouped by colour form, orange and blue. Preserved speci\\xad\\nmens lose their colour, so it was hoped that morphological differences \\nwould enable museum material to be classified. \\nData are available on 50 specimens of each sex of each species, \\ncollected on sight at Fremantle, Western Australia. Each specimen has \\nmeasurements on the width of the frontal lip FL, the rear width RW, \\nand length along the midline CL and the maximum width CW of the \\ncarapace, and the body depth BD in mm. \\nForensic glass \\nOur next example comes from forensic testing of glass collected by \\nB. German on 214 fragments of glass, and taken from Murphy & Aha \\n(1995). Each case has a measured refractive index and composition \\n(weight percent of oxides of Na, Mg, AI, Si, K, Ca, Ba and Fe). \\nThe fragments were originally classed as seven types, one of which was \\nabsent in this dataset. The categories which occur are window float glass \\n(70), window non-float glass (76), vehicle window glass (17), containers '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 25}, page_content='14 1 Introduction and Examples \\n(13), tableware (9) and vehicle headlamps (29). The composition sums \\nto around 100%; what is not anything else is sand. \\nRefractive Index \\n~ -~ \\n~ ---.,., - ;! .,., \\n~ = ; $ ~ 19 ~ a \\n~ ~ !\" ~ ..:.. -~ \\nWtnF WlnNF Veh Con Tabl Head \"\\'nF \\nAluminium \\n~ \\n~ :e \\n0 \\n~ .,., .,., \\n~ \\n~ ;! \\nN .,., \\n0 -I .,., ;? \\niliJ N \\n~ = s ~ ~ \\n0 $ ~ -~ -;:: \\n~ --..:.. ~ - ~ -\\nW1nF WinNF Voh Con Tabl Head WlnF \\nCalcium \\n~ 0 ,; \\n;\\': - ~ \\nN \\n!\" 0 \\ni N \\n~ ~ \\n\"\\' r::J -¢J $ ~ Q ~ \\n..:.. ~ \\n~ 0 \\n0 = 0 \\nWmF WW<F Veh Con Tabl Head \"\\'nF Sodium \\n= .,., ~ \\nCl !!: Q \\n~ \\n-\\nWW<F Voh Con T ... \\nSilica \\n.,., \\n$ t!!l ~ ! \\n~ -\\n~ \\nWmNF Veh Con Tabl \\nBarium \\niii ;::::; --WinNF Veh Con Tobl .,., \\n~ \\n..:.. \\nHoad \\n~ ..:.. \\nHead \\n~ \\n~ \\nHead Magnesium \\n9 il i:5 \\n\"\\' -\\nWinf WII\\'INF Veh Con Tab! Heed \\nPotassium \\nWlnF WlnNF Veh Con Tabl H .. d \\nIron \\n~--------~-----. \\n0 \\nFigure 1.5 shows boxplots of the features. Some discrimination \\nbetween glass types is apparent even from single features; for example \\nheadlamp glass is high in barium (although some examples have none), \\nhigh in sodium and aluminium and low in iron. The three types of \\nwindow glass appear similar, with one exceptional fragment of window \\nnon-float glass having a high refractive index, high barium and calcium \\nand low magnesium and sodium. The containers group also contains \\na couple of exceptions. Characterizing populations with exceptions \\n(especially 2 out of 13) can be difficult, and it may be easier to remove \\nthe exceptions in the training phase. \\nThis example is really too small to divide, so methods have been Figure 1.5: Boxplots of \\nthe features of the \\nforensic glass data. \\nassessed by 10-fold cross-validation using the same random partition This is discussed in \\nfor each method. The best methods have an estimated error rate of Section 2.7. \\nabout 24%. \\nDiabetes in Pima Indians \\nA population of women who were at least 21 years old, of Pima Indian \\nheritage and living near Phoenix, Arizona, was tested for diabetes ac\\xad\\ncording to World Health Organization criteria. The data were collected '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 26}, page_content='1.5 Literature 15 \\nby the US National Institute of Diabetes and Digestive and Kidney \\nDiseases, and are available from Murphy & Aha (1995). A previous \\nreport by Smith et al. (1988) found an error rate of about 24%. The \\nreported variables are \\nnumber of pregnancies \\nplasma glucose concentration in an oral glucose tolerance test \\ndiastolic blood pressure (mm Hg) \\ntriceps skin fold thickness (mm) \\nserum insulin ( f1 U /ml) \\nbody mass index (weight in kg/(height in m)2) \\ndiabetes pedigree function \\nage m years \\nMany of these had zero values where these were impossible, so are taken \\nto be missing values. Of the 768 records, 376 were incomplete (most \\nprevalently in serum insulin). Most of our illustrations omit serum \\ninsulin and use the 532 complete records on the remaining variables. \\nThese were randomly split into a training set of size 200 and a test set \\nof size 332. Methods which can deal with missing values were given \\n100 of the incomplete cases as part of the training set. \\nNote that 33% of the population were reported to have diabetes, \\nso an error rate of 33% can be achieved by declaring all test cases to \\nbe non-diabetic. Our best methods reduce this to about 20%. \\nSome aspects of this dataset were considered by Wahba et al. (1995). \\nData availability \\nAll these datasets are available by anonymous ftp from the Internet site \\nftp.stats.ox.ac.uk IP address 163 .1. 20.1 \\nin directory /pub/PRNN. The datasets and other material are available \\nby pointing your World Wide Web browser at \\nhttp://www.stats.ox.ac.uk/-ripley/PRbook/ \\n1.5 Literature \\nThe classic books on pattern recogmtwn are Duda & Hart (1973), \\nDevijver & Kittler (1982) and Fukunaga (2nd edn 1990), all of which \\npre-date the impact of neural networks on the subject. There are \\na small number of introductory texts (James, 1988; Therrien, 1989; \\nSchalkoff, 1992) and two specialist monographs on kernel methods '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 27}, page_content='16 1 Introduction and Examples \\n(Hand, 1982; Coomans & Broeckaert, 1986). Some conference proceed\\xad\\nings, for example Devijver & Kittler (1987), provide a good overview \\nof applications. \\nClassical statistical techniques are discussed in most texts on multi\\xad\\nvariate analysis such as T. W. Anderson (1984) and Mardia et al. (1979) \\nand in slightly more specialized books by Lachenbruch (1975), Gold\\xad\\nstein & Dillon (1978), Hand (1981) and McLachlan (1992). \\nThere are now very many books on neural networks, particularly \\non parts of the subject not discussed here. Approaches to modelling \\nmemory from the point of view of statistical physics are covered by \\nAmit (1989), Peretto (1992) and Hertz et al. (1991). Haykin (1994) \\nis modern, comprehensive but unselective (and untroubled by real \\napplications). Amari (1993) and Ripley (1993) give two statistical views \\nof the neural network field, and Bishop (1995a) is slanted towards \\npattern recognition. Arbib (1995) provides many short sketches of \\ntopics over a very wide range of neural networks. One important area \\nof neural network methods which we do not consider is the prediction \\nof time series, the subject of a competition analysed by Weigend & \\nGershenfeld (1993), including expository papers. \\nThere is now one text on general machine learning, Langley (1996), \\nand it appears in some artificial intelligence texts (for example, Winston, \\n1992; Russell & Norvig, 1995). There are many more aspects than we \\nshall consider, including incorporating domain knowledge as illustrated \\nby King et al. (1992). Langley & Simon (1995) and Bratko & Muggleton \\n(1995) discuss applications of machine learning with claimed real-world \\nbenefits. \\nBooks which cover more than one of these three areas are rare. \\nKrishnaiah & Kanal (1982) was a very good overview at its time; \\nthe recent edited volumes by Cherkassky et al. (1994) and Michie et \\nal. (1994) contain several good overviews. \\nFace recognition is a popular application of pattern recognition sur\\xad\\nveyed by Samal & Iyengar (1992). Golomb et al. (1991) and Flocchini \\net al. (1992) give two example systems. \\nThere is a very large literature on character recognition, and non\\xad\\nEuropean alphabets with at least hundreds of classes provide a severe \\ntest of pattern recognition methods. The articles by Baird (1993), Cohen \\net al. (1991), Le Cun et al. (1989, 1990a), Gader et al. (1991), Guyon \\net al. (1992), Impedovo et al. (1991), Knerr et al. (1991), Lee (1991), \\nMartin & Pitman (1990, 1991), Pavlidis (1993), Simard et al. (1993), \\nSinger & Tishby (1994), Suen et al. (1992, 1993) and Wakahara (1993) \\nprovide some flavours. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 28}, page_content=\"We regard probability \\nmass functions of \\ndiscrete distributions as \\ndensities. 2 \\nStatistical Decision Theory \\nThis chapter presents basic statistical decision theory for classification \\nproblems with predefined classes. \\nThe framework in its simplest form is as follows. Certain objects \\nare to be classified as coming from one of a fixed number of types, \\nor classes, say 1, ... , K. Each object gives rise to certain measurements \\nwhich together form the feature vector X, belonging to a suitable \\nfeature space f£. This is typically a subset of .IR..P or perhaps of the \\ntype f£ 1 x · · · x f£ P with each f£ J either a given finite set or .IR. The \\nproportion of class k cases in the population under study is some known \\nor unknown nk. Feature vectors from class k are distributed according \\nto the density Pk(x). The task is to classify an object, which means \\nreaching one of K +2 possible decisions 1, ... ,K,~,(!) on the basis of \\nthe observed value X = x; decision k corresponds to claiming 'X is \\nfrom class k ', whereas ~ means 'being in doubt', possibly postponing \\nthe decision until further measurements have been extracted , and (!) \\nsignifies an outlier, an object definitely not belonging to any of the K \\npredefined classes. \\nSection 2.1 treats the idealized case when class densities Pk(x) as \\nwell as class prior probabilities (nk) are known. This gives valuable \\ninsight and also provides limits for the performance of real-life classifiers \\nthat in some way must estimate class densities. In Section 2.2 some \\nof the most important parametric models for classification are studied. \\nThe parameters are typically estimated using maximum likelihood, but \\nalternatives are discussed in Section 2.3 (assuming less of the model), \\nSection 2.4 (taking the variability of the parameters into account) and \\nSection 2.5 (bias correction). \\nSections 2.6 and 2.7 discuss how we assess the adequacy of a \\nparametric model and estimate the performance of a classification rule. \\nThe final section considers 'generalization' , a more abstract way to find \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 29}, page_content=\"18 2 Statistical Decision Theory \\nbounds on the expected performance of a class of classifiers. \\nNot all of the material here is essential for the later applications. \\nThe most important sections are Section 2.1, Section 2.2 omitting 'Bayes \\nrisk consistency' and 'Fitting parametric families when they are wrong', \\nSections 2.3, 2.4, 2.6 and 2.7. \\n2.1 Bayes rules for known distributions \\nIn this present section we assume that the class densities Pk and the \\nprior probabilities nk are known. This makes it possible to construct \\nclassification procedures with well understood optimal properties. Such \\nresults are not directly applicable since the class densities, at least, are \\nunknown in practice, but they will serve as guidelines for the estimated \\nrules of Sections 2.2 and 2.3, and have intrinsic theoretical interest. \\nLet C denote the class label of a random feature vector X, in \\nparticular C = k with probability nk. The classification task is to \\nestimate the true C after having observed X. Let c: :!C ~ {1, ... ,K, .@} \\nbe a classification procedure (also known as a classifier). (We will deal \\nwith outlier decisions in a later subsection.) To determine whether such \\na procedure is 'good' or not one has to agree on reasonable overall \\ncriteria, for example involving the misclassification probabilities \\npmc(k) = Pr{c(X) =I= k, c(X) E {1, 0 0 0 ,K} I c = k} (2.1) \\nand the reject or doubt probabilities pd(k) = Pr{c(X) = .@I C = k}. \\nThe quantities pmc and pd denote the unconditional misclassifica\\xad\\ntion probability Pr{c(X) =I= C} and doubt probability Pr{c(X) = .@} \\nrespectively. \\nMinimizing the expected error rate \\nThe usual way of formalizing a goodness criterion is by means of a \\nloss function. Let L(k, l) be the loss incurred by making decision l \\nif the true class is C = k. One should have L(k,k) = 0 and maybe \\nL(k, .@) = d for all k, whereas the other L(k, l) 's could in principle be \\nany set of positive numbers. If every misclassification is equally serious, \\nthen Marginal notes point \\nout the less important \\nmaterial. \\n{ 0 if l = k (correct decision), \\nL(k,l) = 1 if l =I= k and l E {1, ... ,K} (wrong decision), \\nd if l = fi2 (being in doubt), (2.2) Loss (2.2) is used unless \\notherwise stated. \\nfor k = 1, ... , K and l = 1, ... , K, .@, is a reasonable choice. In \\nwhat follows we will often employ the loss function (2.2) to illustrate \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 30}, page_content=\"A medical example with \\nassessed costs is Table 4 \\nof Titterington et al. \\n(1981, p. 154). 2.1 Bayes rules for known distributions 19 \\nimportant concepts; it is often used when there is no way to assign \\nmore accurate costs. However, we should warn that its use when \\ninappropriate can cause difficulties or even be dangerous; the costs of \\nfailing to spot a disease are usually very much higher than those of a \\nfalse positive in a series of screening tests. \\nThe risk function for classifier c is the expected loss when using it, \\nas a function of the unknown class k : \\nR(C,k) = E[L(k,C(X))IC =k] \\nK \\n= L L(k, l) Pr{c(X) = 11 c = k} + L(k,9&) Pr{c(X) = 9& 1 c = k} \\n1=1 \\n= pmc(k) + d pd(k). \\nThe total risk is the total expected loss, viewing both the class C and \\nthe vector X as random; \\nK K \\nR(c) = ER(c, C)= L nk pmc(k) + d L nk pd(k). (2.3) \\nk=l k=l \\nThis is seen to be the overall misclassification probability plus d times \\nthe overall doubt (or reject) probability. It is also the long-term average \\nloss, the limit of n-1 L.'J=l L(Cj,C(Xj)), where {(Cj,Xj)} is a random \\nsample of size n. \\nFor our first main result, let \\nnk pk(x) p(klx)=Pr{C=kiX=x}= K (2.4) \\nL.l=l n1 PI(x) \\nbe the posterior probability of class k given X = x. Then the following \\nholds. \\nProposition 2.1 The classification rule which minimizes the total risk \\nunder loss (2.2) is \\nc(x) = l,;;K { k if p(k I x) = maxp(ll x) and this exceeds 1-d, \\n9& if each p(k I x) ~ 1-d, (2.5) \\nand for a general loss function is \\n{ k if this attains min L L(j, l)p(j l.x) < d, c(x) = I,;;K . \\nJ \\n9& otherwise. (2.6) \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 31}, page_content=\"20 2 Statistical Decision Theory \\nProof: We have \\nR(c) = E [E[L(C,c(X)) I X]] \\n= L E[L(C,c(x)) I X= x] p(x)dx \\nwhere p(x) = L::f=l TrkPk(x) is the marginal density for X. It suf\\xad\\nfices to minimize the conditional expectation, which we can write as \\n'L:~= l L(k, c) p(k I x), with respect to c, for each x. For c = ~ we have \\n'L:k=l L(k, {J2) p(k I x) =d. Under loss (2.2) the minimand becomes \\n1 -p( 11 X), ... ' 1 -p(K I X), d \\nwhen c = 1, ... ,K, {J2 respectively. Thus we look for the maximum of \\np( 11 x ), ... , p(K I x ), 1 -d, and find the solution given. D \\nUnder loss (2.2) another way to write the optimal rule is to choose \\nthe class with the highest nk Pk(x) provided this exceeds (1 -d) p(x), \\nfrom (2.4). \\nThis optimal classifier is also referred to as the Bayes rule. When \\ntwo or more classes attain the maximal p(k I x) the tie can be broken \\narbitrarily. The value R(c) of the total risk (2.3) for the Bayes rule \\nis called the Bayes risk. This value is the best one can achieve if \\nthe nk 's and Pk 's are known, and provides a benchmark for all other \\nprocedures. For two classes and without the doubt option the Bayes \\nrisk is Emin[p(11x),p(21x)], for any number of classes it is E[1-\\nmaxp(klx)]. Let r(x) = 1-maxkp(klx) . Then with the doubt option \\npmc = E{r(X)J[r(X) ~ d]}, pd = Pr{r(X) > d}, \\nand the Bayes risk is R = pmc + dpd. The error-reject curve plots pmc \\nagainst pd for varying d. Note (Chow, 1970) that \\npmc(d) = -ld ( dpd(O \\nso pd(d) as a function of d determines all the performance quantities. \\nProposition 2.1 highlights the central role of the posterior probabil\\xad\\nities. Most of the rest of the theory presented here can be regarded as \\nways to estimate or approximate the posterior probabilities from the \\ntraining set. This definition follows \\nLehmann (1986), \\nDevijver & Kittler \\n(1982) and many \\nothers; another school, \\nrepresented by \\nBerger (1985), calls the \\ntotal risk the Bayes \\nrisk, and the Bayes risk \\nthe minimum Bayes \\nrisk. Fukunaga (1990) \\ncalls it the Bayes error. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 32}, page_content='2.1 Bayes rules for known distributions 21 \\nExample: Normal classes with common covariance matrix \\nThe most important distribution in statistical theory is the normal \\ndistribution, with the familiar density (.j2no-)-1exp[-!(x-,u)2jo-2] in \\nthe one-dimensional case. We write X \"\\' N{,u, o-2} to signify that X \\nhas this distribution, with mean parameter ,u and variance parameter \\no-2, and will also say that \\'X is normal (,u, o-2)\\'. In p dimensions the \\ndensity is \\nWe write X \"\\' Np{,u, L} or say that X is normal (,u, L) when X is a p\\xad\\ndimensional vector with this distribution. (Thus we omit the qualifying \\n\\'multi\\' or \\'multivariate\\' that often is included.) The expected value is ,u \\nand the covariance matrix is L (Mardia et al., 1979, p. 37). \\nSuppose the feature vectors from class k are Np{.Uk. L}. If we \\ndisregard the doubt option a new feature vector x is allocated to the \\nclass k with smallest value of <5(x, ,Uk)2-2log nk. where \\n<5(x,,uk) = [(x-,uk)TL-1(x- .Uk)]112 \\nis (the definition of) the Mahalanobis (1936) distance from x to the \\ncentre of class k. Since the quadratic term xTL-1 x is common to each \\nclass the optimal rule can be written \\nminimize -2,u[L-1 x + ,u[L-1 ,Uk -2log nk over k = 1, ... , K. (2.8) \\nThis is called (the population version of, or the theoretical version of) \\nlinear discriminant analysis. If the classes are equally likely a priori \\nthen x is classified as coming from the nearest class, in the sense of \\nhaving the smallest Mahalanobis distance to its mean. If in addition L \\nis proportional to the identity matrix then distance can be Euclidean \\ndistance. \\nThe error rate for the optimal rule can be computed explicitly in \\nthe two-class case. One should allocate to class 1 whenever \\nThis can be reorganized as \\n(2.9) \\nwhere Ji = !(,u1 + ,u2). If X comes from class 1 then A\"\\' N{!<52,<52}, \\nin terms of the Mahalanobis distance \\n\"\\' { T -1 )}1/2 u = (,u1 -.U2) L (,u1 -.U2 (2.10) '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 33}, page_content='22 2 Statistical Decision Theory \\nbetween the two classes. Similarly, if X comes from class 2, then \\nA\"\\' N{(-~<52\\n, <52}. Accordingly \\npmc = 1qPr[N{ib2,b2}:::::; log(n2/nl)] \\n+ n2Pr[N{ -~<52\\n, <52} > log(n2/nt)] \\n= n1<1>( -~b +;! log(n2/nd) + n2<1>( -~b-! log(n2/n!)), \\nwhere <I>(·) is the cumulative distribution function for the standard \\nnormal. Note that the error rate is expressed in terms of the one\\xad\\ndimensional normal distribution even when the class distributions are \\np-dimensional normal. In the symmetric case with equal prior probabil\\xad\\nities both class-wise error rates are equal, and the minimum attainable \\nmisclassification rate is pmc =<I>( -~b). \\nExample: three Poisson groups \\nSuppose there are three equally likely groups of Poisson data, with \\nmean parameters A.1 = 10, A.2 = 15, A.3 = 20. Then the optimal rule is \\nto allocate to class 1 if X :::::; 12, to class 2 if 13 :::::; X :::::; 17, and to class \\n3 if X ~ 18. The class-wise success rates, or probabilities of correct \\nclassification, are \\npcc(1) = Pr{X:::::; 121 C = 1} = 0.792, \\npcc(2) = Pr{13:::::; X:::::; 171 C = 2} = 0.481, \\npcc(3) = Pr{X ~ 181 C = 3} = 0.703. \\nThe overall error rate is 0.341. \\nSuppose next that one can obtain two independent measurements \\nX1 and Xz from the object to be classified. How do the allocation \\nrules and the error rates change? Some easy calculations show that one \\nshould allocate to class 1 if X :::::; 12.0, to class 2 if 12.5 :::::; X :::::; 17.0, and \\nto class 3 if X ~ 17.5, where X = (X1 + X2)/2. The revised class-wise \\nsuccess rates are \\npcc(1) = Pr{X:::::; 12.01 C = 1} = 0.843, \\npcc(2) = Pr{12.5:::::; X:::::; 17.01 C = 2} = 0.640, \\npcc(3) = Pr{X ~ 17.51 C = 3} = 0.806. \\nThe overall error rate has been reduced to 0.237. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 34}, page_content=\"This technique is \\nknown as multiple 'hot \\ndeck' imputation in \\nsurvey sampling. 2.1 Bayes rules for known distributions 23 \\nRemarks \\nThe constant d in (2.2) acts as a safety threshold, and should in principle \\nbe specified by the user of the resulting classifier. The inconveniences \\ncaused by a reject have to be judged against the consequences of a \\nmisclassification. In a serious application where the classifier is meant \\nto work routinely on future examples one would typically try several \\nd values on a training set of vectors with known classes, and obtain \\nestimates of misclassification and doubt rates (see Section 2.7) before \\na 'final value' is chosen. Plotting misclassification rate against d is \\nuseful (see Figure 3.5 on page 114). If d is near zero then 'doubt' \\nis inexpensive . This will lead to low error rates but on few classified \\nvectors and a high doubt rate. If on the other hand d ~ 1 -1/K then \\ndecision £0 is so expensive that it never will be used. \\nThere are no restrictions on the type of densities Pl, ... , PK ; in \\nparticular they need not be densities with respect to Lebesgue measure. \\n(For professional probabilists : as long as Pk = dPk/dJ.l for some a-finite \\nmeasure J.l dominating the class distributions Pt, ... , Pk both (2.4) and \\n(2.5) continue to hold. We may in fact take J.l = I:f=I Pk-) Thus some \\nor all of the Pk 's may have discrete components, they may represent \\nnormal distributions with singular covariance matrices, and so on. \\nThe small piece of theory presented here is fairly standard, although \\nthe rigorous derivation of the optimal reject ('doubt') region, by means \\nof the loss function, is less known. The most popular special cases \\nof the optimal rule are the normal distribution cases with common \\nor different covariance matrices; see the example above and those \\ndiscussed in Section 2.2. Indeed, discriminant or classification analysis \\nstarted with a sample version of (2.9), in Fisher (1936). He derived the \\nbest linear rule in the two-class case but from a different perspective; \\nsee Section 3.1. \\nMissing values \\nSome problems (such as the Pima Indians data) have examples with \\nmissing values for some of the features. In principle these are easily \\naccommodated; just compute the posterior probabilities p(c I x*) using \\nthe observed features x·. However, these may be difficult to calculate. \\nOne technique is to simulate the missing features from p(x I x*) and \\naverage p(c I x) over the simulated values. For this to be possible, the \\nmarginal density p(x) must be known. If there are several missing \\nfeatures, the Gibbs sampler (Section A.3) may be used to allow them \\nto be sampled one at a time. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 35}, page_content=\"24 2 Statistical Decision Theory \\nFor normal classes both procedures are easy, as the distribution \\nof some of the features is again joint normal, so we find a modified \\nlinear rule in the observed features. The density p(x I x*) is a mixture \\nof normal distributions (one for each class) and so is easy to sample \\nfrom. \\nSimpler procedures are often used, such as replacing missing values \\nby 'typical' values, for example by the average over observed values. \\nThis is potentially dangerous, as the conditional density p(y I x*) of \\na feature y may have a very different mean from the unconditional \\ndensity. \\nMissing values have been largely ignored in the pattern recognition \\nliterature. They are common in medical diagnosis, but rare in domains \\nwhere data are collected automatically . It is a subject which has been \\ntreated most extensively in the literature on sampling surveys (Little \\n& Rubin, 1987). There the problem may be that 'missing' actually \\nindicates a refusal to respond, and so is informative about the features. \\nThis can also occur in medical diagnosis, where the medical practitioner \\nmay not order a test whose outcome appears certain or not relevant \\nto the diagnosis. It could also be that a feature is missing because it \\nproved to be too difficult to measure. Note that informative missingness \\nof y is only a problem if it indicates a departure from the distribution \\np(y I x*). Thus a missing test whose outcome could be predicted from \\nthe remaining features would not be a difficulty (although the medic \\nmay be predicting from qualitative data which are not recorded). On \\nthe other hand, the refusal to answer a test may well be unpredictable \\nand so informative. Where this is suspected, often the only possible \\naction is to code 'missing' as a value of the feature, and somehow to \\nfind the densities required using the expanded feature(s). \\nOutliers \\nThe concept of outliers does not fit cleanly into the decision-theory \\nframework; one is supposed to have described the whole problem, \\nand 'outliers' suggest incorrect specification. So one way forward is to \\nanticipate outliers and build them into the specification as a separate \\nclass, with a specified TC(J) and class density p(J)(x). Where might these \\ncome from? As outliers express surprise, the class density should \\nperhaps reflect ignorance , and so be a suitable uniform distribution \\nover PI. This is likely to cause difficulties, as for many feature spaces the \\nuniform distribution is not normalizable to a probability distribution. \\nThese can be circumvented; for example for PI = JRP we could take a \\nnormal distribution with a very large variance. However, the difficulties \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 36}, page_content=\"2.1 Bayes rules for known distributions 25 \\npersist, as both the 'shape' of the variance (for p > 1) and the scale of \\nthe variance can affect dramatically the reporting of outliers. \\nAn alternative to assuming ignorance for the class of outliers is to \\nfollow the procedure we will use for all the other classes, and estimate 1T.(!) \\nand p(!)(x) from the training set. Sometimes this is feasible; for instance \\nin reading Zip codes and in object recognition, data are sufficiently \\nplentiful to enable a representative sampling of outliers. But such \\ntraining sets are not commonplace, and often training sets are collected \\nunder carefully controlled circumstances where outliers are less common \\nthan usual (or even removed entirely). \\nOnce the outlier distribution is given, under loss (2.2) outliers are \\ndeclared if \\n1T.(!) Pm(x) ~ (1-d) p(x), max 1T.k pk(x) \\nk \\nIf PCP is 'uniform' this classifies as an outlier when both p(x) = \\n2: 1T.k Pk(x) and each component is small. \\nAnother way to view an outlier would be as an observation x which \\nwas implausible under each of the class densities Pk or under all classes, \\nthat is under p(x) = 2: 1T.k pk(x). Note that these two concepts can be \\nvery different if the classes have very different prior probabilities; the \\nsecond seems preferable as we would want to report as an outlier a \\nmildly-unusual observation for a very rare class. Thus in this approach \\noutliers are detected by first screening observations x and declaring \\nthose with small p(x). How small? This is the same scenario as a pure \\nsignificance test in statistical hypothesis testing (Cox & Hinkley, 1974; \\nLehmann, 1986) and the same ideas apply. Typically we will fix a level \\nrx of acceptable false detections of outliers, and fix a level Pc so that \\nPr{p(X) < Pc} ~ rx. \\nHowever, the integration needed here will often be intractable, and in \\nthe examples we relate p(x) to its average value on the training set. \\nThe two routes lead to the same practical conclusion; declare an \\noutlier when p(x) is small. Note that this is one place where knowledge \\nof the posterior probabilities is not sufficient. We have to be very \\ncareful to ensure that 'uniformity' is an acceptable assumption for PCP; \\nas this is a density it will depend on the particular transformation of \\nthe features used. Often structural constraints on the features will rule \\nout uniformity , and some other plausible guess at p(!) will be needed. \\nThe data on Cushing's syndrome shown in Figure 1.2 on page 11 \\nprovide an illustration of the difficulties of outliers in even a small \\nnumber of dimensions. (Typically there are many more points in \\nmany more dimensions, so the data may be equally 'sparse'.) One \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 37}, page_content='26 2 Statistical Decision Theory \\nof the unknown results seems a clear outlier, both for all three types \\nindividually and from the whole distribution, but so does one of the \\nresults of type c, and the latter is believed to be genuine. (However, \\nthere are only five patients of type c with known results.) Another ofthe \\nunknown results looks like a marginal outlier. In this problem we will \\nassume that a uniform distribution over log excretion rates is plausible, \\neven though there must be an effective maximum and minimum, and \\nwe might perhaps expect the two rates to be correlated. \\nIgnoring the possibility of outliers can lead to misleading results. \\nIn the early 1950s anthropologists were discussing recently discovered \\nhominoid fossils, and in particular whether Australopithecus africanus \\nshould be classified as an ape or a human. Bronowski & Long (1951) \\nconsidered a linear discriminant analysis of teeth between chimpanzees \\nand Homo sapiens and found agreement with Homo but not chim\\xad\\npanzees; Rao (1960) pointed out that they thereby overlooked the fact \\nthat on the full set of variables the sample tooth of A. africanus was \\nimplausible for either population . \\nSometimes outliers are the main interest in a classification problem, \\nin what is known in signal processing as novelty detection . For example \\nin detecting tumorous tissue in mammograms, the tumours are so rare See the cover for an \\nthat what is required is to highlight unusual tissue for further inspection example. \\n(Tarassenko et al., 1995). \\n2.2 Parametric models \\nWe have seen in Proposition 2.1 the central role of the posterior proba\\xad\\nbilities p(k I x), although the consideration of outliers showed that this is \\nnot universal. Since the posterior probabilities are in general unknown, \\nwe have to estimate them from the data, and to do so we use models. \\nThe difference between the parametric models we consider here and \\nthe non-parametric models we consider in Chapter 6 is less clear-cut \\nthan the terms would suggest: the real distinction is between families \\nof probabilities which are quite constrained by having only a few pa\\xad\\nrameters, and those which are so flexible that they can approximate \\n(almost) any posterior probabilities. \\nWe first give some general comments about the use of paramet\\xad\\nric models in classification, including discussion of what the methods \\nactually do when the underlying assumed models are incorrect. We \\nthen present classification rules based on some of the most important \\nparametric models. The most theoretically \\nsatisfying approach \\ncomes in Section 2.4. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 38}, page_content='Figure 2.1: It is not \\nalways necessary to \\nmodel the \\nclass-conditional \\ndensities (upper figure) \\naccurately, as the \\nposterior probabilities \\nin the lower figure are \\neffectively unchanged \\nby most aspects of \\nmodelling the right \\npeak of the \\nclass-conditional \\ndensity shown dashed. \\nOnly the densities in the \\ninterval [1, 2] matter. 2.2 Parametric models 27 \\n\"! \\n\\'\\'I \\n\\' I \\nI \\n~ CC! \\' \\' I \\n·;,n 0 \\' I \\n\\' I c \\nQ) \\n\"C \\n\"\\' 0 \\' \\n0 0 \\n0 2 3 4 \\nX \\n0 \\n\\' \\' ~ CC! \\' :c 0 \\' \"\\' \\' .c \\'\"\\' \\' e 0 0. \\n0 \"\\' ·;:: 0 * N g_ c) \\n0 0 \\n0 2 3 4 \\nX \\nTheoretical and practical issues related to debiasing of maximum \\nlikelihood density estimates, predictive classifiers and robust estimation \\nare addressed in later sections. Our first approach is that of classi\\xad\\ncal statistics, to model either the class densities (this section) or the \\nconditional probabilities (discussed in Section 2.3). It will be helpful \\nto distinguish clearly these two tasks, which Dawid (1976) calls the \\nsampling and diagnostic paradigms. Both give a parametric model of \\nthe joint density p(x, c; 8) of a random sample (X, C) of a set of \\nfeatures and its (reported) classification. In the sampling paradigm, \\ninterest centres on Pk(x;8), and we have p(x,c;8) = ncPc(x;8), with \\nthe prior probabilities (nk) for the classes assumed to be either known \\nor completely unknown. In the diagnostic paradigm, interest centres on \\nthe posterior probabilities p(clx;8), with p(x,c;8) = p(clx;8)p(x;8), \\nbut any information about 8 in the unconditional density p(x; 8) is \\nnormally discarded by conditioning on the observed x \\'s. \\nIn later chapters we will concentrate on the diagnostic paradigm, \\nwhich is illustrated in Section 2.3. The sampling paradigm is considered \\nin this section, Sections 2.4 and 2.5 and Chapter 6. In Chapter 8 (X, C) \\nis modelled simultaneously without stressing the importance of the class \\nC. \\nEach of these approaches has strengths and weaknesses. As Figure \\n2.1 shows, direct modelling of the posterior probabilities may need fewer \\nparameters than modelling via the class-conditional densities, and as the '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 39}, page_content='28 2 Statistical Decision Theory \\nmain quantities of interest, p( c I x ), are modelled directly, the procedure \\nwill often be less sensitive to the modelling assumptions. However, the \\ndiagnostic paradigm does have some disadvantages . We have already \\nseen that we need the marginal density p(x) to handle missing values \\nand outliers, and will see that using unclassified observations is much \\neasier in the sampling paradigm. Thus although users of the diagnostic \\nparadigm almost invariably do not model p(x; 8), it is often wise to do \\nso. \\nGeneral considerations \\nThe optimal classification procedure under loss function (2.2) is given \\nin (2.5) when the class densities are known. It resulted in the Bayes risk \\nK \\nR(c) = EL(C, c(X)) = L nk[pmc 0(k) + d pd0(k)], (2.11) \\nk=l \\nfeaturing misclassification and doubt rates for procedure c. In practice \\nthe Pk \\'s are at least partly unknown, and the statistical task becomes \\none of providing good alternative procedures with Bayes risk as close \\nto R(c) as possible. \\nIt is assumed in this section that the prior probabilities nk are \\nknown and that the class densities are modelled parametrically, say \\npk(x) = Pk(x;8) fork= 1, ... ,K, \\nwhere 8 E E> is the vector of unknown parameters needed to describe \\nthe K class densities. Suppose a training set of the form \\n(2.12) \\nis available, with the nk Xk,j \\'s coming from class k. These give rise to \\nan estimate Ok of 8k. A natural proposal is then the classification rule \\n~ { k ifp(k I x) = maxp(ll x) and this exceeds 1-d, c(x) = I \\ng) if each p(k I x) ~ 1-d, (2.13) \\nwhere parameter estimates are inserted in class densities to produce \\napproximate posterior probabilities \\n~(k I ) = nk pk(x; 0) p X K ~. \\nLl=l n1 PI(x; 8) (2.14) \\nThe rule (2.13) is called the plug-in classifier. Some of the most widely \\nused classification methods are of this form, as shown in the examples \\nbelow. We would estimate 1tk \\nby nk = nd L.:: \"j· '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 40}, page_content=\"Here consistency means \\nalmost sure \\nconvergence to the \\n'true' value. 2.2 Parametric models 29 \\nIt remains to decide exactly which estimator should be plugged in. \\nThe maximum likelihood (ML) estimator has been the most popular \\nchoice in statistical practice, together with modifications to reduce its \\nbias. (It is defined and discussed below.) The widespread use of the \\nplug-in rule with the ML estimator has been caused by the good general \\nreputation the ML method enjoys and the fact that several pioneers in \\nstatistical classification theory have directly or implicitly reco!llmended \\nit. The use has been rather uncritical, though. Although () may be \\nexcellent as an estimator of () there is no guarantee that pk(x; 0) is a \\ngood guess for Pk(x; 0), nor is c(x) necessarily a good approximation \\nto c(x). The performance of plug-in rules and other procedures should \\nreally be judged by the criterion of total risk, R(c) defined in (2.3), if \\n(2.2) is still considered to be the appropriate loss function, or by othe_E \\ncriteria more tied to classification accuracy than to the behaviour of (J \\nas an estimator for e. \\nFor example, to apply Proposition 2.1 we only need to know which \\nof the posterior probabilities is the largest (or which of a weighted \\nsum is the smallest), which requires high accuracy of modelling only for \\nsome parts of the feature space. (If one posterior probability dominates, \\nit does not matter if it is fitted as 0.999 when it is really 0.85.) We \\nknow of no work aimed at this aspect of the problem, although some \\napproaches are closer than others to its goals. \\nThese questions and related problems are returned to later, but first \\nwe give some general comments pertaining to the use of parametric \\nmodels in discriminant analysis. \\nBayes risk consistency \\nA reasonably simple observation that has been taken as support for \\nthe use of plug-in parametric rules is the following: As the training s;:t \\nincreases, that is each of n1, ... , nK grows, then provided only each ()k \\nis consistent and the class densities are continuous in their parameters, c of (2.13) becomes identical to the optimal c and its total risk R(c) \\nconverges to Bayes risk R(c). Many plug-in rules, corresponding to \\na large class of possible estimators 0, have this property; see Van \\nRyzin (1966) and Glick (1972, 1976). \\nThere is an important assumption behind this argument, that the \\nclass densities Pl, ... , PK in fact obey the parametric structure in ques\\xad\\ntion. As statisticians sometimes admit, their parametric models are \\nonly approximations to reality, implying in the present context that \\neven when the size of the training set increases beyond bounds, c of \\n(2.13) will become close to only an approximation to c of (2.5), and the \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 41}, page_content=\"30 2 Statistical Decision Theory \\ntotal risk R(C) will converge to a number greater than R(c). Expres\\xad\\nsions for this limit can be found using the theory presented below. It \\nis often possible to construct procedures that are Bayes risk consistent \\nin the sense that the sequence of total risks converges to the Bayes risk \\nR( c) when the training sets grow. Unless one firmly believes in a certain \\nparametric model the Bayes risk consistent rules will necessarily involve \\nnon-parametric or very flexible parametric methods, a topic returned \\nto in Chapters 4 and 6. \\nThese comments are not meant to imply that parametric models are \\nuseless; they may indeed constitute good and compact approximations \\nto more complicated models. Classifiers built on parametric assump\\xad\\ntions may work excellently. Non-parametric methods often demand \\nfor their successful application far larger training sets than parametric \\nalternatives. Thus there is a trade-off between perhaps simple, easily \\nimplementable algorithms that work well even for moderately sized \\ntraining sets, and non-parametric ones that may behave awkwardly \\nfor small to moderate training sets. The non-parametric ones will \\nnevertheless (nearly always) win if sufficient training data are available. \\nLikelihoods and unclassified observations \\nThe likelihood for the training set ff is \\nK nk \\nt(8;ff) = ITITpk(xk ,J;8)nk(8) \\nk=1}=1 \\nand this applies whether the nk were fixed in advance or resulted \\nfrom a random sample taken from the whole population . We will use \\nL( 8; ff) = log t'( 8; ff) for most of our calculations. Conventionally \\nlikelihoods are only defined up to a factor which does not depend on \\n8 and hence log-likelihoods up to an additive constant. \\nHere we assume that either nk(8) is known, hence does not depend \\non 8 and can be dropped from the likelihood, or completely unknown \\nand forms part of the parameter vector, which is then really tp = \\n(8, n1, ... , 7rK ). The maximum likelihood estimator of tp is the maximizer \\nof the (log-)likelihood. We have \\nL(8,(nk);ff) = LLlogpk(xk,J;8) + Lnklognk. \\nk j k \\nWe can maximize first over the second term; after introducing a \\nLagrange multiplier for the condition L 1rk = 1 we find nk = nkf L nj. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 42}, page_content='2.2 Parametric models \\nPlugging this in gives the log profile likelihood \\nL( 0, (nk); ff) = L L log Pk(xk,j; 0) + const \\nk j \\nwhich is the same as the log-likelihood knowing (nk). 31 \\nIn most cases we have to maximize over 0 directly (numerically or \\nanalytically). Sometimes the parameter 0 divides into separate parts \\nfor each class, in which case we can fit Pk(x; 0) to each class separately \\nby maximum likelihood. \\nIf some of the features are missing, we replace Pk(Xk,j; 0) by \\npk(xZ,j; 0) for the observed features xk,j\" \\nThere are some problems in which observations X are cheap but \\nclassifications C are expensive, so we can envisage having a set of \\nunclassified observations diJ = {xj} in addition to the training set. \\nThese must be regarded as independent samples from the mixture \\ndistribution p(x; 0) = Lk 1tkPk(x; 0), and the log-likelihood (or profile \\nlikelihood) becomes \\nL(O,(nk);ff) = LLlogpk(Xk,j;O) + Llogp(xj;O) (2.15) \\nk j j \\nwhich will couple the class densities even if they could previously be \\nseparated. Note that the extra observations may carry much useful in\\xad\\nformation; consider classes with Np{Jlb ~k} distributions. Given enough \\nunclassified data, we could estimate all the parameters Jib ~b 1tk as pre\\xad\\ncisely as desired, except we would be unable to say which group applied \\nto which class. The classified observations provide the information on \\nthis matching. \\nFitting parametric families when they are wrong \\nWe will give a brief discussion of the behaviour of ML estimates when \\nthe underlying parametric model is not necessarily true. Assume that \\nX 1, ... , Xn are independent and identically distributed with a density \\np(x), and that the parametric model p(x; 0) = pe(x) is forced on the \\ndata, 0 being a q-dimensional parameter belonging to some open \\nparameter set. The ML estimator e maximizes the log-likelihood \\nfunction n \\n(2.16) \\ni=1 \\nwith respect to 0. By the law of large numbers n-1 Ln(O) tends to \\nJ p log Pe dx, the mean of log pe(Xi), with probability 1 (often termed \\n\\'almost surely\\'). '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 43}, page_content=\"32 2 Statistical Decision Theory \\nFor many important parametric models this function has a unique \\nmaximum at a parameter value 8 = 8o. This 8o is not necessarily the \\n'true value' because we have not assumed that p belongs to the family \\nof Po's. In a sense 8o is the value of 8 making Po closest to the true \\np, in that it minimizes the Kullback-Leibler divergence \\nI p(x) \\nd(p,p0) = p(x) log Po(x) dx. (2.17) \\nThis measure is not symmetric in its arguments and therefore not a \\ndistance in the usual sense. It is rather a 'directed' distance from the true \\ndensity to the modelled density, and we think of 80 as_::he 'least false' \\nparameter value. Under weak regularity conditions 8 ~ 8o almost \\nsurely (see for example: Huber, 1967; White, 1982) thus generalizing \\nthe classical consistency result for ML estimators. If the true density is \\nin the parametric family, p(x) = p(x;8o) and d(p,po0) = 0. \\nApplying this result to each of the parametrically estimated class \\ndensities we see that the ML plug-in rule c defined in (2.13) and (2.14) \\nconverges pointwise to a rule c• defined analogously to (2.5) but with \\nposterior probabilities of the form \\np(k I x) = ;k pk(x; 8o) . \\nLt=l nt Pt(x; 8o) \\nFurthermore, \\nR(c) ~ R(c*) almost surely, and R(c*) > R(c). (2.18) \\nThe classical result on the limiting distribution of Jii(ii-80) may \\nalso be generalized to the present agnostic state of affairs where the \\nparametric family does not necessarily contain the true p (Huber, 1967, \\np. 231; White, 1982, Theorem 3.2). \\nProposition 2.2 Under mild regularity conditions \\n(2.19) \\nwhere ~d denotes convergence in distribution and \\nd K _ V o logp(Xi; 8o) an -arp iJ() . \\nIf the true density belongs to the parametric family, J = K. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 44}, page_content=\"The deviance is defined \\nin the glossary. Here \\nthe reference model is \\nthe true distribution. \\nThe second step uses \\nthe independence of X \\nand e. 2.2 Parametric models 33 \\nProof: The ML estimator solves the vector equation un(i)) = 0, where \\nUn(O) = 2:::~1 fa logp(Xi; 0). A Taylor expansion shows that \\nwhere In(O) is the Hessian of the log-likelihood function and 7J lies on \\nthe vector between the ML estimator and 00. This implies \\nJYt(iJ-Oo) = [-n-1In(O)r1n-112Un(Oo) \\n-a J-1Nq{O,K} = Nq{O,J-1KJ-1 }. \\nIf the family contains the true model that J = K is well known \\n(Cox & Hinkley, 1974, p. 108; Lehmann, 1983, p. 118). 0 \\nThe usual definition of the Fisher information matrix is K. The \\nregularity conditions needed imply that J and K are positive definite. \\nWe also need to consider the effect of approximating the log\\xad\\nlikelihood (M. Stone, 1977b; Murata et al., 1991, 1993, 1994). \\nProposition 2.3 Let D = 2 E (logp(X) -logp(X; B)], the expected de\\xad\\nviance on a single test example. Then \\nn x D = E deviance+ 2 q* + 0(1/ y'Yt) (2.20) \\nwhere q trace [K J-1]. If the parametric family contains the true \\ndensity, q* = q, the number of parameters. \\nProof: Let i(x, 0) be the Hessian of the log-likelihood for just one \\nsample. We approximate D via the Taylor expansion about 00 \\n2logp(x; 0) ~ 2logp(x; Oo) + 2 (0-Oof a logp(x; Oo)jaO \\n~ T ~ + (0-Oo) i(x, Oo)(O-Oo) \\n= 2logp(x; Oo) + 2 (0-Oo)T a logp(x; Oo)jae \\n+ trace[i(x,Oo)(O- Oo)(O-Oo)T]. \\nWe assume ad(p,p0)jae = Ealogp(X;O)jae = o at 00, so \\nD ~ 2 d(p, Po0) -E trace (i(X, Oo)(O-Oo)(O-00) T] \\n= 2d(p,po0) + trace[J Var(O)] = 2d(p,po0) +! trace(JJ-1KJ-1] \\nn \\n1 = 2d(p,po0) +-trace(KJ-1]. (2.21) \\nn \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 45}, page_content='34 2 Statistical Decision Theory \\nFor the training set we expand about (j : \\n2 I: log p(~i) \\n. p(X;,Oo) \\nI \\n~ 2 L log p(~;~ -L trace [i(X;, O)(e-Oo)(e-eof] \\ni p(X;,8) i \\n\"\"\"\"l p(X;) * d . • ~ 2 ~ og . ~ + q = evtance + q . \\np(X;,O) \\nChecking the error terms shows the error to be 0(1/ Jii). D \\nThis is the basis of Akaike\\'s (1973, 1974) AIC and Murata et al.\\'s \\n(1991, 1993, 1994) NIC criteria for model selection, which are of the \\nform (deviance+2q*), with q* replaced by q for AIC. M. Stone (1977b) \\nderived NIC while considering cross-validation and AIC, but did not \\ncomment that it might provide a better approximation. Moody\\'s (1991, \\n1992) Pelf is a more general version which we discuss in Section 4.3. \\nTo use (2.20) we replace the expectation of the deviance by the \\nobserved value. The main error comes in the fluctuations of the deviance \\nat Oo, 2 I: logp(X;)/p(X; ; 8o), about its mean 2n d(p, po0), which by the \\ncentral limit theorem (assumed applicable) will be of order Op(Jii), \\nand we have \\nn x D = NIC + 0(1/ Jii) + Op(Jii). (2.22) \\n(The notation Op() is explained on page xii.) \\nNow consider comparing several models via their values of NIC, \\nand choosing the model with the smallest. Equation (2.22) shows that \\nfor large enough n we will choose one of the models with smallest D. \\nOf course, there may be many such models if we have a nested set, so \\nNIC will there choose a model which includes the smallest true model, \\nbut not necessarily the smallest such model. \\nOne major source of the fluctuations in (2.22) is the variability of \\nthe training set (X;), and this is common to all models. However, the \\nclaim by Murata et al. (1994, §5) that for differences in NIC amongst \\nnested models this fluctuation term in the differences is Op(1/ Jii) is \\nfalse. Suppose we have nested models with q1 > q2, 6..q = q1 -q2 and \\nthe smaller model (and hence both) are true. Then as M. Stone (1977b) \\npointed out, \\nAICt-AIC2 = 2(LR test of 1 vs 2)-26..q \"\\'xiq-26..q \\nfor large samples, and the right-hand side has fluctuation Op(1). Even \\nasymptotically we might find AICt < AIC2 and so choose the larger AI C was named by \\nAkaike (1974) as \\'An \\nInformation Criterion \\', \\nalthough it seems \\ncommonly believed that \\nthe A stands for \\nAkaike. NI C is an \\nabbreviation of \\n\\'Network Information \\nCriterion\\'. Some \\ndefinitions of AIC and \\nthe definition of NIC \\ndivide by n, which is \\nfixed. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 46}, page_content=\"2.2 Parametric models 35 \\nmodel. If the models are not nested and equally good we can have \\na fluctuation term in the difference of Op(.j1i); consider the perverse \\nexample of two models which choose the true family for the odd \\nnumbered Xi, fixed e for the even ones and vice versa, so the effective \\ntraining sets are disjoint. \\nTo make use of these results in practice, we have to be able to esti\\xad\\nmate J and K. Now J is the expectation of the observed information, \\nthe Hessian of the negative log-likelihood, with the observed informa\\xad\\ntion evaluated at e0 and the expectation over the true distribution of \\nexamples. Thus we can form a reasonable estimate by replacing the \\nexpectation by the average over a training (or test) set, and replacing \\neo by 0. The same argument suggests estimating K by the variance \\nof ologp(X,O)joe over the training or test set. If qjn is not negli\\xad\\ngible, there is a danger of bias here, especially in estimating K, and \\nhence of underestimating q*. To see this, let U(x,e) = ologp(x;e)joe \\ndenote the scores. Then EU(X, e0) = 0 from the definition of e0, so \\nK = EU(X, eo)U(X, eof. For a training set .l:::i U(Xi, B) = 0, which \\nimposes q constraints on the scores, and the divisor in the variance \\nshould perhaps be n-q. For a test set it is perhaps best to use the \\nvariance with divisor n-1. \\nVery little of the argument here depends on using a maximum like\\xad\\nlihood estimator, and Huber's (1967) results hold much more generally. \\nAll we need is that 8 maximizes 2::::: tp(Xi; e) for a suitably smooth \\nfunction tp playing the role of log p, and that a unique e0 minimizes \\nE tp(X; e). Of course, the definitions of J and K change by replacing \\nlogp by tp. (We use this freedom on page 140.) \\nExample \\nConsider the normal distribution Nq{Jl, ~} as an approximation to a \\ngiven density p on 1Rq. The density is given at (2.7). Some analysis \\n(Huber, 1985, Lemma 12.4) shows that the parameter values (JLo, ~o) \\nthat provide the best approximation according to the Kullback-Leibler \\ncriterion (2.17) are \\nJlo = EpX = j xp(x)dx and \\n~o = VarpX = j(x-JLo)(x-JLo)T p(x)dx. \\nThus when the normal model is used to describe data from a density p \\nthat perhaps is known a priori not to be normal and the ML estimators \\nji, !: are computed, the theory shows that what they really estimate \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 47}, page_content='36 2 Statistical Decision Theory \\nare J-lo, I:o, the population mean and variance. It is worth pointing \\nout that this was proved without using any explicit expressions for the \\nestimates themselves. These are derived below, after which another and \\nmore direct proof of /1 ~ J-lo and ~ ~ I:o can be given. \\nThe normal model and the best linear rule \\nSuppose Pk is the Np{J-lb I:} density for k = 1, ... ,K, as defined in \\n(2.7). There we saw that the Bayes rule is a linear rule in the sense that \\nwe choose the maximum of K linear combinations, and for two classes \\nwe divide the linear combination (J-l2-J.1I}I:-1x (from (2.9)). \\nThe total likelihood for a training data set of form (2.12) is \\nK nk II II II:I-112 exp[-~(Xk,j- }-lk)TI:-1(Xk,j-J-lk}]. \\nk=1 j=1 \\nThis is maximized by flk = Xk = nk\"1 2::}~1 Xk,j and by \\n(2.23) \\nwhere N = 2::~=1 nk is the total training set size (and the maximum will \\nbe infinity unless N ~ p + K ). See, for example, Mardia et al. (1979, \\n§4.2.2). The ML-estimated best linear rule takes the form (2.8) with \\nthese estimates plugged in: \\n. . . 2~T~-1 ~T~-1~ 21 k 1 K (2 24) mm1m1ze -J-lk ~ x + J-lk ~ J-lk-ognk over = , ... , . . \\nFor two classes this is Fisher\\'s (1936) linear discriminant, derived \\nfrom another criterion; this approach stems from Rao (1948). Often \\nthe bias-corrected estimator of I: with divisor N -K is preferred \\n(and N -1 appears in at least one computer package). This makes \\nno difference to the linear rule unless the prior probabilities differ, in \\nwhich case the effect is to change the constant terms to reduce slightly \\nthe influence of the data term relative to the prior. \\nThe best linear rule for the data on Cushing\\'s syndrome on page 11 \\nis shown in Figure 2.2. The equal-covariance normal model does not \\nseem appropriate for this dataset. \\nThe best quadratic rule \\nNow let the model for class k be Np{J-lk, Lk}· The ML estimators can \\nbe found from a likelihood expression as before, and since there are no '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 48}, page_content='Figure 2.2: The decision \\nregions of the best \\nlinear rule for the data \\non Cushing\\'s syndrome, \\ntogether with contours \\nfor p(x) at negative \\npowers of 10 of the \\naverage for the training \\nset. i! .. c • \\nc ;; 2.2 Parametric models \\nu b b b \\nb \\n10 \\nTetrahydrocol\\'ti&one 37 \\n/ \\n50 \\nparameters common to more than one class, Jik and ~k are found by \\nmaximizing the likelihood for class k separately. The result is \\nnk \\nJik = Xk and ~k = : \\'L,(Xk,j- /ik)(Xk,j- Jik)T \\nk j=1 (2.25) \\nwhere we need nk ~ p + 1 for each class for a finite maximum of the \\nlikelihood. This produces the plug-in version of the best quadratic rule; \\n. . . 1 1 I~ I 1 ( ~ )T~-1( ~ ) 1 m1mm1ze 2 og \"\"\\'k + 2 x -Ilk \"\"\\'k x -Ilk -ognk (2.26) \\nover k = 1, ... ,K. The rule goes back to C. A. B. Smith (1947). \\nThe number of estimated parameters has increased dramatically \\nfrom Kp + p(p + 1)/2 for the best linear rule to Kp + Kp(p + 1)/2, \\nso parameter estimates may be rather variable for the quadratic rule. \\nEven though this method is guaranteed to outperform the linear rule \\nfor very large sample sizes, it can very well be outperformed by the \\nlinear rule for moderate sample sizes. \\nSince it may be preferable to use a linear rule, we can ask which \\nlinear rule produces the smallest error rate. This has been considered \\nfor two classes by Riffenburgh & Clunies-Ross (1960), Clunies-Ross & \\nRiffenburgh (1960) and Anderson & Bahadur (1962). (See Anderson, \\n1984, §6.10.2.) The optimal linear rule is not that derived by pooling the \\ncovariance matrices and using (2.24) (for example, with ~ the MLE or \\nthe average of ~i ), although the linear combination used does derive \\nfrom a convex combination of the two covariances. In practice it may \\nbe better to take some intermediate position, and compromise between \\nthe linear and quadratic rules. This is discussed in Section 3.4. \\nThe data on Cushing\\'s syndrome look suitable for quadratic dis\\xad\\ncrimination, since although the numbers in the classes are very small, '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 49}, page_content='the covariance ellipsoids vary very considerably in orientation. The \\nresults are shown in Figures 2.3 and 2.4. The six unknown types are \\nall given quite high posterior probabilities (the lowest is 70%, and the \\ntwo apparent outliers have low values of p(x), roughly w-12 and w-3 \\ntimes the average for the training set. Thus both are rated as outliers \\n(and they were medically, the more extreme being due to difficulties in \\nthe measurement procedure, and the less extreme to another type not \\nrepresented in the training set). \\nIt is possible that l:k is singular in one or more groups. (This \\nhappens in the forensic glass data-none of the samples of tableware \\ncontains any potassium, barium or iron.) A singular covariance matrix \\nimplies that the population for the class lies in a subspace of f£; \\nequivalently it satisfies one or more linear constraints. Then a future \\nexample which does not satisfy those constraints does not come from \\nthe class, and one which does will come from this class (or any other \\nthat has the same constraints). '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 50}, page_content='These equations follow \\nfrom Huber (1981, \\n§8.4). They extend in \\nthe obvious way to a \\ncommon scale matrix \\nfor all groups. See also \\nKent et al. (1994) and \\nLange et al. (1989). \\nKent et al. (1994) show \\nthat the solution is \\nunique for v > 1 and \\nn~p+l. 2.2 Parametric models 39 \\nMultivariate t models \\nThe univariate normal distribution is well known to have shorter tails \\nthan distributions which occur in applied problems, and a t distribution \\nwith a moderate number of degrees of freedom is often regarded as \\na better fit. The multivariate analogue of a t distribution is usually \\ndescribed by the analogue of the distribution of Student\\'s t statistic: \\nthe multivariate t with location vector 1-l and scale matrix ~ is the \\ndistribution of J-l +X /S where X \"\\' Np{O, ~} and vS2 \"\\' X~ (Johnson \\n& Kotz, 1972, §37.3; Mardia et al., 1979, p. 57). (Unfortunately, several \\nvariant definitions exist in the literature, not all of which are actually \\ndensities!) \\nWith this definition, for v > 2 the. mean is 1-l and the covariance \\nmatrix is v~/(v-2). The density \\n(2.27) \\nhas elliptical contours with shape determined by ~ but which spread \\nout more slowly than a normal distribution. The optimal classifier is \\nminimize (v!p) log [1 + ~(x- J-lk)T~k\\'1(x-J-lk)] +!log l~kl-log 1tk. \\n(2.28) \\nIf the prior probabilities are equal and the scale matrix is common to \\nall groups we again have the best linear rule. \\nThe log-likelihood for the multivariate t is similar to that for the \\nnormal, except that the quadratic term Qi = (xi- l-lf~-1(xi -J.l) \\nis replaced by (v + p)log(1 + Qifv). Thus the maximum likelihood \\nestimators of 1-l and ~ are weighted versions of the mean and scale \\nmatrix, with weights wi(J-l,~) = 1/(1 + Qi/v): \\nThe effect of the longer tails of the t distribution is to down-weight \\nobservations which are far from the mean. The maximum likelihood \\nestimators can be found by an iterative algorithm which updates the \\nweights, although it would be wise to choose resistant estimates of \\nthe mean and covariance matrix (see Section 2.5) as starting points. \\nDetails of existence and convergence are a special case of arguments \\nof Maronna (1976) and Huber (1981, §8.6). Note that as llxdl -+ oo \\nits effect on the location estimate goes to zero, whereas on the scale \\nestimate its effect remains bounded but does not vanish. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 51}, page_content=\"One use of a multivariate t is as an agnostic model for dis\\xad\\ntributions with elliptical densities with long tails, in the spirit of \\nrobust statistics (Huber, 1981; Lange et a/., 1989). In that set\\xad\\nting it is interesting to consider what the least false parameter s are, \\nas they indicate what the parameters measure in the population . \\nThey are weighted versions of the mean and variance, weighted by \\nw(,uo,I:o) = 1/ [1 +(X-.uofi:01(X-.uo)/v]. Thus if the true density \\nhas elliptical contours, .uo will be the centre of the ellipses and I:o will \\nbe proportional to the moment matrix of the ellipses (with the constant \\nof proportionality depending on the true density). \\nThe decision rule for multivariate t distributions on 5 degrees of \\nfreedom is shown in Figure 2.5 for the data on Cushing's syndrome. \\nThe number of degrees of freedom was chosen arbitrarily to give fairly \\n'fat' tails; despite this there is little difference from the best quadratic \\nrule. Some of the difference is due to different mean and scale estimates, \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 52}, page_content='2.2 Parametric models 41 \\nbut the differences in the lower right reflect the tail behaviour. The \\nmuch greater uncertainty shown in Figure 2.6 in the lower right also \\nreflects the tail behaviour. \\nA mixed model for discrete and continuous components \\nSuppose X = (A, Yt, ... , Yp) where A is discrete and takes values in \\n{ 1, ... , m} and Y = ( Y1, ... , Yp) T has a continuous distribution. A \\nsimple and sometimes quite effective model for such feature vectors is \\nto postulate Y I {A= a}\"\" Np{llk,a,L} while Pr{A =a I k} = gk(a) for \\nclass k (Olkin & Tate, 1961; Krzanowski, 1975). Many variations exist \\naround this theme; the L matrix which is assumed common here can be \\ntaken to vary with either or both of a and k, for example. There is also \\na possibility of modelling gk(a) if the number of possible values for A is \\nanything but small. (This is termed a conditional Gaussian distribution; \\nsee, for example, Edwards, 1995; Lauritzen, 1996, Chapter 6.) We shall \\nbe content here to illustrate the general principle with the simple model, \\nsometimes called the \\'location model\\'. \\nThe class densities are \\nand so from Proposition 2.1 we find the class k maximizing \\nwhen (a, x) is observed. We need to find and plug in the maximum \\nlikelihood estimators of the parameters. These are straightforward: Jik,a \\nis the mean of observed X from class k with A = a, i: is the observed \\ncovariance matrix (with divisor n) of X -1-Lc,A, and gk(a) is estimated \\nby the proportion of examples in class k with A = a. \\nFinite mixture distributions \\nWe can consider larger parametric models, for example mixtures of \\nnormals which will allow us to model multi-modal class densities. As \\nthis is a way to fit quite general class densities, we defer the most of the \\ndetails to Chapter 6. However, there is one quite commonly used \\'trick\\' \\nto fit class densities by mixtures, and that is to model sub-populations of \\nthe classes. A rather extreme example is that of Oliver et al. (1979), who \\nconsidered 13 cell types in cervical cytology, 5 normal and 8 abnormal. \\nWe have experienced several instances of feature distributions with a \\nclear bimodal structure. Consequently histograms for even well chosen '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 53}, page_content='42 2 Statistical Decision Theory \\ntransformations of data are not well described by fitting a normal \\ndensity. This suggests studying mixtures of two normal distributions as \\na means of describing class densities. The case of three or more normal \\ncomponents of a mixture is similar, but the number of parameters \\nneeded increases quite rapidly. We view the following mixtures of \\ntwo normals for each class density as still being within the realm of \\nparametric modelling. \\nLet X~, ... ,Xn be a random sample from a density which we intend \\nto describe parametrically by \\nThe important problem of fitting data to this class of densities is a \\ndifficult one and is perhaps not yet satisfactorily solved in the literature. \\nThe model is not properly defined until a restriction of the parameter \\nset is made to avoid problems of identifiability; we may exchange \\n(J.L1, I:l) and (J.l2, I:2) and rename q as 1-q to get two representations \\nof the same density. The model is identifiable if one demands q ~ ~ \\nor that the first J.ll component should be to the \\'left\\' of the first J.l2 \\ncomponent, for example. One may check by drawing graphs in the \\none-dimensional case, however, that curves with rather different sets \\nof parameters may still come close to each other, making estimation \\nof the parameters a more confusing and difficult task than usual. The \\ndensity is not necessarily bimodal even when J.ll and J.l2 are different; \\nsee Eisenberger (1964). \\nThe maximum likelihood programme does not work as smoothly \\nand automatically as in the earlier examples. First of all it does not \\nexist in the usual sense, since the log-likelihood Ln is unbounded, with \\nmany singularities. For example, Ln ~ oo as J.ll = X1 and I:1 ~ 0, \\ncorresponding to the \\'explanation\\' q = 1 -~\\' X1 \"\" N{J.ll, 0}, while \\nX2, ... , Xn follow N{J.l2, I:2}. Clearly this is not the solution we want. \\nThe Ln function will usually have several local maxima, and one of \\nthese corresponds to the nth element in a sequence of stationary points \\nthat converge almost surely to the true parameter values. \\nA one-dimensional example of fitting two normals is given in \\nVenables & Ripley (1994, Chapter 9) which illustrates some of the diffi\\xad\\nculties even in that case. They use direct maximization, with derivatives \\nof the log-likelihood being found by automatic symbolic differentiation. \\nUpdating estimates from unclassified data \\nWe saw at (2.15) that we could include unclassified observations in the \\nlikelihood, and this opens the possibility of continuing to estimate the For univariate data, \\nHathaway (1985) \\nestablishes consistency \\nfor the global minimum \\nunder a constraint on \\nthe ratio of the \\nvariances. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 54}, page_content='2.3 Logistic discrimination 43 \\nparameters (and the prior probabilities (nk)) while the classifier is in \\nroutine use. This has two important implications: it enables work to \\nstart with a minimal training set, and it allows the classifier to adapt to \\nslow changes in the class distributions over time. \\nThe log-likelihood (2.15) involves the marginal density p(x; e) which \\nis a mixture of the class-conditional densities. Mixtures are discussed \\nin more detail in Section 6.4, but we only need the application of the \\nEM algorithm (Section A.2). Regard the true classes of the unclassi\\xad\\nfied observations o/1 as missing data. Then we estimate the posterior \\ndistribution of the true class k as nk(x; e) oc nkpk(x; e) and nk as the \\naverage of the nk(x; e) over all observations. Hence e is estimated by \\nmaximizing a weighted log profile likelihood \\nL(e) = L L:logpk(xk,j;e) + L L nk(xj;e)logpk(xj;e). \\nk j j k \\nThe first term comes from the training set !T and the second from the \\nunclassified observations o/1. This is an iterative process, in that e and \\n(nk) are updated alternately with ( nk(xj; e)). \\nWe can apply this programme to the best linear and quadratic \\nclassifiers (Hjort, 1986, §7.2) and to multivariate t distributions. We \\nneed only keep the means and covariance matrices of the classified data, \\nbut since nk(x; e) will change as more examples are collected, all the \\nunclassified data needs to be retained. Approximations for situations \\nwhere retaining the data is computationally undesirable are discussed \\nby Titterington et al. (1985, Chapter 6). \\n2.3 Logistic discrimination \\nLet us return to the normal model for classes with a common covariance \\nmatrix given by (2.7) with Jl = Jlj for class j. If we compare class k \\nwith class 1 we have \\n21 p(klx) \\nog p(11 x) \\n= (x-JldT~-1(x-Jld-(x-Jlkf~-1(x-Jld + 2log nk \\n1rl \\n= 2(Jlk- JldT~-1x-(Jlk + JldT~-1(Jlk-Jld + 2log nk \\n1rl \\n= 2/3[ x + 2rxk \\nsay, a linear function of x. Thus the posterior probabilities obey a \\nlog-linear model of the form \\nlogp(k I x) = logp(11 x) + rxk + f3[ x (2.29) '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 55}, page_content=\"44 \\n8 ,; \\n8 \\n0 \\nci 2 Statistical Decision Theory \\nu b b b \\nb \\n10 \\nTetrahydrocortisone 50 \\nwhich is also known as a multiple logistic model. The case of two classes \\nis much simplified, as \\nlogitp(k I x) =a+ pT x (2.30) \\nfor the logit transform logit(x) = log(x/(1- x)). Thus (2.30) gives the \\nposterior log-odds of class two versus class one, and is known as a \\nlogistic regression. \\nEquation (2.29) is illuminating, as it expresses the posterior proba\\xad\\nbilities, the important quantities in a plug-in rule, directly in terms of \\nthe parameters. This suggests that rather than use maximum-likelihood \\nestimation of Jlk, ~ and hence ak. fJk, we should estimate the latter \\ndirectly. As (2.29) and (2.30) only concern the dependence of C on X, \\nthis is done by conditioning on X. For illustration we consider only \\nthe case of two classes here, and defer the general case to Section 3.5. \\nHowever, comparing Figures 2.2 and 2.7 shows that the two estimates \\nmay give quite different classifiers when the common covariance model \\nseems inappropriate. \\nConditional on X = x, the class C has a Bernoulli distribution \\nwith probabilities p(c I x). Thus if were-express the training set as .o7 = \\n{ ( Ci, Xi), i = 1, ... , n} the conditional log-likelihood for the parameters \\n(}=(a, {J) is given by \\nn n II p(ci I xi)= II p(21 xJ(c;=2)[1- p(21 Xi)]l-l(c;=2) \\ni=l i=l \\nand so if YJ = I(C1 = 2) the conditional log-likelihood is given by \\n(2.31) Figure 2.7: The decision \\nregions based on \\nlogistic discrimination \\nfor the data on \\nCushing's syndrome. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 56}, page_content=\"The loss is even higher \\nwhen there is very little \\noverlap, but then both \\nrules perform well. \\nThe ARE is the ratio in \\nlarge samples of plug-in \\nerror rate minus Bayes \\nrisk; this is more \\nrelevant than the \\nvariability of the \\nparameter estimate \\nitself. 2.4 Predictive classification 45 \\nNote that maximizing (2.31) will not give the same answer as plugging \\nin the maximum-likelihood estimators to give 7J = 2-1(J1k -jll), a = \\nlog(n2jn1)-!(J11 + jl2)T2-1(jl2-jll) as here the likelihood is based on \\nthe conditional distribution. As they are based on less information, the \\ndirect estimators should be less efficient (that is more variable) although \\nthe standard large sample theory applies to show that the estimates are \\nconsistent and asymptotically normal. Efron (1975) demonstrates that \\nthis is the case, and the loss of efficiency can be appreciable when \\nthe class densities overlap, so the classification task is neither easy \\nnor hopeless. If there are two classes with equal prior probabilities, \\nthe asymptotic relative efficiencies are a function of the Mahalanobis \\ndistance (j between the class means, given by \\n0.5 1 1.5 2 2.5 3 3.5 \\nARE 1.00 0.99 0.97 0.90 0. 79 0.64 0.49 \\npmc% 40.1 30.8 22.7 15.9 10.6 6.68 4.01 \\n(The values of pmc are computed from the arguments below (2.10).) \\nOn the other hand, the logistic form (2.30) assumes less and is therefore \\nless likely to be biased. (We will see in Chapter 3 that (2.30) can arise \\nfrom other models of the class densities.) \\nLogistic discrimination is a very important template for many of \\nthe generalizations we will consider, much more so than linear dis\\xad\\ncrimination. We take it up in Chapter 3 as a principle in its own \\nright. \\n2.4 Predictive classification \\nNext we discuss the predictive approach towards estimation of paramet\\xad\\nric densities and posterior probabilities. It is Bayesian in inspiration \\nand flavour even though the 'vague prior' versions of the method can \\nbe used and motivated outside the Bayesian paradigm. Suppose that \\nwe have a parametric family p(x, c; e) for the joint distributions of the \\nclasses and features; this implies parametric models for Pk(x; e), nk(e) \\nand p(k I x; e), although of course not all of these need actually de\\xad\\npend on e. Assume also that we have a prior distribution p(e) for \\ne. Then in principle (and sometimes in practice) we can calculate \\np(k I x) = Pr{k I x;Y}. The predictive approach then acts as if p(k I x) \\nwere the true posterior probabilities, and uses Proposition 2.1 to calcu\\xad\\nlate the optimal rule, which we will call the predictive classifier. The \\ncrucial difference between the plug-in and predictive classifiers is that \\nthe former acts as if the estimated e was the true e whereas predictive \\nmethods average over the uncertainty in e. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 57}, page_content=\"46 2 Statistical Decision Theory \\nThe predictive approach gains very little mention in even compre\\xad\\nhensive texts such as Berger (1985) and McLachlan (1992). This may \\nwell be because it usually makes little difference within the tightly con\\xad\\nstrained parametric families we are considering in this chapter, but it \\nwill be important when we consider much larger families. The books \\nof Aitchison & Dunsmore (1975) and Geisser (1993) are devoted to the \\napproach. Both contain brief accounts of classification, in Aitchison & \\nDunsmore's Chapter 11 under the heading of 'diagnosis'. \\nWithin the Bayesian paradigm this needs no further justification: \\nthe prescribed way to handle unknown parameters is to integrate them \\nout from the conditional distribution given the data. We may however \\nask whether the predictive classifier has any optimality properties in \\nterms of risk. In one sense it cannot, for if we knew the true value of e we must do better. Suppose rather that we extend the framework so \\ne is an unobserved random variable. Let c(x, 3) be a classifier which \\nis allowed to depend on the training set (2.12). Its risk function, the \\nexpected loss when using it, is \\nR(c,k,e) = E [L(k,c(X,3)) 1 c = k,e] \\n= pmc(k, e) + d pd(k, e), \\nwhere Ek,8 denotes the expectation for class k and fixed e. As a \\nfunction of e alone the risk function is \\nR(c,e) = EeL(C,c(X,3)) = pmc(e)+dpd(e). \\nThe overall risk in this framework is \\nR(C) = EL(C,c(X,3)) \\n= j R(c, e) p(e) de= pmc + d pd, \\nwhere pmc = J pmc(O) p(e) de and pd = J pd(O) p(O) de are uncondi\\xad\\ntional misclassification and reject rates, averaged over the unknown e. \\nNote that this criterion makes good sense with any reasonable weight \\nfunction over the parameter space; it does not have to be interpreted \\nas a prior density (although it can be). \\nProposition 2.4 The classifier that minimizes the overall risk under loss \\n(2.2) is \\n-( ) = { k ifp(k I x) = max1 p(llx) and this exceeds 1 -d, \\nc x ~ if each p(k I x) ~ 1-d, (2.32) \\nwhere p(k I x) = Pr{ C = k I X = x; 3}. This is also the rule that mini\\xad\\nmizes the conditional risk given the training data. The extension to other \\nloss functions follows Proposition 2.1. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 58}, page_content='2.4 Predictive classification 47 \\nProof: We condition on the training set: \\nR(c) = EL(C,c(x,.rn = Eg-E[L(C,c(x,.rn 1 .r] \\nand the conditional expectation is the total risk in the conditional \\ndistribution. Now apply Proposition 2.1 to the conditional total risk, to \\nshow this is minimized by a classifier of the form (2.32). D \\nThe full expression for the predictive posterior distribution when \\nthe future observation is independent of the training set is \\np(klx) oc j p(klx;O)p(x;O)p(Oiff)dO (2.33) \\nn n \\ni=l i=l \\nThe posterior density and the integral here readily become intractable. \\nExplicit expressions are given below for some important special cases, \\nand approximations can be provided in other cases. Note that (2.33) \\nand (2.34) depend on the parametrized marginal density of X; it is \\nat this point that the simplicity of logistic discrimination loses out, \\nunless the assumed form of the marginal density does not depend on \\ne. Alternative expressions which simplify in that case are \\np(k I x) = j p(k I x;O)p(O I ff,x)dO \\nn \\np(O I ff,x) oc p(O)p(x; 0) IIp(ci I xi; O)p(xi; 0). \\ni=l \\nand p(O I ff,x) will not depend on x if p(x;O) does not depend on e. \\nIf we work with parametrized class densities in the sampling \\nparadigm, it is easier to use \\nPk(x) = j pk(x;O)p(Oiff)dO. (2.35) \\nIt is helpful to remember that p quantities are just conditional densities \\ngiven ff and so can be manipulated as densities. We consider later \\nwhat happens if the prior (nk) is unknown. \\nSuppose we have to classify m > 1 future examples with feature \\nvectors x~, ... , x~. The approach so far will not be fully efficient if \\np(x; 0) really does depend on e, since all the xj can be used to increase \\nour knowledge of e (Geisser, 1966). We should use \\nn m \\np(O I ff,x~, ... ,x~) oc p(O) II p(ci I xi; O)p(xi; 0) II p(x;; 0) \\ni=l j=l '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 59}, page_content='48 2 Statistical Decision Theory \\nin the diagnostic paradigm, and use this omitting the current xj in \\n(2.35). (This differs from the proposal of Geisser, 1966, 1993, which is \\nto maximize the joint predictive probability of the m classifications. The \\nlatter is optimal under the loss structure that all predictions be correct \\nrather than the number of misclassifications be small. The difference is \\nimportant in statistical image analysis; Ripley, 1988, p. 114.) \\nExample: Poisson distributed counts \\nFor a structurally simple example, suppose that a count variable X is \\nmodelled by Poisson distribution p(x; 8) = exp( -ewx I X!. Let e have \\na gamma (0(, p) prior distribution with density [p(X lf(O()]OIX-l exp( -PO) \\non [O,oo), which has mean rxiP and variance rxlfJ2• Assume that \\nindependent counts X1, ... ,Xn have been observed from p(x; 8). Then \\nit is not difficult to show that e given the data has a gamma(O(+nO,fJ+n) \\ndistribution, where 0 = Xn is the ML estimate for e. Hence the \\npredictive density is \\n1oo ex (p + n )tX+nO , \\np(x) = exp(-8)1 ~ etX+nB-l exp[-(p + n)8] de \\n0 X. r(O( +nO) \\n1 (p + n)tX+nO f(O( +nO+ x) \\n= x! (fJ + n + l)a+nO+x f(rx +nO) \\nWhen 0( and P are sent to zero the expression simplifies to \\n0 ~ \\np(x) = _!_ nn , f(n8 ~ x) \\nx! (n + 1)n8+x r(n8) \\nand when n is large this is close to exp( -0) ex I X!, the ML density \\nestimate p(x; 0). The difference is shown in Figure 2.8. 0 \\nExample: Normal \\nLet p(x;J.L,r.) be the Np{J.L,r.} density, (2.7), and assume (at this stage) \\nthat r. is fixed. Then we can consider each class separately. Choose a \\nprior Np{J.lo,A} for the J.l vector. The ML density estimate based on \\ndata X1, ... ,Xn from p(x;J.L,r.) is p(x;ji,r.) with ji =X. The posterior \\ndensity is \\nThis follows from the fact that '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 60}, page_content='Figure 2.8: Plug-in (left) \\nand predictive (right) \\nprobabilities for four \\nsamples with total 20 \\nfrom a Poisson \\ndistribution. ll) -c:i \\n0 -c:i \\nll) \\n0 \\nc:i \\n0 \\nc:i I \\n0 5 2.4 Predictive classification \\n~ \\n10 15 0 -c:i \\n0 \\nc:i l\" \\n0 49 \\nk \\n5 10 15 \\ncombined with properties of conditional distributions for jointly normal \\nvectors; see for example Mardia et al. (1979, §3.2). The predictive density \\n(2.35) can be worked out from this, but gives quite lengthy expressions. \\nIt is usual to use instead the simplified version that comes from the \\nuniform (and improper) prior on JRP, and which also corresponds to \\nletting the matrix A tend to infinity (in the sense that all its eigenvalues \\ntend to infinity). With this flat prior .u Iff is simply Np{/2, I:/n}. \\nCalculations (Geisser, 1964) give \\np(x) = (2n)-p/21I:I-l/2(_n_)p /2 exp(-!_n_(x- /l)TI:-I(x- /2)] n+1 2n+1 \\n= Np{/2, nt1 I:}(x). \\nThe difference from the plug-in estimate of the density is small when n \\nis moderate or large. The optimal rule is still a linear discriminant and \\nhas the same combinations of the variables, but the slightly increased \\nvariance will affect the cutpoints if the prior probabilities are unequal. \\nUnknown covariance matrix/ ces \\nThe most important case is when the covariance matrices are also \\nunknown. We start with one class and a non-informative prior of the \\ntype (.Uk, I:k) ,...., II:k 1-ao/2, where the symmetric covariance matrix is \\nparametrized by its upper triangle, so this is a density on JRP+P(P+l) /2. \\nIn the quite lengthy calculations that are involved it is more convenient \\nto work with Ak = I:;;1 instead; the density for (.Uk.Ak) is IAkl-a/2 \\nwith a = 2(p + 1) -ao. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 61}, page_content='50 2 Statistical Decision Theory \\nThe choice of a non-informative prior here is not clear-cut, but \\nthe majority view is for ao = a = p + 1 with a minority supporting \\nao = 2p, a = 2. Even for p = 1, Jeffreys\\' (1961) information principle \\nleads to an answer that he rejects, but the choice ao = a = p + 1 \\nfollows from assuming prior independence of J.lk and Lk then seeking \\na non-informative prior for Lk alone (Box & Tiao, 1973, pp. 425-426). \\nThis is Geisser\\'s (1993) choice, following Geisser & Cornfield (1963), \\nand that of Aitchison & Dunsmore (1975) who take a limiting case \\nof a Wishart conjugate prior. Berger (1985, §6.6) advocates using \\nright invariant Haar measures as non-informative priors, which he \\ndemonstrates for p = 1 gives ao = 2. For larger p the results do not \\nfollow directly from his work (since the necessary group isomorphism \\nfails). Hjort (1986) was led to ao = 2p and a= 2 by this approach, a \\nvalue Geisser & Cornfield (1963) derived from a Fisher-Cornish fiducial \\ndistribution. On the other hand Villegas (1969) obtains ao =a= p + 1 \\nfrom a fiducial argument, and this was derived by Fraser (1968) from \\nstructural probability. \\nWe can see the difficulties by examining p = 2 in more detail. \\nThen l: can be specified by K; = (Jf, i = 1, 2 and the correlation p; \\nits determinant is K1K2(1-p2). Jeffreys (1961, pp. 176, 187) variously \\nadvocates the priors d(J1d(J2dp/(J1(J2 and d(J1d(J2dp/(Jw 2(1 -p2)312, \\nand the latter corresponds to jl:j-312. \\nAfter extensive manipulation (Geisser & Cornfield, 1963; Hjort, \\n1986) we find \\n-( ) -p/2( + 1)-p/2 r( !(nk + P-a+ 1)) I~ 1-1/2 Pk X = 1t »k 1 \"\\'-\\'k r{:z(nk- a+ 1)) \\n1 \\n[ 1 T~ 1 ] -:z(nk+p-a+l) \\nx 1 + nk + 1 (x-Jlk) 1:;; (x-jlk) . (2.36) \\nHere Jlk and i:k are the usual ML estimators defined at (2.23) on \\npage 36. Figure 2.9 displays three estimates of a one-dimensional normal \\nbased on a sample of size 10; the ML plug-in estimate, the estimate \\nwhich is unbiased on log scale, and the predictive estimate. Note that \\nthese three do not estimate comparable quantities, as the predictive \\nestimator takes the uncertainty of the parameters into account in a way \\nthat the other two do not, but all are potential estimates to be used in \\nProposition 2.1. \\nFor p = 1 the two approaches to choosing the prior agree. The \\npredictive density is a (scaled) t distribution centred on }l.k. The general \\nform is known as a multivariate t distribution on (nk + 1-a) degrees '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 62}, page_content='Figure 2.9: Estimates of \\nthe density based on a \\nrandom sample of size \\n10 from N{O, 1 }. The \\n\\'plug-in\\' estimate is \\nshown with a solid line, \\nthe predictive estimate \\n(2.36) with a dotted \\nline, and the unbiased \\nestimate on log scale \\n(2.38) with a dashed \\nline. \\nThe multivariate t is \\ndefined in the glossary \\n(see \\'t distribution\\') and \\non page 39. Aitchison \\n& Dunsmore (1975, \\np. 255) give a different \\nparametrization which \\naffects the \\ninterpretation of their \\nresults. 2.4 Predictive classification 51 \\n\"\\' ci \\n~, \\nI \\\\ I \\\\ I \\\\ ... \\nci I \\nI I I \\nI I \\n\"\\' I I ci I: ·.I I: I 1.: I t.: I \\n\"\\' 1: I ci I I \\nI I. ··.\\\\ 1.: . \\\\ \\n( \\\\ ci I \\\\ I \\\\ .J \\\\· ·/ \\'\\\\> . . ·/ ............ \\n·\\'/ ,·. \\n0 \\nci \\n-4 -2 0 2 4 \\nof freedom with location vector Pk and scale matrix \\nFor the point of view of classification we assume independent non\\xad\\ninformative priors for each class. Then the predictive class densitj_es \\nhave a larger spread than the normal distribution with variance :Ek, \\nbut the same shape of contours, and so the optimal classifier is a \\nquadratic rule. The difference from the plug-in quadratic rule is to \\nmove the decision boundaries to be more nearly equidistant from the \\nclass centres (apart from allowing for unequal prior probabilities of the \\nclasses). \\nVery similar ideas can be applied to the case of a common within\\xad\\nclass covariance matrix, leading to \\nwhich is a multivariate t distribution on N -K -a + 2 degrees of \\nfreedom with location Pk and scale matrix \\n(1 + 1/nk)N ~\\xad\\nN-K-a+2 '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 63}, page_content=\"52 \\n~ \\n8 .,; \\n~ \\niil \\n0 \\n0 \\n0 \\n:!l \\n0 2 Statistical Decision Theory \\nu b b b \\nb \\n10 \\nTetrahydrocortisone 50 \\nFor equal group sizes and prior probabilities we exactly recover the \\nbest linear rule. \\nThe calculations here are from Hjort (1986); versions of these for\\xad\\nmulae are given by Aitchison & Dunsmore (1975) (up to the differences \\nin the meaning of their multivariate t) and Geisser (1993). This ap\\xad\\nproach is originally due to Geisser (1964, 1966). \\nThe differences between the predictive and plug-in approaches will \\nbe small or zero for roughly equally prevalent classes. In other cases, \\nfor example screening for rare diseases or when very few data are \\navailable, the differences can be dramatic as shown by the examples in \\nAitchison & Dunsmore (1975, §§11.5-11.6). The latter do have groups \\nwith nk only slightly greater than p, for example p = 8 and n2 = 11 \\nwhen fitting a covariance matrix to each class, which would be seen \\nas over-fitting in the plug-in approach. (Indeed, one might choose not \\nto use all the variables, or perhaps to restrict the class of covariance \\nmatrices considered.) \\nAitchison et al. (1977) conducted a small-sample simulation compar\\xad\\nison of the plug-in and predictive methods for two multivariate normal \\npopulations. They were (correctly) criticized by Moran & Murphy \\n(1979) for using the accuracy of the estimation of the log-odds as the \\nbasis of comparison rather than error rates, and for including mainly \\nequal sample sizes of the two classes. Moran & Murphy's results show \\nvery little difference in the error rates, and show that for estimation \\nof the log-odds the debiasing methods of Section 2.5 are effective in \\nremoving the dramatic optimism of the plug-in method where it occurs. \\nVlachonikolos (1990) extended these calculations to some simple \\ncases of the 'location model' for mixed discrete and continuous data \\ndiscussed on page 41. Figure 2.10: The \\ndecision regions of the \\npredictive quadratic \\nrule for the data on \\nCushing's syndrome, \\ntogether with contours \\nfor p(x) at negative \\npowers of 10 of the \\naverage for the training \\nset. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 64}, page_content=\"Figure 2.11 : The \\nuncertainty of the \\npredictive quadratic \\nrule for the data on \\nCushing's syndrome. \\nThe greyscaJes represent \\nthe maximum posterior \\nprobability of a class, \\nwith light grey as one \\nand black as zero. \\nThe Dirichlet \\ndistribution is defined \\nin the glossary. 2.4 Predictive classification \\n10 \\nT.....n.,.CIIOOOI1IIol lt 53 \\n50 \\nWe return to the data on Cushing's syndrome shown on page 11. \\nWith the predictive estimates, the classification is less certain, especially \\nof the two apparent outliers which are now both classified as c (with \\nprobabilities 61% and 68%). However, both still seem outliers, with \\nvalues of p(x) roughly 0.2% and 6% of the average for the training \\nset. These much less dramatic values and the tendency to classify both \\noutliers as c reflect the great uncertainty in the class distribution for \\nthat class. \\nUnknown class prior \\nWe need to consider the effect of unknown prior class proportions (nk) \\nin (2.35) as we would need to justify plugging-in the 'obvious' estimates. \\nUnder random sampling the observed numbers (nt, ... , nK) follow a \\nmultinomial (n, p1, ... , PK) distribution. The natural choice for a prior \\nis a Dirichlet(a;) distribution . The posterior is a Dirichlet(n; + a1) \\ndistribution so \\nsay, since the priors and hence posteriors for e and (nk) are indepen\\xad\\ndent. Then \\nso we act as if we had plugged in the estimate 1ik. The difference \\nThe MLE is it= ntfN. between this and plugging in the MLE will be negligible unless the \\nclass sizes are very small or the prior extremely strong. \\nFor two classes the Dirichlet distribution reduces to the beta distri\\xad\\nbution with parameters (a1,a2). We can ask how to represent ignorance \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 65}, page_content='54 2 Statistical Decision Theory \\nby the Dirichlet parameter vector a. Three suggestions in the two\\xad\\nclass case are a; = 1 (used by Bayes and Laplace), a; = 0 (which is \\nimproper) and a; = 1/2 (Berger, 1985, §3.3.4; Geisser, 1984). Good \\narguments can be made for any of these; fortunately in our setting \\nthey will be very similar. This suggests the simplest choice for the \\nDirichlet of a; = 0 as a vague prior, which gives the simple plug-in \\nrule 1ik = nk/ n. \\nHyperparameters \\nIn some circumstances the prior p( 8) is specified only up to a family \\np;.(8) of priors; 2 is known as a hyperparameter. This occurs most often \\nin pattern recognition when the prior is used to express \\'smoothness \\' of \\nthe posterior probabilities p(k I x; (}) as a function of x, and 2 is then \\nthe degree of smoothness. (See, for example, Section 4.3.) \\nHow should 2 be chosen? Within the predictive Bayesian frame\\xad\\nwork, the solution is clear; we give a prior to 2, called a hyperprior . If \\nthis contains parameters, they too are given a prior, and so on. This \\nis sometimes known as hierarchical Bayes (Berger, 1985, §3.6, 4.6) and \\nthe analysis is in principle obvious, since the effective prior is \\np(8) = J p;.(fJ)p(2)d2. \\nThis may be awkward to use if p;.(8) has been chosen to simplify \\ncomputation, and the integration over 2 may be postponed to a late \\nstage in the calculation . Thus we may find Pr{k I x,ff,2} and then \\nintegrate this with respect to the density p(21 x, !T). \\nOther approaches have been proposed, and in some cases advocated \\nstrongly. Empirical Bayes methods use the data to choose 2, which \\nentails a data-dependent prior which purist Bayesians do not allow, \\nbut is sometimes seen as an approximation to hierarchical Bayes and \\nsometimes as desirable in its own right. (Maritz & Lwin, 1989, is devoted \\nto empirical Bayes methods; Berger, 1985, §4.5, gives references to many \\nstrands.) \\nLet us consider empirical Bayes as an approximation to p(k I x) by \\nPr{k I x, 5\"\",1}. If p(21 x, 5\"\") is highly concentrated about one value \\n1(x, 5\"\"), and if we can estimate this value easily and well, empirical \\nBayes will provide a considerable comp~tational simplification. (Often \\nx will not be at all informative, so 2 can be computed once the \\ntraining set is given.) How could we find 1? Good (1965, 1983) \\n(the latter a compendium of earlier work) calls one method \\'type II '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 66}, page_content='2.5 Alternative estimation procedures 55 \\nmaximum likelihood\\' or \\'ML-II\\'. This is to choose A. to maximize the \\nmarginal density of the data; if we ignore x this is \\nm(§\"IA.) = j t(O;§\")pJc(O)dO. (2.37) \\nNote that ML-II is equivalent to maximizing p(A I§\") oc m(§\"l A.)p(A.) if \\np(A.) is constant, and so its 1 is likely to be a good estimate of A.o(x, §\") if \\nm(§\"l A.) has a sharp peak. Deely & Lindley (1981) discuss conditions \\nfor such approximations. In the usual empirical Bayes context the \\nassumption is of many problems with different e but the same 2, \\nso this condition follows from the asymptotic normality of maximum \\nlikelihood estimation of A. from independent samples from m(§\"l A.). \\nThis is not the usual situation in pattern recognition applications, where \\na single large training set allows increasingly precise inferences about \\ne but provides just one sample to estimate A.. ~ \\nThe empirical Bayes methods usually ignore the variability in 2; \\nPX(O) will be more concentrated than p(O I 3). This may not matter in \\nthe centre of the distribution, but may be material in applications such \\nas ours of finding p(k I x) since p(k I x; 8) is often a highly non-linear \\nfunction of 8 and the empirical Bayes methods often tend to produce \\nfitted probabilities which are too extreme. \\n2.5 Alternative estimation procedures \\nThe maximum likelihood estimator ek is not always the very best \\nestimator of ek (we habitually use a different estimator of the variance \\nof a normal population), and even in ~ses where it is well-chosen \\nfor ek the plug-in density estimate Pk(x; ek) is not necessarily the best \\nestimate of Pk(x; ek). In view of (2.5) and (2.4) our interest lies more \\nwith the densities themselves than with the parameters that describe \\nthem, so here we consider alternative estimation procedures, principally \\nwithin the sampling paradigm. \\nDebiasing density estimates \\nOne route is to modify estimators so as to make them unbiased or \\nless biased, if they are not unbiased in the first place. For example, \\nthe ~ estimator of (2.23) has expected value NNK r., and statisticians \\nnormally use the modified version with denominator N -K instead of \\nN. The same remark applies to the modification of ~k of (2.25) which \\nuses denominator nk-1 instead of nk. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 67}, page_content=\"56 2 Statistical Decision Theory \\nThe densities themselves are more directly involved in the classi\\xad\\nfication problem. It is possible to find an unbiased estimator of a \\nnormal density (Ghurye & Olkin, 1969), but it is more natural to find \\nan unbiased estimator of the log density, and hence of differences in \\nlog densities. This is the appropriate plug-in estimator if the interest \\nconcentrates on the log-odds or on logp(k I x), as in the experi~ents of \\nAitchison et al. (1977) and Moran & Murphy (1979). Suppose ~ is the \\nunbiased estimator of ~ based on m degrees of freedom (so m = nk -1 \\nfor group k, or m = N-K if a common covariance is assumed). Then \\nlogp*(x) = -~plog(2n)- ~[log 11:1 + Bp(m)J \\n1 [m-P-1 r-1 PJ -2 m (x-J1) ~-(x-ji)-~ \\nis the unique unbiased estimator for log p(x; ,u, ~), where \\np-1 (2.38) \\nBp(m) = plog(~m)-L tp(~(m- i)) and tp(z) = r'(z)/r(z) \\ni=O \\nfor the digamma function tp (Abramowitz & Stegun, 1965, p. 258). \\nWhen n goes to infinity there is agreement with the plug-in estimator. \\nThe proof depends on E 1:-1 = m~-1 /(m-p -1) and that jml:!/1~1 = \\nIJg-1 Zi where Zi are independent X~-i random variables (Mardia et \\nal., 1979, pages 85 and 73 respectively). \\nMoran & Murphy (1979) give explicitly the effect of this bias \\ncorrection on the linear and quadratic classifiers. The plug-in version of \\nthe two-class linear discriminant (2.9) is to allocate to class 1 whenever \\nthe estimate of logit p( 11 x) is positive, or \\n(ji1 -ii2f~-1(x-~) + log(n!/n2) > 0. (2.39) \\nUsing the unbiased estimator of the log density gives the rule \\nN-K-p -1 [ r-1 ~] p [ 1 1] N _ K (ji1-Ji2) ~-(x -71) + 2 n1 -n2 + log(n!/n2) > 0. \\n(2.40) \\nThe effect of the data over the prior class probabilities is reduced, but \\nthe constant term will also be important if the class sizes are very \\ndifferent in the training set. \\nFor the quadratic discriminant, the effect of the debiasing is to \\nincrease the effective variance by a factor nkf(nk-p-1) over the usual \\nnk/(nk- 1), and to add a constant which depends on nk. \\nThis debiasing is usually unimportant, but can make a difference if \\nfor some class(es) nk is only a little larger than p + 1, as Figure 2.12 \\nshows for the data on Cushing's syndrome. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 68}, page_content=\"Figure 2.12: The \\ndecision regions of the \\ndebiased quadratic rule \\nfor the data on \\nCushing's syndrome, \\ntogether with contours \\nfor p(x) at negative \\npowers of 10 of the \\naverage for the training \\nset. 2.5 Alternative estimation procedures \\ni! o. o a \\n10 \\nTelrahydrooortlsone \\nRobust estimation of parametric densities 57 \\n50 \\nThe normal distribution is a convenient abstraction, but all careful \\nstudies show that real distributions do not quite follow a normal distri\\xad\\nbution but have slightly heavier tails. In addition we should consider \\nthe possibility of outliers, that is examples which do not belong to the \\nclass under consideration (for example, they might be wrongly labelled \\nin the training set). This has led to discussion of robust estimators of \\nthe normal mean and variance for use in 'plug-in' linear and quadratic \\nclassifiers (for example, McLachlan, 1992, §5.7) \\nOur classifiers should be different in the two scenarios. If the \\ndistributions are non-normal, then we need to take into consideration \\nthat the tails will be longer, and assuming a t distribution will be more \\nappropriate. As we saw on page 39, this leads to robust estimators \\nof the means and variances of the t distribution, but with a common \\ncovariance matrix and equal prior probabilities the Bayes rule is still \\nthe best linear rule. This suggests that for the linear rule it is reasonable \\nto plug in robust estimators, whereas for the quadratic rule the rate of \\ndecay of the densities in the tails is crucial. \\nOn the other hand, if we believe that the true class densities are \\nclose to normal but that we have outliers, it will be desirable to plug in \\nrobust estimators, since the aim of robust estimation is to characterize \\nthe uncontaminated populations, and it is the latter we wish to use in \\nthe Bayes rule. \\nRobust estimation in multivariate problems is trickier than appears \\nat first sight, since simple extensions of univariate methods (based \\non down-weighting extreme observations as we saw for fitting the \\nmultivariate t) can fail completely with a fraction 1/p of outliers \\n(Huber, 1981, pp. 227-8). The most recent approaches (for example, \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 69}, page_content=\"58 2 Statistical Decision Theory \\nRousseeuw & van Zomeren , 1990) start by finding a central 'core' of the \\ndata, use the shape of this to identify outliers and then take the mean \\nand covariance matrix of the 'cleaned' data (adjusting the scale of the \\ncovariance to compensate for the effect of cleaning). A specific method \\nfinds the minimum-volume ellipsoid containing L(n + p + 1)/2J data \\npoints. This ellipsoid is used to define a Mahalanobis distance, and all \\npoints within the 97.5% point of distance from the ellipsoid centre are \\nretained. Finding the ellipsoid (even approximately) is time-consuming. \\nWeighted estimation \\nIt is quite common in medical diagnosis for the abundance of the classes \\nin the training set not to reflect their importance in the problem. Often \\nwhen the training data are a random sample from the population, the \\nvast majority of cases are 'normals' yet the cost of mis-classifying a \\ndiseased case as normal is t times higher than that of a false positive. \\nIn screening problems t can be ten or more. \\nFor clarity, suppose we have just two classes, 'diseased' , d, and \\n'normal', n. The effect of differential costs is to move the decision \\nthreshold , so we will declare a positive result when the odds in favour \\nof 'diseased' are not too adverse (better than 1 : t). If we estimate \\nthe posterior probabilities p(k I x) from the training data by plug-in \\nmethods, we would expect to learn p( n I x) much more accurately than \\np(d I x). Under some circumstances this can lead to serious bias in \\nthe estimators. Consider two normal distributions within the sampling \\nparadigm. For the best quadratic rule, we estimate the class-conditional \\ndensity Pk(x) from the examples from class k. For the best linear rule, \\nthe common covariance matrix I: is estimated from both population s, \\nand hence is principally determined from the sample of 'normals'. This \\nis fine if the covariance matrix really is the same in each group, but can \\nlead to biased estimates if the covariance matrix in the 'diseased' group \\nis somewhat different from that in the 'normal' group. The effect of this \\nbias on the posterior probabilities is much more pronounced when the \\nclasses are unequally represented . \\nThe biases in the diagnostic paradigm are often more serious. The \\nplug-in decision rule is to declare a case 'diseased' if p( d I X; e) > c for \\nc = 1/(1 +t) less than 0.5, often much less. We have already noted that \\nplug-in rules tend to produce estimated posterior probabilities which \\nare too extreme, and this will result in a bias when c is small. A further \\nbias results from the disproportion of the two classes in the training \\nset, resulting in underestimation of p(d I x) (since there are many more \\n'normal' cases to be fitted). \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 70}, page_content=\"The reduction in the \\nsize of the training set \\ncan have considerable \\ncomputational benefits, \\nso sampling might be \\npreferred. 2.6 How complex a model do we need? 59 \\nThere are two ideas to alleviate these biases. A simple idea is to \\nuse a biased sample in the training set; this normally means randomly \\nsubsampling the 'normal' group. Let nk denote the numbers of the \\nclasses in the training set, and nk the proportions in the population. \\n(We assume that these are known, for example estimated from the \\noriginal training set.) The fitted probabilities p(k I X; e) then estimate \\nquantities proportional to p(k I x)nk!nk> the posterior probabilities under \\nbiased sampling. Thus the decision rule is to declare a case 'diseased' if \\np(d I x; 8)ndlnd 1 \\n--~:---->-\\np( n I X; e)nn Inn t \\nor p(d I x; e) > 11(1 + tndnnlnnnd). If we under-sample the 'normal' \\ngroup by a factor of t, this declares 'diseased' if the odds exceed one. \\nThis degree of bias in the training set puts the two groups on an equal \\nfooting in the parameter estimation, thereby reducing the estimation \\nbiases. \\nWhen the biased training set is created by subsampling a larger \\ntraining set, it seems wasteful to discard data on the 'normal' group. \\nThis suggests using weighting rather than sampling. In a weighted \\nprocedure, all the 'normal' examples are used, but their contributions \\nto the log-likelihood are weighted by a factor w and, in the sampling \\nparadigm, the size of the 'normal' sample is regarded as wnn. Taking \\nw ~ 1lt will minimize the estimation biases, since the decision rule \\ndeclares 'diseased' if p( d I X; e) > 1 I ( 1 + t (J) ). \\nThese palliatives can also be applied when there are several diseased \\ngroups. The formulae can be extended quite easily by working with the \\nodds of each disease to 'normal'. \\n2.6 How complex a model do we need? \\nAdequacy of a model is not usually an absolute criterion; rather we ask \\nhow complex the model needs to be within families of models. For ex\\xad\\nample, we can ask how many input features to use for a linear classifier. \\nIn this section we concentrate on the adequacy of a parametric model \\nfor the class densities or posterior probabilities. In the next section we \\nlook more directly at the effect of model inadequacy on performance, \\nand in the final section of this chapter we consider absolute bounds on \\nperformance, averaged over training sets. \\nWe have seen two distinct modelling problems. In Section 2.2 we \\nmodelled the class densities Pk(x; 8); in Section 2.3 we modelled the \\nposterior probabilities p(k I x; 8). It is important to realize that a model \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 71}, page_content=\"60 2 Statistical Decision Theory \\nmay be adequate for the posterior probabilities without being adequate \\nfor the class densities (see Figure 2.1 on page 27). \\nWe will consider the traditional statistical approaches to model \\ncomplexity which are applied to both problems. These fall into two \\ncamps. \\n1 Iterative selection of a model. For example, in choosing the number \\nof features to use in a linear discriminant or a logistic discriminant \\nthere are many variants of stepwise procedures, modelled on those \\nused for regression problems. Backward selection starts with all \\npossible features, and drops them one at a time. Forward selection \\nstarts with no features, and adds one at a time. Stepwise procedures \\nstart somewhere (usually with all features) and at each step consider \\neither adding or dropping a feature, and choose the best single \\nstep before iterating. There are a large number of families of \\nmodels considered in later chapters with direct analogues of this, \\nfor example selecting centres in radial basis function models, the \\nnumber of hidden units in a feed-forward neural network and the \\nnumber of components in a mixture distribution. \\nThe distinctive feature of this approach is that the selection is made \\nby a series of pairwise comparisons: is the larger model sufficiently \\nsuperior to the smaller one? \\n2 Penalizing the fit by a measure of the complexity of the model. In \\nthis approach the search is in principle over all models within the \\nfamily. Normally we would expect the largest models to fit best, but \\nthe penalty on size will tend to ensure that the smallest adequate \\nmodel is chosen. In practice we may have to confine the search to \\nonly some of the models in the family: this could even be done \\nby a stepwise search as in 1. The penalty is often motivated by \\npredicting the degree of fit on a test set. \\nWe have left open the measure of fit to be used. The most common \\nis the log-likelihood evaluated at the ML estimate. It is often more \\nconvenient to work with the deviance, minus twice the log-likelihood \\nshifted to be zero for the 'perfect' model. In the classification context \\nthe perfect model has p(k I x) = 0 or 1, with 1 for the class which \\nactually occurs. \\nIn iterative selection we can use likelihood-ratio tests, or equiva\\xad\\nlently differences in deviances. For a regular problem, the reduction in \\ndeviance on adding q further parameters has an asymptotic chi-squared \\ndistribution on q degrees of freedom provided the smaller model is ad\\xad\\nequate (Lehmann, 1986, §8.8). Iterative selection normally works by This may not be \\npossible as a function \\nof the features actually \\nobserved. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 72}, page_content=\"Akaike (1985) reviews \\nthe rationale in a more \\nleisurely way than the \\noriginal papers. 2.6 How complex a model do we need? 61 \\nchoosing some conventional significance level (often 10%) to decide a \\ncompanson. \\nThe penalization methods themselves have three schools. The most \\ncommon one is based on the idea that the deviance will be smaller on \\nthe training set than on a test set of comparable size, since we actually \\nchose the parameters to minimize the deviance on the training set. How \\nlarge would the difference be on average (over training and test sets)? \\nAkaike's (1973, 1974) AIC criterion is based on the answer 2p, where \\np is the number of free parameters. This does assume that the model is \\ncorrect. The criterion NIC of Murata et al. (1991, 1993, 1994) is based \\non the answer 2p* where \\n(2.41) \\nand J and K are defined in Proposition 2.2. As pointed out there, \\nJ = K if the model is adequate, so p* = trace(/) = p. Both criteria \\nfollow from (2.20), and are based on asymptotic normality of the \\nparameter estimates. (Moody, 1991, 1992, uses his effective number of \\nparameters in the same way.) Note that whereas the deviance plus \\n2p* may be a poor estimate of the mean test-set deviance because of \\nthe variability over training sets, differences of this measure between \\nmodels may be acceptably good estimates of the differences in test-set \\ndeviances. Note the may in the previous sentence; the fluctuations \\nare normally small enough to distinguish models whose mean test-set \\ndeviances differ appreciably, but they can dominate if the mean test-set \\ndeviances are nearly the same (see page 34). \\nAsymptotically in the size of the training set, the use of AIC may \\nchoose a model which is at least as large as the correct one with \\nprobability one (see Shibata, 1976; Hannan & Quinn, 1979; M. Stone, \\n1979; this is established for the order of an autoregressive time series, \\nfor instance). Thus if there is no correct model in the family, AIC \\nwill tend to choose larger and larger models as more training data \\nbecomes available. The fact that it tends to overshoot the correct size \\nhas led to modifications (BIC) which penalize complex models more \\nseverely (Akaike, 1977, 1978; Schwarz, 1978) and to justifications based \\non allowing the complexity of the true model to depend on n (Shibata, \\n1980, 1981). An alternative way to estimate the expected deviance on \\na test set of the same size as the training set is to use cross-validation \\nand the other methods of Section 2. 7 on the deviance. \\nA general programme for measuring and controlling the complex\\xad\\nity of fitted models based on minimum description length (MDL) or \\nminimum message length (MML) is given by: Rissanen (1983, 1987, \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 73}, page_content=\"62 2 Statistical Decision Theory \\n1989); Wallace & Freeman (1987); Barron (1990, 1994); Barron & \\nCover (1991). In this the deviance of a model is penalized via the mini\\xad\\nmum length of a binary code needed to represent it. Now in most cases \\nwe can only (over-)estimate this length by providing a specific encoding, \\nand the extension to continuous parameters is via discretization. There \\nare many variants within this programme. \\nVapnik's (1982) structural risk minimization is a similar idea, using \\na bound on test-set risk based on the work of Section 2.8 and discussed \\nfurther there. \\nThe third idea is to estimate more directly the performance on a \\ntest set, by cross-validation and allied methods which we discuss in the \\nnext section. \\nThe predictive approach \\nBayesian methods provide an interesting view of these measures, as dis\\xad\\ncussed by Smith & Spiegelhalter (1980) for linear models (but asymp\\xad\\ntotically most methods are locally linear). In the Bayesian formulation, \\nmodels are compared via Pr{ M I Y}, the posterior probability assigned \\nto model M, which requires a prior distribution (p M) over models \\nand the ability to integrate out the parameters following the predictive \\napproach : \\np(Y I M) = j p(Y I M, 8)p(8) de \\nso the ratio in comparing models M1 and M2 is proportional to \\np(Y I M2)/p(Y I MI), known as the Bayes factor. Note that if the \\nmodels are nested, the priors will correspond to a prior over the \\nparameters of the larger model which gives positive probabilities to \\nzero values of some of the parameters. Then model choice will involve \\nthe controversial testing of 'precise hypotheses', where classical and \\nBayesian methods are often in conflict (Berger & Delampady, 1987). \\nNote that the predictive approach does not actually select a model, \\nbut averages the predictions of the models, with weights proportional \\nto the Bayes factors. This seems not to be widely used, possibly for \\ncomputational reasons, but can be very effective. It is used for simple \\nlogistic models by Stewart (1987), and in time series prediction by West \\n& Harrison (1989). Geisser (1993, §4.1) argues that the only possible \\nloss function which would suggest choosing just one model is one which \\nembodies an extreme principle of parsimony, that only one model is \\nacceptable. Even those who argue for restricting the class of models \\n(such as Madigan & Raftery, 1994) show that averaging is much better Cheeseman ( 1995) \\nclaims 'for most \\ninteresting problems \\nachieving this goal is \\nNP-hard', referring to \\nfinding the model with \\nthe minimum message \\nlength. \\nWe might want to \\nrestrict attention to one \\nor a few classifiers for \\ncomputational speed. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 74}, page_content=\"L is the log-likelihood. \\nVarious results are \\nknown on the \\nlarge-sample accuracy \\nof (2.42), but our uses \\nwill be as \\napproximations far \\nfrom asymptotia. Kass \\n& Raftery (1995) \\nsuggest that at least 5p \\nand preferably 20p \\ntraining samples are \\nrequired. 2.6 How complex a model do we need? 63 \\nthan any single model. The posterior probability may be spread over \\nmany models: Moulton (1991) reports an example in which the top \\n800 models out of 212 = 4096 are needed to account for 90% of the \\nposterior probability . In pattern recognition our sole concern is future \\ndecision making, but in other applications the use of a single model \\nmay be more acceptable or desirable (see, for example, the arguments \\nof Geisser, 1987 and A. F. M. Smith, 1991). \\nThese ideas go back at least to Box & Tiao (1962). Bernardo & \\nSmith (1994) give an overview of the current philosophical discussion, \\nalso to be seen in Draper (1995) and its discussion and references. \\nWe will see other ways to choose combinations of models in the next \\nsubsection, and examples of using estimated Bayes factors to combine \\nposterior probabilities from different models in Section 5.5. Another \\nidea is to use Markov chain Monte Carlo ideas such as the Gibbs \\nsampler to integrate over both the model space and the parameter \\nspace, as considered by George & McCulloch (1993) and Madigan & \\nYork (1995). \\nSuppose we just use the Bayes factor as a guide. The difficulty is in \\nevaluating p(.r I M). Asymptotics are not useful for Bayesian methods, \\nas the prior on (} is often very important in providing smoothing, yet \\nasymptotically negligible . We will assume that p((} 1.r) is approximately \\nnormal with~ mean fJ and covariance matrix V. One approximation \\nis to take (} as the mode of the posterior density and V as the \\ninverse of the Hessian of -log p(B 1.r) (since for a _p.ormal density \\nthis is the covariance matrix); we can hope to find (} and V from \\nthe maximization of log p( (} I .r) = L( (}; .r) + log p( 8) + const. Let \\nE(8) = -L(8;.r) -logp(8), so this has its minimum at fJ and Hessian \\nthere of v-1. Then \\np(.r I M) = j p(.r I 8) p(8) de= j exp -E(8) de \\n~ exp-E(B) j exp[-~(8-fJ)Tv-1(8-B)] d(} \\n= exp -E(B) (2n)P/21VI1/2 (2.42) \\nfrom (2.7). (This is sometimes known as a saddle-point approximation \\nor Laplace's method: Lindley, 1980; Tierney & Kadane, 1986. This and \\nother approximate methods are discussed by Evans & Swartz, 1995.) \\nThus \\nlog p(.r I M) ~ L(B; .r) +log p(B) + ~log 2n + ~log I VI. (2.43) \\nIt may be feasible to use this directly for model choice, as was proposed \\nfor nested models by Kass & Vaidyanathan (1992). \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 75}, page_content=\"64 2 Statistical Decision Theory \\nIf we suppose e has a prior which we may approximate by \\nN{Oo, Vo}, we have \\nlogp(ff I M) ~ L(ii;ff)-i(B-8o)Tv0-1(0-8o)-i log !Vol+ i log lVI \\nand v-1 is the sum of v0-1 and the Hessian H of the log-likelihood \\nat B. Thus \\nlogp(ff I M) ~ L(B; ff)-i(B-8o)rv0-1(B-Oo)-i log IHI. \\nIf we assume that the prior is very diffuse we can neglect the second \\nterm, so the penalty on the log-likelihood is -!log IHI. For a random \\nsample of size n from the assumed model this might be roughly \\nproportional to -( i log n) p provided the parameters are identifiable. \\nThis is the proposal of Schwarz (1978) derived following the ideas of \\nSmith & Spiegelhalter (1980, §2.1). Others argue for retaining different \\nterms in (2.43); for example Draper (1995, p. 57) retains the second term \\nof (2.43), drops the third and replaces log I VI by -log I HI. The latter \\nmight be damaging (Raftery, 1993) and is often of no computational \\nbenefit. \\nThe assumption that the prior can be neglected is a strong one, since \\nwe may not obtain much information about parameters which are rarely \\neffective, even in very large samples. For example, suppose we have \\nseparate parameters for each class-conditional density Pk(x; 8). Then \\nwe will learn very little about the parameters of very rare classes, and \\nthe effective sample size in the expression for BIC for the parameters \\nfor class k will be nk not n. Since we would expect nk oc n, the leading \\nterm in n is still -( i log n )p, but as ( i log n) will be quite small for \\npractical n ( 5.75 for n = 100 000 ), replacing i log IHI by (!log n)p \\ncan be quite misleading. We should be interested in comparing different \\nmodels for the same n, and in many problems p will be comparable \\nwith n. It seems best to use (2.43) directly. \\nKass & Raftery (1995) review many of the approaches to approxi\\xad\\nmating Bayes factors; Gelfand & Dey (1994) provide one of the clearest \\naccounts of the multitude of variations which have been proposed for \\nmodel choice. \\nImproper priors over e (those that are not integrable) lead to \\ndifficulties, since p(ff 1M) will be unknown up to a constant factor, \\nand might be infinite. It may be possible to resolve this by taking limits \\nof results with proper priors (but often improper priors were chosen \\nto make the integrations feasible). Other approaches are discussed \\nby Kass & Raftery (1995, §5.3) including the device of an 'imaginary \\ntraining sample' used by Spiegelhalter & Smith (1982). It is not clear This is often called \\nBIC. Jeffreys (1961, \\npp. 248, 272, 277, 343) \\ndiscussed special cases \\nmany years before in \\nearlier editions. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 76}, page_content=\"2.6 How complex a model do we need? 65 \\nhowever that any of these methods is totally satisfactory, especially \\nwhen the models under consideration have very different numbers of \\nparameters with improper priors. The difficulty shows up in (2.43). \\nWith an improper prior we may treat logp(e) as constant, but there is \\nno reason to suppose that it is the same constant for different models, \\nalthough careless workers in the neural networks field often do so. \\nWe can relate these calculations to the derivations of AIC and NIC. \\nThe present derivation is less asymptotic and does take account of a \\nprior. It produces a factor of log n (for some appropriate n) rather \\nthan 2 on the penalty for the deviance, which curbs the tendency of \\nAIC to overshoot the true model size. (Remember the Bayes factor \\nis intended for use in averaging not selecting models.) Note that the \\napproach of this subsection necessarily assumes that the model is true \\nwhile calculating p(ffl M). We defer further consideration of NIC to \\nSection 4.3, where we consider other methods of parameter estimation. \\nCombining models \\nWe have already mentioned that the full predictive approach 1s to \\naverage models by \\np(k I x) = LPm(k I x) Pr{m Iff} \\nm \\nrather than choosing one model (unless our loss function includes costs \\non multiplicity of models). We now consider other ideas for combining \\nmodels, using ideas which are developed in other contexts in Section 2.7. \\nM. Stone (1974, pp. 126--7) mentioned the idea of using cross\\xad\\nvalidation not to choose between models but to combine them. In \\nour context this would amount to combining the posterior probabilities \\n(either plug-in or predictive) from a series of M models. The predictive \\nviewpoint motivates Stone's suggestion of \\n(2.44) \\nm \\nfor .a set of constants ( ocm), perhaps confined to a probability distribution \\nand chosen by cross-validation. We could also allow the weights to \\ndepend on the class k. \\nThis idea has been developed (independently) and extended by \\nWolpert (1992), under the name of stacked generalization, and applied \\nby Breiman (1992) in the regression context. Breiman's work is precisely \\nwithin Stone's setting, and shows that in his simple examples it does \\nindeed help to confine attention to non-negative weights (although he \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 77}, page_content=\"66 2 Statistical Decision Theory \\nseems not to have considered a unit-sum constraint on the weights). \\nThe idea of averaging (both simple and weighted) for regression neural \\nnetworks has been suggested many times. (A selection of references is \\nBaxt, 1992; Benediktsson & Swain, 1992; Bridle & Cox, 1991; Hansen \\n& Salamon, 1990; Lincoln & Skrzypek, 1990; Pearlmutter & Rosenfeld, \\n1991; Perrone & Cooper, 1993; Srihari, 1992; and Xu et al., 1992.) \\nLeBlanc & Tibshirani (1993) took up the same thread, but also \\nconsidered estimation by the bootstrap. Both cross-validation and the \\nbootstrap can be seen as methods to correct the bias of the deviance \\n(or other measure) on the training set as a function of ( ~Zm), and \\nare used exactly as for the apparent error in Section 2.7. The bias\\xad\\ncorrected estimate of the deviance is then minimized over ( ~Zm). Their \\nexperiments considered non-negativity and unit-sum constraints, but \\nnot both together! \\nWolpert's ideas were much more general than those picked up by \\nlater users. Although (2.44) is suggestive, we might only to want to use \\nit locally in the feature space, and so could allow the weights to vary \\n(slowly) with x. In his general scheme, we use the outputs of all the 'level \\n0' models (under leave-one-out cross-validation) and the true response \\nas inputs to a 'level 1' procedure which then makes the final decision. \\nAt its simplest we could take the predictions from M classifiers and \\nlearn how best to combine them to give a single classification. But \\nthe outputs of the models can be their posterior probabilities, and we \\ncan also pass the inputs through a 'do-nothing' level 0 procedure. So \\nstacked generalization includes any method of combining the outputs \\nof the models, possibly varying with x. \\nA similar approach is taken by Jacobs et al. (1991) in which they \\ntrain all the classifiers simultaneously and the level-0 classifiers are not \\nrequired to work well over the whole input space. This approach is \\ndiscussed in Section 8.5. \\n2. 7 Performance assessment \\nThe title of this section begs the question of what is meant by perfor\\xad\\nmance. Since we have identified the Bayes rule on the basis of total risk \\n(expected loss) this seems a suitable basis for performance evaluation. \\nWhen loss (2.2) is used, it is often helpful to plot the expected rate Figure 3.5 on page 114 \\nof misclassification against the expected rate of 'doubt' classification, is an example. \\nusually termed the reject rate. \\nTo be explicit, the error rate is the probability of making a definite \\nerroneous classification (including outlier (!)) for a future randomly \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 78}, page_content=\"We use use ±2 \\nstandard errors as an \\napproximate 95% \\nconfidence interval. 2. 7 Performance assessment 67 \\nchosen sample, previously called pmc, and the reject rate is the prob\\xad\\nability of declaring doubt, previously denoted pd. Our discussion will \\nconcentrate on pmc, but entirely analogous statements follow about \\npd. Also, statements made about error rates can be replaced verba\\xad\\ntim by ones about average cost for other losses. A later subsection \\non 'confusion matrices' discusses more detailed information on error \\npatterns. \\nThe apparent error rate pjilC is the proportion of errors made when \\nclassifying either a training or a test set. If the training set is used, pjilC \\nwill (usually) be biased downwards. \\nError rate estimation \\nThe easiest way to assess the error rate is to choose a test set independent \\nof the training set (and validation set if used), to classify its examples, \\ncount the errors and divide by the size M of the test set. The reject rate \\nis estimated by the proportion of test-set examples which are rejected. \\nThese measures are clearly unbiased estimates under all circumstances, \\nbut they can be highly variable, and having to use a test set may waste \\ndata which could otherwise have been used for training. The idea of a \\ntest set is sometimes called the hold-out method and goes back at least \\nto Highleyman (1962a). \\nSimple calculations show that the test set needs to be large for the \\nerror rate to be estimated at all accurately. The estimate pjilC = R/ M \\nfor an error count R has a binomial(M, pmc) distribution. Thus pjilC \\nhas variance pmc(1- pmc)/M ~ 1/4M. Suppose that pmc is around \\n5% and we wish to know it to around 1%. Then we will want \\n2J0.05 X 0.95/ M ~ 0.01 \\nor M ~ 1900, which is considerable. (Here we use the normal approxi\\xad\\nmation to the binomial, which is justified at such sample sizes.) \\nNote that the task of comparing the error rates of two classifiers is \\nrather easier as they use the same test set, a point often overlooked in \\nthe literature and taken up in a later subsection. \\nWe can also calculate the error rates conditionally for each class, \\njust by counting within each class. If we know the prior probabilities \\nnk, the estimator I: 1tk pjile(k) (2.45) \\nk \\nestimates pmc. This form is important when the test set is a deliberately \\nbiased sample, which can be a good idea when almost all the errors \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 79}, page_content='68 2 Statistical Decision Theory \\noccur in uncommon classes. In general we would expect it to be less \\nvariable, but there is a problem that it will be undefined if nk = 0 for \\nany class, which complicates the theoretical analysis. \\nRisk averaging \\nSuppose we knew the posterior probabilities p(k I x) and are using the \\nBayes rule. Consider a random pair (X, C) from the whole population. \\nThen \\nP(correct I X= x) = P ( C = argmaxp(c I x) I X= x) = maxp(c I x) c c \\nand so \\n1-pmc = P(correct) = E [ maxp(c I X)]. c (2.46) \\nBoth I [ C = arg maxc p( c I x)] and maxc p( c I x) are conditionally un\\xad\\nbiased estimators of P(correct I X = x), but the second averages over \\nP ( C I X = x) and so has a smaller variance. Let Z = 1-maxc p( c I X) ~ \\n1-1/K. Then EZ = pmc and EZ2 ~ (1-1/K) EZ, so \\nVar[maxp(c I X)] = Var(Z) ~ (1 -1/K)pmc- pmc2 \\nc \\n= Var(J [C = argmaxp(c I x)])-pmc/K. c \\nNote that (2.46) does not depend on knowledge of the true class C, \\nwhich can be useful if the authenticity of the classifications of the test \\nset is in doubt. This can also be an advantage if examples are cheap but \\naccurate classification is expensive, as in almost any form of automated \\ndata collection which needs human classification . This approach is \\nsometimes known as risk averaging. \\nOf course, we only very rarely know the posterior probabilities, but \\n(2.46) can be used if we believe we have accurate estimates of them. \\nIt will be much better to use predictive estimates p(k I x) rather than \\nplug-in estimates p(k I x; (i) as the latter ignore the variability of e and \\nso tend to underestimate small probabilities, often quite severely. The Compare Figures 2.4 \\nestimate from (2.46) can be based on either the training or the test set; and 2.11. \\nif the training set is used there will be some bias, the size of which \\nwill depend on the number of parameters. When the probabilities are \\nestimated maxc p(c I x) will be biased, both because we cannot usually \\nfind unbiased estimators of p(c I x) and because the maximum is a \\nnon-linear operation. We would expect the bias to be small for a test \\nset; calculations for one particular classifier are given in Section 6.2. \\nIf this method is to be used, it would seem desirable to check if the \\nestimates p(k I x) are reliable, which is itself a check of the adequacy '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 80}, page_content=\"2. 7 Performance assessment 69 \\nof the model. The methods are part of the more general methodology \\nof verifying if probability forecasters (such as 'the probability of rain \\ntomorrow is 60%') are well calibrated (see, for example, Dawid, 1982, \\n1986). The basic idea is that if p(k I x) = 11 say, amongst the examples \\nfor which we predict 17, the proportion which occur should be about 1'7· \\nWe can apply this either to the posterior probabilities of each class, or \\nto the probability maxk p(k I x) of a correct classification. The test set \\nthen provides a set of independent events Ei and predicted probabilities \\nfh We can test the calibration by using a non-linear logistic regression \\non Oi (say by the methods of Chapter 4) and test if the identity is \\nadequate. If it is not, we can even use this regression to re-calibrate the \\nprobabilities (an idea for a linear logistic regression going back to Cox, \\n1958). Various methods for fitting such regressions will be described in \\nChapters 4 and 5, and an example is shown in Figure 3.6 on page 115. \\nThere is also a literature on methods of numerically assessing prob\\xad\\nability forecasts. The most common measures are (half-)Brier scores \\n(Brier, 1950) which is the sum of squared differences between the pre\\xad\\ndicted probabilities and the indicator function that the event occurred \\n(or, equivalently, the sum of (1-p)2 where p is the forecast probabil\\xad\\nity of the positive or negative event which occurred), and logarithmic \\nscoring (Good, 1983) which sums the negative log probability of the \\nevent which occurred. Note that logarithmic scoring computes the \\nconditional log-likelihood as used in logistic regression. \\nThe effect of using risk averaging with inaccurate probability esti\\xad\\nmates can be severe; on p~ge 228 there is an example in which the \\nerror rate is underestimated by a factor of more than two. \\nCross-validation \\nOften the use of a test set is regarded as too wasteful of scarce classified \\ndata. Can we avoid this by dividing the training set? If we divide \\nthe training set into two halves, we could train with one half and test \\nwith the other. As the halves are independent samples, the resulting \\nestimator is unbiased. Furthermore, we can swap the halves and still \\nobtain an unbiased estimator, and so the average of the two estimators \\nremains unbiased. \\nThe drawback of this approach is that the estimate is an unbiased \\nestimate of the performance using just half the data. Can we do better? \\nYes, at the expense of more computation. Suppose we randomly divide \\nthe training set into V pieces. Then we can use one piece to test the \\nperformance of the classifier trained on the remaining (V-1) pieces. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 81}, page_content='70 2 Statistical Decision Theory \\nThis is again unbiased, and we can average the V such estimates. For \\nmoderate V such as 5 or 10, the loss of performance from a smaller \\ntraining set will usually be small enough, at the expense of V times \\nthe computation (although this can be done in parallel if several CPUs \\nare available). (A technical nicety is how to do the average; should \\nwe weight by the size of the pieces if they are unequal? Weighting is \\ncommon practice, as it corresponds to counting the number of errors \\nmade by the cross-validated classifiers.) \\nThe extreme version of this strategy is to take V very large. Then \\neach test set will contain zero or one examples. This suggests (but does \\nnot quite justify) the leave-one-out estimator, in which each observation \\nis tested on the classifier trained on the remaining (N-1) observa\\xad\\ntions, suggested by Mosteller & Wallace (1963), Hills (1966), Lunts \\n& Brailovsky (1967) and Lachenbruch & Mickey (1968) and often re\\xad\\ndiscovered. The leave-one-out version of cross-validation apparently \\nrequires a large amount of computation, but for some classifiers this \\ncomputation can be reduced to a similar level to classifying a test set \\nof n examples (see pages 100, 184 and 200). \\nThe leave-one-out estimator is a balanced version of cross-validation \\nin that the sets are chosen of exactly equal size. This can be applied \\nto the V -fold version as well, and indeed we may choose to balance \\nthe subsets on other characteristics such as regions of the feature space \\nor even the numbers taken from each class. Are these variants valid? \\nA test-set error rate is an unbiased estimator of pmc provided that \\nC is sampled from its conditional distribution given X, and that X \\nis sampled with density p(x); no independence is needed to justify the \\nunbiasedness of an average. This justifies all the approaches except \\nchoosing fixed numbers from each class, but including the leave-one\\xad\\nout estimator. If we choose fixed numbers from each class, we obtain \\nunbiased estimates of the class-conditional error rates pmc(k), and \\nhence an unbiased estimate if the prior probabilities are known or \\nestimated in the usual unbiased way (from other data). \\nChoosing V = N should give the most accurate assessment, as \\nthe true size of the training set is most closely mimicked; it also \\nnormally involves the most computation. There is another argument \\nin favour of smaller V. Dropping just one observation assesses the \\nclassifier via 0(1/ N) perturbations from the training set. On the other \\nhand the sampling variations in the parameter estimates are (usually) \\n0(1/ JN), so for large N we end up extrapolating these from much \\nsmaller perturbations. Thus cross-validation estimates of performance \\nfor large V might be expected to be (and are often reported to be) Leave-one-out \\ncross-validation is also \\nknown as ordinary \\ncross-validation. \\nThis is sometimes called \\nstratified \\ncross-validation. \\nA good example is a \\nclassification tree, where \\ndropping any single \\nexample might not alter \\nthe chosen topology, \\nonly the fitted \\nprobabilities at single \\nleaf. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 82}, page_content='Stone\\'s argument was \\nalso given by Liu (1993, \\n1995). 2. 7 Performance assessment 71 \\nrather variable; taking a smaller V can give a larger bias but smaller \\nvariance and mean-square error. \\nWe have concentrated on cross-validation for the error rates, but \\nit is also possible to use cross-validation for the \\'smoothed\\' measures \\nsuch as 1-maxc p(c I X) discussed in the previous subsection. \\nEstimation and model choice \\nCross-validation is also commonly used for model choice. (Cover, 1969, \\nis the first advocate of which we are aware.) This can be used to estimate \\neither a measure of performance (such as error rate) or a measure of \\nmodel fit (such as deviance). M. Stone (1974) and Geisser (1975) \\npointed out that cross-validation could also be used for parameter \\nestimation; just choose the parameter value which minimizes the cross\\xad\\nvalidated measure of performance or fit. Then if we want to assess the \\nperformance of the resulting classifier by cross-validation, we have to \\ndo so by a double layer of cross-validation. \\nM. Stone (1977a, b) considered various asymptotics for model se\\xad\\nlection by cross-validation. Consider first cross-validating the deviance \\n(under ML estimation). This is a sum of terms D; over examples in \\nthe training set. We follow the usual notation in which (i) refers to a \\nquantity based on the training set with the i th example deleted. From \\nTaylor expansions we have \\n2: D;(8(i)) = D(8) + I:[8(i)- 8]T v;(e;) \\ni \\n8(i)-8 = D\"(8;)-1 D\\'(8(i)) \\nfor 0;, 7J; convex combinations of 8(i) and 8 (and since L,Ni Dj(8(i)) = 0 \\nby definition). Under consistency all the estimators converge to Oo, so \\nl:D;(8u)) \"\\'D(8) + I:v;(Oo)TD\"(Oo)-1D;(Oo) \\n= D(8) + trace[D\"(Oo)-1 I:v;(Oo)D;(Oo)T] \\nand the limit of the second factor on the right-hand side is 2p• by \\nthe arguments in Section 2.2. Thus leave-one-out cross-validation of \\nthe deviance is asymptotically equivalent to using NIC to correct the \\ndeviance. This suggests that model choice by NIC and by leave-one\\xad\\nout cross-validation are asymptotically equivalent. (For finite classes of \\nmodels this argument will prove so unless two or more models have \\nthe same D(Oo); all true models have the same value, zero.) \\nM. Stone (1977a) gave heuristic arguments and examples for asymp\\xad\\ntotic consistency of cross-validatory assessment (which follows from '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 83}, page_content=\"72 2 Statistical Decision Theory \\nunbiasedness and a law of large numbers) and asymptotic efficiency of \\ncross-validatory estimation. \\nImproving on cross-validation \\nCross-validation was used to estimate the performance (error-rate, loss, \\ndeviance) on a test set by constructing a pseudo test-set from the \\ntraining set. We now take a different viewpoint , of accepting that \\nthe performance measure on the training set is biased, but trying to \\nestimate that bias, and correct it using our estimate. For concreteness \\nwe will work with error rates (although the principles apply much \\nmore widely). We need to distinguish between the pmc, the true error \\nrate for our classifier trained on this training set, E pmc, its average \\nover training sets, and pmc0, the true error rate with the 'least false' \\nparameter eo plugged in. We can then aim to correct the bias of pmc \\nas an estimator of either pmc or pmc0. The first is most relevant for \\nperformance assessment of this classifier, the second if we use pmc0 as \\nan upper bound for the Bayes risk (which for large parametric families \\nmay be close to the Bayes risk) or as a lower bound on the achievable \\nperformance within this parametric family. In either case pmc will be \\nbiased. The two biases are E[pmc- pmc] and E pmc-pmc0, and \\nin each case we will correct pmc by subtracting an estimate of the \\nappropriate bias. \\nHow do we estimate the biases? The method of Quenouille (1949), \\nlater termed the jackknife by Tukey, is sufficiently similar to leave\\xad\\none-out cross-validation to have caused considerable confusion in the \\nliterature . Suppose en is an estimate of (} based on n observations, \\nand that its mean has the expansion \\n~ a1 a2 E (}n = (} + - + 2 + · · · . n n \\nThen each leave-one-out estimator e(i) has mean \\n~ al a2 \\nE (}(i) = (} + n-1 + (n-1)2 + ... \\nas does their average 0. Now consider ne-(n-1)0. This has mean \\n~ ~ 3 nE (}-(n-1)E (} = (}-n(n _ 1) + O(n-) \\nand so much smaller bias for large n. Thus the jackknife estimator of \\nthe bias is (n-1)[0-e]. \\nThe most obvious application of the jackknife is to reduce the bias \\nof pmc as an estimate of pmc0. The expansion needed is valid under \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 84}, page_content='2. 7 Performance assessment 73 \\nvery mild regularity assumptions for ML plug-in classifiers, and the \\nbiased-reduced estimator of pmc0 is \\n--n-1\\'\"\\'--n pmc--n-L.....t pmc(i) \\ni \\nusing E pmc = pmc0 + at/n + O(n-2). \\nIt is less obvious how jackknifing can be used to estimate the bias \\nE[pmc- pmc]. Efron (1982, Chapter 7) sketches how to do so. The \\nidea is to compare the error in predicting the omitted sample with that \\nin predicting the (n-1) remaining samples. Let ei be the indicator \\nof the error of predicting the class of Xi from §{i), and pmc(i) the \\napparent error rate on fitting to §{i)· Then the estimator of the bias is \\n(n-1) --\\nn m~an[pmc(i)- ei] \\nwhere the scale factor is a sample-size adjustment. Under mild regularity \\nconditions we would expect E pmc = pmc0 + bjn + O(n-2), and so \\nEei = pmc0+b/(n-1)+0(n-2) and E pmc-pmc = (al-b)/n+O(n-2). \\nThen our bias estimator has mean \\nn -1 [ a1 2 b 2] --pmc0 + --+ O(n-) -pmc0----O(n-) n n-1 n-1 \\n= al -b + O(n-2) \\nn \\nas required. The complete jackknifed estimate of pmc is \\npmc + (1 -1/n) m~an [ei-Pri1Cu)1 \\n! \\nwhich has bias O(n-2). \\nIn our current notation the leave-one-out cross-validated estimate \\nof pmc is 2::: ei/n, and so it implies pmc -meani ei as its estimate of \\nthe bias. Efron (1982, Chapter 7) gives a suggestive argument why the \\nrelative difference between the two bias estimates might be Op(1/n), and \\nhence there would be little practical difference. Our arguments show \\nthat if we drop the sample-size correction, thereby making a relative \\nerror of 0(1/n), the difference between the two bias corrections is \\npmc-meani pmc(i) which has a mean of O(n-2), and is often Op(n-2). \\nNote that the computational effort of these two estimators of pmc is \\nalmost the same. \\nThe bootstrap is loosely related to the jackknife, and conceptually \\nsimpler. Suppose we make a new sample of size n by resampling with '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 85}, page_content=\"74 2 Statistical Decision Theory \\nreplacement from our sample, and calculate an estimate e• from the \\nbootstrap sample (as it is called). Then the variability of e·-7i should \\nmimic that ~ 7i-e, in particular the mean of the first should estimate \\nthe bias of e. This can be used by actually resampling B times and \\naveraging, or sometimes by finding the mean analytically. \\nIn our problem we can bootstrap pmc to estimate pmc0, or boot\\xad\\nstrap pmc -pmc to estimate the bias correction. This bias is the \\ndifference between the classifier's apparent error and true error, aver\\xad\\naged over training sets. To bootstrap this we replace .r by .r· and the \\nmean over x by the average over the points in .r. Thus the bootstrap \\nestimate of the bias is the average (over bootstrap samples) of the error \\nrate on the training set .r· minus that on the larger set .r. Of course, \\nwe will evaluate the error rate on the distinct members of .r• using \\nweights for multiple members. \\nImmediately we see a snag, as the example from .r being predicted \\nmay be in the bootstrap training set .r•, and if it is we may expect to \\npredict it well, for some classifiers far too well. Efron (1983) proposed \\nthe '.632' bootstrap, which considers only the predictions of those \\nmembers of .r not in .r•; specifically for each point x; estimate the \\nerror by averaging over those bootstrap samples not including x;, then \\naverage over points to get eo. The final estimate is then \\n0.368 pmG + 0.632 EQ. \\nHere 0.632 is shorthand for (1 -1/e), the limit for large n of the \\nprobability that a given observation from .r appears in .r·. \\nBootstrap methods may also be used to estimate the precision of \\nthe apparent error rate pmc, using the variability of pmc • about pmc \\nto estimate the variability of pmc about pmc, for example to estimate \\nthe variance of pmc by the variance of pmt•. But we have to be \\ncareful, as we should really be interested in the mean square error of \\nthe bias-corrected estimator, not of pmc, and the bias correction is \\nitself an estimate. Efron & Gong (1983) suggest the mean square error \\nof the bootstrap samples used to estimate the bias gives a lower bound \\non the mean square error of the bias-corrected estimator. \\nIntroductions to the bootstrap are given by Efron (1982), Efron & \\nGong (1983) and Efron & Tibshirani (1993); Efron (1983, 1986) contain \\ncomparisons of error rate estimation methods including those based \\non the bootstrap. Other comparisons within the pattern recognition \\nliterature are given by Chernick et al. (1985) (for linear classifiers), \\nCrawford (1989) (for classification trees) and Jain et al. (1987) and \\nWeiss (1991) (for k-nearest neighbour classifiers). These show some Note that !T. contains \\nsome of the members of \\n!T more than once, and \\n(usually) some not at \\nall, so as a set !T• is \\nsmaller. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 86}, page_content=\"2. 7 Performance assessment 75 \\nsupport for the '.632' estimator, but by no means universal improvement \\nover leave-one-out cross-validation. The title of this subsection has been \\nchosen in optimism, since the full power of the bootstrap (for example, \\nused in conjunction with ideas such as (2.46)) seems not to have been \\nfully tested. \\nConfusion matrices \\nThus far we have concentrated on a single measure of performance, the \\noverall error rate. This is natural within our decision-theory framework, \\nbut we may want more detail to help understand where a classifier is \\nfailing. The next level of detail is the class-conditional error rates \\npreviously termed pmc(k), that is the error rate amongst examples \\nof class k. Further, we may want to know which classes are being \\nconfused, and so we may wish to know \\neiJ = Pr{ decision j I class i} \\nwhich is sometimes called the confusion matrix. Note that the decisions \\ncan include 'doubt', ~-\\nThe most obvious way to estimate ekj is from the pattern of errors \\non a test set, and sometimes the term 'confusion matrix' refers to the \\nmatrix of counts of the events 'true class i decided as j'. \\nAlmost all the methods we have discussed apply equally to the class\\xad\\nconditional error rates. We just consider only those examples with true \\nclass k, and in some cases (such as the jackknife) need to take the sample \\nsize as the number of class-k examples. Although Efron (1986) derived \\nthe '.632' estimator for the overall error rate for two groups, it extends \\nreadily to class-conditional rates (Hjort, 1986). Once the conditioning \\non class k is accomplished, estimating the confusion matrix merely \\namounts to accounting for which errors were made. \\nThe one method that has a less obvious extension is the use of \\nthe posterior probabilities at (2.46), since this looks at the predicted \\nrather than true class. Extensions were considered by Schwemer & \\nDunn (1980), Basford & McLachlan (1985) and McLachlan (1992). As \\nat (2.46) we at first assume that the posterior probabilities are known. \\nThen \\neij = Pr{ decision j I C = i} = Pr{ C = i, decision j} /ni \\n= E{p(i I X)J[c(X) = j]} /rri = L 1tk Ek{p(i I X)J[c(X) = j1}. \\nk 1tj \\nWe can form an unbiased estimator of eij by replacing the expectations \\nin the final expression by averages over a test set. If the (rri) are \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 87}, page_content=\"76 2 Statistical Decision Theory \\nunknown and are estimated as usual by (ni/n), the estimator simplifies \\nto \\neij = _!_ :t p(i I X!) I [c(Xl) = j] \\nni 1=1 \\nwhich can also be seen to be a ratio of unbiased estimators. This sug\\xad\\ngested to Basford & McLachlan replacing the conditional probabilities \\nand the Bayes classifier c(x) by estimates. Then it may happen that \\nL:j eij =I= 1, so their final estimator \\nn n \\neij = LP(i 1 Xt)I[C(Xt) = j] / LP(i I Xz) \\n~1 ~1 \\nis formed by re-normalizing the estimator to sum to one. Note that \\nunlike the estimator of the unconditional error rate, this may be biased \\neven if the posterior probabilities are correct. \\nComparing error rates \\nA study to compare the error rates of difference classifiers is an exper\\xad\\niment, and should be designed and analysed as such. There is much \\nknown from many years of theory and experiments in the statistics \\nliterature; an excellent basic reference is Box et al. (1978). Experiments \\nin our field are computer experiments and have much in common with \\nwork in the field of simulation; Kleijnen & van Groenendaal (1992) \\nprovide a non-technical introduction in that context which is amplified \\nin Kleijnen (1987). Important ideas from that field include importance \\nsampling and stratified sampling, both of which can be used to design \\n'difficult' test sets and to compensate for the increased difficulty in esti\\xad\\nmating error rates. For example, we might arrange for rare patterns to \\nbe well-represented in the test set, but down-weighted (as in the study \\nof Candela & Chellappa, 1993, Blue et al., 1994). \\nIt should always be possible to give some idea of the variability of \\na quoted performance estimate. For test-set error counts we can use \\nthe binomial distribution or the normal or Poisson approximations to \\nit. For other measures such as the smoothed error counts based on \\n1-maxc p(x I x) we can use the sample variance, as each example in the \\ntest set is assumed to be an independent sample from the population \\non which we are trying to predict the error rate. \\nHowever, a crucial observation is that since the same test set is used \\nfor each method, the comparisons between methods are usually much \\nmore accurate than the standard errors suggest. (In the terminology of \\nthe design of experiments we have a paired comparison, or a blocked \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 88}, page_content=\"Table 2.1: Test-set error \\ncounts (out of 120) for \\nthe Leptograpsus crabs \\nexample, from \\nRipley (1994c). \\nThis uses the variance \\nof a binomial \\ndistribution . \\nAn exact test has \\nprecisely the \\ndistribution claimed \\nunder the null \\nhypothesis. 2.8 Computational learning approaches 77 \\nmethods 4-way sex only sex only \\ncolour given yes yes no \\nlinear discriminant 8 8 8 \\nlinear discriminant on log variables 4 4 4 \\nquadratic discriminant 11 9 8 \\nquadratic discriminant on log variables 9 7 7 \\nexperiment if there are more than two methods.) Consider the first \\ntwo lines of Table 2.1. As frequently happens, the 4 errors made in \\nthe second line are also made in the first line. The standard error of \\nthe difference between 4/120 and 8/120 assuming separate test sets is \\ny'0.01642 + 0.02272 ~ 3.37/120 so a naive comparison would conclude \\nthat there was no significant difference. \\nMore appropriate methods are available, such as McNemar's test \\n(Fleiss, 1981). Let nA and n8 be the number of errors made by \\nmethod A and not method B, and vice versa, so in our example \\nnA = 4 and nB = 0. Then McNemar's test (with continuity correction) \\nrefers \\nInA -nBI-1 \\ny'nA + nB \\nto a N(O, 1) distribution, and an exact test refers nA to a binomial \\n(nA + n8, 1/2) distribution. This suggests that we need nA ~ 5 for \\na significant difference (but this is only sufficient if n8 = 0 ). Thus \\nlarge test sets are needed to distinguish between classifiers of similar \\nperformance; to detect a 1% difference in error rate needs at least 500 \\nexamples. So although the difference here is suggestive, the sample size \\nis too small for a definitive conclusion . \\nOne pitfall to be avoided is to give too much emphasis to statistically \\nsignificant results. In an experiment in which method A with error rate \\n29.8% is significantly better than method B with error rate 30.1 %, it \\nis clear that the difference is unlikely to be of practical importance, \\nespecially if we estimate the Bayes risk as 6% by the methods of \\nChapter 6. \\n2.8 Computational learning approaches \\nOne recent strand of theory looks at what Valiant (1984) called 'the \\ntheory of the learnable' and has since become known as PAC-learning \\n(for probably almost correct). Suppose we have a training set of n \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 89}, page_content=\"78 2 Statistical Decision Theory \\nsamples, and use these to fit a classifier g from a class !#' of possible \\nclassifiers. If the class !#' is not too large and includes the true classifier, \\nwe would expect that for large enough n the fitted classifier g would \\nbe 'close' to the true classifier f. Thus the theory addresses the question \\nof how large the training set needs to be. \\nHow should we measure closeness? The obvious way is to compare \\nthe decisions made by f and g for a randomly chosen sample from \\ng( x rl, where rl is the set of classes, and ask that they agree with high \\nprobability, that is \\nPr{g(X) -=/= f(X)} < c, (2.47) \\nsay, for some pre-specified E. (This implies that the true error rate of the \\nclassifier g exceeds the Bayes risk by less than c.) The left-hand-side \\nof this statement is a random variable, as it depends on the training \\nset. In PAC-learning we ask that (2.47) be true for a high proportion \\nof training sets, say with probability exceeding 1 -b, for a sample \\nsize no more than polynomial in 1/c, 1/b and that g be fitted in time \\npolynomial in n. Here we are more concerned with the sample size \\nthan the computational complexity of finding g. \\nIn the 'noise-free' case when there is a f E !#' which correctly \\nclassifies any training set, Pr{g(X) -=!= f(X)} is the error rate of g, so \\n(2.47) corresponds to low error rate. In the 'noisy' case we actually \\nstudy the difference between apparent and true error rates. \\nThe bounds used in studying PAC-learning are often called worse\\xad\\ncase bounds since they apply to any distribution over g( x rl, provided \\nthat both the training set and future samples are drawn (independently) \\nfrom the same distribution. Much of the theory currently available \\napplies only to two-class problems, and the results are most refined in \\nthe noise-free case. \\nA warning. The results of this section are often misinterpreted. They \\napply over all possible training sets !T, and assert that events occur for \\nmost (or few) training sets for a given model. As such they are subject \\nto the usual criticism of frequentist methods, that we cannot know if \\nour particular training set is an exception. But the difficulty here is \\nparticularly acute, as the model will have been chosen on the basis of \\nthe training set, indeed often on the basis of the sort of performances \\nthat these bounds guarantee. A typical claim is \\n'If our network can be trained to classify correctly a fraction 1-(1-y )E \\nof the n training examples, the probability that its error-a measure \\nof its ability to generalize-is less than € is at least 1 -o.' \\nHowever, the probability is in fact guaranteed to be less than 1-b over \\nall !T, including those our model will not fit well, not the conditional \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 90}, page_content='For each g with overall \\nerror rate at least E, the \\nprobability of getting n \\ncorrect samples is at \\nmost (1-E)\", and one \\nof at most r classifiers \\nis chosen. 2.8 Computational learning approaches 79 \\nprobability asserted. To use these results (correctly!) in performance \\nassessment , they have to be applied to the whole procedure including \\nmodel choice for every problem, with any exceptions included in the \\nprobability b. (This is pointed out by Wolpert, 1994b.) We have never \\nseen this done. \\nThe results are interesting theoretically, and perhaps useful in model \\nchoice, but are very conservative. Much better bounds should be \\npossible using the knowledge of the world gained from the examples to \\nfit a class of models. We give examples of the \\'dimensions\\' used in the \\nresults in later chapters. \\nFinite sets of classifiers \\nThe simplest approach is to assume that there is a finite number r of \\ndistinct classifiers in ffi\\' (Blumer et al., 1987). Then the probability \\nthat a g chosen consistent with (that is correctly classifying all of) a \\ntraining set of size n yet having overall error rate at least E, is at most \\nr(1-E)n. We can invert this bound to show that \\nlog r + log -} log r + log ! \\n----,,----,-:---\"- ~ ----\"--log(1- E) £: (2.48) \\ntraining samples are enough to ensure that if we classify the training \\nset correctly, the true error rate is less than £: with probability at least \\n1-b. \\nNow suppose that we cannot find a classifier in our class correctly \\nclassifying all cases, so the apparent error rate pmc is non-zero. There \\nare several possible bounds. The number of errors is a binomial ( n, pmc) \\nrandom variable so the Bienayme-Chebychev inequality gives \\n~ pmc(1- pmc) Pr{lpmc(g)- pmc(g)l > £:} ~ 2 nE \\nwhich, allowing for the r possible classifiers, gives \\nr \\nn ~ 4bE2 \\nwhich is much worse than the noise-free bound for small £: or large r. \\nThe bound of Hoeffding (1963) gives for each classifier \\nPr{pmc < pmc-e} ~ exp-2ne2 \\nand twice this probability for a two-sided bound, so \\nlogr +log-} \\nn ~ 2~:2 '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 91}, page_content=\"80 2 Statistical Decision Theory \\nsuffices to bound the optimism in the apparent error rate by E for \\na proportion (1 -<5) of training sets. This is better, but still has \\nrate O(c-2). To overcome this we have to look at relative error with \\nChernoff's (1952) bound: \\nPr{pmc < (1-y)pmc} ~ exp-!ny2pmc; \\nthis gives \\nlogr +log-} n ~ 2 2 y E \\nif we confine attention to classifiers with pmc > E. Thus just as for \\nthe noise-free case we consider the case in which we are doing badly \\nand are not aware of it from the fit on the training set. Taking y ---+ 1 \\ngives double the previous bound on the sample size for a perfectly-fitted \\ntraining set. \\nA combination of absolute and relative error is obtained by using \\nthe metric \\nd ( ) _ lr-sl \\nv r,s ---'-----'----v+r+s \\nused by Pollard (1986) and Haussler (1992). For large v this behaves \\nlike absolute error, for small v like relative error. Note that for \\narguments in [0, 1] (such as error rates) we have \\nlp-ql v+2 ~dv(p,q)~lp-ql, \\nand that de(P, q) > ! implies IP-ql > E, and is equivalent for p = 0. \\nWe have the bound (Haussler, 1992, Theorem 1) \\nPr{ dv(pmc, pmc) > oc} ~ 2 exp -nvoc2 \\nwhich translates into the sample size bound \\nIogr +log~ \\nn~ 2 voc \\nTo compare this bound with the previous ones, note that \\nPr{lpmc-pmcl >c} ~ Pr{dv(pmc,pmc)>c/(v+2)} \\n~ 2 exp -nvc2 j(v + 2)2 \\nwhich is minimized by v = 2 as 2 exp -nE2 /8. For a Chernoff-like \\nbound we have \\nPr{pmc < (1-y) pmc} ~ Pr{ dv(pmc, pmc) > Y pmc } v +pmc \\n~ 2exp-nv [ ypmc ]2 \\n=2exp[-!ny2pmc] v +pmc The precise form of the \\nbound used here is due \\nto Angluin & Valiant \\n(1979). In Bather (1996, \\np.340) Chernoff says the \\nbound should be named \\nafter Herman Rubin. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 92}, page_content='This concept is \\ndiscussed very clearly \\nby Pollard (1984). \\nAnthony & Biggs \\n(1992) give full but \\nopaque versions of the \\nproofs of Blumer et al. \\n(1989) for the noise-free \\ncase, except for \\nmeasurability \\nconditions. 2.8 Computational learning approaches 81 \\non taking v = pmc. \\nNote that none of the bounds in this subsection depend on the \\nnumber of classes, as they work directly with error events. \\nInfinite number of two-class classifiers \\nA key quantity in the PAC-learning results is the Vapnik-Chervo\\xad\\nnenkis (or VC) dimension d of :F. Consider the set of functions \\n{0, l}n -+ {0, 1} induced by evaluating functions in :F at the n points \\nof the training set. (That is, the induced function gives the predicted \\nclass for each of the possible class assignments to the training set.) Let \\nthe number of distinct functions be A(n). In many cases this number \\nwill be 2n for small n, since all possible functions are induced. Let d \\nbe the largest value of n such that A(n) = 2n for some training set of \\nsize n, or infinity if there is no such number. Then it turns out that for \\nd < oo we have \\n(2.49) \\nthe last inequality holding for n ~ d ~ 1 (Blumer et al., 1989, Propo\\xad\\nsition A2.1). This result is sometimes called Sauer\\'s lemma; its history \\nis traced by Assouad (1983). Note that these bounds apply to all train\\xad\\ning sets; from now on we will use A(n) to denote the maximum over \\ntraining sets of size n, and to use the results we will replace it by one \\nof the bounds in (2.49). \\nWe will give the most precise results available for two classes (from \\nBlumer et al., 1989) then sketch how they are derived. (There are benign \\nmeasurability conditions which we ignore.) \\nAs a simple example of the VC dimension, consider [!( c: Rm and \\nclassifiers of the form m \\nsign(LaiXi >b) \\ni=l \\nwhich we shall meet under the name of perceptrons in Section 3.6. \\nCover (1965) showed that these rules have VC-dimension m + 1 (as \\nfollows from Proposition 3.1 on page 119). On the other hand, for \\nbinary inputs, there are between 2m(m-l)/2 and 2m2 different functions \\ngenerated by perceptrons (Muroga, 1971), so the bound given by {2.48) \\nis of the form \\nm2 log 2 + log t n ~ ------\"\\xad\\n€ \\nThe following proposition usually gives considerably tighter bounds: '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 93}, page_content='82 2 Statistical Decision Theory \\nProposition 2.5 (Blumer et al., 1989) \\nLet d denote the (finite) VC dimension of ff. \\n(i) Given 0 < E\\' < 1, the probability that there is a classifier g E ff \\nconsistent with n training examples and with true error rate greater than \\nE\\' is bounded above by \\n(ii) If \\n(4 2 8d 13) 4 [ 2 12] n ~max ; log2 b\\'-; log2-; or n ~; log2 b + dlog2-; \\nthe bound in (i) is less than (J. \\n(iii) For given 0 < E\\' ~ 1/8, (J ~ 1/100, d ~ 2 and \\n(1-E\\' 1 d-1) \\nn < max -E\\'-log 2 b\\' 32E\\' \\nfix an algorithm to select a classifier g for each possible training set \\nof size n. Then there is a probability distribution on PI x { 0, 1} and a \\nfunction f E ff which correctly classifies examples with probability one, \\nbut the probability exceeds (J that the algorithm gives a classifier with \\nerror rate exceeding E\\'. \\nThe lower bound in (iii) was shown by Ehrenfeucht et al. (1989) by \\nexhibiting a suitably malicious distribution, which concentrates on d \\npoints and gives probability 1 -8E\" to one of them. The conclusion \\nfrom the proposition is that to achieve a high-probability guarantee \\nof an error rate of less than E\\' we must take the size of the training \\nset to be at least of order d/E\" log(d/E). This is, however, very much \\na worse-case bound, and such empirical evidence as there is (such as \\nCohn & Tesauro, 1992) suggests that practical performance is closer to \\nthe lower bound given by (iii), and can even be well below that bound \\nfor any \\'normal\\' distribution of examples. \\nSimilar results are known for the case with no perfect classifier: \\nProposition 2.6 Let d denote the (finite) VC dimension of ff. \\n(i) (Vapnik & Chervonenkis, 1971) For any E\\' > 0 \\nPr{ sup lpmc(g)- pmc(g)l > E\\'} ~ 4L\\\\(2n) exp[-nE2 /8] \\ngES&\\' \\nand the probability is less than (J if \\n16 [ 4 32e] n ~ E\\'2 log b + d log E\\'2 . (2.50) The constants in (2.50) \\ncan be improved. The \\nfactor 1/8 in the \\nexponent can be \\nremoved (Parrondo & \\nVan der Broeck, 1993) \\nat the expense of \\nincreasing the constant: \\nthe claims of Vapnik \\n(1995, pp. 66, 85) are \\nnot supported by the \\nbelated proofs in \\nVapnik (1998). '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 94}, page_content='2.8 Computational learning approaches 83 \\n(ii) (Vapnik, 1982, with a slight improvement by Anthony & Shawe\\xad\\nTaylor, 1993) For any 11. > 0 \\n{ pmc(g) -piTie(g) } Pr sup > 11. ::( 4Ll{2n) exp -ina2 \\ng .jpmc(g) (2.51) \\nand the probability is less than <5 if \\n8 [ 4 16e] n ~ 11.2 log J + d log 11.2 . \\n(iii) (Blumer et al., 1989) Given 0 < E, y ::( 1, the probability that there is \\na classifier g E ~ with true error rate pmc exceeding E and apparent \\nerror rate piTiC < (1-y)pmc is bounded above by \\n4Ll(2n) exp -h2nE. \\nThis probability is less than (J if \\nn ~max(+ log~, 1\\n~d log 1\\n26) or n ~ --:.-[log~+ dlog 1\\n~e] . YE u YE YE YE u YE \\n(iv) (Haussler, 1992, Theorem 3) For any 11. > 0 \\nPr{ sup dv ( piTie(g), pmc(g)) > 11.} ::( 4L\\'l(2n) exp[-!nva2]. (2.52) \\ngE.?F \\nIf we can find a classifier g E ~ which fits the training set exactly \\nwe could apply Proposition 2.5(i) or Proposition 2.6(iii) with y = 1, \\nbut the bound given by Proposition 2.5 is smaller. Part (iv) implies \\nsomewhat weaker versions of parts (i) and part (iii) with factors 1/16 \\nand 1/8 in the exponent (using v = 2,11. = E/4 and v = E,IJ. = y/2 \\nrespectively). \\nProposition 2.6(i) has been used to give upper bounds for PAC\\xad\\nlearning by Pearl (1979) and Abu-Mostafa (1989). This bound can be \\nimproved for large nE2; Devroye (1982) has \\nPr{ sup lpiTie(g)- pmc(g)l > E} ::( 4e4€+4\\nf\"2 Ll(n2)exp[-2nE2] (2.53) \\ngE.fF \\nand Alexander (1984) has \\nPr{ sup lpiTie(g)- pmc(g)l > E} ::( 16(ynE)4096(d+l) exp[-2nE2] (2.54) \\ngE.fF \\nfor nE2 ~ 64, which translates to a guarantee for \\nnE2 ~ [log ~6 + 1024(d + 1)log 2048~ + l)] ,64. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 95}, page_content=\"84 2 Statistical Decision Theory \\nSo far we have considered these results as bounding the size of the \\ntraining set. It is possible to change our point of view and seek upper \\nbounds on the true error rate; in fact given the probability framework \\nwe will have (conservative) upper confidence limits corresponding to \\nprobability 1 -!J. From (2.48) we have an upper bound of the form \\n(log r -log !J)/n if we can always fit n training cases exactly. From \\npart (i) of Proposition 2.5, in the same case the upper bound is \\n2d 2ne 2 2 \\ne ~ -; logz d + n logz b \\nwhich shows convergence at slightly less than 0(1/n). On the other \\nhand, if the Bayes risk is non-zero, we obtain convergence at rates \\naround 0(1/ ..[ri) from Proposition 2.6 parts (i) and (iii). These give \\nlpmc-pmcl ~ -d log -+ log -8 [ 2ne 4] \\nn d lJ \\n--· 4 [ 2ne 4] pmc -pmc ~ .JPrliC n d log d + log b . \\nDevroye (1988) considered the expected maximal difference (over \\nclassifiers) between pmc and pmc rather than confidence limits for \\npmc, and also looked at the direct calculation of d(m) for practical \\nfamilies of classifiers. \\nOne way to look at these results is in terms of empirical risk min\\xad\\nimization. In the noise-free case we select a classifier which makes the \\nminimum number (zero) of errors on the training set. For the noisy \\ncase it is convenient to choose a classifier with the same property, as \\nthen the upper confidence limits are tightest on the true error rate. This \\namounts to a parameter estimation strategy, although often it will not \\nlead to a unique parameter estimate. If we then consider families §m \\nof models of increasing flexibility, we expect to obtain a lower apparent \\nerror rate as m increases, but a confidence limit on the error rate which \\nwill decrease and then increase. Vapnik's (1982, 1992) structural risk \\nminimization chooses the model class to minimize this bound. (Note \\nthat minimizing bounds, especially those as loose as these appear to be, \\nmay not be a good idea!) \\nOutline of proofs \\nWe will only give the main ideas of the proofs, omitting details of \\nmeasurability. It may be puzzling that d(2n) appears in Propositions 2.5 \\nand 2.6 since we have only n training samples. The reason is an idea \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 96}, page_content=\"2.8 Computational learning approaches 85 \\nthat Pollard (1984) calls symmetrization. Suppose we consider two \\nindependent training sets of size n. Let 11 be the true error rate and rfi \\nbe the apparent error rates on the two sets. Then \\nPr{sup 1111-111 > e} ~ 2Pr{sup 1111-1121 > !e} (2.55) \\ngE.'F gE.'F \\nfor n > 2je2. This reduces the computation to comparing two indepen\\xad\\ndent training sets of size n, thus to events on a training set of size 2n. \\nThis will be used in the proof of (2.50). \\nProof of (2.55): \\nFix a classifier with true error rate 11 and consider n1Ji> which has a \\nbinomial (n, 11) distribution. By the Bienayme-Chebychev inequality, for \\nn > 2/e2 we have \\nNow condition on the first sample and choose a g which maximizes \\nthe left-hand side of (2.55). Then conditionally we have \\nPr{ I1J2(g)- Yf(g)l ~ !e I g} ~ ! \\nsince the second sample is independent of the first. Thus unconditionally \\nand \\n~ Pr{ I1J1 (g)-Yf(g)l > e, I1J2(g)- Yf(g)l ~ !e} \\n~ !Pr{l171(g)- Yf(g)l > e} = !Pr{ sup ll11-111 > e} \\ngE.'F \\nwhich suffices. D \\nNow consider the class /Fe of classifiers with true error rate 11 at \\nleast e; we will show \\nPr{1J1 = 0 for some g E /Fe} \\n~ 2 Pr { 1J1 = 0, 1J2 ~ 11/2 for some g E /Fe} (2.56) \\nfor n > 8/e. Again condition on the first sample and choose g E /Fe \\nwhich is consistent with it (if possible). Then conditionally \\n(~ 1 I ~ =) p (I~ I 1 ) 4(1-'1) 4 1 Pr '72 < 2'1 g E :#'e ~ r '72-11 > 2'1 ~ ~ -~ 2· \\nn17 ne \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 97}, page_content=\"86 2 Statistical Decision Theory \\nThus as before \\nPr{17t = 0,172 ~ !11 for some g E 37'e} ~ Pr{17t(g) = O,l72(g) ~ !11(g)} \\n~ !Pr{17t(g) = 0}. \\nConsider the right-hand side of (2.56). We condition on the locations \\nof the 2n training points, and consider only their order. Of the \\n22n possible assignments by classifiers in 37' of labels to points, at \\nmost ~(2n) will occur. Suppose there are t erroneous assignments \\nof labels by an eligible classifier; we must have t > !n11 ~ !nf'. \\nThe probability (over permutations) that these all occur in the second \\nsample is (;);e;) ~ 2-t ~ 2-ne/2, so the probability of the event \\n{17t = 0, ll2 > 1112 for some g E 37'e} is at most ~(2n)2-ne f2\\n, which \\nwith (2.56) establishes part (i) of Proposition 2.5 for nf' > 8, hence \\n~(2n) > 8. The (uninteresting) remaining cases can be proved by \\nshowing Pr{l72 < !11 }~! actually holds for n > 2/€. \\nPart (ii) of Proposition 2.5 follows from part (i) using the second \\nbound in (2.49). To cover all the cases, consider a bound of the form \\nA~(2n)e-Bn ~A (2~n) d e-Bn ~b. \\nNote that logx ~ x-1 can be manipulated to give logx ~ Cx-logCe. \\nTake C = r~.Bjd for 0 < r~. < 1. Then \\nr~.Be \\nrt.Bn ~ dlogn +log d' \\nso it suffices to choose n satisfying \\nA 2 (1- r~.)Bn ~log b + dlog r~.B· \\nThe inequalities come from r~. = 1/2 and 8/ log 2 ~ 12. \\nWe return to (2.55). We condition on the 2n examples and only \\nconsider their order. Let ei = I (error on sample i ). Then half the \\nright-hand side of (2.55) is \\nn \\nPr{ sup I L)ei- ei+n)l > nf'/2}. \\ngE§ i=l \\nConsider random permutations of the total sample. The terms ei -ei+n \\nare bounded by ±1 and have a symmetric distribution with mean zero. \\nWe need only consider permutations which swap elements between the \\ntwo sets, so consider Yi = ±(ei-ei+n) independently with probability \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 98}, page_content='2.8 Computational learning approaches 87 \\na half for each sign. Hoeffding\\'s (1963) inequality asserts that the \\nprobability over these permutations is bounded by \\nn \\nPr{jl:.::Yij > 1nc} ~2exp-2n(c/2)2/4=2exp-nc2/8 \\ni=l \\nand averaging over the remainder of the permutation distribution does \\nnot affect the bound. As before, allowing any g E .\\'F gives rise to at \\nmost ~(2n) assignments of errors. With (2.55) this gives (2.50). \\nPart (iii) of Proposition 2.6 follows immediately from part (ii) on \\ntaking rx2 = y2c. (The reduction of the constant to 4 is from Anthony \\n& Shawe-Taylor, 1993.) We will prove part (ii). Choose g E .\\'F as \\nbefore, and assume 1J = pmc(g) > rx2 and nrx2 > 4, or the bound \\nis trivial. Since i]2 has a binomial (n, 1J) distribution with n1J > 4, \\nPr{ih > 1J} ~ 0.32768 > 1/4. (The worst case occurs with 1J = \\n1 -1/n and n = 5, hence this value.) Now ift < 1J -rxJil and \\nif2 > 1J imply if2 -ift > rxJ[1(ift + if2) J. (Show this by minimizing \\n[if2-ift]/rxJ[1(ift + if2)] over ift, which is clearly achieved by taking \\nthis as large as possible, and its bound is least restrictive if 1J = \\'h.) \\nThus \\n~ Pr{ift < 1J -rxJii} ~ Pr{ift < 1J -rxjii, if2 > 1J} \\n~ Pr{if2-ift > rxJ[1(ift + if2)]}. \\nAs before we consider random swapping permutations. If all Yi are \\nzero then if2 = ift and the probability is zero, so suppose there is at \\nleast one Yi + 0. Hoeffding\\'s inequality (1963, Theorem 2) gives \\nn 2n \\nPr{if2-if1 > rxJ[1(ift + if2)]} = Pr{L Yi > rxJ[1n Lei]} \\ni=l i=l \\n2nrx21 L~~ ei \\nS:: exp 2 z-l = exp \\n\"\"\" 42:¥/ 2 \"\\'2n \\nnrx L....i=l ei s:: exp _lnrx2 \\n4l:IYil \"\"\" 4 \\nMore general problems \\nThe methods based on VC dimension are confined to two classes, since \\nthe VC dimension itself is. There are several extensions we might need \\nto consider: \\n1 more than two classes; \\n2 loss functions other than the error rate; '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 99}, page_content=\"88 2 Statistical Decision Theory \\n3 characterizations other than the VC dimension, since this is either \\nunknown or infinite for many of the classifiers we would want to \\nconsider in practice. \\nThese are beginning to be addressed, notably by Haussler (1992). An \\nalternative approach for multiple classes is to use the VC dimension of \\nthe graph of the classifier, a subset of [!( x ~. as in Shawe-Taylor & \\nAnthony (1991). \\nThe loss functions we consider will be bounded by 0 and 1. Since \\nour decision space is finite, this amounts to a possible re-scaling, but \\nexcludes working with the deviance, for example, unless probabilities are \\nbounded below. Techniques are available for unbounded loss functions \\n(Pollard, 1990). \\nThe general technique to replace the VC dimension is one of ap\\xad\\nproximating the infinite class of functions by a finite €-cover, that is \\na set of functions such that any function is within distance € of a \\nmember of that set. Let %(€) be the smallest number of points in \\na €-cover, which is closely related to the maximum number At{€) of \\npoints at least € apart which can be packed in: in fact \\nJt(2€) ~ %(€) ~ At(€). \\nThen the bounds will be in terms of the expected value of % applied \\nto the family of functions evaluated at the training set. For example \\n(Haussler, 1992, lemmas 13 and 14) \\nPr{ sup dv(R(g), R(g)) > o:} ~ 2E %(o:v /8) exp -o:2vnj8 \\ngE$' \\nwhere R and R are the risk and estimated risk respectively , and L1 \\ndistance is used between loss functions. Very similar ideas occur in the \\nmethod of sieves (Grenander, 1981; Geman & Hwang, 1982). \\nThe concept of pseudo-dimension (Pollard, 1990; Haussler, 1992) \\ngeneralizes the VC dimension and provides a convenient way to bound \\ncovering numbers. Consider orthants of IR.P, possibly shifted to a new \\norigin. The pseudo-dimension is the largest n for which there is a \\ntraining set of n points such that the loss functions evaluated at those \\npoints meet all the orthants for some origin. Clearly for {0, 1 }-valued \\nfunctions the pseudo-dimension is the VC dimension. If the set of \\nloss functions for all rules under consideration has pseudo-dimension \\nd then (Pollard, 1984, p. 27; Haussler, 1992, Theorem 6) \\n[2e 2e] d .!V(€) ~ At(€) < 2 -;log-; Such notions are often \\nreferred to as metric \\nentropy. \\nAn orthant is specified \\nby giving the sign of \\neach coordinate. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 100}, page_content='2.8 Computational learning approaches 89 \\nfor any training set, where e is measured in an Lt distance. These two \\nresults combine to give the bound \\n8 [ 8 8e] n ~ -2-log ~ + 2d log -\\n0( v u ocv \\nto ensure that Pr{dv(R(g),R(g)) > oc} with probability at least 1-~-'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 101}, page_content=''),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 102}, page_content=\"3 \\nLinear Discriminant Analysis \\nIn this chapter we discuss methods which arise in statistics and in \\npattern recognition based on linear combinations of the feature vectors \\n(so we assume that the feature space f!l is contained in 1lV ). These \\nmethods provide the templates for generalization to flexible non-linear \\nmethods discussed in the next two chapters as well as being of interest \\nin their own right. \\nWe can identify three distinct ways in which the idea of approxi\\xad\\nmating a function f from f!l to IRK can be used to produce a classifier, \\nalthough all have variations on their themes. \\n1 Take fk(x) = p(k I x) = E[J(Y =k) I X= x] and f(x) = (fk(x)). The \\nBayes rule chooses a maximizer of fk(x). Define target tk to be the \\nkth unit vector. Since \\nIIYII denotes the norm llf(x) _ tk112 = -2fk(x) + 1 + llf(x)ll2 \\nof a vector y . \\nThe L1 distance \\nbetween x and y is \\nLilxi-yi[. the Bayes rule amounts to choosing the nearest target to f(x). This \\nleads to ways to approximate by f(x; 8) based on choosing e to \\nmake the predictions for the training set as close as possible to the \\ntargets. \\n2 Dietterich & Bakiri ( 1991, 199 5) consider coding the class targets \\ntk to be widely spaced in :!l' = {0, 1 }m for m > k, and learning a \\nfunction f from f!l to [0, l]m. The classifier then chooses the nearest \\ntarget in :!l' to the prediction f(x) for a new example. The actual \\ncoding is done using error-correcting codes, and the distance is L1• \\nWe can view this approach as training a classifier for m pseudo\\xad\\nclasses, and then mapping the distribution over pseudo-classes to \\nthe K real classes. \\n3 We have seen that the Bayes rule maximizes log p(k I x), and the \\nmultiple logistic model (2.29) is a linear model for these log posterior \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 103}, page_content=\"92 3 Linear Discriminant Analysis \\nprobabilities. Variants which are less principled but commonly used \\nare separate logistic models of each class versus the rest or versus a \\nreference class (see Section 3.5). \\nWe vary slightly our usual notation: there is a training set of n ob\\xad\\nservations (or examples) ofa p-variate pattern, and these observations \\nare classified into g groups. Note that the groups need not coincide \\nwith the classes, and in the less flexible methods of this chapter it may \\nbe desirable to split some of the classes. For example, in symbol recog\\xad\\nnition we might divide the class for sevens into crossed and uncrossed \\nsevens. If we let the classifier choose the best group and then assign to \\nits class we would be using a cost structure which penalizes the wrong \\nchoice of group rather than class. The cost structure based on groups \\ncorresponds to adding posterior probabilities over groups to form the \\nposterior probability for the class, then choosing the class with the \\nlargest posterior probability. \\nLet X denote the n x p matrix of examples, and G the n x g \\nmatrix of indicator functions for the groups (i.e. gij = 1 if and only if \\nobservation i belongs to group j ). Note that GT G = diag (ni), where \\nni is the number of observations for group i. A typical example will Remember x is a row \\nbe denoted by x and is a row vector where necessary (as it is a row of vector. \\nX ), and T denotes the transpose of a vector or matrix. \\n3.1 Classical linear discrimination \\nWe will normally assume (to ease the notation) in this section and in \\nSection 3.2 that X has been centred; each feature variable has had its \\naverage subtracted. \\nWe saw on page 36 that if we assume the probability model in which \\nthe observations for group j are normal with mean l'j and common \\ncovariance matrix I:, the Bayes rule is to allocate a future observation \\nx to the group for which \\n-2 logp(j I x) = (x-P.j)I:-1(x-l'jf-2 log 1tj + const (3.1) \\nis smallest. The first term on the right is known as the Mahalanobis \\ndistance from x to the group mean. Expanding this out we find \\n-2logp(jlx) = -2xi:-1p.J +p.ii:-1p.J -2logni+const+xi:-1xT (3.2) \\nwhich is a linear term in x plus a quadratic term which does not \\ndepend on the class. Since we wish to maximize p(j I x) or equivalently \\nto minimize (3.2), we may as well maximize the linear terms \\nLDAj = 2xi:-1 p.J-Jlji:-1 p.J + 2 log 1tj. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 104}, page_content='3.1 Classical linear discrimination 93 \\nThe space f!{ = JR.P is partitioned by hyperplanes, another sense \\nin which this is linear discrimination. For the special case of g = 2 \\ngroups, comparing LDA2 with LDA1 amounts to computing the linear \\nfunction LDA2-LDA1 and choosing group 2 if and only if it is positive. \\nMore generally, the comparison can be done in a space of dimension \\nat most g -1, and the distances computed in such a space, as we shall \\nshow more formally. \\nIn practice the population quantities /lj,\"f. are estimated by mj, W, \\nwhere W is the within-group covariance matrix defined below, rather \\nthan the maximum likelihood estimates considered in Section 2.2. \\n(Other estimates are considered in Sections 2.4 and 2.5, but very rarely \\nused in applied statistics.) \\nFisher\\'s linear discriminant \\nThe classical method of linear discrimination was described by Fisher \\n(1936) for two classes and extended to more by Rao (1948) (but some\\xad\\ntimes attributed to Bryan, 1951). It uses a different criterion not based \\non the decision theory of Chapter 2; it seeks a linear combination xa \\nof the variables which maximizes the ratio of its between-group vari\\xad\\nance to its within-group variance. This is appealing even if multivariate \\nnormality is implausible. \\nThese terms come from the analysis of variance and are defined for \\na variable y = (yi) as follows. Let mj denote the mean of y in group \\nj, let m = (mj) and let [i) denote the group of observation i. The \\nn x g matrix G indicates which group each observation belongs to, so \\ngij = I (j = [i] ). Then the within-group variance is defined to be \\nW _ Li(Yi- m[iJ)2 IIY-Gmll2 \\ny-n-g -n-g \\nand the between-group variance is \\nB _ Li(m[iJ-:Yf = \\ny-g-1 IIGm-ylf \\ng-1 \\nWe can extend these definitions to the multivariate observations X by \\ndefining M as the g x p matrix of group means and \\nW =(X -GMf(X -GM) \\nn-g \\' B = (GM -lxf(GM -lx). \\ng-1 \\nThen the linear combination xa has variances aT W a and aT Ba, and \\ntotal variance \\nTs _ r(n-g)W+(g-1)B a ra-a 1 a. n-'),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 105}, page_content=\"94 3 Linear Discriminant Analysis \\nIt may be that the training sample is known not to be a random \\nsample from the underlying distribution, but that the numbers nj of \\nobservations from each group were pre-specified. (This is often done \\nto ensure that sufficient information is obtained on rare groups.) Then \\nbetter estimates are obtained by weighting the observations in group \\nj by nnj/nj in forming B, W and x. The interpretation remains \\nunchanged. (Note that the maximum likelihood estimates are not \\nweighted in this circumstance; the weighted versions are thought to be \\nmore accurate when the group covariance matrices actually differ.) \\nThe classical computational approach is to seek a rescaling of \\nthe variables xS such that their within-group covariance matrix is \\nthe identity matrix I and then perform an eigenvalue decomposition \\nof B expressed on these variables. It will suit some of our further \\ncomputations better to rescale the variables so that the total variance \\nis nl /(n -1). The rescaling is achieved by taking a matrix S such that \\nsrxrxs = nl. This can be done in a number of ways: a simple one \\nis to use the QR decomposition (Golub & Van Loan, 1989) of X: \\nQX = [ ~] \\nwhere Q is a n x n orthogonal matrix and R is a p x p upper triangular \\nmatrix. Take S as the solution to RS = Jill. Then on the rescaled \\nvariables X' = X S we have X' T X' = nl. There will be difficulties if \\nthe covariance matrix does not have full rank, and this can be hard to \\nidentify numerically. (For example, a column differing only in the fifth \\nsignificant digit could be constant up to rounding error or could be an \\nextremely precise measurement of, say, refractance.) \\nA fairly safe procedure is first to rescale all variables to unit vari\\xad\\nance (and detect any constant variables) then to use the singular value \\ndecomposition X = U A V T. Then small singular values correspond to \\nnearly constant combinations. We would take S = JiiV A -1, but small \\nsingular values should cause concern, since they correspond to linear \\ncombinations which are nearly constant and whose varia·nce is likely to \\nbe determined inaccurately from the training set. \\nWe now work with these rescaled variables. The matrix GT G is \\ndiagonal containing the numbers nj of observations on each group. \\nLet T = diag ( y'nlnj) so \\nTGTGT = nl. \\nThe group means are given by the g x p matrix M = (GT G)-1GT X= \\nn-1 T2GT X. Since X has been centred the column sums of M See the glossary. \\nModified procedures \\nare discussed in \\nSection 3.4. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 106}, page_content='3.1 Classical linear discrimination 95 \\n(weighted by group size) are zero, hence M has rank r ~ min (p, g-1 ). \\nConsider the singular value decomposition of r-1 M = U A V T. Since \\nwe do not wish to assume that either g or p is larger, we will assume \\nthat U is g x r, A is r x r and V is p x r. \\nFrom the singular value decomposition we find \\n(g -1)B = (GM)T(GM) = VATuT(TGTGT)UAVT = nVA2vT \\n(n-g)W = XTX -(g -1)B = nl -nVA2VT = nV[I -A2]VT \\nwhich incidentally shows that the singular values are at most one. (Note \\nthat one can occur; it corresponds to a linear combination which is \\nconstant within the groups but has different values on two groups. If \\nso it is the desired linear combination.) The original problem reduces \\nto finding a linear combination xa of the rescaled variables which \\nmaximizes the ratio \\naTV[I -A2]VTa· \\nLet b = yT a. The ratio is I: Afhf I l.:(1-Af)bf, which is maximized \\nby taking only b1 non-zero. Thus on the original variables a is \\nproportional to the first column of SV. The linear combination is \\nunique up to a scale factor unless A2 =AI. \\nThe linear combination found by this process is called the first linear \\ndiscriminant or the first canonical variate. Subsequent columns of S V \\ngive further linear discriminants which maximally separate the group \\nmeans subject to being uncorrelated with previous linear discriminants \\n(since on the variables rescaled by SV both the W and B covariance \\nmatrices are diagonal). The first J ~ r linear discriminants maximize \\nthe ratio of the determinants of the between-group to within-group \\ncovariance matrices for J -dimensional linear transformations of the \\noriginal variables. (This follows immediately since the determinant is \\nthe product of the eigenvalues.) \\nThe linear discriminants are usually scaled so that they have within\\xad\\ngroup variance one (unless constant on groups). We have only defined \\nr of them but a further p-r can be chosen by taking further columns \\nwhich are orthogonal to the columns of V. \\nFor the linear discriminant variables the group means differ only in \\nthe first r variables. The quantity (n-g)Af /(g -1)(1-Af) measures the \\nratios of the between- to within-group variances on the i th canonical \\nvariate. We can show graphically the difference between groups by \\nplotting the data on the first few canonical variates, often the first \\ntwo. Although the original probability model corresponds to classifying \\nusing all r dimensions, it may be better not to use canonical variates '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 107}, page_content=\"96 3 Linear Discriminant Analysis \\nwhich have low discrimination between groups, so we may wish to \\nuse only those dimensions for which the ratio of between-group to \\nwithin-group variance is appreciable. \\nLet A be the matrix whose columns define the linear combinations \\nfor the canonical variates, specifically \\nA = diag ( J(n-g)/n(l -~1)) S V. \\nThen the transformed variables XA minus the appropriate group means \\nare uncorrelated with unit variance (since AT W A = I). Thus on the \\ncanonical variates, Mahalanobis distance is Euclidean distance. Since \\nthe group means differ only on the first r variates, the Mahalanobis \\ndistances to the group means can be computed from the distance in the \\nfirst r dimensions plus a quantity from the remaining dimensions which \\ndoes not depend on the group. The Bayes rule minimizes the Euclidean \\ndistance to the mean in the first r dimensions minus 2log 1tj. With just \\ntwo groups this process does find the linear variable LDA2-LDA1, \\nfor it computes distances only in the first dimension, the only one on \\nwhich the means differ. \\nSo far we have only considered finding the linear combination(s) \\nrequired. Fisher's procedure cannot tell us the threshold between the \\ntwo groups in classification. It seems common practice to classify by \\nchoosing the group whose mean is nearest in the space of canonical \\nvariates. Since in that space Euclidean distance is the within-group \\nMahalanobis distance, this corresponds to the Bayes rule if (and only \\nif) the prior probabilities are equal. \\nThe problems of rank-deficiency and various solutions are discussed \\nby Cheng et al. (1992); the solutions given here are much simpler and \\nmore transparent. \\nCanonical variates and canonical correlation \\nThe name 'canonical variate' comes from a connection with canonical \\ncorrelation analysis, which seeks linear combinations xa and yb of \\nmaximal correlation. Since X is centred we have \\nbryrxa \\ncorr ( xa, yb) = -----,=::=============== jbTVar(Y)b aT(XTX)a JbT Var(Y)b nllall2 \\nNow if we take Y = G we have \\nGTX = (GTG)M = (GTG)TUAVT = nT-1UAVT. (3.3) \\nWithout loss of generality we can centre Y. Let b' = urr-1b. Then \\nVar(yb) = IIGbll2 = n11T-1bf = nllb'll2 and bTyTxa = nb'T AVTa, This is a form of \\nshrinkage: see \\nSection 3.4. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 108}, page_content='Figure 3.1 : Linear \\ndiscriminant plots for \\nthe Leptograpsus crab \\ndata. The left plot is \\nfrom the original \\nvariables and the right \\nplot from variables on \\nlog10 scale. The blue \\nspecies is shown by \\ntriangles, the orange \\nspecies by squares, and \\nthe symbols for females \\nare filled. \"\\' ... 1U \"E: \\n~ \"\\' \\n~ . § 0 \\nc \\n~ \"\\' \"0 • c 8 9l ..,. 3.1 Classical linear discrimination \\no\\'bo \\n0 q, !; \\n!; co \\'\"\\'c \\'b \\n\\')~\\\\\\' ~~~ \\nc c. o • \\n... ..... ·(·· ~· •t1· • ··\\'=\" •• . \\n·5 0 5 \\nfirst canonical variate ... * \"E: \\n~ \"\\' \\n~ <: 0 g \\n~ \"\\' \"0 • \\n<: \\n0 \\n~ ..,. . \\n..... I I \\n·-I o I :\\'\": o • I I \\' .. \\n\\' • \\n~ ~ ~ ~ 0 2 4 6 \\nfirst canonical variate 97 \\nso the correlation is maximized by taking xa as the first canonical \\nvariate and b\\' proportional to the first coordinate· vector ( 1, 0, ... ) T. \\nThen b is the first column of E> = T U and the correlation achieved \\nis ,tt, Subsequent combinations with maximal correlation are given \\nby subsequent discriminant variables and columns of E>, and achieve \\ncorrelations A.;. We refer to the columns of e as scores. Note for future \\nuse that (GE>f(GE>) = nl and so the scores are uncorrelated and have \\nsum of squares n over the training set. \\nExamples \\nThe crabs data are shown on the first two linear discriminants in \\nFigure 3.1. As the measurements are lengths, we also considered \\ntaking logarithms, and as the figure shows this does increase slightly \\nthe separation between the groups. As there are four groups, the \\nlinear discriminants span three dimensions, and for the variables on \\nlog scale the ratios of between- to within-group standard deviations \\nare 25.5, 16.9 and 2.9 on the three discriminants. Thus the first two \\nlinear discriminants explain 99.1% of the variance between the groups. \\nClearly the first linear discriminant expresses the difference between the \\nspecies, the second that between the sexes. On log scale they are given \\nby \\nLt = 72FL + 22RW + 22CL -151 CW + 41 BD \\nL2 = -6 FL -56 RW + 88 CL -49 CW + 13 BD \\nwhich shows that the differences between the sexes are principally in the \\nratio of length to width, and that the blue form has a wider carapace \\nthan the orange form relative to its other measurements. \\nNext consider the forensic glass dataset. Figure 3.2 shows the first \\ntwo canonical variates which account for 93.14% of the between-group \\nvariation. The variables are not on a common scale, but we can rescale '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 109}, page_content='98 3 Linear Discriminant Analysis \\n\"\\' \\n\"\\' 1ij ..,. \\n·~:: \\n\"\\' > \\n~ ·c: C\\\\1 0 \" ~ \\n\"0 \" 8 0 \\n\"\\' (/) 2 4 6 \\n2421:42444 \\n~ 4 \\n2 6 5 4 \\n1 ~~2 2 2 \\n2 2 \\n2 65 2 \\n1 ~· ~ 1 1,11 31!1 1 \\'!; 31 3 1 \\n\\'l\\' e3 \\n\\'I\" \\n-5 0 5 \\nfirst canonical variate \\neach variable to have unit variance. The weights given by the first two \\ncanonical variates are then \\nRI Na Mg Al Si K Ca Ba Fe \\n0.95 1.94 1.07 1.67 1.90 1.02 1.43 1.15 -0.05 \\n0.09 2.58 4.31 0.86 2.33 1.21 3.38 1.71 0.02 \\nso the amount of iron appears to be unimportant, and the two most \\nimportant variables in the second variate are magnesium and calcium. \\nHowever, as the sum of the compositions is close to 100%, the variables \\nare highly collinear which does not help interpretation. The plot is \\ndominated by groups 4 to 6, with suspicions that groups 4 (containers) \\nand 5 (tableware) are not homogeneous. As the boxplots of Figure 1.5 \\non page 14 show, this problem has a far from normal distribution, and \\nprobably has mixed distributions for some of the composition variables \\n(with a positive probability for zero). Linear discriminant analysis has a \\ncross-validated error rate of 38%. All of the vehicle glass is misclassified \\nas window glass, and there is considerable confusion between float and \\nnon-float window glass. The cross-validated confusion matrix is \\nWinF WinNF Veh Con Tabl Head \\nWinF 47 20 3 0 0 0 \\nWinNF 20 49 0 4 2 1 \\nVeh 11 6 0 0 0 0 \\nCon 0 6 0 6 0 1 \\nTabl 0 3 0 0 5 1 \\nHead 1 1 0 2 0 25 \\nThe predictive form of linear discriminant analysis (page 51) gives \\nalmost identical results. Figure 3.2: Linear \\ndiscriminant plots for \\nthe glass fragments \\ndata. The coding is 1 = \\nwindow float glass, 2 = \\nwindow non-float glass, \\n3 = vehicle window \\nglass, 4 = containers, 5 \\n= tableware and 6 = \\nheadlamps. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 110}, page_content='Figure 3.3: Density \\nestimates of the \\nnon-diabetic (left) and \\ndiabetic group (right) \\non the canonical variate \\nfor the Pima Indians \\ndata. 3.1 Classical linear discrimination 99 \\nSince the within-group Mahalanobis distance is Euclidean distance \\nin the space of canonical variates, it is useful to scale plots so that the \\naxes have equal scales. Then circularly symmetric scatter plots for each \\ngroup indicate that the assumptions of normality and equal covariances \\nare realistic (or not, as in this example). \\nThe Pima Indians diabetes data have two groups, so plots of the \\ncanonical variates are not very useful. The within-group covariance \\nmatrices are quite similar, although blood pressure and pedigree are \\nuncorrelated in the non-diabetics group, and strongly negatively corre\\xad\\nlated in the diabetics group, which generally has slightly higher vari\\xad\\nability. The correlation is due to just one woman, who has the highest \\nobserved pedigree, and the second-lowest blood pressure. \\nStandard linear discrimination makes 67/332 errors on the test \\nset. Choosing a subset of variables by cross-validation on the training \\nset suggests that no reduction is worthwhile . The predictive version \\nmakes one fewer error. Both are making most of their errors (42/109) \\non the group reported to have diabetes. In this example quadratic \\ndiscrimination does significantly worse (84/109), making many more \\nerrors on the non-diabetics group. \\nWe can see something useful by plotting the first (and only) canon\\xad\\nical variate, Figure 3.3, as there is a suspicion of multi-modality in \\nthe diabetics group, and skewness in the non-diabetics. The density \\nestimates used were kernel methods (Section 6.1) with automatically \\nchosen bandwidths . \\n... \\nci \\n\"\\' ci \\n\"\\' ci \\nci \\nq \\n0~---- -------- -------------- ~ \\n·4 ·2 \\nVariable selection \\nIt is sometimes desirable to consider only those variables which make a \\nuseful contribution to discriminating between the classes. For classifica\\xad\\ntion it may be desirable not to have to measure unimportant variables. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 111}, page_content=\"100 3 Linear Discriminant Analysis \\nFor interpretation it may be easier to concentrate on the important vari\\xad\\nables. In either case we need a variable-selection procedure . McKay & \\nCampbell (1982a, b) provide a comprehensive introduction to practical \\nissues in variable selection. \\nComputer programs are widely available to select feature variables \\nto be used in linear discrimination. This is an example of our consider\\xad\\nations of Section 2.6, and the simplest stepwise methods are commonly \\nused. There are two main approaches. One is to use significance tests \\nfor the value of individual features under the normal model (there are \\nmany such tests; McLachlan , 1992, §12.2), the other is to use the error \\nrate (Hermans et al., 1982). \\nCross-validation \\nLinear discrimination is one of those classifiers for which leave-one-out \\ncross-validation can be computed without complete re-fitting (Fukunaga \\n& Kessell, 1971; Hjort, 1986, §12.1). Suppose we wish to re-train the \\nclassifier omitting example Xj which is of class c, and find its predicted \\nclass. To do so we need to update our estimates of the Mahalanobis \\ndistances Ll]k from Xj to the group means l'k· We find \\nfor k i= c. As these formulae do not involve new matrix operations, \\nthey can be computed relatively quickly. \\nFor the best quadratic rule, updating is somewhat easier, as we only \\nhave to update the Mahalanobis distances to the mean of class c plus \\nthe determinant of fc. We find \\nhence \\nwhich can be evaluated with modest additional computation. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 112}, page_content=\"This follows from \\nresults of Efron (1975) \\ndiscussed in Section 2.3. 3.2 Linear discriminants via regression 101 \\n3.2 Linear discriminants via regression \\nConsider first the case of two groups, and let Y be the class indicator \\nfor class 2. Then the posterior probability for class 2 is \\np(21 x) = E(Y I X = x). \\nRemarkably, although the conditional mean is not linear in x, linear \\nregression of y on x can be used to find the linear discriminant. With \\nequal prior probabilities and group sizes, future observations can be \\nclassified by predicting via the linear regression and selecting group 2 \\nif and only if the prediction exceeds 0.5. This was established by Fisher \\n(1936) by a direct calculation, reproduced by T. W. Anderson (1984, \\n§6.5.4) and Hand (1981). We will take another approach which gives \\ngreater insight and provides an extension to more than two groups. \\nThus for two groups, if the normal model for the population holds, it is \\nmore efficient to use linear rather than logistic regression, even though \\nthe population regression E(Y I X = x) is logistic not linear. \\nWe can think of the linear regression as the best linear approxima\\xad\\ntion to the posterior probabilities. As a principle of classifier design \\nthis has been used (Duda & Hart, 1973; Devijver & Kittler, 1982; \\nFukunaga, 1990) under the name of minimum (mean) square error clas\\xad\\nsifiers. Unlike the linear discriminant (for more than two groups), that \\nprocedure classifies by the nearest target or equivalently the largest \\ncomponent. An alternative would be to find the linear classifier which \\nminimizes the total risk (Highleyman, 1962b). This is much harder, and \\nhas only been achieved for two general normal populations (when the \\nBayes rule is quadratic); see Section 2.2. \\nThe manipulations which follow are of some interest, but would \\nnot be used to actually calculate a linear discriminant in preference to \\nthe methods of Section 3.1. Their importance lies in the realization \\nof Breiman & Ihaka (1984) that non-linear regressions could be used \\nin place of linear regression, thus providing one way to use non-linear \\nregressions for classification problems. \\nOnce again we assume that X is centred and that in the algebraic \\nformulae we work with the rescaled variables xS, which have covariance \\nmatrix nl /(n-1). \\nThe derivation of the canonical variates via canonical correlations \\nshows that the 'scores' for classes given by the columns of E> = T U \\nhave a special place, as these are best predicted by the corresponding \\ncanonical variate. However, we can first observe that if we regress the \\nclass indicators G on X using the rescaled variables we have regression \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 113}, page_content=\"102 3 Linear Discriminant Analysis \\ncoefficients \\nusing (3.3 ). Thus the predicted values are of the form x V AA for a full\\xad\\nrank matrix A and so span the same space as the linear discriminants. \\nThis implies that the regression will perform the reduction to r ~ \\nmin(p, g -1) dimensions. (Since X is centred, the predicted values \\ncannot predict a constant term. If g > p+ 1 there will be no reduction.) \\nIf g = 2 then either r = 1 or the groups have the same mean, and we see \\nimmediately that linear regression will find Fisher's linear discriminant, \\nup to a constant factor that is not needed for classification purposes. \\nFor more than two groups Breiman & Ihaka (1984) showed how \\nto find 0 by minimizing the residual sum of squares over the scores \\nas well as the coefficients, but it seems as easy to apply standard \\nlinear discrimination methods to the predicted values. That is, the \\ndata are replaced by the fitted values, classification is done by using \\nMahalanobis distances to the group means based on their within-class \\ncovariance matrix, and lower-dimensional plots can be made after a \\nsingular value decomposition. \\nNote that finding the maximum of the linear functions xp does \\nnot give the linear discriminant classifier, for Pk = nk/(n -l)S:r1mk \\nwhere ST is the total variance matrix, whereas LDAk uses W, the \\nwithin-group covariance matrix. There are, however, ways to calculate \\nthe classifier in the space of predicted values of the regression. \\nTwo groups \\nIn the case of two groups we can achieve a worthwhile simplification. \\nUnless the groups have the same mean, r = 1 and we need only \\nconsider ). = .A.1. There are only two possible scores with mean zero \\nand sum of squares n over the data; e = ±(-~, vn;Jil2f. The \\ncoefficients for the regression of the score variable on X are (using \\n(3.4)) \\nand so the predicted values y are ). times the first column of (xS) V \\nand have within-group variance W = nJ2(1 -J..2)/(n-2). The group \\nmeans are M = TUAVT = 0AVT and these are mapped to MP = \\n0AA = ).28. The correlation achieved is J.., so the residual sum of \\nsquares is 1 -J..2 times the total sum of squares n, and the residual \\nmean square s2 = n(1-J..2)j(n-2). A regression package \\nwill use divisor n -p \\nfor s2. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 114}, page_content='3.2 Linear discriminants via regression 103 \\nThe linear discriminant chooses class 2 if and only if \\n2 2 LDF = ~( )m2-ml -m2-ml 1 7r2 0 Y x W 2 W + og n1 > · \\nNow \\n(3.5) \\nLet us change y from the scores to be the indicator of class 2. This is a \\nlinear transformation, so we can linearly transform the fitted regression. \\nAfter some manipulation we find \\nLDF _ y(x)-1/2 (n-2)(n2 -nl) 1 n2 - - + og-s2 2n1n2 n1 \\nwhich is directly computable from the regression. Note that if we use \\nthe proportions in the training set to estimate the prior probabilities, the \\nconstant term has leading terms in an expansion in powers of 1i2 -1/2 \\nas \\n8(7i2-1/2) -32 (! -!) (7i2-1/2)3 \\nn 3 n \\nso the dividing point for two groups on y(x) will differ negligibly \\nfrom 1/2 unless the proportions in the training set are very different. \\n(Remember that s2 will often be very small since the targets are zero \\nand one and the fit can be very good.) \\nMore than two groups \\nFor more than two groups the simplest procedure is to apply standard \\nlinear discriminant techniques in the space of fitted values. However, \\nBreiman & Ihaka (1984) (who appear not to have realized this) extended \\nsome of the calculations to more than two groups. We work with the \\nregression of G0 on X. Remember that we can replace the data X \\nby the fitted values F of the regression on G on X, and perform a \\ncanonical correlation analysis on these to find 0, since the canonical \\nvariates are linear functions of F as well as of X. \\nThe predicted values are proportional to the canonical variates, \\nso W is diagonal expressed in these variables, and the Mahalanobis '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 115}, page_content='104 3 Linear Discriminant Analysis \\ndistance is a sum of r terms similar to that for the two-group case. Let \\nsr = n(1-.A})I(n-g) be the residual mean square for the ith regression. \\nThe linear discriminant between group t and group s becomes (using \\n(3.5)) \\n~ [(~·( ) -eis + eit) eit-eis (n-g)(8ft-efs)J 1 1rt \\nL.....t Yr X 2 2 + 2 + og \\ni si n ns \\n= ~ [ll(y(x)- ts)diag(1lsi)ll2-n: g 11tsll2] \\n1 [11 ~ d\" I 2 n-g 2] nr -2 (y(x)-tr) mg(1 Si)ll --n-lltrll +logns \\nwhere ts is the row of E> corresponding to group s. Thus we choose \\nthe group t to minimize \\nll(y(x)-tr)diag(1lsi)f- n-g -2lognt (3.6) \\nnr \\nsince lltrf = nlnr. (This is a formula of Breiman & Ihaka apart from \\na difference in divisor for W.) Since \\ndiag(1lsd = J(n-g)ln [1-A2r1/2 \\nthe first term is the Mahalanobis distance between the predicted value \\nand the target in the space of predicted values for the scores E> (since \\nwe saw that (n-g)W = nV[I-A2]VT ). \\nNote that (3.6) corresponds to a distance between the predicted \\nvalues and targets (which are the k th unit vector for class k ), with \\nquadratic form [(n-g)ln]E>[1- A2]-1E>r. (This is not a metric since \\nit ignores variation in the g th coordinate, corresponding to a shift in \\nlevel of all the predicted values.) Thus the linear discriminant chooses \\nthe nearest target in this distance, adjusted by ( n -g) I nr + 2log nt. \\nBreiman & Ihaka noted that the linear combinations E> can be \\nfound by minimizing the residual sum of squares, since for combination \\ne this is \\nwhere ( = ur r-1e, and so is successively minimized subject to II( II = 1 \\nby taking ( as the coordinate vectors or e as the columns of T U = E>. \\nThe condition is that II GO II = 1, so that the scores for the groups have \\nunit variance over the training set. Let F be the matrix of fitted values. \\nThen the problem is to maximize IIF8f subject to II r-1811 = 1, or \\nIIFTU(II2 subject to IIU(II = 1. The solutions are then the columns '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 116}, page_content=\"3.3 Robustness 105 \\nof TV* where v· is the matrix of right singular vectors of FT. But \\nit is simpler to consider the canonical correlations of the fitted values \\ndirectly. \\nHastie et al. (1994) have independently re-interpreted the work of \\nBreiman & Ihaka to the same conclusions, via an entirely algebraic \\nroute. \\n3.3 Robustness \\nWhen the crabs data were re-entered by a clerk she made an error in \\nrow 98 which should have read \\nsp sex FL RW CL cw BD \\nBl F 17.4 16.9 38.2 44.1 16.6 \\nbut was entered as \\nsp sex FL RW CL cw BD \\nBl F 17.4 16.9 438.2 4.1 16.6 \\nWe expected this to be disastrous for the discriminant analysis, but \\nin fact it turned out to make rather little difference to the plots on \\nthe first two canonical variates. The effect of the errors is to inflate \\nthe within-group variance for the variables CL and CW which are then \\nheavily down-weighted in the second canonical variate. It happens that \\nin this example there is enough structure for the remaining variables to \\nshow almost the same discrimination . \\nThis example does suggest that it would be wise to have a robust \\nform of linear discrimination . The choice of canonical variates depends \\non the estimated within-group covariance matrix W and the matrix M \\nof group means. Robust estimators of means and covariance matrices \\nare discussed in Section 2.5, and our experiments used the minimum\\xad\\nvolume ellipsoid method discussed there. \\nThe effect of using robust estimators can be seen for the glass \\nfragments data by comparing Figures 3.2 and 3.4. Even more of the \\nvariation (97.83%) is explained by the first two canonical variates (using \\nthe robust measures of variance) and the central 'core' is more concen\\xad\\ntrated, with one example of class 2 being shown up as a considerable \\noutlier, perhaps closer to class 4 (containers). But what this example \\nshows is that the groups do not have a common covariance matrix. The \\ncross-validated error is much worse at 46.7%. \\nWe mentioned on page 99 that the covariance matrix for the Pima \\nIndians data was influenced by one unusual observation, so we tried l \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 117}, page_content=\"106 3 Linear Discriminant Analysis \\n.. \\n6 2 2 2 2 \\n2~·~~4 \\n2 2 5 \\n1 5 5 \\n2 \\n-10 0 2 \\n~ 2 \\n4 \\n4 4 .. \\n6 2 \\n4 25 \\nfirst canonical variate 6 6 \\n' ' \\n10 \\nrobust estimation. This increased the test-set error rate for linear dis\\xad\\ncrimination considerably (but not significantly) to 76/332. For quadratic \\ndiscrimination there is a large increase to 100/332. Using multivariate \\nt discrimination gives 77/332, better than any other form of quadratic \\ndiscrimination. \\nRobustification of discriminant analysis has been considered a num\\xad\\nber of times in the literature; we first saw it in Campbell (1980b, 1982). \\nBroffit et al. (1980) used trimmed estimators of mean and covariance, \\nbut these are less satisfactory (as they are not affinely equivariant). \\n3.4 Shrinkage methods \\nWhen discussing the best quadratic rule in Section 2.2 we mentioned \\nthat if the training set is not large, we might do better to use the \\nbest linear rule than attempt to estimate all the variance matrices ~k\\xad\\nThere is evidence (for example, Marks & Dunn, 1974) that if the true \\nclass covariance matrices ~k are similar, a linear discriminant may \\noutperform the quadratic discriminant in small samples. This suggests \\nother compromises. We might take some convex combination of the \\nequal and unequal estimated variance matrices, say \\n~ ~ \\n~k(a) = (1-a)nk~k +an~ \\n(1-a)nk +an (3.7) \\nwith parameter 0 < a < 1 chosen to maximize performance (using a \\nvalidation set or cross-validation). We describe this as shrinking the \\ncovariance matrices towards a common value, and hope thereby to \\nobtain a biased but less variable estimator. Figure 3.4: Robustified \\nlinear discriminant plot \\nfor the forensic glass \\ndata. Two points of \\nclass 4 at (-4.7,33.7) \\nhave been omitted. \\nCompare this to \\nFigure 3.2 on page 98. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 118}, page_content='Some accounts just \\nconsider W + ).[, but \\nfinding the linear \\ndiscriminants is \\nunchanged by an \\noverall scale change in \\nw. 3.4 Shrinkage methods 107 \\nWe might wish to shrink our estimator of the common covariance \\nmatrix L. Recall that our algorithm for linear discriminant analysis was \\nfirst to rescale each variable to zero mean and unit variance, and then \\nseek a transformation to variables which are uncorrelated (using xS ). \\nSuppose we use the singular value decomposition X = U AVT to do \\nso; the variables XV are uncorrelated with variances proportional to \\n.A.i, ... , A~, and trace(XT X)= L AJ = np. Thus if the original variables \\nwere positively correlated, there will be some linear combinations of \\nhigh variance and some of low variance. The variance of the latter will \\nnot be determined at all precisely, so it is conceivable that we will find \\nthe first canonical variate taken in the direction of a combination of \\nfeatures that happens by chance to be nearly constant within groups. \\nOne way to avoid this is to shrink the .At towards a common value, \\nand this is precisely the effect of using the eigendecomposition of \\n(1-y)Sr + yi. In linear discrimination it is more usual to shrink W, \\nthat is to use \\nL(y) = (1-y)W + yi. (3.8) \\nOf course, this is only appropriate if the variables have been rescaled to \\nunit variance. Many accounts (for example, Campbell, 1980a) replace I \\nby a diagonal matrix, but this is equivalent to rescaling the variables if \\nwe use the diagonal matrix of the variances of the variables. Campbell \\n(1980a) suggested that finding the first canonical variate to be in a \\ndirection of low within-group variance was common in applications, \\nand that shrinkage was important in interpreting the coefficients of the \\nlinear discriminants . \\nBoth aspects of shrinkage were considered by Friedman (1989) \\nunder the name of regularized discriminant analysis. He took a convex \\ncombination of the within-group and pooled covariance matrices to \\nestimate Lk. and then shrunk that estimate, to obtain a shrunken \\nquadratic discriminant analysis. Specifically, he used a plug-in quadratic \\nrule with \\nI:k(a, y) = (1-y)I:k{a) + 2:. trace[Lk{a)] I \\np \\nI:k(a) = (1 -a)nki:k +ani:. \\n(1-a)nk +an \\nThe parameters a and y are chosen to minimize the cross-validated \\nerror rate; Rayens & Greene (1991) point out that the minimum will \\noften occur over a wide range of values of (a, y ). The largest value of \\n(a, y) is chosen (in lexicographical order, so first the largest a for any \\ny, then the largest y for that a). '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 119}, page_content=\"108 3 Linear Discriminant Analysis \\nThe idea of shrinkage is better known for regression under the name \\nof ridge regression (Hoerl & Kennard, 1970a, b) where the (centred) \\nmatrix xr X is replaced by xr X + cD where c is an adjustable \\nconstant and D is a diagonal matrix which will be the identity if the \\nvariables are on a common scale, and otherwise could be the diagonal \\nmatrix of the variances of the variables. Given that we have seen \\nthat the linear discriminant may be computed by regression, we might \\nwonder if the connection is exact: it is. Other forms of shrinkage are \\nconsidered in regression (Sen & Srivastava, 1990; Frank & Friedman, \\n1993), such as dropping small principal components. \\nThere are other intermediate positions between linear and quadratic \\ndiscrimination. We can restrict the class of covariance matrices Lk, for \\nexample to be proportional to each other (Owen, 1984; Flury, 1986; \\nEriksen, 1987) or to have equal correlation matrices (Manly & Rayner, \\n1987). More generally, we can parametrize Lk = AkUkDkU[ where Uk \\nare the eigenvectors, Ak the sum of the eigenvalues and Dk a diagonal \\nmatrix of the eigenvalues divided by their sum. The proportionality \\nrestriction forces equality of Uk and Db and commonality of corre\\xad\\nlations forces commonality of Uk. Other restrictions which might be \\nimposed are commonality of 'shape' (equal Dk) and/or 'size' (equal Ak) \\nas discussed by Banfield & Raftery (1993) and Flury et al. (1994). \\nThus far we have only considered shrinking W. We mentioned \\non page 96 using J < r canonical variates when allocating future \\nexamples, retaining only those dimensions in which the group means \\nare well separated, say those for which the ratio of the between-groups \\nto within-groups variance is around one or more. This is equivalent to \\nsetting the difference between the group means to zero in the dropped \\ndirections, and so is a form of shrinkage to zero of the between-groups \\ncovariance matrix B; we could use a less extreme form of shrinkage \\nin which these directions are down-weighted. However, we can down\\xad\\nweight by either reducing B or increasing W, so this is merely a \\ndifferent perspective. \\nThe most common sources of strong correlations between the vari\\xad\\nables are when features are measuring essentially the same quantity and \\nwhen the features are a discretized signal or image. In both cases we \\nmay be able to design a better composite feature, and we will certainly \\nbe able to design a better form of shrinkage. In the regression context \\nfor a discretized signal, we would assume that the quantity of interest \\nwas really an integral J {J(t)f(t) dt which is approximated by a sum \\nat the reported values of t (Hastie & Mallows, 1993), and so would \\nassume that fJ(t) was smooth. There penalized regression could be used \\n(as in Section 4.3). In the discrimination context Hastie et al. (1995) This is a special case of \\nthe algebra of Hastie et \\na/. (1995). \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 120}, page_content='3.5 Logistic discrimination 109 \\napply this idea by modifying w to w + n for a pre-specified n and \\nfinding the equivalence to canonical correlation analysis and to ridge \\nregression (for xr X + nn) in the same way as in Section 3.2. \\nIn practice in these high-dimensional situations it may be preferable \\nto re-parametrize the signal, for example by a spline basis, and discard \\nthe fine detail before any computation is done. After all, these methods \\nare still assuming normality, and that is likely to be a serious limitation. \\nWe will need to be very careful with shrinkage if some i:k is \\nsingular, since this is likely to make it easy to identify that class, and \\nshrinkage may reduce the performance dramatically. \\n3.5 Logistic discrimination \\nLogistic discrimination is important both as a more direct way of \\nestimating posterior probabilities and hence the Bayes rule, and also as \\na method which is much easier to generalize. \\nWe saw in Section 2.3 that using a normal model for each class (or \\ngroup) density with a common covariance matrix gave rise to (2.29): \\nlogp(k I x) = logp(11 x) + ak + xfh. \\nMore generally, we could model log p(k I x) -log p( 11 x) by some para\\xad\\nmetric family of functions, say gk(x; 8) (with g1(x) = 0 ). Then we \\nestimate \\np(k I x) = exp gk(x; e)~ . \\nL:j exp gj(x; 8) (3.9) \\n(For the linear model we take the parameter vector e to contain all the \\nak and /h.) In the neural network literature this is known as softmax \\n(Bridle, 1990a, b), and in the (earlier) statistical literature as a multiple \\nlogistic model. Classification is done by using the (estimated) Bayes \\nrule, taking the maximum of p(k I x) (under the usual loss function \\n(2.2)). This procedure is known as logistic discrimination. \\nSeveral remarks are needed. First, (3.9) can arise from other prob\\xad\\nability models, sometimes with transformed x variables. Some of its \\nearliest applications were to epidemiological studies where such a direct \\nparametrization was very natural, and other models leading to (3.9) are \\nconsidered by J. A. Anderson (1972) and Cox & Snell (1989). Another \\nhas two classes and independent binary features (Minsky, 1961) with \\nprobabilities eik under class k, for then \\nlogp(21 x) -logp(11 x) = L Xi[log ei2 -log On] \\ni \\n+ (1-Xi)[log(1- 8i2) -log(1- Oil)]. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 121}, page_content='110 3 Linear Discriminant Analysis \\nIt is normal to fit logistic regressions and log-linear models by \\nmaximum likelihood, but we have to consider carefully the likelihood \\nto be used (as we do below). The standard methods (Cox & Snell, 1989; \\nMcCullagh & Neider, 1989) regard x as fixed and take the observed \\nclass y to have a Bernoulli distribution with probability distribution \\n(p( 11 x), p(21 x)) for two classes, and a multinomial distribution for \\nmore. Thus the likelihood of the training set is \\nt(8;ff)= rrp(c;lx;)= IT expgcj(x;;~~ (3.10) \\n; i L:j exp gj(X;, 8) \\nwith deviance \\n(3.11) \\nWe note that if gk(x; 8) = ak + xfh, the log-likelihood is concave, so \\nany local maximum of the likelihood is a global maximum. \\nIn the special case of two groups 0 and 1 the log-likelihood is \\nL(8; ff) = L c; p(21 x;) + (1-c;) [1 -p(21 x;)] \\nand hence the deviance is \\n\" [ C; (1-C;) ] D(8) = 2 ~ C; log p(1l X;) + (1-c;) log (1 _ p(1l X;)) · (3.12) \\nAn alternative way to specify the probabilities p(k I x) is to give \\ng -1 logistic models of the form \\nlogp(k I x) = logp(11 x) + gk(x; 8) (3.13) \\nwhich is the same model as equation (3.9) (as replacing gk(x; 8) by \\ngk(x;8)-gt(x ;8) leaves (3.9) unchanged). However, the models (3.13) \\ncould be fitted separately comparing each group with group 1, and this \\ngives different maximum likelihood parameter estimates . The pairwise \\ncomparison uses less information and so will be less efficient, but for \\nlinear models Begg & Gray (1984) show the loss is often negligible. An \\napproach which is common in neural networks is to consider a logistic \\nmodel for each p(k I x) against the rest, that is \\n~ \\np(k I x) = exp gk{x; 8) ~ . \\n1 + exp gk{x; 8) (3.14) \\nThis has the disadvantage that there is no guarantee that the estimated \\nprobabilities will sum to one, and no compensating advantages. This \\nmodel is however appropriate if the classes are not mutually exclusive, \\nfor example if they indicate diseases which could conceivably occur \\ntogether. This is a special case of \\nthe computations on \\npage 152 which show \\nthat the Hessian is \\nnon-positive definite. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 122}, page_content='The empirical \\ndistribution gives \\nprobability 1/n to each \\nobserved Xj. 3.5 Logistic discrimination 111 \\nLikelihoods \\nWe derived likelihood (3.10) as a conditional likelihood for a training \\nset of size n which was a random sample from the whole population. \\nIt will also be appropriate if the feature vectors are selected rather than \\nsampled. We will lose information if we know p(x; 8) as a function of \\n8, so we are implicitly assuming that p(x) is completely unknown. \\nThe issue is more complicated when the numbers of examples \\nin each group are fixed, which implies that we will not be able to \\nestimate the prior probabilities of the groups. The natural model is for \\nPc(x) = p(x I c) not p(c I x). However \\nlogp(x I k; 8) = logp(k I x; 8) -log nk + logp(x; 8) \\n= logp(11 x) + ock + xpk -log nk + logp(x; 8) \\nand it is clear that we can only estimate (k = ock -log nk. The log\\xad\\nlikelihood formed by conditioning on all the observed classes is \\nL(8;ff) = L logp(xi I ci; 8) = \"L:logp(ci I xi; 8) -log ncj + logp(xi; 8). \\nIf we assume that p(x; 8) is entirely unknown (apart from normalizing \\nto a probability density), we can maximize over this as well as (k and \\nPk· The maximum likelihood estimate of p(x) is then the empirical \\ndistribution of the observed values (so not a genuine density), leaving \\nthe maximization of the profile likelihood for pk, \\nL logp(xi I ci;O) = L logp(ci I xi; /3) + const. \\nThis is exactly what we get from the other forms of sampling, and so \\nwe will obtain the same maximum likelihood estimates of Pk· This \\nis a streamlined version of the arguments of J. A. Anderson (1972) \\nand Prentice & Pyke (1979). Note that we have maximized over an \\ninfinite-dimensional parameter p(x), so standard likelihood theory does \\nnot apply; Anderson avoided this by considering only discrete x, but \\nPrentice & Pyke and Cosslett (1981) prove analogues of the standard \\nasymptotic results. \\nScott & Wild (1986) also consider a different approach. Suppose the \\nsample was chosen to have nk cases of class k, but the prior probability \\nis known to be nk. A natural idea is to weight the cases of class k by \\nWk = Nnk/nk. that is to use a weighted log-likelihood of the form \\nLWei logp(ci I xi). (3.15) \\nThis is precisely what we would use if combining identical cases in the \\ntraining set, and is the common practice in survey statistics. Scott & '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 123}, page_content='112 3 Linear Discriminant Analysis \\nWild show that the profile likelihood approach is more efficient if the \\nlogistic model is true, but the weighted form may be preferable for \\nestimating the least false parameter if the model is reasonable but not \\nprecisely true. \\nOrdinal logistic models \\nThe approaches thus far are appropriate if the classifications are purely \\nnominal. Some classifications are naturally ordered, such as grades of \\nfoodstuffs and the severity of a disease. For these a model which takes \\naccount of the ordering is desirable. McCullagh & Neider (1989, §5.2.2) \\nadvocate models for Yk = P(Y :( k I x), on logistic (log[yk/(1- Yk)]) \\nor complementary log-log (log[-log(1- Yk)]) scales. Suppose there is \\nan unobserved variable Z (say the true quality of the foodstuff) of \\nwhich Y is a grouped version, so Y = k if and only if (k-l < Z :( (k. \\nSuppose also that Z has a logistic distribution with mean IJ(x); then \\nexp[(k -IJ(x)] \\nYk = P(Y :( k I x) = P(Z :( (k I x) = 1 [( ( )] + exp k-11 x \\nwhich naturally gives rise to a logistic model \\nlogitP(Y :( k I x) = (k + IJ(X) \\nfor Yk that differs only in intercept for each category. Giving Z \\nthe extreme-value (or Gumbel) distribution rather than the logistic \\ndistribution leads to \\nlog{ -log[1- P(Y :( k I x)]} = (k + IJ(x). \\nIn both cases the linear model part is then IJ(x) = xp. \\nIf we regard the intercepts (k as unknown (but necessarily in\\xad\\ncreasing) this analysis can be extended to grouped versions of </J(Z) \\nfor an unknown but monotonic transformation ¢(·), since this will be \\nequivalent to grouping Z. \\nThe likelihood for the parameters (which are ((k) and the parame\\xad\\nters in 11 ) then follows from our earlier considerations, since a model \\nfor P(Y :( k I x) implies one for p(k I x) = P(k-1 < Y :( k I x). In \\nnumerically maximizing the likelihood we will have to remember the \\nordering constraint on ((k). \\nAnderson & Phillips (1981) illustrate the linear ordinal logistic \\nmodel for data on severity of back pain. It does lack flexibility, for \\nthere must be a linear projection on which the class distributions occur \\nin the assumed order for the fit to be adequate. If we allow a non-linear \\nfunction 11 (Mathieson, 1996) this may be much easier to achieve. Higher efficiency as \\nusual means smaller \\nvariance in large \\nsamples. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 124}, page_content='3.5 Logistic discrimination 113 \\nInfinite estimates \\nA difficulty which arises with the conditional likelihood is that the \\nmaximum likelihood estimates may be infinite (or, as some prefer to \\nsay, fail to exist). Consider just two classes. If there is a direction such \\nthat the projection pT Xi completely separates the two classes, it should \\nbe clear that the posterior probabilities can then be made arbitrarily \\nclose to one for every example by taking fJ -+ oo with the appropriate \\nsign. Indeed, the methods of Section 3.6 are designed to find such \\nprojections where they exist. \\nAlbert & Anderson (1984), Albert & Lesaffre (1986), Santer & \\nDuffy (1986), Silvapulle & Burridge (1986) and Lesaffre & Albert (1989) \\nconsider this in detail (apparently unaware of the earlier parallel de\\xad\\nvelopment given in Section 3.6). With more than two groups, some \\ngroups can be completely separable from others on a linear projection, \\nor (quasi-complete separation) \\np(j I Xi; 8) ~ p(k I Xj; 8), k -:/= j \\nfor all xi from class j and some fJ. The maximum likelihood estimate \\nhas an infinite component. This occurs for the tableware group in the \\nforensic glass dataset. \\nWe feel too much has been made of this. The difficulty is an inap\\xad\\npropriate parametrization, and the limits for infinite II fJ II of the fitted \\nposterior probabilities remain perfectly suitable fits, albeit sometimes \\npredicting probability zero or one. \\nPredictive approach \\nWe considered the predictive framework for the diagnostic paradigm at \\n(2.33). Suppose we assume that we know nothing about the marginal \\ndensity p(x). Then the posterior density is of the form \\nlogp(8 I 5\") = L(8;§\") + logp(8) + const. (3.16) \\nThen for large n t~e posterior density p( 8 I 5\") will be approximately \\nnormal with mean 8 and covariance matrix V, say, and we can approx\\xad\\nimate these by the maximizer and the inverse of the Hessian of (3.16). \\nThe integration over 8 could be performed approximately (Aitken, \\n1978) or by simulation using importance sampling from the approxi\\xad\\nmate normal distribution (Ripley, 1987, §5.2). Since the approximate \\nnormal distribution will have short tails, it is better to choose a longer\\xad\\ntailed distribution, for example by increasing the variance slightly or \\nusing a multivariate t distribution. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 125}, page_content='114 3 Linear Discriminant Analysis \\nIf the maximum likelihood estimate is (partially) infinite, the local \\napproximation will break down, but with a proper prior the predictive \\napproach will give a sensible fit. Indeed, a proper prior which restrains \\nthe size of the parameter vector is a form of ridge regression (see \\nSection 3.4) and has advantages even in non-Bayesian views. For \\nexample, it avoids the embarrassment of predicting probability zero \\nfor events that happen (as might happen with the plug-in rule if the \\ntraining set is linearly separable but the populations are not). \\nBias correction of the parameters of a linear logistic discrimination \\nhas been considered extensively, but Byth & McLachlan (1978) showed \\nthat the bias of the plug-in estimates of the posterior probabilities \\np(k I x; 0) was of smaller order than 1/n and so much less important \\nthan for the best linear classifier discussed in Section 2.5. \\nExamples \\nFirst we consider the Pima Indians data. The simplest approach is \\na direct logistic regression on the seven explanatory variables. This \\nmade 66 errors on the test set, an error rate of 19.8%. However, some \\nof the features had insignificant coefficients, and a stepwise selection \\nprocedure to choose the fit with the smallest value of AIC dropped \\nblood pressure and skin thickness. Its test set performance was also 66 \\nerrors (but not the same ones). \\nThe number of pregnancies varies from zero to seventeen, and this \\nseems unlikely to enter linearly. To test this, we allowed separate \\ncoefficients for 0, 1, 2, 3, 4, 5 and 6+ pregnancies, but the fit was little \\nbetter, and the predictions worse (69 errors). If we allow a polynomial \\nin age, AIC chooses a cubic; the number of errors is increased to 69. \\n\"\\' C\\\\1 \\n0 \\n0 \\nC\\\\1 0 \\n~ error rate \\nc:i \\n~ \\n~ \\n0 \\nreject rate\\'\\',, \\nI() \\n0 0 \\n0 ci \\n0.30 0.35 0.40 0.45 0.50 \\ndoubt cost Figure 3.5: Error and \\nrejection rates against \\ndoubt cost d for a \\nlogistic discrimination \\nmodel for the Pima \\nIndians data. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 126}, page_content='Figure 3.6: Calibration \\nplot for a logistic \\ndiscrimination model \\nfor the Pima Indians \\ndata. The \\'rug\\' of ticks \\nshows the events which \\noccurred in the training \\nset against the predicted \\nprobabilities. The \\nsmooth curve is a \\nkernel regression (see \\nSection 6.1). 3.5 Logistic discrimination 115 \\nC! ·~ \\n\"\\' 0 \\n<0 \\n0 \\n\" 0 \\n\"\\' 0 \\n0 \\n0 \\n0.0 0.2 0.4 0.6 0.8 1.0 \\npredicted probability \\nWe can use this example to illustrate some of the concepts we saw in \\nChapter 2. If we allow a low enough cost of \\'doubt\\', this will be chosen. \\nFigure 3.5 (computed on the test set) shows a fairly small reduction in \\nerror rate with rejection in this example; if we allow 10% of the patients \\nto be rejected, the error rate drops from 19.9% to 15.7%. We can also \\ncheck if the predicted probabilities look well calibrated. Figure 3.6 was \\ncomputed on the training set and shows no serious departures. \\nFor the forensic glass data the fitting algorithms converge slowly \\nbecause of the partial separation of the classes, but produce satisfactory \\nfitted probabilities and an error rate of 26.2% with confusion matrix \\nWinF WinNF Veh Con Tabl Head \\nWinF 50 17 3 0 0 0 \\nWinNF 19 55 0 2 0 0 \\nVeh 6 7 4 0 0 0 \\nCon 0 2 0 11 0 0 \\nTabl 0 0 0 0 9 0 \\nHead 0 0 0 0 0 29 \\nThe fit is rather better than linear discriminant analysis, not surprising \\ngiven the very non-normal nature of these data. The cross-validated \\nestimate of the error rate was 36% with confusion matrix \\nWinF 46 19 5 0 0 0 \\nWinNF 19 47 2 3 3 2 \\nVeh 7 6 4 0 0 0 \\nCon 0 3 0 9 0 1 \\nTabl 0 2 0 0 7 0 \\nHead 0 2 0 2 1 24 \\nsince the cross-validation runs did not find the \\'right\\' separation vectors. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 127}, page_content=\"116 3 Linear Discriminant Analysis \\n3.6 Linear separation and perceptrons \\nHistorically , special attention has been given to situations such as \\nFigure 3.1 on page 97 in which the two species are completely separated \\non the first linear discriminant. We say two groups are linearly separable \\nif there is a linear function of the variables, say xa + b, which is positive \\non one group and negative on the other. A function which computes \\na linear combination of the variables and returns the sign of the result \\nis known as a perceptron after the work of F. Rosenblatt (1957, 1958, \\n1962). (There are also publications by Block, 1962; Block et al., 1962.) \\nTheir interest now is in their continuing influence on the thinking in \\nthe field of neural networks. \\nLet us add a column of 1's to x and add b to a. Let z = x/1/xll \\non the first group and z = -x/llxll on the second group. We then \\nseek a linear combination a such that za > 0 for every example in the \\ntraining set. Since the training set is finite, we can choose (j > 0 so \\nthat za > (j. Indeed, we can achieve this for any (j > 0 by rescaling a. \\nOne approach to the problem would be to choose a by least squares \\nto make za as near one as possible, or to regress y = ±1 on x which as \\nwe have seen gives the linear discriminant up to a scale factor. (However, \\nthere is no guarantee that the linear discriminant will linearly separate \\nthe groups if they are linearly separable, and it is easy to construct \\nexamples in which it will not, Figure 3.7.) A more direct formulation is \\nto minimize the number of errors, but as that is a discrete measure, the \\noptimization is difficult. The sum of the degree of error \\nwill be zero if and only if linear separation can be achieved. This is \\nequivalent to solving the linear programming problem zia ~ b, and \\nlinear programming methods can find a solution or show that none \\n+ + \\n+ + + \\n+ \\n+ Analytical evidence that \\noptimizing the number \\nof errors made by a \\nperceptron is hard is \\nprovided by Hoffgen et \\na/. (1995) who showed \\nthat the problem of \\ndetermining if there is a \\nsolution with at most \\nk ~ 1 misclassifications \\nis NP-hard. (For k = 0 \\nit is reducible to a \\nlinear programming \\nproblem, so solvable in \\npolynomial time.) \\nFigure 3.7: A dataset \\nwith the least-squares \\nline (solid) and a linear \\nseparator (dashed). \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 128}, page_content=\"3.6 Linear separation and perceptrons 117 \\nexists (Minnick, 1961; Muroga et al., 1961; F. W. Smith, 1968; Grin old, \\n1969). (We need to introduce ~ to avoid the trivial solution a = 0.) \\nWe introduce n artificial variables u; and also split a = a+ -a-into \\npositive and negative parts. Then the problem becomes \\nsubject to \\nand n \\nmin 'l:.:u; \\nu;,a+,a-i=l \\nwhich can be solved in a finite number of steps, and has a zero solution \\nif and only if the classes are linearly separable. \\nIn the late 1950s a number of researchers were interested in simpler \\nbut iterative solutions, in which the value of a was adjusted after each \\nexample was presented. The derivative for the least-squares problem \\nlly-Xaf is 2XT(Xa-y) and so a steepest descent procedure would \\nbe of the form \\n(3.17) \\nFor small enough 11 this process converges to the space of least-squares \\nsolutions. \\nRather than compute the sum on the right-hand side and update a, \\nwe could update after each pattern was considered. This gives the rule \\n(3.18) \\nknown as Widrow-Hoff learning (after Widrow & Hoff, 1960) or the \\ndelta rule. The patterns are presented cyclically until convergence, which \\nwill need 1J ---+ 0. \\nRosenblatt's perceptron learning rule replaced the term x;a in (3.18) \\nby the output of the perceptron, the sign of xa. Thus a is changed only \\nif the current pattern is misclassified, and so the rule is of the form \\na+---a+ 211zT I(z;a:::; 0). \\nNo generality is lost by taking 11 = 1/2, since we can rescale a. \\nRosenblatt showed that this rule will converge in a finite number of \\nsteps to a linearly separating combination if one exists. Let a• be a \\nsuitable combination chosen so that z;a• ~ 1 for all members of the \\ntraining set. If the rule changes a we have z;a :::; 0 so \\n!Ia +~a-a·ll2 = !Ia-a·ll2 + 2z;(a-a·)+ 1 :::; !Ia-a· f-1. \\nThis shows the rule terminates in at most !lao-a•ll2 steps. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 129}, page_content='118 3 Linear Discriminant Analysis \\nThis result is known as the perceptron convergence theorem. Its \\nlimitations were explored by the first edition of Minsky & Papert \\n(1988) published in 1969. They showed that the coefficients needed to \\nachieve linear separation (with fixed {J) could grow very rapidly with \\nthe size of the problem and the finite number of steps needed by the \\nperceptron rule could become very large. There is after all another rule \\nwhich will terminate in a finite number of steps: try all integer-valued \\na in order of increasing length, and no one would advocate that rule. \\nMinsky & Papert also considered the behaviour of the rule when \\nthe two groups were not linearly separable, and stated that llall would \\nremain bounded. (Their proof was completed by Block & Levin, 1970.) \\nThus if the a belong to a fixed-precision set (as they will do in real \\ncomputation) the rule will eventually cycle. In particular there is no \\nimmediate way to deduce whether the rule will ever terminate, and \\ncycling can be hard to detect, as the cycle length is unknown. \\nThere are a number of variants of the perceptron updating rule. For \\nexample, 11 can be chosen just large enough to correctly classify the \\ncurrent case. Ho & Kashyap (1965) have other algorithms, discussed \\nin detail in Duda & Hart (1973, §§5.6-7). It is also possible to extend \\nthe procedure to K > 2 categories. In that case the natural classifier \\nwould be to choose the largest of K linear discriminants xak. Let a \\nbe the concatenation of the vectors ak. Then correct classification of \\npattern x in class k is equivalent to (0 ... , -x, 0 ... 0, x, ... O)a > 0 with \\nthe negative element in position j, for each j not equal to k. Thus each \\nexample x generates g-1 examples in the Kp-dimensional problem. \\nApplying the perceptron updating rule to this problem is equivalent to \\nthe updating rule \\nwhen pattern x is from class i, and j is a class with a larger value \\nof xaj. Since this is the perceptron rule in the transformed problem, \\nthe convergence proof still holds. F. W. Smith (1969) extends the linear \\nprogramming approach to more than two classes. \\nThe traditional simplex algorithm for linear programming has ex\\xad\\nponential worst-case behaviour, but recently a number of provably \\npolynomial algorithms have been devised. Mansfield (1991) has imple\\xad\\nmented one of these, Khachiyan\\'s algorithm, for perceptron learning. \\nThe algorithm is quite simple and related to quasi-Newton methods \\nof optimization (Gill et al., 1981). It is guaranteed to find a solution, \\nif there is one, in (p + 1)3log(p + 1) + (p + 1)2log[n(p + 1)] iterations \\nwith binary inputs, where an iteration involves finding a misclassified \\nexample and updating the perceptron weights. Prior to Minsky & \\nPapert, Muroga et a/. \\n(1961) had shown that \\nthe weights in a \\nlinearly-separating \\nperceptron with n \\nbinary inputs could be \\nchosen to be integers \\nless than \\n(n + l)(n+l)/22-\", and \\nMuroga (1965) showed \\nthat there were \\nproblems where integer \\nweights of Q(2\") are \\nrequired (The Q \\nnotation is defined on \\npage 178.). Hampson & \\nVolper (1986) showed \\nthat in some sense an \\naverage problem needs \\ninteger weights of \\nQ(2\"12). More recently \\nHastad found an \\nexample which needs \\ninteger weights at least \\nas large as \\nn\"122-\"e0<205851. These \\nresults are reviewed and \\nproved by \\nParberry (1994). \\nThis bound was \\nreduced to O(p2log p) \\nby Maass & Tunin \\n(1994) using a more \\nrecent algorithm. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 130}, page_content='3.6 Linear separation and perceptrons 119 \\nIt is possible to extend the ideas of separation by linear planes \\nto other surfaces (for example conic sections) or to piecewise linear \\nsurfaces. These have been considered by Mangarasian (1968) and \\napplied to the diagnosis of breast tumours in Mangarasian et al. (1990); \\nWolberg & Mangarasian (1990) and subsequent papers. \\nCapacity questions \\nWe can ask how many random patterns a perceptron with p inputs \\ncan learn reliably; that is can be classified without error. There will \\nbe a finite limit since the patterns must be linearly separable; this \\nis irrespective of the existence of an algorithm to learn the patterns. \\nCover (1965) showed that the asymptotic answer is 2p patterns. In \\nother words, for large p we expect to be able to store most sets of up \\nto 2p patterns without error, but attempts to store more than 2p have \\na very low probability of success. \\nProposition 3.1 The probability that N patterns randomly chosen from \\nany continuous distribution in RP and randomly divided into two groups \\nare linearly separable is one for N ~ p + 1 and in general is \\n21-N min~l,p) (N ~ 1) \"\\' <l> (-=-2p___,-=N-) \\ni=O l JN \\nfor large N. \\nProof: Let C(n,p) be the number of assignments of classes 1 and \\n2 to n patterns in p variables which are linearly separable. We will \\nshow by induction that this does not depend on the patterns themselves \\nprovided they are in general position in RP (that is, not collinear). All \\nthe assignments (yi) are linearly separable for n ~ p + 1, since we can \\nsolve the system x;a + b = Yi of n equations in p + 1 unknowns to \\nfind a separating hyperplane. (This system will be singular only if the \\npoints are not in general position.) Thus C(n,p) = 2n for n ~ p + 1. \\nNow consider adding an example to n linearly separable patterns. \\nIf the new point lies on the same side of every separating hyperplane, \\nonly one label for the new point gives a linearly separable set. In \\nthe other case, by continuity , there must exist a separating hyperplane \\npassing through the new point and either label for the new point gives a \\nlinearly separable set. Of the C(n,p) sets of linearly separable patterns, \\nprecisely C( n, p-1) will pass through the new point (since this reduces \\nthe problem to one of dimension p -1 ). Thus \\nC(n + l,p) = 2C(n,p-1) + [C(n,p)- C(n,p-1)] = C(n,p -1) + C(n,p). '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 131}, page_content='120 3 Linear Discriminant Analysis \\nBy induction we find the probability to be \\nmin(N-l ,p) (N 1) \\nC(N,p)/2N = 21-N ~ ~ = P(X ~ min(N- 1,p)) \\nwhere X has a binomial (N-1, 1/2) distribution. The approximation \\nthen follows from the normal approximation to the binomial for large \\nN-1. D \\nThe approximation on the right-hand side goes rapidly from 1 to 0 \\nas N increases through 2p, since for large n a binomial (N -1, 1/2) \\ndistribution is tightly concentrated about N /2. \\nSupport-vector machines \\nIf two classes are linearly separable, there will be a continuum of weight \\nvectors a which give rise to separating hyperplanes . Amongst these we \\ncan choose a hyperplane with maximal distance to the nearest example, \\nachieved by minimizing llall2 whilst insisting that za ~ 1. Finding \\nthis hyperplane is a quadratic programming problem, and the usual \\nKuhn-Tucker optimality conditions show that there will be a subset of \\nexamples Zi (known as support vectors) for which zia = 1 and that the \\noptimal a is a linear combination of these zi. \\nThe advantage is choosing the optimal hyperplane is to reduce \\nthe VC-dimension of the space of solutions (which is proportional \\nto a bound on llaf ). If the two classes are linearly separable then \\n(Vapnik, 1995, Theorem 5.2) the expected error rate on future examples \\nis bounded by the expected number of support vectors divided by n-1. \\nThus finding a small number of support vectors might indicate good \\ngeneralization properties. \\nOf course, linear separation in the original feature space is quite \\nrare, but as for generalized linear discrimination (page 121) we can \\nexpand the feature space by using polynomials or even radial-basis \\nfunction networks and sigmoidal functions. These can give rise to very \\nlarge feature spaces, but generalization may remain acceptable if the \\nnumber of support vectors remains small, which was the case in the \\nexperiments reported by Vapnik (1995, Section 5.7). \\nBy jointly minimizing the sum of the degree of error (page 116) and \\nII a 112 these ideas can be extended to non-separable two-class problems \\n(Cortes & Vapnik, 1995). '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 132}, page_content='4 \\nFlexible Discriminants \\nLinear combinations of the features will not always suffice to discrim\\xad\\ninate the groups. It is quite common to include ratios by including \\nfeatures on log scale as we did for the Leptograpsus crabs in Chapter 3. \\nWe can also allow non-linear functions of the features by including \\npolynomial terms or dividing the range of the feature and including \\nindicator terms for parts of the range. A more sophisticated alternative \\nis to expand a feature on a spline basis such as B-splines (see below). \\nThese all amount to linear discrimination in a larger space of features, \\nsometimes called generalized linear discrimination (Duda & Hart, 1973). \\nWe will continue to assume n cases from g groups, which may or \\nmay not be the classes. We saw at the beginning of Chapter 3 three \\nmain ways to use a family of functions f: fl£ ~ lRg to approximate \\nthe Bayes rule. All these ideas apply equally here, but we will now \\nconsider much more general and flexible classes of functions. Another \\nlarge class is the subject of Chapter 5. \\nThe first approach was to estimate f(x) from the training set within \\nour parametric family, and choose the class which maximizes fk(x) or \\nhas nearest target tk. (This includes the Dietterich & Bakiri, 1991, \\n1995, approach, which specializes the choice of targets.) As f(x) is \\na regression , it is natural to fit () by least squares. Since in the \\nconventional version f represents (p(k I x)), the outputs are sometimes \\nre-normalized to sum to one. \\nThe second approach was to fit f(x) within the parametric family, \\nbut then to use the predicted values as the variables in a linear discrim\\xad\\ninant analysis. (This appears to have been the motivation of Breiman \\n& Ihaka, 1984.) This amounts to finding the nearest group mean in the \\nMahalanobis distance given by the within-group covariance matrix of \\nthe fitted values (or, equivalently, of the residuals). Note that this differs \\nfrom the first approach in using a different metric and in minimizing '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 133}, page_content='122 4 Flexible Discriminants \\ndistance to the group means rather than the targets. Formula (3.6) does \\nallow us to consider distances to the targets provided the group sizes \\nare equal, but the metric there is not the Mahalanobis distance. Thus \\nthe essential difference is the metric used. \\nThe second procedure appears preferable to the first if the predicted \\nvalues are approximately normal with a common covariance matrix. \\nHowever, in practice the fit is often very good except at a few points \\nwhich therefore dominate the residuals and the estimate of the common \\ncovariance matrix. This suggests that it is desirable to use a robust \\ndiscriminant analysis, and it may be better to accept the safe choice of \\nthe Euclidean metric. \\nThe third approach is to use the parametric family within a multiple \\nlogistic model of the form (3.9), which is often the most theoretically \\nsatisfying but needs the ability to fit by maximum likelihood rather \\nthan least squares. \\nFrom the predictive viewpoint, these methods are all (in principle) \\nparametric, and so we need to average over the uncertainty in the fitted \\nparameters . Since there will usually be many more parameters than for \\nlinear families, it will be more important to average over the greater \\nuncertainty. In practice this can be nigh impossible, as in the high\\xad\\ndimensional parameter space the integration is more difficult and it is \\nunlikely that the asymptotics which suggest a normal approximation will \\nbe appropriate unless n is much larger than the number of parameters. \\nWe are only aware of such issues having been studied for neural \\nnetworks, so discuss them in that context in Section 5.5. \\n4.1 Fitting smooth parametric functions \\nWe discuss some of the possible ways to describe more general functions \\nof the feature variables. We consider first methods using univariate \\nfunctions f : !![ ---+ 1R. \\nAdditive models and smoothers \\nAn additive model is of the form \\np \\nf(x) = rt + L gJ(XJ) (4.1) \\n}=1 \\nfor smooth but unknown functions g1 (Friedman & Silverman , 1989; \\nHastie & Tibshirani, 1990), which could encompass the effect of trans\\xad\\nformations (such as square or log or even an arbitrary polynomial) of \\neach feature. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 134}, page_content='The notation [y]+ \\nmeans max(y, 0). \\nSometimes the free \\nparameters are reduced \\nby end conditions ; for \\nnatural cubic splines the \\nsecond and third \\nderivatives vanish at \\nthe boundaries. \\nSmoothing splines are a \\nspecial case of \\nregularization, to be \\nconsidered in \\nSection 4.3. 4.1 Fitting smooth parametric functions 123 \\nOne choice of the smooth functions g(x) of a single feature is to \\nuse splines. Splines are defined by M knots ~; which we can consider \\nin increasing order. Then within an interval [~;, ~i+d a spline is a \\npolynomial of degree d (often three) and at the knots the first ( d -1) \\nderivatives are continuous. This can be written as \\nd M \\ng(x) = :~::::>x;xi-1 + LfJ;[x- ~;]~ \\ni=O i=l \\nwhich shows that there are M + d + 1 free parameters. There are \\nother bases which have better numerical properties such as B-splines \\n(de Boor, 1978; Green & Silverman, 1994 ). In any basis we can write \\nM+d+l \\ng(x) = L {J;c/J;(x). (4.2) \\ni=l \\nIt remains to choose the parameters {3;. For a regression spline these \\nare chosen by least squares. Cubic smoothing splines are the solution to \\nthe minimization problem \\nM \\nL[y;- g(~;)]2 +A j g\"(u)2 du \\ni=l \\nand the parameters in (4.2) can be found by solving a sparse system \\nof linear equations . Figure 4.1 shows the effect of the smoothness \\nconstraint on a smoothing spline: however many knots are included, \\nover-fitting is prevented by the smoothness term. Thus smoothing \\nsplines are normally preferable to regression splines, except that the \\nchoice of A is computationally demanding , and .?. = 0 can be an \\nadequate approximation with a small number of knots. \\nOther smoothing algorithms are also used, for example the loess \\nsmoother (Cleveland et al., 1992) which uses robustly-fitted locally\\xad\\nweighted polynomials. Let h(x; ¢) be a polynomial of degree d (usually \\none or two). Then the fitted value g(x) is found by fitting h(x; ¢) in the \\n~eighbourhood of x and reporting the fitted value at x. Specifically, \\ncPx is chosen to minimize \\nand g(x) = h(x, \"¢x). The loss function p(u) could be u2 but might \\npenalize large departures less severely, in the spirit of robust statistics. \\nFinally, the parameter r controls the smoothness, and is chosen to '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 135}, page_content='124 \\n0 \\nci \\n\"\\' 9 \\n0.0 0.5 4 Flexible Discriminants \\n.. . .. \\n.···~· . ······... . \\n· .. \\n.· .. \\n1.0 1.5 2.0 2.5 3.0 \\ninclude a neighbourhood of cxn points at point x. This approach is \\neasily extended to smoothing d dimensions, for small d. \\nIf the smooth functions in an additive model are written in terms \\nof basis functions, as for polynomials and splines, we have \\np dfj \\nf(x) = ex + L L f3 jk</J jk(x j ). (4.3) \\nj=l k=l \\nIf the smoothing procedure chooses parameters by least squares, we \\nhave a linear regression in an extended space of features spanned by \\nthe functions </Yjk(Xj). On the other hand, if as in smoothing splines \\nthe functions minimize a sum of squares plus a penalty, we will have \\n(approximately or exactly) a penalized linear regression. The method \\nBRUTO used in the examples is described in Hastie & Tibshirani (1990, \\npp. 262-3). This adaptively chooses the smoothness ofthe splines and \\nthe number of terms in (4.1), including the possibility of linear functions \\nand dropping features completely. \\nA general procedure to fit additive models is known as back-fitting \\n(Hastie & Tibshirani, 1990). This holds all but one of the additive terms \\nconstant, removes that term and fits a smooth term to the residuals \\nagainst the feature. In symbols, the model is \\nf(x)-ex-L gj(Xj) = g1(x!) (4.4) \\nHI \\nand any smoothing algorithm (including loess) can be applied to the \\nleft-hand side. Smoothing is applied a feature at a time until the process \\nconverges (which it will under mild conditions). Figure 4.1: The effect of \\nvarying the number of \\nknots in a smoothing \\nspline for fixed A.. \\nBased on an example of \\nWahba & Wold (1975). \\nFor 50 or more knots \\nthe curves are \\nindistinguishable. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 136}, page_content=\"4.1 Fitting smooth parametric functions 125 \\nThus far we have considered penalized least-squares fitting. Linear \\nmodels can be used to fit a logistic regression by maximum likeli\\xad\\nhood via local linearization to give a weighted least-squares problem \\n(McCullagh & Neider, 1989) and this is often solved iteratively in a \\nhandful of iterations. The extension to additive models is immediate \\n(Gu, 1990; Wahba et al., 1995). \\nAn extension of this approach is to allow smooth functions of a \\nsmall number of the features as terms in the additive model (for exam\\xad\\nple, by C. J. Stone, 1985). Adding pairwise then three-term functions \\n(and so on) is common practice in statistics, and these are known as \\ninteraction functions. Examples including functions of two features are \\ngiven by Hastie & Tibshirani (1990, §9.5), Wahba (1995) and Wahba et \\nal. (1995). \\nPima Indians diabetes \\nIn Section 3.5 we saw that a linear logistic discrimination model worked \\nwell for the data on diabetes amongst female Pima Indians, and that \\na polynomial in age improved the fit. We could consider if smooth \\nnon-linear terms in age or other variables such as plasma glucose levels \\nand the body mass index might help the fit. \\nThis was tried with several smoothers. The improvement in fit as \\nmeasured by AIC or NIC was marginal, but the test-set error rate was \\nunchanged or increased. BRUTO (which fits by penalized least squares) \\nselected only linear terms, dropping blood pressure and skin thickness. \\nWahba et al. (1995) built an additive model including an interaction \\nterm in glucose concentration and body-mass index and a categorical \\nterm for the number of pregnancies (0, 1 or 2, 3-5, 6 or more). In our \\nexperiments this did less well than the linear model. \\nProjection pursuit regression \\nAdditive models do not allow interactions between the features in \\nPI'. Perhaps the simplest way to allow interactions is through linear \\ncombinations (projections) of features: \\nr \\nf(x) =a+ L gj(tXj + PJ x) \\nj=l (4.5) \\nwhich is projection pursuit regression (PPR; Friedman & Stuetzle, 1981). \\nSometimes the components of ( 4.5) are called ridge functions because a \\npeaked gj gives a topographic ridge in two dimensions. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 137}, page_content=\"126 4 Flexible Discriminants \\nThis is a surprisingly general class of functions, as it can approxi\\xad\\nmate uniformly arbitrary continuous functions over compacta (Diaconis \\n& Shahshahani, 1984; L. K. Jones, 1987, 1992; Zhao & Atkeson, 1992). \\n(This is sometimes referred to as the 'universal approximation' prop\\xad\\nerty.) As PPR encompasses feed-forward neural networks, these results \\nfollow from (but are a little easier than) those of Section 5.7 where the \\nfunctions gj are restricted to one function, the logistic. However, ridge \\nfunctions provide better (in the sense of fewer parameters) approxima\\xad\\ntions to some functions than others (Donoho & Johnstone, 1989; Zhao \\n& Atkeson, 1992), which Zhao & Atkeson express as working better \\nfor 'angular smooth functions' than for 'Laplacian smooth functions'. \\nWith multivariate regression we have to decide whether to use \\ncommon non-linear terms for the different independent variables. This \\nis usually done, so that for example for projection pursuit regression \\nwe have \\nr \\nfk(x) = '1k + LYkjcPj(aj + 13[ x). (4.6) \\nj=l \\nThis shows that the fitted values lie in a (r + 1 )-dimensional space. Since \\nthe scale of cPj is not otherwise fixed, we can choose cPj(aj + 13[ x) to \\nhave zero mean and unit variance over the training set. \\nAlgorithms \\nThe original algorithm for PPR has been superseded by SMART \\n(Friedman, 1984). This constructs the (approximate) least-squares fit \\niteratively. A maximum value M for r is specified, and terms are \\nadded to ( 4.5) one at a time until M terms are present. Then at each \\nstep the least effective term is dropped and the model re-fitted, until r \\nterms are left (and this process can also be used to help select r by \\nlooking at the fit). Some of the details can be changed by the user, and \\nwe only describe the 'highest' level of optimization. \\nBackfitting is used to fit the model, and when the j th term is being \\nconsidered, the direction 13j is optimized by a Gauss-Newton procedure \\n(see Section A.5). This finds a local minimum of the least-squares \\ncriterion for a model with r terms. A new term is introduced by an \\ninitial direction 131, and the process continues until convergence. When \\nM terms have been added, the least important (measured by Lk IYkjl) \\nis dropped, the reduced model re-fitted and the process continued. \\nThe precise algorithm for scatterplot smoothing is not intrinsic to \\nSMART, and we have also use spline smoothers. Friedman used his \\nown 'super-smoother', which uses a local linear fit to k/2 data points \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 138}, page_content=\"See the glossary. 4.1 Fitting smooth parametric functions 127 \\neach to the left and right of the point x at which g(x) is required. (The \\nuse of a rectangular window allows fast updating as x scans along.) \\nThe value of k is chosen from three possibilities by cross-validation \\nat x, and then this choice is smoothly interpolated between the three \\nsmoothers. \\nHwang et al. (1991, 1992a, b, 1994a, b, 1996) replaced the super\\xad\\nsmoother by Hermite polynomials, which tends to produce smoother \\nfunctions, and Roosen & Hastie (1994) have also tried smoothing \\nsplines. Most of the examples were tried both with Friedman's super\\xad\\nsmoother and our own implementation of smoothing splines. \\nHinging hyperplane s \\nBreiman's (1993) hinging hyperplanes are the special case gj(x) = \\n[x]+ = max(x, 0) of PPR. These suffice to approximate arbitrary \\ncontinuous functions , by the results of Section 5.7. It is attractive to use \\nthe same projection direction for both positive and negative versions of \\nthe function, when we have \\ng(a + f3T x) =a max(a + f3T x,O) + b min(a + f3T x,O). \\nA little thought shows that this is a linear function plus a multiple of \\nmax(±(a + f3T x)), which suggests adding an overall linear function to \\nf(x) to avoid a wasteful fit using a projection and two components to \\nrecover a linear term. \\nThese have universal approximation properties by the methods of \\nSection 5.7, as using two such functions we can approximate a step \\nridge function. Further, we will have rate of convergence results for \\nsuitable smooth functions of order 0(1/ .jY). (Breiman's Theorem 3 is \\nmore restrictive than our Proposition 5.3 in assuming a higher degree \\nof smoothness although it may thereby use a smaller r.) \\nThe attraction to Breiman of hinges was a fast way to fit one \\nhinged hyperplane by least squares. This is used within the back-fitting \\napproach of PPR. His reported CPU times seem comparable with the \\nstate-of-the-art in fitting neural networks, which have the advantage \\nof using smooth (indeed, infinitely differentiable) functions. The fast \\nalgorithm is based on the idea that once we know which side of the \\nhinge the points fall, fitting the hinged hyperplanes is simple (a linear \\nleast-squares problem, which can be updated as points change sets). \\nThus we choose an initial hinge, divide the points, fit, divide again on \\nthe fitted hinge and repeat. This gives a local minimum, so may be run \\nfrom several starting hinges. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 139}, page_content=\"128 4 Flexible Discriminants \\nBreiman also considers variable selection, that is restricting {31 to \\npick just some features. This can ease the search (and could also be \\napplied to full PPR) but does assume that the features have some \\nindividual meaning. \\nMARS (multivariate adaptive regression splines) \\nFriedman's (1991) multivariate adaptive regression splines allow for \\ninteractions more explicitly by \\nM Km \\nf(x) = rx + L f3m 11 cf>km(Xv(k,m)) (4.7) \\nm=l k=l \\nwhere \\nand \\nand tkm is an observed value of Xv(k,m)· The degree is the largest Km; \\nif this is one the model is additive. The functions were chosen to allow \\nfast least-squares fitting algorithms. (Terms cf>km and cf>k,m+l are added \\ntogether.) The components are splines corresponding to a penalty on \\nthe first rather than second derivative, discussed further in Section 4.3. \\nOnce again back-fitting is used, and terms are only considered for a \\nhigh-order product if they are interactions of terms which occur in the \\ncurrent fit. \\nThe precise details of the selection of terms are an 'engineering \\ndetail' discussed at length in Friedman's paper and its discussion. His \\nalgorithm has a forward phase followed by a backward phase. Terms \\nare chosen to add or delete depending on a lack-of-fit criterion, which \\nis the residual sum of squares divided by (1-Cjnf. Here n is the \\nnumber of observations, and C is the number of parameters plus a \\nmultiple (2-4) of the number of terms M. The backwards elimination \\nstep aims to produce a model with comparable performance but fewer \\nterms. In the additive case, over-fitting is avoided by reducing the \\nnumber of knots rather than via a smoothness penalty. \\nMARS produces fits which are continuous but not differentiable, \\nwhich can be visually unappealing. Friedman suggests 'smoothing' out \\nthe piecewise linear functions cf>kJ• for example by replacing them by \\ncubics (and re-fitting the coefficients). \\nPIMPLE \\nBreiman's (1991) IT-method with program PIMPLE is another way to \\ninclude interactions (and we will meet a third as interaction splines). As [y]+ means max(y,O). \\nHowever, the splines \\nare fitted by least \\nsquares, that is as \\nregression splines rather \\nthan smoothing splines. \\nThis is related to the \\nGCV penalty discussed \\nin Section 4.3. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 140}, page_content='Figure 4.2: Non-linear \\ndiscriminants for the \\ncrabs data via \\nprojection pursuit \\nregression. The left plot \\nuses linear \\ndiscrimination on the \\npredictions, the right \\nplot a robust version. 4.1 Fitting smooth parametric functions 129 \\nfor MARS, Breiman considers a sum of products of splines, this time \\ncubic splines, but the method appears to be geared towards small num\\xad\\nbers of products of quite accurate fits, rather than as in MARS large \\nnumbers of inaccurate products. Back-fitting is used. The variable\\xad\\nselection strategy starts with a small number of knots in the splines, \\nincreases this and then considers deleting individual knots, using gener\\xad\\nalized cross-validation (Section 4.3) to decide when to stop adding and \\nwhen to delete. \\nBreiman suggests that in three or more dimensions the product \\nterms will be identifiable (up to their ordering in the model) and so can \\nbe interpreted; his discussants are less confident. \\nExamples \\nWe first consider the data on Leptograpsus crabs. BRUTO selected FL, \\nCL and CW to enter linearly, with a slightly non-linear term for RW but a \\nroughly parabolic term for BD. MARS of degree 1 chose a single break \\nof slope for each feature (and a small departure from linearity) except \\nfor BD. If interactions are permitted, several two-term interactions and \\none three-term interaction are chosen. Although the models are quite \\nnon-linear, the discriminant plots are virtually unchanged. \\nA difficulty (Ripley, 1994c) with our second approach (using LDA \\non the fitted values of a regression) is that a small number of outliers \\nwhich are not fitted well can distort the within-group covariance matrix. \\nFigure 4.2 shows the first two canonical variates for a projection-pursuit \\nregression (with r = 3) for the crabs data, in which a number of outliers \\nhave appeared, together with bunching of points which are predicted \\nparticularly well. Sometimes using a robustified discriminant analysis \\nwill help, as Figure 4.2 shows . \\n... D \\n\" ~oJl\\'b 1U ,tiJ66 ·c \"\\' ~ D D o\" \\n~ 6 ~ 6 6 D \\nD \\n\" 0 • tt .. . \\n0 \"-l.\\\\...t \" \"\\' .... » .. <.> \\n\" \\'l\\' :~~ AA ...... -: \\n\" :-: -.1i) 0 l \\\\•. \\n~ ..,. • • . . \\n• • I ••• . :· ~ \\n~· •.-... . : \\n-6 -4 -2 0 2 4 6 -10 -5 0 5 \\nfirst canonical variate first canonical variate \\nFigure 4.3 shows a series of non-linear discriminant plots for the \\nforensic glass data. The BRUTO fit is very similar to linear discrim\\xad\\nination. When we examine the terms of the additive model, we find '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 141}, page_content='130 4 Flexible Discriminants \\nco \\n\"\\' \\n\"\\' 0 \\n0 \\n~ L---------~--~-- ----~ ~ L_--------~ ~--------~ \\n-5 0 5 10 -5 0 5 \\n(a) (b) \\n. i\" 5 ... ... \\n4 f \\n2 i \\n\"\\' ~ \"\\' tl I l\\'• \\n0 \\' \"\"\\' 2j 0 #\\': 42 # 2 .., \\n\" 2 \\n<)\\' \\') <)\\' \\'\" 3 \\\\ \\n\\'.., 6 \\'\\' \\\\ \\n-6 -4 -2 0 2 4 6 8 -6 -4 -2 0 2 4 6 \\n(c) (d) \\nthat iron oxide has been omitted, and the term in calcium is slightly \\nnon-linear ; all the other terms are selected to enter linearly. The \\ncross-validated error rate was 35%, a little better than the full linear \\ndiscriminant analysis (but not significantly so). \\nMARS with maximum degree 1 fits an additive modeL This intro\\xad\\nduces non-linear terms in RI, Mg, Si, K, Ca and Ba, and drops the \\nrest. The fitted functions are shown in Figure 4.4. This achieves a \\ncross-validated error rate of 32.2%. \\nAs more interaction terms are introduced in MARS these principally \\ndistinguish groups 5 (tableware) and 6 (headlamps) from the rest. As \\nFigure 4.3 shows, the linear discrimination model on the transformed \\nfeatures becomes much less plausible. The performance improved to \\n29% both with pairwise interactions and with unlimited interactions. \\nProjection pursuit regression was also used to produce new features \\nfor use in linear discrimination. There are six classes here, so if we \\nchoose the number of ridge terms r < 5 there will be collinearity in \\nthe new features. It does seem desirable to choose at least five ridge \\nfunctions, which must necessarily be rather smooth functions to avoid \\nover-fitting. This was borne out by our experiments , which achieved \\na cross-validated error rate of 42% if Friedman\\'s super-smoother was \\nused, but 35.5% if smoothing splines were used with a relatively large \\nvalue of A.. Using just three ridge functions made the performance Figure 4.3: Non-linear \\ndiscriminant plots for \\nthe glass data: \\n(a) using BRUTO; \\n(b, c, d) using MARS \\nwith maximum degree \\n1, 2, and unrestricted. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 142}, page_content=\"Figure 4.4: Fitted \\nfunctions for the \\nMARS additive model \\nfor the forensic glass \\ndata. The y scales are \\narbitrary. 4.2 Radial basis functions 131 \\nI ~I \\n' Rl Mg \\n~I \\n• ~I \\n' s; K \\n~I i ~I ,, \\nCa Ba \\nconsiderably worse as the two types of window glass were barely \\nseparated. \\nPima Indians \\nFitting MARS and PPR models by least squares to the Pima Indians \\ndiabetes data did not improve the fit over linear methods, with a typical \\ntest-set error rate of 75/332. \\n4.2 Radial basis functions \\nWe return to ways of parametrizing f or the log probabilities as a linear \\ncombination of basis functions. For a one-dimensional x splines are a \\nnatural choice. For higher dimensions we could use multidimensional \\nsplines (Section 4.3), but radial basis functions or RBFs have been more \\nwidely advocated (Powell, 1987, 1992; Broomhead & Lowe, 1988; Lee \\n& Kil, 1988; Moody & Darken, 1989; Poggio & Girosi, 1990a, b; \\nMusavi et al., 1992). \\nRBFs are approximations of the form \\ny = o: + L /3jG(IIx- Xjll) \\nj (4.8) \\nfor centres Xj. Examples of G proposed include the Gaussian G(r) = \\nexp -r2 /2, the multiquadric G(r) = J(c2 + r2) (Hardy, 1971, 1990; \\nKansa, 1990) and the thin-plate-spline function G(r) = r2log r. (The \\nRCU network of Reilly et al., 1982, has G(r) = I(r < ro).) It is easy \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 143}, page_content=\"132 4 Flexible Discriminants \\n(but not as useful) to extend the definition to general kernels G(x-Xj) \\nor G(x,xj)· For multivariate approximations we just take a and /3j to \\nbe vectors, that is we take different linear combinations of the same \\nbasis functions. \\nWhen G is Gaussian, (4.8) can be seen as extending the notion of \\napproximating a probability density by a mixture of known densities. \\nThe norm II II is unspecified, and could be Euclidean distance or \\na Mahalanobis distance, when the densities would have a common \\ncovariance matrix. In general we might want to consider different \\ncovariance matrices for each component, leading to the form \\ny =a+ L /3jG(IIAj[x- Xj] II). \\nj \\nGirosi et al. (1995) call this form hyper basis functions. (4.9) \\nA variant of radial basis functions which is sometimes considered \\n(Moody & Darken, 1989; Xu et al., 1994) is the normalized form \\n2.:: · /3jG(IIx- Xjll) y = --===1=--------l.:jG(IIx-xjll) . (4.10) \\nApproximation properties \\nThe class of radial basis functions has similarly good approximation \\nproperties to those of ridge-function methods (such as projection pursuit \\nregression and feed-forward neural networks). There are many possible \\ncases to consider, depending on whether the centres and G are fixed or \\nadaptive (chosen for each dataset). We will only give a flavour of the \\nresults. \\nPark & Sandberg (1991) studied the subclasses of (4.9), \\nY =a+ Lf3jG(II[x-xj]ll/aj) \\nj \\ny = a+ L /3jG(II [x-Xj] 11/a) \\nj (4.11) \\n(4.12) \\nin which G is fixed but the covariance matrices are proportional, \\nincluding the special case in which the aj are identical. (This is still \\nmore general than (4.8).) Provided G is continuous, bounded and \\nwith a finite and non-zero integral, they show that the class (4.12) \\nis dense in Lp for every p E [1, oo ), and can uniformly approximate \\ncontinuous functions on compact sets. (This is sometimes referred to \\nas the 'universal approximation' property.) Thus for any function f(x) \\nthere is a set of centres (xj) and a a> 0 such that (4.12) is close to f \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 144}, page_content=\"Here H'·2 is the space \\nof L2 functions all of \\nwhose derivatives up to \\norder s are in L2 and \\nso includes the \\nGaussian. 4.2 Radial basis functions 133 \\nin the appropriate norm ( Lp or the maximum difference on a compact \\nset). \\nThe uniform approximation on compact sets of continuous func\\xad\\ntions for Gaussian RBFs of the form (4.11) can be shown via the \\nStone-Weierstrass theorem (Girosi & Poggio, 1990; Hartman et al., \\n1990). Girosi & Poggio (1990, Appendix C) state a more general result \\nfor (4.8) and piecewise continuous G arising from (4.14) below, but \\ntheir proof is of pointwise rather than uniform convergence. (The \\nproof can be completed by consideration of the discretization error in \\na Riemann integral, using the modulus of (uniform) continuity.) Thus \\nwe still have uniform approximation with a fixed basis function G. \\nResults on the rate of approximation are available for smooth \\nenough target functions f (Girosi & Anzellotti, 1993), using the meth\\xad\\nods described in Section 5.7. They considered the class (4.8) for \\nG E L2(JR.P) and the class of targets f which are in L2(JR.P) (for \\nLebesgue measure) and can be expressed in the form \\nf(x) = { G(x-y) d.A.(y) \\nJJR.P \\nfor a signed measure A of bounded total variation (that is, the difference \\nbetween two finite measures). Thus functions in the class are at least as \\nsmooth as G; for example, for the Gaussian G the sharpness of peaks \\nis firmly controlled. Then the L2 rate of convergence is bounded by \\nII.A.II/ J1l for n terms, when G is scaled to II Gil = 1. Further, if G is in \\nH8\\n•2 for s > p/2, there is uniform convergence at rate 1/ Jll. \\nAs class (4.12) corresponds to the rescaled kernel G, the results of \\nGirosi & Anzellotti also apply to target functions of their class for any \\nrescaling of G (although the constant in the bound will vary with the \\ntarget function). This class of functions is, however, strictly smaller \\nthan L2. (Consider the function x-1/4 /(lxl < 1).) \\nFitting \\nFinding the coefficients a and {31 in (4.8) to (4.12) is usually done by \\nleast squares and is easy; for fixed centres x1 and fixed scale parameters \\naJ these are linear regression equations, so least-squares fitting reduces \\nto solving linear equations. \\nThis leaves the issues of finding the centres and any scale factors. \\nOne possibility is to take every training example as a centre, but this \\ncan lead to over-fitting. This suggests taking a representative collection \\nof training examples, for example a random or stratified sample (e.g. \\nLee, 1991). There are other ways to take a representative collection of \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 145}, page_content=\"134 4 Flexible Discriminants \\npoints not necessarily within the training set. For example, the k-means \\nalgorithm (Section 9.3) of cluster analysis chooses k points in P£ to \\nminimize the sum of squares from each training point to the nearest of \\nthe k points. These clustering algorithms do not take the classes of the \\ntraining examples into account. \\nMusavi et al. (1992) designed an agglomerative clustering algorithm \\n(see Section 9.3) which only merges clusters of points with the same \\nclass. The cluster means then provide the centres for the basis functions. \\nThe algorithm will choose the number of clusters and hence the number \\nof basis functions, but has a 'clustering parameter' 11 which controls \\nthe agglomeration process. Other ideas are the SOM (Section 9.4) and \\nLVQ (Section 6.3) methods of Kohonen. LVQ aims for cluster centres \\nwhich provide a good set for nearest-neighbour classification. \\nMoody & Darken (1989) considered finding centres by k-means \\n(using the method of (6.9) on page 202) and this has often been \\nused. Note that this (and most other ways of choosing representatives) \\ndepends on the choice of a metric in P£, and so is most appropriate for \\n(4.12) or (4.8) rather than for (4.9). \\nMoody & Darken explored choosing the scale functions O'j in \\n(4.11) from a heuristic using the P-nearest neighbour distance. Musavi \\net al. (1992) chose the matrices Ai in (4.9) (equivalently the covariance \\nmatrix of the Gaussian basis function) by fitting a maximal ellipsoid \\naround the centre which includes no training examples of another \\nclass, and taking this as an isodensity surface containing 95% of the \\nprobability . Note that this procedure is very sensitive to outliers and \\nfaulty training-set classifications. \\nLeonard et al. (1992) extend the Moody-Darken method by adding \\nfurther outputs designed to signal extrapolation and to give confidence \\nintervals for the predictions. \\nIt is of course possible to minimize the least-squares fit over the \\nparameters (J or (J j, and even over the centres of the basis functions. \\nAs the least-squares fitting is partially linear, this will be computation\\xad\\nally quite feasible and seems to be the proposal of Poggio & Girosi \\n(1990b), although considered and rejected as too demanding by Moody \\n& Darken. Wetterschereck & Dietterich (1992) considered optimiz\\xad\\ning over both parameters O'j and centres. On their (single) example \\nthey found that optimizing over centres was particularly important to \\nachieve a competitive performance by RBF methods, but that choosing \\nthe O'j by a local heuristic was counter-productive. \\nYet another idea is to choose cluster centres from a large class of \\ncandidates (for example, all training examples) by a stepwise regression \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 146}, page_content=\"4.2 Radial basis functions 135 \\nprocedure. This was the idea of Chen et al. (1991). Any sensible \\nselection strategy could be used, and the method extended to choosing \\nfrom a small number of scale factors at each candidate centre. \\nPoggio & Girosi (1990b, §IV.D) consider alternatives to least-squares \\nfitting for 'unreliable data'. They replace the square function in the sum \\nof squares or in (4.13) below by \\nwith E positive and f3 large. This enforces a quadratic penalty only up \\nto about ±-JE, and has the same effect as are-descending M-estimator \\nin robust regression (Huber, 1981; Hampel et al., 1986; Rousseeuw & \\nLeroy, 1987). \\nThis plethora of methods provides a difficulty in assessing RBF \\nmethods; no two workers use the same class of RBFs and method of \\nfitting. There is a range of compromises being made between speed \\nof fitting and accuracy of approximation. RBFs are often claimed \\nto be much faster than ridge-function methods on the basis of their \\npartial linear fitting, yet if the centres are varied (or regularization \\nused; Section 4.3) their computational load seems as large as their \\ncompetitors. However, there is considerable scope for inspired choices \\nof centres in specific problems (as in Roberts & Tarassenko, 1995). \\nPotential functions \\nThe concept of potential functions has a variety of meanings within the \\npattern recognition literature. To some users it is synonymous with ker\\xad\\nnel methods (Section 6.1). Its origins (Bashkirov et al., 1964; Aizerman \\net al., 1964a, b, 1965; Braverman, 1965; Arkedev & Braverman, 1966) \\nare close to those of radial basis functions. Suppose we consider each \\nobservation X; as having some 'charge' q; and measure the 'potential' \\nat another point x. It will be of the form \\nand this provides another way to approximate by a smooth function \\n(except perhaps at the data points). The 'potential' is to be chosen by \\nthe user, and so could subsume both kernel methods and radial basis \\nfunctions . \\nThe potential-function classifier is trained to attempt to correctly \\nclassify all the samples of the training set by adjusting the qi by a \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 147}, page_content='136 4 Flexible Discriminants \\nperceptron-like procedure. Indeed, it can be seen as applying perceptron \\nideas to generalized discriminant functions. \\nPotential functions can also be used to approximate probability \\ndensity functions (Kashyap & Blaydon, 1968; Tsypkin, 1966). \\nThe method is considered in more detail in books by Meisel (1972) \\nand Young & Calvert (1974). It seems to have disappeared from view \\nuntil revived as the study of radial basis functions. \\n4.3 Regularization \\nAn alternative to reducing the number of basis functions in fitting \\nRBFs is to allow one per training example, but to control directly the \\nsmoothness of the fitted function, as is done for smoothing splines in \\none dimension. Indeed, since splines are so useful in one dimension, \\nthey might appear to be the obvious method in more. In fact they turn \\nout to be rather restricted and little used. \\nThis section is dominated by least-squares fitting, since regulariza\\xad\\ntion has been most explored in approximation theory. We know of \\nno exact solutions (such as smoothing splines) for other forms of our \\nproblem such as fitting multiple logistic models by maximum likelihood. \\nBishop (1991) considers (4.12) with a centre at every example in the \\ntraining set, but adds a penalty term when fitting. Let f(x) denote the \\napproximating RBF. Then the term to be minimized is \\nL IIYi-f(x;)ll2 + AC(f), \\ni \\nwhere i refers to the i th training example. This is an example of a \\ngeneral process termed regularization in which other penalties C(f) may C(f) is sometimes \\nbe considered. The parameter A controls the smoothness and degree called a stabilizer. \\nof fit. For A = 0 the fits will usually be exact (from interpolation \\nproperties of RBFs) and as A ---+ oo the fitted function becomes flat. \\nBishop chooses A by trial and error. The sum over .examples can be \\nseen as an approximation to an integral over fl£. \\nAdding a penalty C(f) has a Bayesian interpretation. The first term \\nis proportional to the log-likelihood if we assume that the noise variance \\na} in a regression is known, so if we take a prior over functions f \\nwhich is proportional to exp -2Aa}C(f), minimizing a penalized sum \\nof squares is equivalent to maximizing the posterior density over f. \\nThis is a MAP estimator (see Section A.1) and is widely used in image \\nanalysis (following Geman & Geman, 1984). However, the warnings \\nabout MAP estimation given in Section A.1 must be borne in mind. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 148}, page_content=\"4.3 Regularization 137 \\nSuppose we add a penalty of the form C(f) = II P f 112 for a differ\\xad\\nential operator P. We can then consider the function f minimizing the \\npenalized sum of squares \\nL IIYi-f(xdf + .?ciiP/112 (4.13) \\nover all (smooth enough) functions f, not just those represented by a \\nform of RBFs. Exactly as for smoothing splines, the general solution \\n(Poggio & Girosi, 1990b; Wahba, 1990) is of the form \\n(4.14) \\nwhere G is the (symmetric) Green's function of P P, P is the adjoint \\ndifferential operator, and n(x) is a function in the null space of P. \\nIf the operator is translation or rotation equivariant, so will G be. \\nThus equivariance under rigid motions leads to Green's functions of \\nthe radial basis function form. The coefficients Ci satisfy the linear \\nequations \\n(4.15) \\nand n(x) is chosen by least squares. \\nThe Gaussian RBF arises from the (non-intuitive) penalty functional \\n(Poggio & Girosi, 1990b, pp. 95-96), and the null space gives the \\nconstant IX. The penalty is more obvious when expressed in the Fourier \\ndomain, and Girosi et al. (1995) consider the class of penalties \\nC(f) = j J~s)J2 ds \\nG(s) (4.16) \\nfor some positive symmetric function G that tends to zero as llsll ---+ oo. \\nHere the tilde denotes Fourier transformation, and it turns out (Dyn, \\n1987; Madych & Nelson, 1990; Girosi et al., 1995) that G is the Fourier \\ntransform of the function G(x-xi) in (4.14). In this formulation the \\nGaussian RBF arises from G(s) = exp-,BIIsf. \\nAlthough regularization is theoretically interesting, it demands the \\nsolution of large systems of linear equations (4.15). In the case of \\nsmoothing splines in one dimension this is a banded system and can \\nbe solved quickly, but in general it will take O(n3) operations for n \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 149}, page_content='138 4 Flexible Discriminants \\nexamples and so be prohibitively slow. It remains possible to use a \\nsmaller set of basis functions as in the previous subsection and to use a \\npenalty functional to control the smoothness . We could use a general\\xad\\npurpose optimizer to minimize the penalized measure of fit over all \\nparameters rather than solve (4.15) within a loop. \\nAlthough we have considered only least-squares problems in this \\nsubsection, similar considerations apply to other deviance functions, \\nsince they can be approximated locally at the optimum by a weighted \\nsum of squares function, and this continues to have a solution of the \\nform (4.14) with appropriate modifications to (4.15). \\nAnother approach to regularization is to add noise during training \\n(see, for example, Sietsma & Dow, 1991). Suppose we add a moderate \\namount of white noise to the target values Yi· Then (Webb, 1994; \\nBishop, 1995b) a second-order Taylor expansion shows that the effect is \\napproximately (despite Bishop\\'s title) the same as using the regularizer \\nC(f) = ~ E [ ( 8\\n~~~)) 2 \\n+ H.fi(X)-Yil ;;\\n1~]. \\n1,] \\nBishop shows that for small added noise and a good fit the first term \\ndominates (since E[fi(X) -Yil will be small), so the regularization is \\nmainly by the expected squared length of the first derivative. With \\nnon-least-squares error functions, local linearization gives a weighted \\nleast-squares approximation and hence an expected weighted sum of \\n(ofi(X)/8Xj)2. \\nMultidimensional splines \\nSmoothing splines in one dimension arise from the regularization \\npenalty J g\"(u)2 du on the sum of squares at the data points. This \\npenalty does not generalize immediately to higher dimensions, and a \\nsuitable generalization took some years to emerge. Thin-plate splines \\nuse the penalty \\n!! a2j(x,y) 282j(x,y) 82f(x,y) d d \\nox2 + oxoy + oy1. X y \\non 1R.2 which is invariant under rigid motions, and \\nm! j [ amj(x) ]2 \\na a dx 2: OCJ ! • • • OCn! OX 1 \\n• • • OXn\" tXJ+··+an l \\nin JR.P. We have to take 2m > p to ensure this is a smoothing \\npenalty, since a radially symmetric bump of width h will have penalty \\nproportional to h2m-p which should go to infinity as h -+ 0. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 150}, page_content=\"4.3 Regularization 139 \\nThe solution to the penalized least-squares problem is of the form \\nf;.(x) = cp(x) + L c;G(IIx-xdl) \\ni \\nfor a polynomial cp of total degree at most m -1 and \\nG(r) = { r2m-p log r if 2m-pis even \\nr2m-p otherwise \\n(Duchon, 1977; Meinguet, 1979; Wahba, 1990, p. 33). Thus univariate \\nsmoothing splines and our two-dimensional example correspond to \\nm = 2. The higher-dimensional cases lose the computational simplicity \\nof smoothing splines since the matrices are no longer banded. Note \\nthat m = p = 1 gives a penalty of the integrated square of the first \\nderivative, and the solution is piecewise linear, as used in MARS. We \\ncan only use a second-derivative penalty in p ~ 3 dimensions, and \\neven in two dimensions there are edge effects to consider (Green & \\nSilverman, 1994, Chapter 7). \\nAdditive models, including those with interaction terms such as \\nMARS, can be put within the spline framework by choosing a suitable \\nregularizer (Wahba, 1990, Chapter 10). Summing penalties gives an ad\\xad\\nditive model, and including terms which involve two or more variables \\ngives rise to interaction terms, most simply for m = 1 (since for m > 1 \\nthere are interactions between polynomials and splines to consider). \\nThese are known as additive and interaction splines respectively; the \\nlatter are considered by Barry (1986), Gu & Wahba (1991) and Gu et \\nal. (1989). \\nTensor product splines are similar to MARS in that the interaction \\nterms are products of functions of a single feature. They arise from a \\npenalty of the form (4.16) in which G is a product of functions of a \\nsingle feature. \\nFitting additive models with interaction terms via splines gives rise \\nto a potentially large number of parameters A to be considered, one \\nfor each term. For m = 2 there are 22 A's to choose even for p = 4, \\nwhich is beyond current methods to select. \\nAsymptotic theory \\nOnce a regularization term is added to the fitting criterion, for exam\\xad\\nple the deviance, the asymptotic distribution theory of the parameter \\nestimate will be changed, whether or not the model is true, except in \\nthe unlikely event that the penalty is completely ineffective. From their \\nintended purpose, we would expect the effect of regularization terms to \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 151}, page_content='140 4 Flexible Discriminants \\nbe to introduce bias, even asymptotically, but to decrease the variance. \\nThis raises the possibility of juggling the amount of the penalty so that \\nthe bias decreases as n --+ oo but the variance remains under control, \\nindeed decreases to zero at a rate close to 0(1/n). \\nThis general programme is often considered \\'non-parametric\\' since \\narbitrarily complex models will be needed to fit a true model outside \\nthe assumed class of functions . (For instance, consider Gaussian RBF \\nfunctions with centres at data points, and approximating an exactly \\nlinear function.) The results of Section 2.8 provide sufficient bounds on \\nthe complexity of the model needed for n points which can allow the \\npenalty to be varied with n, and a heuristic outline is given by Geman \\net al. (1992) with a more detailed account by White & Woolridge (1991) \\nand White (1990). As an example of these techniques we can consider \\na projection pursuit regression with the number r of terms growing at \\nO(n1-\") for E > 0, and control the smoothness of the terms to grow at \\nO(log n), thereby achieving risk consistency. \\nThis asymptotic programme has little to do with the performance \\nin realistic situations, since there is not usually a large enough training \\nset relative to the complexity of the fitted classifier. For useful results \\nwe need an approximation, not a limit theorem. Moody (1991, 1992), \\nLiu (1993, 1995) and Murata et al. (1993, 1994) attempt to provide this \\nin a limited set of circumstances. They assume a regularization penalty \\nproportional to n (unrealistic according to the previous paragraph, \\nwhich suggests it should grow less rapidly, but much better than con\\xad\\nstant regularization). Then we can use the results of Propositions 2.2 \\nand 2.3 in Section 2.2 with Lt = Et + A.Ct replacing -logp, where E1 \\nis a term in the fit criterion (for example the contribution of an example \\nto the negative log-likelihood) and Ct = C/n. This leads to \\nNIC = 2[E(O) + A.C(B) + p*] \\nwhere p• = trace[KJ-1] and \\nd K _ V oLt(8o;X) an -ar o8 . \\nIf the penalty A.C(O) measures the smoothness of the fitted function, it is \\nthe same for both the training and test sets, so the expected difference \\nin E between test and training sets of size n is also p• (which is \\nMoody\\'s formulation). Sometimes if the penalty is an integral over \\nthe feature space of the form C(8) = Jq-G[f(x; 8)] dx it is replaced by \\nC(l}) = * 2::: G[f(Xi; 8)], which has expectation C( 8) for both training '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 152}, page_content=\"4.3 Regularization 141 \\nand test sets. Thus a penalty of the form C(O) can also be deleted from \\nNIC. \\nMoody (1991, 1992) calls 'the effective number of parameters', Pelf, \\nthe estimate of p• obtained by replacing the expectations in J and K \\nby averages over the training set, and ()o by 0. There is a potential \\nbias in using the training set to estimate p• since it shows how many \\neffective parameters are needed to approximate the true distribution \\nover a set of size n, and that could be much smaller than the number \\nneeded to approximate the whole distribution. Further, as we suggested \\nin Section 2.2, a divisor n -p for the estimate of K would be more \\nappropriate. \\nWe now return to the approximation of Bayes factors in Section 2.6. \\nWe can always regard p(()) oc -.IcC(()) as a prior density for the \\nparameters (), although it may well be an improper prior (with infinite \\nintegral). Then choosing () to minimize the negative log-likelihod plus \\n.IcC(()) is equivalent to MAP estimation, and we can use (2.43) as an \\napproximation to logp(:Y I M). \\nChoosing A. \\nTo actually use regularization, we have to select all the A's. If they \\nare derived from Bayesian priors as in Section 5.5, they are already \\ndetermined, but otherwise they are treated as free parameters (and \\nare also so treated in some empirical Bayes schemes). We can use the \\nmethods of Section 2.6, in particular cross-validation. However, this has \\ntwo disadvantages. The first is computational unless updating schemes \\nare known (as for univariate smoothing splines; Silverman, 1985). \\nThe second is that cross-validation is not invariant under orthogonal \\ntransformations of the data vector in a regression problem. Generalized \\ncross-validation (GCV) computes the average adjustment of the fit on \\nleaving each point out, rather than producing the adjustment for each \\npoint (Craven & Wahba, 1979). Thus in a regression problem we have \\nGCV(./c) = _.!._ l:f:t IIYi-JA(xi)ll2 \\nN [1-trace(A(.Ic))/ N]2 \\nwhere A,t is the matrix mapping the data vector to the vector of \\nthe fitted values. Where several A's are involved, A and GCV be\\xad\\ncome a function of all of them, and a simultaneous minimization is \\nneeded; Wahba (1990) reports using up to 10 2 's for additive interaction \\nsplines. Except for univariate splines, the computation is dominated \\nby the computation of A(./c). Hastie & Tibshirani (1990, §9.4.3) suggest \\napproximating traceA(./c) by one plus the sum of the traces of the \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 153}, page_content='142 4 Flexible Discriminants \\nmatrices for the individual smoothers minus one, which seems ad hoc \\nbut is much faster. This is used in BRUTO. \\nFor non-least-squares problems there seems no general alternative \\nto V -fold cross-validation. Hastie & Tibshirani ( 1990, §6.9) propose \\na version of GCV which weights the deviance by the divisor of the \\nleast-squares form, but this seems unsupported theoretically except as \\na local weighted least-squares approximation (given by Wahba, 1990, \\np. 113). '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 154}, page_content=\"What we denote by W;j, \\nthe weight on the link \\nfrom i to j, is more \\noften denoted Wji· 5 \\nFeed-forward Neural \\nNetworks \\nA great deal of hyperbole has been devoted to neural networks, both in \\ntheir first wave around 1960 (Widrow & Hoff, 1960; Rosenblatt, 1962) \\nand in their renaissance from about 1985 (chiefly inspired by Rumelhart \\n& McClelland, 1986), but the ideas of biological relevance seem to us \\nto have detracted from the essence of what is being discussed, and are \\ncertainly not relevant to practical applications in pattern recognition. \\nBecause 'neural networks' has become a popular subject, it has collected \\nmany techniques which are only loosely related and were not originally \\nbiologically motivated. In this chapter we will discuss the core area of \\nfeed-forward or 'back-propagation' neural networks, which can be seen \\nas extensions of the ideas of the perceptron (Section 3.6). From this \\nconnection, these networks are also known as multi-layer perceptrons. \\nA formal definition of a feed-forward network is given in the glos\\xad\\nsary. Informally, they have units which have one-way connections to \\nother units, and the units can be labelled from inputs (low numbers) \\nto outputs (high numbers) so that each unit is only connected to units \\nwith higher numbers. The units can always be arranged in layers so \\nthat connections go from one layer to a later layer. This is best seen \\ngraphically; see Figure 5.1. Each unit sums its inputs and adds a con\\xad\\nstant (the 'bias') to form a total input Xj and applies a function fj to \\nXj to give output Yj· The links have weights Wij which multiply the \\nsignals travelling along them by that factor. The input units are there \\njust to distribute the inputs, so have f = 1. Thus a network such as \\nFigure 5.1 represents the function \\nYk = !k ( rxk + L Wjk/j( rxj + L. wijxi)) \\nj~k I~] (5.1) \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 155}, page_content=\"144 5 Feed-forward Neural Networks \\nBias units \\nHidden \\nlayer(s) \\nfrom inputs to outputs. The functions fJ are almost invariably taken \\nto be linear, logistic (with f(x) = t(x) = ex /(1 +ex)) or threshold \\nfunctions (with f(x) = I(x > 0) ). A variant is to take hyperbolic \\ntangent units with f(x) = tanh(x) = (ex-1)/(ex + 1) = 2t(x)-1, but \\nthis only introduces a linear transformation which can be absorbed \\ninto the weights (except at the output units). Only threshold units give \\na genuine multi-layer extension of the perceptron, and such networks \\nwere considered in Rosenblatt's work. \\nThe general definition allows more than one hidden layer, and it \\nalso allows 'skip-layer' connections from input to output. If all units in \\na layer have the same function fh or f0, we have \\nYk = fo ( rxk + L WikXi + L Wjdh ( aj + L WijXi)). (5.2) \\ni->k j->k i->j \\nThe bias terms can be eliminated by introducing a new unit 0 (the \\nbias unit) which is permanently at + 1 and connected to all other units. \\nWe set wo1 = a1. (This is the same idea as incorporating the constant \\nterm in the design matrix of a regression by including a column of 1's.) \\nThis is shown in Figure 5.1. The general form is then \\nYk = fo ( L WikXi + L Wjk/h (L WijXi)). (5.3) \\ni--->k }->k i-+j \\nNote that if we have logistic units in the hidden layer, adding skip-layer \\nconnections is not really more general, since we can add another unit \\nper output in the hidden layer with input weights wik!G and output \\nweight G to just unit k. Then for large G we only use the central, \\nlinear, part of the range of the logistic function. However, skip-layer \\nconnections can be easier both to implement and to interpret. Figure 5.1: A generic \\nfeed-forward network \\nwith a single hidden \\nlayer. To avoid \\nover-crowding bias \\nunits are shown for \\neach layer, but they can \\nbe the same unit. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 156}, page_content=\"5.1 Biological motivation 145 \\nA neural network with a single logistic output unit can be seen as \\na non-linear extension of logistic regression. With many logistic output \\nunits, it corresponds to linked logistic regressions of each class vs the \\nothers. \\nThe terminology of neural networks can be very confusing: Fig\\xad\\nure 5.1 is sometimes referred to as having three layers (which seems \\nvisually correct), two layers (as the input layer does nothing) and one \\nhidden layer (as the states of the units in the central layer cannot be \\ninspected from outside the 'black box'). We will refer to the inputs, the \\noutputs and the hidden layer, since we will almost always have only \\none hidden layer. \\nWe will extend our notation to allow every unit j to have an input \\nXj and output Yj· The inputs to the whole network are the inputs to \\nthe input units, and the outputs from the whole network are those of \\nthe output units. The signal paths through the network are determined \\nby the equations \\nXj = LWijYi· \\ni-+j (5.4) \\nWe can even drop the condition on the sum by defining W;j to be zero \\nfor all non-existent links. When programming it is useful to number \\nthe units by layer, so all units in the first layer precede all those in the \\nfirst hidden layer and so on. Then we know w;j = 0 unless i < j. \\nWe will briefly consider how such functions came to be suggested, \\nand the theory which shows that they form large and flexible classes of \\nfunctions. However, in practice the main issues are how the parameters, \\nthe weights, should be chosen, and how the architecture (the number \\nof layers and the number of units in each, as well as which connections \\nto include) is selected. \\n5.1 Biological motivation \\nThe original biological motivation for feed-forward networks stems \\nfrom McCulloch & Pitts (1943) who published a seminal model of a \\nneuron as a binary thresholding device in discrete time, specifically that \\nnj(t) =I (L Wijn;(t -1) > 8;) \\ni-+j \\nthe sum being over neurons i connected to neuron j. Here n;(t) is \\nthe output of neuron i at time t and 0 < W;j < 1 are attenuation \\nweights. Thus the effect is to threshold a weighted sum of the inputs \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 157}, page_content=\"146 5 Feed-forward Neural Networks \\nat value ei. Real neurons are now known to be more complicated; \\nthey have a graded response rather than the simple thresholding of \\nthe McCulloch-Pitts model, work in continuous time, and can perform \\nmore general non-linear functions of their inputs, for example logical \\nfunctions. Readers may worry that this model can only allow non\\xad\\nnegative weights. This is so, but neural systems have both excitatory \\nand inhibitory connections, so whereas each connection can effectively \\nhave a weight of just one sign, it would be possible to envisage both \\nan excitatory and an inhibitory connection, to simulate the effects of \\nweights of either sign. \\nThere is also a wider motivation based on human abilities in pattern \\nrecognition. A driver can recognize that a traffic light has changed to \\nred and take the appropriate action (or decide not to) in well under one \\nsecond. Neurons are rather slow devices by the standards of electronic \\ncomputers, with messages travelling at up to 100m/s, and of low \\nbandwidth-perhaps 100 bits/s (MacKay & McCulloch , 1952). This \\nallows rather few steps in the computation, most famously expressed in \\nFeldman's (1985) concept of a 'one hundred step program', since there \\nis time for at most 100 steps within a human reaction time. Human \\nbrains must make up for this lack of speed by massive parallelism, \\nand given the speed of messages this parallel computation must be \\nhighly distributed. Brain scientists currently envisage vision as being \\nperformed in a series of layers in the brain, which naturally suggests \\na layered architecture for the distributed computation. (There is some \\nevidence for feedback from later layers to earlier layers in human vision, \\nbut this is recent and controversial.) \\nFurther, human beings can learn tasks such as driving; it seems \\nextremely implausible that we are pre-programmed to recognize red \\ntraffic lights nor (except for advocates of Lamarckian evolution) that \\nthis is an inherited adaptation. Something in our distributed neural \\ncomputing system changes through experience. This could involve \\nadding or removing connections or units, but most emphasis has been \\non exploring the consequences of changes in the strengths of connections \\nrather than their topology. \\nThe book of Hebb (1949) has been very influential in thinking \\nabout how connection strengths should be changed. His approach is \\nnot quantitative, and so is all-embracing. He considers that the con\\xad\\nnection strength between units should be increased if they are activated \\ntogether, and this is often taken to suggest reinforcing the connection \\nproportionally to the product of their simultaneous activations, that is \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 158}, page_content=\"Earlier, Cybenko (1988) \\nhad showed that two \\nhidden layers suffice. \\nUniform convergence \\non compacta is defined \\nin the glossary. 5.2 Theory 147 \\nThis is known as the Hebbian learning rule, widely used in other forms \\nof neural network. \\nThis idea that there could be a simple adaptation rule for connection \\nweights used universally in learning found fruit in the experiments of \\nRosenblatt (1957, 1958, 1962) and Widrow (Widrow & Hoff, 1960). As \\ndiscussed in Chapter 3, they used very simple units, in one or few layers, \\nwith weights expressed by motor-driven potentiometers. Widrow-Hoff \\nlearning (also known as the delta rule) was an iterative algorithm of the \\nreinforcement type to fit a linear regression or discriminant. This seems \\nto have been the inspiration for the rules used for 'learning' in neural \\nnetworks in their renaissance. Similar rules were proposed for learning \\nin potential function systems by Aizerman et al. (1964a, b, 1965). \\n5.2 Theory \\nEquations (5.1) and (5.2) are thus far merely suggestive ways to \\nparametrize multidimensional input-output relationships. They are \\nrather general classes of functions, something which took a long time \\nto be appreciated. Cybenko (1989), Funahashi (1989), Hornik et al. \\n(1989), Carroll & Dickinson (1989), Stinchcombe & White (1989) and \\nmany later authors have shown that neural networks with linear out\\xad\\nput units and a single hidden layer can approximate any continuous \\nfunction f uniformly on compacta, by increasing the size of the hidden \\nlayer, and this implies many other types of approximation. There are \\nalso some results on the rate of approximation (how many units are \\nneeded to approximate to a specified accuracy), but as always with such \\nresults they are no guide to how many units might be needed in any \\npractical problem. These results are in fact rather easy to prove, very \\nmuch easier than most published proofs, so we give complete proofs in \\nSection 5.7. \\nA heuristic reason why feed-forward networks might work well with \\nmodest numbers of hidden units is that the first stage allows a projection \\nonto a subspace of :!{ of much lower dimensionality, within which the \\napproximation can be performed. In this feed-forward neural networks \\nshare many of the properties of projection pursuit regression (PPR; \\nSection 4.1). Indeed, for theoretical purposes the two are essentially \\nequivalent. We consider only linear output units, as clearly logistic, \\nsoftmax or other transforms can be applied to the outputs of either \\nfamily. Clearly ( 5.1) is a special case of PPR ( 4.5), taking the smooth \\nfunctions to be logistic functions. Conversely, if we have a general PPR \\nYk = IXk + L gj(!Xj + L /3j;X;) \\nj i \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 159}, page_content='148 5 Feed-forward Neural Networks \\nwe can approximate each smooth function gj as a sum of shifted \\nlogistic functions (a special case of the results of Section 5.7), so \\ngj(X) ~ LYjlt(bj[ + (j[X) \\nI \\nYk ~ cxk + LYjd(cx}I + L f3}uxi) \\nj,/ i \\nand on writing j, l as a single index the right-hand side is seen as a \\nspecial case of (5.1). \\n5.3 Learning algorithms \\nKnowing that an approximation exists is useless without some way to \\nfind it, and it was this step which held up research in neural networks \\nfor many years. The original idea of the Rumelhart-McClelland school \\nwas to fit the parametrized function by least squares. Suppose we have \\nexamples (xP, tP), and that the output of the network is y = f(x; w). \\nThen the parameter vector w is chosen to minimize \\nE(w) = L IW-f(xP;w)ll2 \\np (5.5) \\nas would be done in non-linear regression (Bates & Watts, 1988; Seber \\n& Wild, 1989). (Note that this is a sum of squares over both output \\nunits and input examples.) As this is a minimization problem, we can \\nuse general algorithms from unconstrained optimization (Section A.5), \\nand we shall see that this seems the most fruitful approach. \\nNote that E(w) is a differentiable function only for differentiable \\nunits, and from now on we assume differentiable units, thereby excluding \\nthreshold units. Indeed, it was the change from the threshold units of \\ngenuine multi-layer perceptrons to logistic units which enabled effective \\nalgorithms to train these networks to be found. These algorithms all \\nrequire the gradient of E(w) with respect to the weights. \\nThe Rumelhart-McClelland group used a form of steepest descent \\nto reduce ( 5.5), with update rule \\n(5.6) \\nand since the partial derivative can be written in the form (see the next \\nsubsection) '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 160}, page_content=\"5.3 Learning algorithms 149 \\n(originally with the sign of <5 reversed) this has become known as \\nthe generalized delta rule. (Here and later the superfix P refers to \\ncalculations involving example p.) Further, as the <5 'scan be computed \\nfrom output to input across the network (see the next subsection) both \\nthe process of calculating the derivatives and the descent algorithm are \\nknown as back-propagation. \\nAlternative discrepancy functions for logistic regressions have been \\nconsidered in Sections 2.3 and 3.5; most ofthese have been re-discovered \\nwithin the neural networks literature. The conditional log-likelihood \\n(2.31) for a two-class problem has been widely suggested (Solla et al., \\n1988; Bichsel & Seitz, 1989; Hinton, 1989a; Bridle, 1990a, b; Holt & \\nSemnani, 1990; Spackman, 1992; van Ooyen & Nienhuis, 1992). This \\nis often summed over multiple logistic output units to give \\n[ tp 1 tp] \\nE = L L tf log ~ + ( 1 -tk) log -~ \\np k Yk 1-Yk (5.7) \\nthe terms in t log t being chosen so that E ~ 0 with equality only for \\na perfect fit. \\nThe log-linear approach to classification gives rise to what Bridle \\n(1990a, b) termed softmax. This is no different from the multiple logistic \\nmodels considered several times in earlier chapters, but for convenience \\nwe will recap the notation here. We have \\n(5.8) \\nand 0e classifier chooses the class max1m1zmg p(k I x) and hence \\ngk(x; 8). To use this with a neural network, we take the Yk to the \\noutputs of a network with linear output units, and compute probabili\\xad\\nties by \\nexpyk \\nPk = · l::jexp Yj (5.9) \\nMinus the log-likelihood for the multinomial distribution is then \\nsummed as usual over examples. The targets tk will usually be one for \\nthe correct class, and zero for the others, and the probabilities are given \\nby (5.9) computed from (yk). From now on we will assume that exactly \\none of the targets is one, all the others are zero, so L:j tj = 1. The \\noutputs (Yk) can all be changed by the same additive constant without \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 161}, page_content='150 5 Feed-forward Neural Networks \\nchanging the probabilities or the fit, so there is a degree of redundancy. \\nIt is often convenient to take gk(x; 8) = 0 for some preferred class k. \\nThe error criteria of robust statistics (Huber, 1981; Hampel et al., \\n1986) may be used to replace least squares, as in Chen & Jain (1994). \\nLiu (1994) uses a t distribution for regression errors, and hence a \\ndifferent robust error criterion. \\nBack-propagation \\nRecall that each unit has input Xj = Ei ..... j WijYi and output Yj = fj(xj). \\nEach form of the fit criterion E is a sum over examples, so we calculate \\nthe derivatives of EP, which can then be summed over examples. For the \\nrest of this subsection we consider one example and drop the superfix \\nP. \\nIn these calculations we will take partial derivatives of E with \\nrespect to weights Wij and with respect to inputs Xi and outputs Yi of \\nunits. We have to make clear precisely what is kept fixed. (The literature \\nwith very few exceptions does not. Werbos, 1994, has a concept of \\nordered derivatives for this.) When we take partial derivatives with \\nrespect to weights, we regard E as a function of all the weights, so \\nchanges in a weight Wij affect the input and output of unit j and \\nall units connected to j, including some output unit(s). When we take \\npartial derivatives with respect to an input or output, we allow all other \\nsignals in the network which depend on the input or output to follow \\ntheir usual dependence. Thus all weights and all inputs (and hence \\noutputs) of other units in the same and earlier layers are kept fixed. We \\nevaluate 8Ej8xj by noting that Xj only affects the outputs through \\nyj, and this only acts through connections to output units. \\nFor the first derivatives we have \\naE aE axj oE , oE -8-= -8 -8-= Yi-0 = yJ1·(Xj)-0 = y/Jj (5.10) Wij Xj Wij Xj Yj \\nif we define [Jj = 8Ej8xj. The first equality comes from the dependence Sometimes bj is given \\nof E on the weights only through the outputs, the second from Xj = a sign change, to set \\n2: Wij Yi ( 5.4 ). We note that w;j .._ wij + 17 L yf bj \\nFor output units oEjoyj can be calculated directly from the form \\nof E. It is customary to express fj(xj) in terms of yj; for logistic units '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 162}, page_content=\"The results here are \\nonly needed by the \\nwriters of programs, so \\nothers may wish skip to \\nthe next subsection. \\nOur development \\nfollows Ripley (1994b) \\nand extends that of \\nBishop (1992). 5.3 Learning algorithms 151 \\nwe have f'(x) = y(1-y). For an output unit o we have the expressions \\n(jo = 2y0(1-Yo)(Yo-t0), logistic output unit, least squares \\n(jo = (Yo -t0), logistic output unit, entropy fit \\n(jo = 2(yo-to), linear output unit, least squares \\n(jo = ( E1 t1) Po-to, softmax \\nFor units in earlier layers we have \\n(jJ = f'/xJ) ~E = fj(xj) L WJk :E = fj(xj) L :: ~x~ \\nYJ k:j->k Xk k:j->k k YJ \\n= fj(xJ) L WJk(jk. (5.11) \\nk:j->k \\nthe sum being over units k fed by unit j. (The first and last equalities \\nfollow from definitions; the second traces the effect of the output of an \\ninternal unit via the units to which it is connected.) Since the formula \\n( 5.11) for (); only contains terms in later layers, it is clear that it can \\nbe calculated from output to input on the network. This simple idea \\nhas been re-discovered many times, and much credit has been given \\nfor it. Werbos (1974) had the idea of organizing such computations as \\na recursive computation, but its modern use stems from Rumelhart et \\nal. (1986) and Rumelhart & McClelland (1986, Chapter 8). It is often \\ndiscussed as a forward pass to calculate the outputs from the inputs, \\nfollowed by a backward pass to calculate ([J;) and hence oEjow;1. In \\ncontrol theory (Bryson & Ho, 1969, §2.2) the idea occurs in a more \\ngeneral form if the weights are considered as control inputs to the \\nlayers. \\nSecond derivatives \\nWe can find the Hessian of the fit criterion E with respect to the \\nweights by extending the derivation of (5.10). We use the symmetry of \\nthe Hessian matrix to require that j is never in a later layer than l in \\nthese expressions. (This ensures that x 1 is changed by w;1 but not by \\nWk/, although Xt could be changed by both.) Then we have \\nH(w)iJkl = 82£ = Yi _!__ oE = Yi o(yk[Jl) \\n, OWijOWk/ OXj OWkt OXj \\n[ oyk 8(),] oyk = Yi (jl-+ Yk-= Yi (jl-+ Yi Yk h11 (5.12) OXj OXj OXj \\nwhere hjl = 8[Jif8xj = o2EjoXj0XI = (j{jjjax,. The first term is zero \\nunless j = k or there is a path from j to k. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 163}, page_content='152 5 Feed-forward Neural Networks \\nFor a general network with a single hidden layer (allowing connec\\xad\\ntions from input to output) the first term must be zero unless j = k is \\na unit in the hidden layer. Thus we have: \\n1 If both j and l are output units the first term is zero, and so we \\nhave a2E \\na a = Yi Yk hjl· Wij Wk[ \\n2 If j is in the hidden layer and l in the output layer \\na2E -~-= Yi (b1l(j = k)fj(xj) + Yk hjl] awijaWki (5.13) \\n= Ydj(xj) [btl(j = k) + Yk L Wjmhmt] (5.14) \\nJ-+m \\non differentiating (5.11) with respect to x1• \\n3 If j and l are both in the hidden layer \\na2 E abt a [ , ] a a = Yi Yk -a = Yi Yk -a fL(xt) L W[mbm Wij Wk[ Xj Xj 1-+m \\n= Yi Yk [l(j = l)fj(xj) ~ Wjmbm \\nJ-+m \\n+ Jj(xj)f/(xt) L L WjmWtnhmn] . (5.15) \\nj-+m 1-+n \\nThese expressions only involve hjt for units in the output layer. If we \\ndifferentiate the expressions given for b0 we find \\nhoo = 2yo(1 -Yo)(2Yo -to+ 2toYo-3y;), \\nhoo = Yo(1 -Yo), \\nhoo = 2, \\nhjt = pjl(j = l)-PjPl, logistic output unit, least squares \\nlogistic output unit, entropy fit \\nlinear output unit, least squares \\nsoftmax \\nIn the first three cases the off-diagonal terms of (hjt) are zero. \\nThe double sum in the third case simplifies for softmax, for if we \\ndefine Hj = L.j-.m WjmPm we have \\n= Yi Yk [1(j = l)fj(xj) ~ Wjmbm \\nJ-+m \\n+Jj(xj)f/(xt){~ WjmWtmPm- HjHt}]. \\nJ-+m '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 164}, page_content='5.3 Learning algorithms 153 \\nBuntine & Weigend (1994) give rules for finding second derivatives \\nin more general networks, in particular those with more than one hidden \\nlayer, but the results here suffice for the networks used in practice. \\nSeveral algorithms use the Hessian H(w) only to compute H(w)v \\nfor a few directions v (for example as part of a line search along \\ndirection v, although that only needs vT H(w)v ). Pearlmutter (1994) \\nshows how to compute Hv without computing the whole matrix H, \\nby a technique he calls 9P-backpropagation (and which also appears in \\nWerbos, 1988). Define the operator \\na 9P[f(w)] = a,f(w + rv) (5.16) \\nas the directional derivative in direction v. \\nWe want to compute Hv = (9l[oEjowiJ]). Let ~i = oE/oYi· We \\ncompute oE I OWij by \\ni-+} \\n~} = LWJkbk \\n}-+k \\nafter computing b0 from the formulae above (5.11). Applying the 9l \\noperator using the normal rules for a derivative operator, we find \\n(Hv)iJ = Yi9l[bj] + 9l[yiJbJ \\n9P[bj] = fj(xj)9l[~j] + fj(xj)9l[xj]~J \\n9l[~j] = L Vijlh + Wjk9l[bk] \\n}---+k \\n9P[yi] = J;(x;)9l[xi] \\n9P[xj] = L ViJYi + WiJ9l[yi] \\ni---+j \\nThe last two equations then form a forward pass, the first three a \\nbackward pass (started by 9P[b0] for the output units o ). Whether \\nthese are easier to use than (5.13-5.15) will depend on the application, \\nand in particular on how special v is. \\nThe classic algorithm \\nThe basic back-propagation algorithm (5.6) has been modified in many \\nways. In the original Rumelhart & McClelland experiments (1986, '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 165}, page_content=\"154 5 Feed-forward Neural Networks \\np. 330) 'momentum' was added, that is exponential smoothing was \\napplied to the correction term, so we have \\nWij +-Wij -1} [(1-a):~} + e<(dwij)previous]. (5.17) \\nThey also considered the 'on-line' version of (5.17), that is \\nWij +-Wij -17'yf b)+ ct'(dWij)previous (5.18) \\nand updating the weights after every example. This only makes sense if \\nthe examples are presented in a random or unstructured order, in which \\ncase the momentum creates an approximation to (5.6). In contrast, (5.6) \\nand ( 5.17) are sometimes known as 'batch' algorithms. \\nThis algorithm can be implemented by a form of distributed com\\xad\\nputing on a (two-way) network, with outputs being passed forward and \\nthen b 's being passed back. \\nThere seem to be three motivations for the 'on-line' algorithm. One \\nis the biological motivation of learning from every experience. Another \\nis that it can converge faster than the batch version. Suppose that the \\ntraining set contains large numbers of exact or near duplicate examples. \\nThen the average over a small proportion of examples will provide a \\ngood approximation to E or its derivatives, and we would expect the \\non-line methods with a small momentum term to do well. However, in \\nthat circumstance the alternative is to use a small sample of examples \\nin the batch algorithm, at least in the early stages of training. The third \\nis a belief that by introducing 'noise' into the algorithm (the random \\nchoice of which example to present) local minima in the optimization \\nare more likely to be avoided. (We return to this on page 156.) \\nIterative algorithms need both a starting point and a stopping rule. \\nThe starting point is usually taken to be a random set of weights. Some \\ncare is needed that they are not taken to be too large, for if all the \\ncombinations ,E1 wiJxf are initially large, the hidden units start in a \\n'saturated' state (with outputs very near zero or one). \\nThe stopping rule does need a form of central control. The earliest \\nidea was to stop when (if) E became small. This is often fine in \\nlogical problems, where no example is ever mis-classified, but can result \\nin poor generalization. Very many ad hoc stopping rules have been \\nproposed. One which seems popular is to have a validation set, and \\nstop training when the error measure on the validation set starts to rise. \\nThis is dangerous, as we have often encountered examples in which \\nafter an initial drop the error on the validation set rises slowly for a Exponential smoothing \\nis a method of \\nsmoothing time series, \\ntaking an exponentially \\ndecaying weighted \\naverage over past \\nvalues. We can compute \\nYt = (1-IX}l:::~ IXiXH \\nby \\nYt = (1 -1X)Xt + IXYt-1· \\nGeneralization is \\ndefined in the glossary; \\nit refers to the test-set \\nperformance. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 166}, page_content='5.3 Learning algorithms 155 \\nlarge number of iterations, then falls dramatically to a small fraction of \\nits previous minimum. Thus one can never know if the minimum error \\non the validation set has yet been attained. It is also not uncommon to \\nuse the test set rather than a validation set, as the use of a validation \\nset is thought wasteful. (One example in a textbook is in Thornton , \\n1992, p. 199.) \\nThe issue of when to stop is important , and we know of no sat\\xad\\nisfactory rule for this algorithm. Folklore suggests that disasters have \\nbeen saved because its convergence is so notoriously slow that users \\ncannot afford the computer time to overfit the training set. Much has \\nbeen made of the idea of stopping before convergence (for example by \\nFinnoff et a/., 1993). Note that the fitted weights will depend on the \\nstarting point if early stopping is used. This complicates the analysis of \\nearly stopping procedures. Wang et a/. (1994) attempt to study early \\nstopping of a linear regression problem, which is a neural network \\nwith no hidden layer and a single linear output unit. The algorithm \\nstudied is (5.6), batch learning without momentum. (They also allow \\nfixed transformations of the inputs, which adds nothing to the analysis.) \\nThey assume that the data were generated by random samples from \\na linear regression. Then there is an optimal stopping point before \\nconvergence, but this has a delicate dependence on the size n of the \\ntraining set and the starting point; their actual results are useful only if \\nthe starting point is taken to converge to the true value as n increases. \\nThat the starting point must be critical can be seen by considering \\nwhat happens if the initial weights happen to be the true weights, when \\nthe expected test-set performance will normally steadily decline during \\ntraining. \\nThe batch version of the classic algorithm can converge for fixed 1J, \\nbut the on-line version will continue to wander unless 1J is reduced to \\nzero. During training we want 1J to be large to approach the (local) \\nminimum rapidly, but small to avoid large excursions about the local \\nminimum. Both Amari (1967) and Heskes & Kappen (1991) studied the \\neffect of the choice of 1J, specifically finding the covariance matrix of w \\nat time t to be proportional to 1J, for large t and small 1J. From this \\nvarious rules have been used to adapt 1J; Amari\\'s original suggestion \\nwas to increase 1J if successive steps had an angle of less than 90°, and \\nto decrease 1J otherwise. White (1989b) uses results from stochastic \\napproximation to give conditions under which convergence is bound to \\noccur to a local minimizer ( 2:: 1Jn diverges, 1/1Jn -1/1Jn-1 be bounded \\nand 2:: 1J~ < oo for some d > 1 ) which are satisfied by 1Jn oc n-\" for \\nO<K~l. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 167}, page_content=\"156 5 Feed-forward Neural Networks \\nVariants of the classic algorithm \\nA number of ideas have been proposed to speed up the convergence of \\n(5.6). Many are reviewed by Jacobs (1988); for example, the constants \\nrt and IX in ( 5.17) can be chosen adaptively for each weight Wij· Some \\nfurther references are Schmidhuber (1989), Silva & Almeida (1990), \\nTollenaere (1990), Darken & Moody (1991), Salomon (1991) and Eaton \\n& Oliver (1992). One scheme that is popular is Quickprop (Fahlman, \\n1989) which uses a crude line-search over rt for each parameter. It \\nretains the immediate past value of the weight update, and fits a \\nquadratic using the past and current derivatives. If this has its minimum \\nat a sensible value the latter is used as the new weight, otherwise a \\nnumber of heuristics are used. The details are given at the end of this \\nsubsection. \\nAnalogies have been drawn between the on-line algorithm and \\nstochastic approximation (for example by White, 1989a, b, 1992), which \\ncan be seen as an algorithm of the form \\nn s:n Wij +-Wij-1'ln Yi uj \\nwith rtn ~ 0 for a sequence of randomly chosen examples, and which \\nthen converges to a local minimum of the least-squares criterion. In\\xad\\njecting further noise (Kushner, 1987; White, 1989a; Styblinski & Tang, \\n1990; Gelfand & Mitter, 1991), for example \\nfor independent Gaussian en and rtn oc 1/log(n + 1), can lead to \\na global minimum, analogously to simulated annealing. However, See the glossary for \\nstochastic approximation is not a very effective way to solve a least- simulated annealing. \\nsquares problem, not least because the magnitude of the current f(xP; w) \\nis ignored. \\nOne of the difficulties encountered by the classic algorithms is that \\nlogistic units may become 'stuck' at the wrong extreme, in which case it \\ntakes many steps to change them to the opposite extreme, since in (5.10) \\nwe have f'(x) = y(l-y) which is small if y is very near zero or one. \\nFahlman (1989) has suggested an offset, using say f'(x) = 0.1+y(1-y) . \\nvan Ooyen & Nienhuis (1992) argue for the entropy fit because for the \\noutput units b0 does not go to zero for Yo tending to the extremes \\nunless the fit is correct, and demonstrate that this leads to faster \\nconvergence in their examples. (However, saturation can still occur \\nat the internal units.) This effect has led to a number of claims of \\nalgorithms reaching local minima when they are merely in very nearly \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 168}, page_content=\"For a regression neural \\nnetwork with no hidden \\nunits this will be \\nequivalent to ridge \\nregression. 5.3 Learning algorithms 157 \\nflat regions. Genuine local minima do occur (Section 5.4), and this can \\nbe checked by considering the Hessian at a supposed minimum. \\nOne way to avoid saturation is to discourage large weights and \\nhence large inputs to units. Weight decay (Hinton, 1986) modifies the \\nclassic algorithm to \\nW;j +--Wij -17 LYf c5} -217AWjj \\np (5.19) \\nwhich tries to reduces the magnitude of the weights at each step. We \\ncan see that (5.19) is steepest descent applied to \\nE +A L w~ = E + J.C \\nij (5.20) \\nsay, a form of regularization. This will only make sense if the inputs \\nand outputs have been (roughly) rescaled to the range [0, 1] to be \\ncomparable to the outputs of the hidden units. In other problems \\nit may make sense to used a weighted sum of weights, and/or to \\nuse different weight decay parameters for groups of weights. Other \\nfunctions C have been used, for example (5.22) on page 170. \\nWith linear output units and the least-squares error criterion, the \\nselection of the output weights is a linear least-squares problem, and \\nas in a regression or an RBF network this can be solved without \\niteration; this also applies to skip-layer weights. (In the parlance of \\nnon-linear regression we have a 'partially linear' problem.) This has \\nbeen incorporated into a number of variant algorithms, for example by \\nShepanski (1987) and Hrycej (1992, Chapter 9). \\nAnother approach has been to turn the discrete-time update into \\nthe system of continuous non-linear differential equations \\ndw;i 8E -=-17-dt n awij (5.21) \\n(Owens & Filkin, 1989; Weiss & Kulikowski, 1991). Then the classic \\nalgorithm is seen as a very simple fixed-step Euler integrator for this \\nsystem, and much more sophisticated integration schemes can be used, \\nespecially those which are designed for stiff systems, those in which the \\nHessian has eigenvalues of very different magnitudes . Effectively these \\nschemes allow long steps for some linear combinations of the weights, \\nand short steps for others. \\nA similar approach for the discrete-time update has been to use \\nversions of the Kalman filter; see for example Singhal & Wu (1989), \\nRuck et al. (1992) and Chandran (1994). \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 169}, page_content=\"158 5 Feed-forward Neural Networks \\nThere have been numerous published comparisons of these variants, \\nbut these must be treated with circumspection, as the effectiveness of \\nad hoc devices can depend on the problem to be treated and how free \\nparameters are chosen, as well as on the starting point and the quality \\nof the implementation. \\nDetails of Quickprop \\nFahlman (1989) modifies the gradient 8Ej8wij both by including his \\noffset and a small oo-4) weight decay; let the modified gradient be \\ndenoted g(k) at the k th iteration for weight Wij. We take a quadratic \\napproximation along the line between the gradients g(k-1) and g(k) \\nand look for its minimum. This amount to finding the zero for a linear \\napproximation to the gradient, which occurs at w + a(k)8Ej8w(k -1), \\nwhere g(k) \\na(k) = g(k-1)-g(k) \\nThis is replaced by 1.75 if it exceeds 1.75 or is uphill along g(k) (when \\nthe quadratic would give a maximum). A learning rate is needed to \\nstart, to re-start for a ~ 0 and is also used if the gradient and the last \\nupdate have the same sign. Thus the update rule for WiJ becomes \\nWiJ +-Wij-0.55 I [gij(k)dwij(k- 1) > O]gij(k) + aij(k)dwij(k- 1). \\nThis can be seen as a combination of a line-search strategy through the \\ndependence on the last step, and gradient descent to re-start when the \\nthe line search is close to a minimum. Beware that both the value of \\n17 = 0.55 and the weight decay are not scale-free in E, and so will need \\nto be adjusted for regression networks. \\nOther algorithms \\nMany other algorithms have been proposed, but by far the most \\neffective in our experience are those which treat the minimization of the \\nfit criterion E or E + A.C as a general optimization problem. Steepest \\ndescent is generally regarded as a poor strategy for optimization, and \\nthe most widely used methods for optimizing differentiable functions \\nare based on approximations by quadratic functions. \\nIntroductions to these methods are provided in Section A.5 and by \\nFletcher (1987), Gill et al. (1981), Nash (1990) and Press et al. (1992). \\nFor realistic numbers of weights (up to thousands) quasi-Newton meth\\xad\\nods work well. For larger problems the storage of the approximate \\nHessian can be too demanding, and conjugate gradient methods or \\nthe limited-memory BFGS quasi-Newton method (see Section A.5) can The values of 'I and \\nthe weight decay are \\ntaken from Fahlman's \\npublicly distributed \\ncode. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 170}, page_content='The field of numerical \\noptimization is as large \\nas that of feed-forward \\nneural networks and \\nthe practical experience \\nvery substantial. Two \\nimportant issues are \\nwhen conjugate \\ngradient algorithms are \\nre-started, and how the \\nline searches are done. \\nFefferman & \\nMarkel (1994) consider \\nmore hidden layers. 5.3 Learning algorithms 159 \\nbe used. Our experience is that these all work well; a quasi-Newton \\nalgorithm was used for all the examples in this book. There are spe\\xad\\ncialized algorithms for non-linear least squares, but these are designed \\nfor exactly-fitting functions and in general use are less effective. \\nThere are at least two reasons for the superiority of these algorithms. \\nOne is that they try to solve the original minimization problem, not \\nthe system of equations setting the derivatives to zero, and so are able \\nto use the information provided by the size of the objective which is \\nunavailable to equation-solving algorithms. Another is that a Taylor \\nexpansion will show that locally the objective is well approximated by a \\nquadratic function, and this enables the algorithms to have super-linear \\nconvergence (see Section A.S). In practice this means that once they get \\nclose to a local minimum, they reach it to machine accuracy in a few \\niterations, this being especially effective with quasi-Newton methods. \\nThere is much collective wisdom in implementing these methods \\nwell, and the reader is advised to use a well-tested implementation \\nfrom an expert. They are part of almost every package of numerical \\nanalysis procedures, and both Nash (1990) and Press et al. (1992) \\npublish code. Unfortunately many of the published comparisons in \\nthe neural networks field have used their own implementations without \\nfully understanding the issues nor documenting the precise procedures \\nused. \\nWith algorithms that can actually solve the optimization problem \\nto machine accuracy in a modest time, we can explore whether we have \\nfound a local minimum, by looking at the eigenvalues of the Hessian \\nat the solution (which should all be positive, at least up to computing \\ntolerances), and also if more than one local minimum exists. We find it \\nto be the norm that choosing enough different starting values will lead \\nto more than one local minimum being found. Now there will always \\nbe a number of local minima of the same value, since the hidden \\nunits are not identifiable, and can be permuted without changing the \\nfunctional form. Further, the signs of all input and output weights to \\na single hidden unit can be reversed, and with suitable changes to the \\nbiases the fitted function is unchanged. Sussmann (1992) and Albertini \\net al. (1993) consider precisely when different sets of weights can give \\nthe same fitted function for a single hidden layer. However, we expect \\nto find local minima with different values of the objective function. \\n(Some simple examples are given by Gori & Tesi, 1992, and we will see \\nexamples in the next section.) \\nWeight decay helps the optimization in several ways. When weight \\ndecay terms are included, it is normal to find fewer local minima, and \\nas the objective function is more nearly quadratic, the quasi-Newton '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 171}, page_content=\"160 5 Feed-forward Neural Networks \\nand conjugate gradient methods exhibit super-linear convergence from \\nmuch farther from the local minimum and so converge in many fewer \\niterations. There seems no reason ever to exclude a regularizer such \\nas weight decay. If no regularizer is used, the Hessian is usually \\nalmost singular at a local maximum, and this slows the convergence of \\nsecond-order optimization methods (Saarinen et al., 1993). \\nThe idea of using general-purpose optimization algorithms is a very \\nobvious one, much re-discovered for neural network fitting. Some early \\nreferences are Watrous (1987), Battiti & Massuli (1990) and Beigi & Li \\n(1990, 1993) for quasi-Newton methods and Kramer & Sangiovanni\\xad\\nVincentelli (1989), Makram-Ebeid et al. (1989) and Johansson et al. \\n(1991) for conjugate gradient methods; Battiti (1992) gives a review. \\nIt is very easy to give an algorithm guaranteed to reach a global \\nminimum (Baba et al., 1994), for example by taking a random step in \\nthe weight space from a distribution with positive density (for example, \\nany Gaussian) and accepting the step if the new weights are better \\nthan the old. Such algorithms will not be practical ones in the weight \\nspace of a non-trivial network. (The proof that this algorithm works is \\nsimple. Fix e > 0. We assume that there is a minimizing w, say w0, \\nand E(w) is continuous . Then there is a ball around w0 with E(w) \\nwithin e of the minimum. The random step will hit that ball with \\npositive probability, and in an infinite sequence of steps will hit with \\nprobability one. The move will be accepted unless the current solution \\nhas E{w)-E(w0) <e.) \\n5.4 Examples \\nWe start with the data on Cushing's syndrome. If we add just two hidden \\nunits to direct input-output connections and use a softmax output \\nlayer, we find many solutions that fit the data exactly (that is, predict There are 21 weights. \\nprobability one for the observed class for each example). Figure 5.2 \\n(a) and (b) show two such solutions; the lines are rough because the \\nposterior probabilities vary so fast with x that the contouring routine's \\ninterpolation is inadequate. Adding even a minimal amount of weight \\ndecay produces a much smoother solution (see Figure 5.2(c)). With a \\nweight decay of A.= 0.01 the solutions are quite smooth, but seventeen \\nlocal minima with distinct values of E +A.C were found. Two commonly \\nfound solutions are shown in the figure, with values of E + A.C :::::: 4.18 \\nand 5.93. The largest local minimum found had E + A.C :::::: 7.65. The \\nHessian at the minimum showed that in each of the seventeen cases \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 172}, page_content='Figure 5.2: Neural \\nnetwork fits to the data \\non Cushing\\'s syndrome. \\nThe network used had \\ntwo hidden units and \\nconnections from the \\ninput to output layer. \\nFigures (a) and (b) are \\ntwo solutions without \\nweight decay which fit \\nperfectly. Figures (c) \\nand (d) show two local \\nminima each, for \\nA= 0.001 and A= 0.01 \\nrespectively . The \\ndashed lines correspond \\nto the local minimum \\nthat fits less well. \\nFigure 5.3: Neural \\nnetwork fits to the data \\non Cushing\\'s syndrome \\nwith A= 0.01. Part (a) \\nhad five hidden units, \\npart (b) twenty. 5.4 Examples \\n0 \\n0 \\n\"\\' \\na a \\n~ a ub b \\na u :g \\n0 L---~~------------~ \\n~ on \\n- 0 -b on \\' 0 a \\nb \\non a \" \\' 0 0 a \\' 5 \\n\"b b \\n5 10 \\n(a) \\n10 \\n(c) 50 \\n50 g \\n\"\\' \\ng a a \\nd a ub b \\na u :g \\n0 L---~--------------~ \\n0 0 \\n\"\\' \\n0 b on 0 a \\' \\' \\' b \\non a \" \\' q a 0 5 \\n\\' \\n~ \\nub b \\n\\' \\n5 10 \\n(b) \\nb \\n10 \\n(d) 50 \\n50 161 \\nthese are well-defined local minima, since it was positive-definite with \\neigenvalues which were well away from zero. \\nAdding more hidden units while keeping the weight decay constant \\nmakes only a little difference to the solution, as Figure 5.3 shows. \\n8 8 cc \\n\"\\' \"\\' \\n~ \\n0 0 on 0 a ub b on 0 a ub b \\na \" a \" on on 0 0 0 0 \\n5 10 50 5 10 50 \\n(a) (b) \\nFor the Pima Indians diabetes data we tried fitting a neural network \\nwith a single logistic output unit, using the conditional likelihood \\n(2.31 ). Omitting the hidden layer is equivalent to fitting a logistic \\ndiscriminant. Adding weight decay (up to A = 0.01) changed the \\nperformance marginally. Adding even one hidden unit increased the \\nerror rate to the mid 70s/332, and no non-linear neural network fit \\napproached the (linear) logistic discrimination. \\nForensic glass \\nWe have been assessing the performance of classifiers on the forensic \\nglass data by 10-fold cross-validation. Since we will expect multiple \\nlocal minima to occur, we will have to be careful to define precisely '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 173}, page_content='162 5 Feed-forward Neural Networks \\nwhat procedure is to be cross-validated. We could choose to start \\nfitting at a random set of weights (the same random set for each \\ncross-validation experiment, or a different one for each) or to start \\nfrom the fitted weights to the whole dataset. The latter will bias the \\nresults slightly, but may mean that the fitting can be done slightly more \\nquickly. \\nWe chose to use the same initial random weights for all the cross\\xad\\nvalidation runs, in part so we could explore the effects of different \\nstarting points. We know that tableware and headlamp glass can be \\nlinearly separated from the remaining classes, and this leads to very \\nslow convergence if weight decay is not used. To avoid this, the \\nsmallest value of weight decay used was A. = w-4. The quasi-Newton \\noptimization procedure used took around 4 times as many iterations \\n(BFGS updates) as the number of weights, and about 1.2 function \\nevaluations per iteration. \\nWith no hidden layer and A. = 0.001 the cross-validated error rate \\nwas 37.8%, slightly worse than that of multiple logistic regression (the \\nsame procedure without the weight decay). Adding two hidden units \\nreduced this to about 30.4 to 34.1 %, and four and eight hidden units \\ngave solutions in the range 24.8 to 29.9% depending on the starting \\npoint. Clearly the different local minima have quite different perfor\\xad\\nmances, and in some cases the best fit to the training set corresponds to \\nthe worst cross-validated performance. The simplest way to overcome \\nthis is to average across different solutions. We saw in Chapter 2 that \\nthe best quantities to average are the posterior probabilities, so we \\naveraged these across fits from ten separate starting points for each \\ncross-validation run. The table shows the error rates (% ). \\n0.0001 \\n0.001 \\n0.01 # (hidden units) \\n2 4 8 \\n30.8 23.8 27.1 \\n30.4 26.2 26.2 \\n31.8 29.9 29.9 \\nThis hows that only a small amount of regularization by weight de\\xad\\ncay is needed in this dataset; this is related to the near-linear separation \\nof some of the classes. These results are the best for fitting a flexible \\ndiscriminant model (and by far the most time-consuming). The sharp \\nvariation in performance with A. and the number of hidden units in \\nthis example is unusual, and can be traced to the success in separating \\nthe rare classes. This averaging is quite \\ntime-consuming since \\n100 fits are required; it \\ntook about an hour per \\nrun on a Spare 20 \\nworkstation. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 174}, page_content='5.5 Bayesian perspectives 163 \\nThis is the best performance found in this example for a flexible \\ndiscriminant method, but it is comparable with the much simpler nearest \\nneighbour rule of page 201, which took around a second. The cross-\\nvalidated confusion matrix is quite different: \\nWinF WinNF Veh Con Tabl Head \\nWinF 56 11 3 0 0 0 \\nWinNF 10 59 3 3 0 1 \\nVeh 8 1 8 0 0 0 \\nCon 0 4 0 8 0 1 \\nTabl 1 0 0 1 7 0 \\nHead 1 1 0 1 1 25 \\nand different decisions were made in 47 examples. This suggests that \\ncombining the two classifiers might well improve the overall perfor\\xad\\nmance, but it did not do so appreciably. \\n5.5 Bayesian perspectives \\nThe Bayesian view of decision theory can add considerable insight to \\nthe fitting of neural networks, although this insight has sometimes been \\nclouded in the literature by the confusion of poor approximations with \\nexact calculations. \\nSetting the weight decay \\nAn important question when using weight decay is how to set the \\nparameter(s) A. A Bayesian perspective (Buntine & Weigend, 1991; \\nRipley, 1994b) helps. Suppose E is the negative log-likelihood, up to a \\nconstant, or half the deviance. Then if we take a prior distribution over \\nweights with density p(w) oc exp -AC(w), the minimizer of (5.20) will \\nmaximize the posterior density for the weights. For the weight decay \\nof (5.20) this prior corresponds to independent Gaussian weights with \\nmean zero and variance 1/2A. As the logistic function saturates for \\ninputs beyond around ±3, the standard deviation of the total input \\nmight be expected to be around 2. (Remember that one motivation for \\nweight decay is to avoid unnecessary saturation of logistic units.) If \\nthere is a small number of inputs scaled to the range [0, 1], this suggests \\nthat the standard deviations of the weights should be around 5, which \\ncorresponds to A = 1/50. This argument is rather conservative , as we \\ndo want some weights to saturate, but suggests the range A~ 0.001-0.1 \\nas a basis for exploration. Experience shows that A is not critical to \\nwithin a factor of 5. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 175}, page_content='164 5 Feed-forward Neural Networks \\nFor the sum-of-squares error criterion, E is not half the deviance, \\nand must be rescaled. In that case the deviance is of the form \\nP log(E/P) for P examples. Suppose u; expresses the value of E/P \\nthat we expect to achieve. Then \\nP log(E/P) = P log(u;) + P log(E/Pu;) ~ P log(u;) + P[E/Pu; -1] \\nso an appropriate scaling is E /2u;. This suggests choosing A. in the \\nrange (0.002-0.2)u;. When least-squares fitting is used with outputs \\nin the range [0, 1], this suggests values of u; ~ w-4-10-2 might be \\nappropriate. \\nWith the softmax criterion (5.9) we will lose the symmetry of the \\nclasses if we set the output for one class to zero, so it usual to include \\nall classes in the network. The weight decay resolves the redundancy \\nover shifting all outputs, and gives a local minimum of E + A.C (rather \\nthan a saddle point). \\nOne criticism of weight decay emerges from this interpretation. \\nBecause it implies independence and the number of weights is often \\nlarge, the opinion expressed about S(w) = 2: w~ is strong, being a \\nrescaled chi-squared distribution with degrees of freedom the number \\nof weights. It is therefore potentially dangerous if A. is set incorrectly. \\nThe predictive approach \\nThe Bayesian perspective goes much deeper, and has been the subject \\nof partial implementations and considerable controversy (Buntine & \\nWeigend, 1991; MacKay, 1992a-e; Wolpert, 1993). We will confine \\nattention here to classification problems. Our aim is to model p(k I x) \\nby a K-output neural network. For K = 2 we will use one logistic \\noutput unit to model p(21 x), and for K > 2 unordered classes we \\nwill use a multiple logistic model, also known as softmax. The weights \\nare parameters, and are given a prior. It is usual to use the weight\\xad\\ndecay prior. This itself has parameters, one or more A., often called \\nhyperparameters. \\nThe predictive Bayes approach approximates p(k I x) by averaging \\np(k 1 x; w) over the posterior distribution for the weights w. This is of \\nthe general form (2.34 ), that is \\nn \\np(w Iff) oc IT p(yP I xP; w)p(xP; w) p(w), p(w) = j p(w; A.) p(A.) dA.. \\np=l \\nNote that we are assuming no model for the marginal distribution \\np(xP; w), and so assume that x carries no information about w. (If this \\nis false or unreasonable it can lead to fallacies.) '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 176}, page_content=\"5.5 Bayesian perspectives 165 \\nSuch formulae can be misleadingly simple. For the moment consider \\nfixed A. Fitting a neural network by minimizing E + AC is equivalent \\nto maximizing the posterior density over w. Since we normally find \\nseveral quite sharp local minima for E + AC, the posterior density will \\nnormally be sharply peaked at more than one point. Averaging the \\nnon-linear function p(k I x; w) over such a density is computationally \\ndifficult. As we saw in Chapter 2, the 'plug-in' approach ignores this \\ndifficulty, and uses p(k I x; w) for one fitted set of weights. A more \\ngeneral way to approximate the posterior is to take a multivariate \\nGaussian distribution about each local maximum of p(w Iff) (Buntine \\n& Weigend, 1991; Ripley, 1994c). At each local minimum of E +AC we \\ncan find the Hessian H(w), and take a local Gaussian approximation \\nto the likelihood surface, of the form N(w, H(w)-1 ). The total mass of \\nthat Gaussian is then proportional to \\nIH(w)l-1/2 exp -E(w) (2ntw12 \\nwhere nw is the number of freely-varying weights. Normalizing over \\nall the local minima found gives a mixture of Gaussian distributions \\nas an approximation to the posterior distribution of w. (This is closely \\nrelated to the single Gaussian approximations discussed in Section 2.6.) \\nWe can average p(j I x; w) for future x by simulating w from this \\nposterior density. (Often the effect of the spread about the peaks is \\nso small that it is sufficient to average over the peaks. The weights \\ngiven to the peaks can be radically different from their relative heights.) \\nMore exactly our mixture of Gaussians can be used as a density for \\nimportance sampling (Ripley, 1987, §5.2) in integrating p(j I x; w), since \\np(w Iff) can be calculated (up to a constant) from the fit of the network \\nat w. Finally, a general approach is Monte Carlo integration which can \\nbe very inefficient unless the sampling is chosen to 'fill' the space of w \\neffectively (as in Neal, 1993, 1996), and only very small examples have \\nbeen demonstrated at a very large computational cost. \\nExample \\nWe can compare the seventeen local minima found for the Cushing's \\nsyndrome data with two hidden units and A= 0.01. Six of them carry \\n5% or more of the total mass, with fitted values and weights \\nE+AC \\n% 4.544 \\n24.4 5.928 \\n21.1 6.198 \\n17.5 4.502 \\n6.9 4.269 \\n6.1 4.180 \\n5.7 \\nThe second and sixth are shown on Figure 5.2, and the (approximate) \\npredictive classifier is shown in Figure 5.4. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 177}, page_content='166 5 Feed-forward Neural Networks \\nThe choice of priors \\nIn the last subsection a weight decay prior was used with a A. which \\nwas assumed known. If we have a hyperprior for A., the extension is \\nquite easy: we can sample from this hyperprior, apply the procedure \\nfor each sample based on p(w; A.), and average over samples. (It may \\nbe better computationally to use a weighted average over a very coarse \\ngrid of values of A..) Two other approaches are to use a vague prior for \\nA. (Buntine & Weigend, 1991) and to estimate A. by ML-II empirical \\nBayes (Berger, 1985, p. 99) as advocated by MacKay (1992a-e). The \\nusual vague prior for a (squared) scale parameter such as A. has a \\nuniform density on log scale and so has improper (un-normalizable) \\ndensity 1/ A on (0, oo). This can be integrated out, to give the prior \\ndensity \\n[\"\"\"\\' ] -nw /2 p(w) oc L...t wl \\nwhich is the vague prior on S(w) = I:: wl and is again improper. \\nEffectively there is no regularization assumed, since this density for \\nS(w) has all its mass at infinity. This makes intuitive sense, since 1/2..1. \\nis the prior variance of the parameters, and if we express no opinion \\nabout it, it will be allowed to be as large as needed. This does not accord \\nwith our prior beliefs. Whereas fixed A. is a strong opinion, perhaps \\ntoo strong, a vague hyperprior is too weak an opinion. A more sensible \\nchoice of hyperprior would be a gamma distribution for A. Note that a \\ngamma hyperprior for 1/ A. would be a scaled chi-squared distribution \\nfor the variance of the weights, and so the prior distribution of the \\nweights would be a multivariate t distribution centred at zero. \\nAlthough MacKay sometimes claims to use a uniform prior for \\nlog A., he uses ML-II empirical Bayes (which omits p(A.)) to choose 1 \\nand so is effectively using a uniform hyperprior on (0, oo) for A. The \\nusual asymptotic justification for ML-II empirical Bayes is missing here, \\nbut the assumed prior independence of Wij may allow the posterior \\nfor A. to be quite concentrated, since we have nw pieces of information Figure 5.4: Predictive \\nneural network fit to \\nthe data on Cushing\\'s \\nsyndrome with two \\nhidden units \\nand A.= O.ol. \\nThis hyperprior was \\nalso proposed by \\nNeal ( 1996). \\nnw is the number of \\nweights. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 178}, page_content='5.5 Bayesian perspectives 167 \\nabout A.. However, prior independence is a dubious assumption , and \\nif nw is large and comparable with n (which it often is) there is no \\nreason to suppose that p(A. I§\"\") will be highly concentrated about one \\npoint. Our experiments showed this often not to be so. \\nThe use of the weight decay prior is convenient, but our prior beliefs \\nare really on the functions represented by the network and not on the \\nparameters (weights) per se. This is the approach of the regularization \\npenalties, which can be alternatively expressed as priors over the family \\nof functions realized by the network. These have been most explored \\nfor regression networks, but Buntine & Weigend (1991) do give a small \\nexample for a classification network. Bishop (1993) uses the regularizer \\ndiscussed in Section 4.3 for RBFs, \\nThis is intended for regression networks , with linear output units. (The \\nimplied prior is exp -A.C(f).) For this choice of regularizer Bishop \\nshows that ac 1 awij can be calculated quite easily via the chain rule. \\nFor classification problems such a regularizer might be appropriate \\nwhen applied to the total inputs to the logistic or softmax output stage. \\nNowlan & Hinton (1992a, b) motivate a prior for the weights which \\nis a mixture of Gaussians by its encouragement for similar values of \\nthe weights. This had proved useful in networks with a hand-tuned \\narchitecture (but not for arbitrary groupings of weights). They choose \\na fixed number of mixture components (not necessarily centred at zero) \\nand optimize the penalized log likelihood over the parameters in the \\nprior as well as the weights. \\nMAP estimation \\nMAP is a common abbreviation for maximum a posteriori, summarizing \\na predictive distribution by its mode. Most of the controversy in this \\narea comes from inappropriate use of MAP estimates. The predictive \\napproach is quite clear; we should average over the hyperprior (if any) \\nand prior on the weights. This is done by mapping the posterior distri\\xad\\nbution over weights to one over fk(x) = p(k I x; w), and averaging over \\nthe weights w and hyperparameters . Finally we maximize the expected \\ncost over decisions. As we have seen, this programme is computa\\xad\\ntionally daunting, and early work (such as Buntine & Weigend, 1991) \\nimplicitly or explicitly approximated the posterior density p(w I §\"\") by \\na mode. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 179}, page_content=\"168 5 Feed-forward Neural Networks \\nThis approximation is often not appropriate and can be misleading. \\nIt needs to be stressed that plugging in the MAP estimator of the \\nweights does not give the MAP estimator of the function f from inputs \\nto outputs. If it is really necessary to use MAP as an approximation, it \\nwould be better to take the MAP estimate off= (fk) rather than that \\nof w. Wolpert (1994a) gives a computational programme for finding a \\ncorrection term to the posterior density of w to skew the mode towards \\nthe MAP estimate off, but the problem of multiple maxima remains, \\nand in practice it appears to be easier to average, as well as being \\ntheoretically correct. \\nOne difficulty with approximating distributions by their modes is \\nthat the mode depends on both the underlying measure for the density \\nand the parametrization. For example, in the weight-decay regular\\xad\\nization log A and 1 /2A. (the implied variance) are equally plausible \\nparameters, yet their modes will not map to the mode of A. Indeed, \\napproximating by the mode seems safe only when the parameter itself \\nhas a physical meaning and so a natural scale, or when the posterior \\ndistribution is so concentrated that the parameter is effectively constant. \\nOptimization over both the weights and hyperparameters has been \\nadvocated by MacKay (1992b) and Nowlan & Hinton (1992b). This \\nis further from the full Bayesian procedure, and so seems even more \\nlikely to mislead. \\n5.6 Network complexity \\nOne of the simplest and most commonly asked questions is \\n'How many hidden units should I use? Are there any rules of \\nthumb?' \\nThere are rules of thumb, but these are as unreliable as those for \\nthe complexity of a multiple or polynomial regression. The answer \\ndepends on the unknown underlying function f which the neural \\nnetwork f(x; w) is approximating. There are three ways to control the \\ncomplexity of the functions represented by a network: to cut out links, \\nto change the number of hidden units, and to change the regularization \\n(such as weight decay) parameter. We consider each in turn. \\nThe statistical background in Sections 2.2 and 2.6 is needed to \\nappreciate these methods. Many of those results can be extended to \\nmethods of parameter estimation which minimize some criterion E \\n(including any penalty A.C) and so in particular to penalized likelihood \\nmethods and the Bayes MAP estimate. However, the extensions are not \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 180}, page_content=\"Wald tests are \\nlarge-sample \\nequivalents of \\nlikelihood ratio tests: \\nsee Cox & Hinkley \\n(1974) or Lehmann \\n(1986). \\nSee the glossary. 5.6 Network complexity 169 \\nvery useful, as asymptotically the criterion E which grows proportion\\xad\\nally to n (the size of the training set f7) will swamp any penalty and \\nso not alter the results. Since the point of a penalty is to produce better \\nbehaviour for moderate n, a different asymptotic regime is needed. It \\nfollows from what we have said about the prevalence of local minima \\nthat the asymptotic theory is not normally relevant when fitting neural \\nnetworks; n is often not much greater than the number of parameters. \\nNeural networks are 'black box' models used for prediction. For \\nprediction performance, it is almost always better to make smooth \\nchanges (shrinkage or regularization) than to select parts of the model: \\nit is often computationally preferable too. It is important to remember \\nthe merits of combining models discussed in Section 2.6. \\nPruning networks \\nThe usual selection procedures can be used, including stepwise selection \\nand AIC. However, it makes little sense to set individual weights to \\nzero, and whole internal units and the weights on their connections are \\nadded or deleted. Often some form of cross-validation (Section 2.6) is \\nused to decide how many hidden units to use. \\nThe neural network community has developed some fanciful names \\nfor these ideas. Optimal Brain Damage (Le Cun et al., 1990b) and \\nOptimal Brain Surgeon (Hassibi & Stork, 1993; Hassibi et al., 1994; \\nBuntine & Weigend, 1994) are both methods to 'prune' weights, that \\nis to choose to set some of the weights to zero. Both are approximate \\nversions of the Wald test, which considers the ratio of a parameter \\nto its standard error. (The large-sample theory computes the standard \\nerror as the appropriate diagonal element of the Fisher information \\nmatrix K.) The methods differ in how crudely the standard errors are \\nestimated. OBD uses just the inverse of the diagonal of the observed \\ninformation matrix. OBS replaces the Fisher information matrix by the \\ncovariance matrix of the scores oL(fi;Xi)/oe. The empirical covariance \\nmatrix can then be inverted incrementally by the Sherman-Morrison\\xad\\nWoodbury formula. It seems simpler to compute and invert the Hessian \\nmatrix, and Proposition 2.2 gives us more accurate asymptotic formulae \\nwhich do not assume the model to be true. \\nReed (1993) gives a partial survey of pruning algorithms in the \\nneural network literature. \\nThere are well-known difficulties in setting parameters to zero. Near\\xad\\ncollinearities can mean that if one weight is set to zero, the standard \\nerrors for others are drastically reduced, so it can be unsafe to set more \\nthan one weight to zero at a time. For classification problems the Wald \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 181}, page_content='170 5 Feed-forward Neural Networks \\ntest is known to have little power when the true weight is large (Hauck \\n& Donner, 1977), and so can be most misleading. \\nAnother approach is to encourage small weights during training, so \\nthese can subsequently be set to zero, and perhaps hidden units and \\nall their connections removed thereby. This is done by an extension of \\nweight decay, for example with penalty \\nw?.jW2 \\nc = L 1 +1~2.jW2 (5.22) \\nlj \\n(Weigend et al., 1990, 1991, 1992) (which corresponds to a very improper \\nprior with free parameter W ) or to base the penalty on the total squared \\nweights reaching a hidden unit (Chauvin, 1989; Hanson & Pratt, 1989). \\nLevin et al. (1994) extend the idea of principal components regres\\xad\\nsion, in which the inputs are first linearly transformed to their principal \\ncomponents and then only some of the principal components are used \\nin the final regression. Like all pruning methods, this reduces the \\nvariance of predictions at the expense of bias. This idea is applied to \\nthe input layer, reducing the inputs to the principal components, and \\nthen to subsequent layers in the neural network. Thus no weights nor \\nunits are actually removed, but the weights are restricted to lie in a \\nlower-dimensional space. \\nSelecting the number of hidden units \\nSelecting the number of hidden units in a neural network is in principle \\nno different from selecting regressors in a linear regression or the order \\nof a polynomial regression. The main ideas that have been developed \\nin that field are pruning by small steps (backward selection) discussed \\nin the last subsection, incremental construction (forward selection; the \\nnext subsection), and minimizing some measure of performance over \\nthe class of possible models. \\nAll the candidate measures aim to predict the performance on a \\ntest set, and so to select the model with the best performance on the \\ntest set. The most general idea is to use cross-validation (Section 2.7). \\nThere is a particular difficulty in cross-validation for neural nets. How \\ndo we move from the whole set to a subset for training? Do we start at \\nthe fitted weights for the whole data? This could bias the procedure . If \\nwe start at another random starting point, we could end up at a very \\ndifferent solution even with the whole data, let alone with a subset. \\nThis shows that the learning procedure for a neural network is not well \\ndefined, as there are often multiple local minima of rather different \\nperformance. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 182}, page_content=\"The motivation of \\nMoody & Utans (1995) \\nseems to have been to \\nsave time by re-training \\nfrom a local minimum \\non all the data. For the \\nforensic glass example \\nwith 10-fold \\ncross-validation we \\nfound re-training took \\nat least half as long as \\ntraining from scratch. 5.6 Network complexity 171 \\nMoody & Utans (1992, 1995) suggest viewing each local minimum as \\na separate model. This is appealing, but flawed. Consider a real example \\nwe encountered of around 100 examples and a binary classification. All \\nbut two of the examples could be fitted very well by a simple logistic \\nregression model with no hidden units, but the other two examples had \\napparently been attributed to the wrong class. Fitting a net with two \\nhidden units did much better overall, but had at least ten non-equivalent \\nlocal minima. Now dropping either of the badly-fitted examples changed \\nthe nature of the fitted function completely. If we cannot track the local \\nminimum over dropping just one example, a local minimum cannot be \\nsufficiently well defined to be used within a cross-validation procedure. \\nThe approach taken in Section 5.4 of averaging across the local minima \\nis much stabler, and only a little more time-consuming . \\nIt is important to realize that penalty terms such as weight decay and \\nregularization change the problem completely , as they often impose a \\nlimit on the complexity of the fitted functions irrespective of the number \\nof hidden units. The use of splines for smoothing in one dimension \\n(Section 4.1) provides graphic evidence of this. The smoothness of the \\nfit can be controlled either by restricting the number of knots or the \\ndegree of regularization; when regularization is •1sed (that is, smoothing \\nsplines) the fitted function will remain smooth however many knots and \\ndata points are taken. Figure 5.3 shows a similar phenomenon for fitting \\na neural network with weight decay. \\nThe NIC penalty of Murata et a/. (1991, 1993, 1994) and Moody's \\n(1991, 1992) Pelf discussed in Section 4.3 were developed for this \\napplication, but do assume a strong single local minimum. For the \\nfits of Section 5.4 for the data on Cushing's syndrome this condition \\nis not met. The fits shown with weight-decay constant A, = 0.1 have \\nPelf in the range 2-5 depending more on the local minimum than on \\nthe number of hidden units. There is too much uncertainty in Pelf for \\nit to be useful in such a small example. Ripley (1995) found it quite \\neffective in a regression problem with one input and a training set of \\n100 examples. \\nIncremental network construction \\nThere have been a number of ideas to grow networks incrementally, by \\nadding hidden units one at a time in the same or extra layers. Many \\nof the ideas were first developed for perceptron (threshold) units and \\nare surveyed in Gallant (1993, Chapter 10). The 'pyramid' algorithm of \\nGallant (1990) adds units one at a time in a new layer, each unit being \\nconnected to all previous units. Frean's (1990) 'upstart' algorithm is l I \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 183}, page_content=\"172 5 Feed-forward Neural Networks \\nfor binary outputs. It starts with a perceptron, then adds two hidden \\nlayer units to attempt to correct (separately) the positive and negative \\nmistakes. As these algorithms are of very limited scope (binary outputs, \\nthreshold units), we refer the reader to the references for detailed \\ndescriptions. Other early construction algorithms are described by \\nAsh (1989) and Moody (1989). \\nMoody & Utans (1995) call 'SNC' the heuristic construction algo\\xad\\nrithm which adds units to the hidden layer in groups of C units, and \\nfirst trains the new units before re-training all the units. They imple\\xad\\nmented this for C = 1, when it is an example of back-fitting (described \\nin Section 4.1). \\nCascade correlation (Fahlman & Lebiere, 1990) is a particular way \\nto grow a 'pyramid' network, and seems the only iterative construction \\nalgorithm that is at all widely used. Initially just input-output connec\\xad\\ntions are used. At each subsequent stage a new unit is added which has \\nas inputs the original inputs and the outputs of all the previous units, so \\neffectively a further hidden layer containing just one unit is added, with \\na prescribed set of skip-layer connections. However, only the additional \\nweights (the input and output weights of that unit) are fitted at that \\nstep; weights to hidden units once fitted are never altered. Further units \\nare added until some pre-specified measure of fit is achieved. (Their \\nexamples are of noiseless classification, and units are added until the \\nwhole training set is correctly classified.) This is called by its authors a \\ncascade architecture. \\nThe 'correlation' in the name of the algorithm comes from the \\nway that the new unit's weights are selected, although this involves \\na covariance not a correlation. If there is just one output unit, the \\nweights are selected to maximize the absolute value of the covariance \\n(over training-set examples) between the output value of the unit and \\nthe prediction error before that unit is added. If there are multiple \\noutputs, the sum of this measure over outputs is used. In what can \\nbe seen as a means to avoid bad local minima and 'flat spots', several \\nattempts are made to maximize this objective from random starting \\npoints, and the best taken. Then all the weights to the output units \\n(including that from the new unit) are re-trained. \\nIt is illuminating to contrast this algorithm with the SMART algo\\xad\\nrithm for projection pursuit regression (page 126). Cascade correlation \\nuses a layered architecture; SMART uses only the original inputs but \\nalso fits to the residuals. In cascade correlation no global optimization \\nis done, whereas in SMART all the weights are periodically re-fitted, \\nand a pruning stage removes units one at a time. This re-fitting of \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 184}, page_content=\"See Section 2.6. 5. 7 Approximation results 173 \\nweights will take more CPU time, but is likely to produce a smaller \\nnetwork with better generalization properties. This is borne out by the \\nempirical comparisons of Hwang et al. (1994b) on the example used \\nby Fahlman & Lebiere, which shows that projection pursuit regression \\nproduces a smoother fit and hence a better generalization. \\nLee et al. (1990) take an approach they term structure-level adapta\\xad\\ntion. This adds units to the network during training where they appear \\nto be most useful, and also removes units which appear not to be ef\\xad\\nfective. The rules to do so are ad hoc; for example a new unit is added \\nin parallel to one whose input weights appear to fluctuate continually \\nduring on-line training based on the heuristic idea that that unit is \\ntrying to follow two (or more) different features. \\nBayesian model choice \\nRipley (1995) found approximate Bayes factors by importance sampling \\nusing the multiple Gaussian approximation to the posterior density \\np(w Iff) discussed on page 165. This was for a curve-fitting problem \\n(one input, one output) selecting both the number of hidden units and \\na single weight decay parameter A.. The results are similar to those \\nusing NIC. \\n5.7 Approximation results \\nWe first prove the 'universal approximation ' result. We are concerned \\nwith functions f:lR.n ~ JRP for n inputs and p outputs. We wish \\nto approximate a given function f by g from some specified class of \\nfunctions , such as (5.1). Uniform approximation on compacta means \\nthat given a compact set K c 1R n and e > 0 we can find a function \\ng within our class such that llf(x)-g(x)ll < e for all x E K. We do \\nnot write out all the fine details, on the understanding that readers who \\nare interested will be familiar with simple arguments in mathematical \\nanalysis. By 'ramp units' we mean \\ng(x) = max(O, min(x, 1)) \\nwhich can be made up of two of Breiman's (1993) hinge functions. \\nIf we can approximate the posterior probability function fk(x) = \\np(k I x) uniformly on compacta, we certainly will have enough to solve \\nany practical classification problem. The restriction to compacta obvi\\xad\\nates the need to extrapolate correctly. It is reasonable to assume that \\nthis f is continuous , but it need not be (consider the task of classifying \\nreal numbers as rational or irrational). \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 185}, page_content='174 5 Feed-forward Neural Networks \\nProposition 5.1 Any continuous function f: 1R n -+ .IRP can approximated \\nuniformly on compacta by functions of the form (5.1) with linear output \\nunits and logistic units in the hidden layer, and also by networks with \\nthreshold units or ramp units in the hidden layer. \\nProof: Our proof proceeds by building up the class of functions we \\ncan approximate (uniformly on compacta). \\n(a) Our first step is to take n = p = 1. A compact set is contained \\nin a bounded interval, say [a, b], and any continuous function f \\ncan be uniformly approximated on [a, b] by a step function with \\nsteps of size less than c /2. (For each x E [a, b] define the interval \\nI(x) = (t(x), u(x)) where t(x) = max{y < x : lf(y)-f(x)l ~ c/4} \\nand u(x) = min{y > x : lf(y)-f(x)l ~ c/4}. The open sets \\nI(x) cover [a,b], hence so does a finite collection J(x;). Sort \\nthe x; into increasing order, and let g take the value f(x;) on \\n[t;, u;) = [u(x;_l), u(x;)). Then ldg(t\\';)l = lf(x;-1-f(x;)l ~ lf(x;-1-\\nf(t\\';)l + lf(x;)-f(t;)l ~ 2e/4.) A step function is in class (5.1) \\nfor threshold units, and sums of logistic or ramp functions can \\napproximate a step function arbitrarily closely except at the steps, \\nand certainly to within one half of the largest step size. \\n(b) We then extend the result to trigonometric functions of the form \\nll?=1 cos(w;x+tp;) for any n. By repeated use of the cos(A+B) for\\xad\\nmula, this can be written as a sum of the form I:j aj cos(wjx + tpj). \\nEach term in this sum is continuous, and so can be approximated \\nby step (a); hence the whole sum can, as well as linear combinations \\nof these functions, including arbitrary trigonometric polynomials on \\n.IR\". \\n(c) Any continuous function f:.IR\" -+ 1R can be approximated by a \\ntrigonometric polynomial. This is a well-known result in Fourier \\ntheory, but we give an elementary proof as Proposition 5.2. \\n(d) Fix the compact set K and c > 0. Each component function fj of \\nf is continuous, so we can find functions gj within our class such \\nthat \\nsup 1/j(x)-gj(x)l < c/ JP \\nxEK \\nand so \\nsup llfj(x)-gj(x)ll <c. \\nxEK \\nThe function (g1(x), ... ,gp(x)) is within the class (5.1), since we can \\ntake separate groups of hidden units for each coordinate function. \\nD The idea of this proof is \\nbased on that of \\nDiaconis & \\nShahshahani (1984), \\nwho used it to show the \\nuniversal approximation \\nproperty for projection \\npursuit regression. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 186}, page_content=\"5.7 Approximation results 175 \\nWe can easily extend the proof to other types of unit in the hidden \\nlayer; all we need is the ability to approximate a step function in the \\nsense used in the proof. It is clear that, for example, any cumulative \\ndistribution function could be used. The literature sometimes refers \\nto such functions as sigmoidal, particularly if they are continuous, \\nalthough that term is also used for just the logistic and hyperbolic \\ntangent functions. \\nWe now give a probabilistic proof of the only slightly tricky step. \\nThis is an easy consequence of the (advanced) Stone-Weierstrass theo\\xad\\nrem (Simmons, 1963, p. 160). \\nProposition 5.2 Any continuous function f: 1R n ~ 1R can be uniformly \\napproximated on compacta by trigonometric polynomials. \\nProof: Fix the compact set K. By an affine transformation of the \\ncoordinate system we may ensure that K c [0, 1]n. Now re-parametrize \\neach coordinate invertibly by Pj = sin(xj/max{2,n}), and use the \\nresult of the second half of the proof to approximate h(p) = f(x) by a \\npolynomial hN(P) on the simplex \\n9 = {p I Pi ~ 0, PI + · · · + Pn ~ 1} · \\nThen f N(x) = hN(P) is a trigonometric polynomial which approximates \\nf to the required accuracy. \\nLet Y = (Yt, ... , Yn) be a sample of size N from a multinomial \\ndistribution with parameters p = (Pt •... , Pn) E 9. (As the probabilities \\nneed not sum to one, we complete the definition by taking a category \\nn + 1 which can occur with probability 1-EPi·) Define \\nWe will show that hN converges uniformly to h on 9. Let M = \\nmaxpE.9' lh(p)l < oo and choose b > 0 so that \\nwhich we can by continuity and compactness. Then \\nhN(P)-h(p) = Ep[{h(Y/N)-h(p)}I(IIY/N-pll <b)] \\n+ Ep[{h(Y/N)- h(p)}I(IIY/N- Pll ~b)] \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 187}, page_content='176 5 Feed-forward Neural Networks \\ngives \\nn \\nE \"\"\\' n ~ 2 +2M~ (j2NPi(1- Pi) \\ni=l \\nE n \\n~ 2 +2M b2N < E \\nfor large enough N. (The fourth step uses the Bienayme-Chebychev \\ninequality.) 0 \\nUniform convergence on compacta is a strong form of convergence, and \\nimplies many others. In particular, it implies L2(f.l) convergence for any \\nprobability measure f.l on IR\". For the purposes of classification this \\nis also sufficient, since it implies that decisions made by the classifiers \\nderived from f and from g agree with high probability (except in \\nartificial cases where several decisions are equally good). \\nKurkova (1991, 1992) gives uniform approximation results for net\\xad\\nworks with two hidden layers for which only the output weights are \\nvaried, including bounds on the numbers of units needed. \\nHornik et al. (1990) give approximation results for continuous func\\xad\\ntions and their derivatives; these would take us too far into approxi\\xad\\nmation theory even to state. Stinchcombe & White (1990) give approx\\xad\\nimation results for networks with bounded weights (for a fixed bound \\nbut using very many hidden units). \\nBarron (1993) gives results on the rate of convergence in Lz(f.l) \\nfor some (but not all) functions f: IR\" ---+ IR; this overlaps work by \\nL. K. Jones (1992). A typical result is \\nProposition 5.3 (Barron, 1993, Proposition 1) Suppose f:IR\"---+ IR has \\na Fourier representation of the form \\nwith \\nCt = { llwllf(w)dw < oo. }F_n This says that the \\nvariance of the \\ndifference (f-g)(X) \\ngoes to zero as g \\napproximates f. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 188}, page_content=\"5. 7 Approximation results 177 \\nThenfor each N, f can be approximated by afunction oftheform (5.1) \\nwith N hidden units (of logistic, threshold or ramp form), in L2(J.l) on \\nBr = {II x II < r} with error at most (2rC f)/ JN. Further, we can take \\na0 = f(O) and l:j lwjol ~ 2rCJ for the hidden-to-output weights. \\nProof: For rigour, a lot of details are needed in the calculation, for \\nwhich we refer the reader to Barron's paper. However, the main ideas \\nare quite simple. By the choice of a0 we can assume f(O) = a0 = 0. \\nSince f is real, it can be represented in the form \\nf(x) = r [cos(wT X+ b(w))-cos(b(w))] if(w)i dw \\nJ{wfO} \\nand so is a probability integral of functions of the class \\nGcos = {II: II [cos(wT x +b)-cos(b)] j IYI < rCJ} \\n(with pdf if(w)illwll/rCJ ). Thus f is in the closure of the convex hull of \\nsuch functions. Each of these is approximated by convex combination \\nof single-step functions of height less than 2rC1, and then the step \\nmay be approximated by a logistic or ramp unit. This shows that the \\napproximation is possible. \\nThe rate of convergence then comes for free, by a lemma attributed \\nto Maurey. Our class of approximating functions is the convex hull of \\nGt = {/U(a + bT x) I IPI < 2rCJ} \\nand each member has norm at most (2rC1 f. Fix N, and suppose 7 = 2:;:1 p;g; is a function within the convex hull of Gt within distance ~ of f. We consider randomly selecting (with replacement) N \\nmembers (say g*) from {g;} with probabilities {p;}, and take their \\n(unweighted) average f*. Then Ef* = 7 and \\nThis shows that there must be a realization f* with \\nand b was arbitrary. L. K. Jones (1992) gives a constructive version of \\nthis lemma. [] \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 189}, page_content=\"178 5 Feed-forward Neural Networks \\nThis result has often been regarded as surprising in that the rate \\nof convergence does not vary with n; typically the rate of convergence \\nin approximation theorems is O(N-cfn), with c depending on how \\nsmooth f is assumed to be. It has been said to break the 'curse of \\ndimensionality'. This is not correct; the conditions do depend on n \\nand impose increasingly strict smoothness on the class of functions as \\nn increases. First, Br becomes much smaller as n increases; the radius \\nneeded to include the unit hypercube is Jii. (Other forms of the result \\ninvolve L1 norms, but with the same rate of increase.) Second, the \\nintegral for Ct is dimension-dependent; indeed considering radially \\nsymmetric functions f(x) = f(llx!l) suggests that normally Ct grows \\nexponentially fast in n. Results of DeVore et al. (1989) show that \\nto approximate all functions with r bounded derivatives to accuracy \\n1/ JN in L2(J.t), at least Q(Nn/2r) units are needed, so the 'curse \\nof dimensionality' cannot be broken by neural networks (nor by any \\nsimilar non-linear method). \\nThe condition Ct < oo is not immediately interpretable. It does \\nimply that f is continuously differentiable, with a gradient whose \\nFourier transform is integrable. Inspection of Gcos shows how the \\nfunctions are actually restricted; Girosi & Anzellotti (1993) point out \\nthat functions with Ct < oo are precisely those which can be expressed \\nas a convolution with II x 111-n, an increasingly restrictive constraint as \\nn increases, and one which Barron (1993, p. 941) shows is satisfied iff \\nhas ln/2J + 2 continuous derivatives. \\nMhaskar & Micchelli (1992) showed that n(mn/r+(n+2r)/r2\\n) units \\nare needed for n ~ 2 to approximate all continuous functions with \\nr bounded derivatives on the unit cube in 1R n uniformly to within \\ndistance 1/m. \\nIt is easy to extend the approximation results to functions with a \\nbound on b = (wij) for each hidden unit j, just by checking how well \\nsuch functions can approximate step functions. If we impose the limit \\nllbll ~ B, we increase the approximation error by less than \\n1 + 2log(rB) \\nrB \\n(Barron, 1993, pp. 936-937). This is an increasingly stringent restriction \\non b as n increases. \\nInverse functions \\nThe universal approximation results for single-hidden-layer networks \\napply to continuous functions f. In some applications we need to Often c is the number \\nof continuous \\nderivatives of the \\nfunction. \\nHere y = Q(x) means \\ny / x is bounded below \\nfor all x > 0. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 190}, page_content=\"5. 7 Approximation results 179 \\napproximate the one-sided inverse of such a function, that is given \\ne > 0 and an compact set K within the range of f, we need to find \\na function <J>:JRP -:IR.n such that 11/(<J>(x))- xll < e for all x E K. \\nThis arises in control theory, where f describes how the plant responds \\nto control inputs, and 4> is the control mapping needed to achieve \\n(approximately) a feasible output of the plant. \\nSontag (1992) points out that not only may 4> need to be discontin\\xad\\nuous, but it can be outside the class of functions which can be approxi\\xad\\nmated by single-hidden-layer networks, even those with threshold units. \\nHowever, networks with two hidden layers and threshold units do suf\\xad\\nfice, as they can approximate the indicator function of any polyhedral \\nregion, which cannot be achieved with only one hidden layer. \\nDimension bounds \\nTo use the results of Section 2.8 we need to know (or bound) the \\n'dimension' of families of neural networks. A few results are known. \\nFirst consider threshold units and one output (so we can consider the \\nVC dimension). Suppose there are M computational units and W \\nweights in toto. Baum & Haussler (1989) showed that for any number \\nof hidden layers the VC dimension is bounded by \\nd :=:;; 2Wlog2eM \\nand for a single hidden layer of H units we have \\nd ~ 2plH /2J ::::::pH= W ___E_2 p+ \\nwhere p remains the number of inputs. The upper bound follows from \\nM \\n.1(m) :=:;; IJ .1i(m), \\ni=l \\nwhere .1i(m) :=:;; (emjdi)d; refers to unit i with ki inputs and VC \\ndimension di = ki + 1. Since W = 2:: di, if m = 2 W log2 eM \\nM \\n.1(m) :=:;; IJ(em/di; :=:;; (MemjW)w <2m. \\ni=l \\nThe lower bound comes from a construction of Baum (1988) which \\nshows that a single-hidden-layer net with 2j hidden units can separate \\n2jp vectors in JRP. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 191}, page_content='180 5 Feed-forward Neural Networks \\nShawe-Taylor & Anthony (1991) extend the bound d ~ 2W log2 eM \\nto multiple (threshold) output units, where d is now the VC dimen\\xad\\nsion defined via graphs of functions. Other bounds are given by \\nBartlett (1993). Maass (1994a, b) shows that VC dimensions of order \\nQ( W log W) can be achieved with two hidden layers, but it is not \\nknown if the true order is O(W) or O(W log W) for a single hidden \\nlayer. (These results apply to networks where both the number of \\ninput units and the number of hidden units are allowed to increase.) \\nSakurai (1993) has Q(W log W) results for networks with one hidden \\nlayer and real inputs (whereas Maass considered binary inputs). \\nIf we allow the units to be logistic, the VC dimension increases, \\nbut its value is not known; indeed only recently has it been shown to \\nbe finite (Macintyre & Sontag, 1993), by decidedly advanced methods \\nfrom mathematical logic. Indeed, this result is subtle, since with cos \\nunits (and hence projection pursuit regression) the VC dimension is \\ninfinite. For sigmoidal neural networks, Karpinski & Macintyre (1995a, \\nb) showed that the VC-dimension is 0( W4), and Koiran & Sontag \\n(1996) showed Q(W2). \\nFor more than one output and for non-threshold units, the pseudo\\xad\\ndimension is more appropriate. Haussler (1992) gives bounds for net\\xad\\nworks with bounded weights. A specialization of his Theorem 11 to \\nlogistic units (including output units) is: \\nProposition 5.4 Suppose there are d ~ 0 hidden layers and a total of \\nW adjustable weights. Suppose that the average of the input (non-bias) \\nweights to units in layer i is bounded by bi. Then \\nProof: Haussler (1992, Theorem 11). D \\nBartlett & Williamson (1996) bound the VC-dimension for a single\\xad\\nhidden-layer network by 2 W log2(24e W D) if the inputs are restricted \\nto {-D, ... ,D}. Karpinski & Macintyre \\ngive an explicit bound \\nfor a single-output \\nnetwork which has a \\nleading term of \\n(W M)2 /2 where M is \\nthe number of \\nsigmoidal units '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 192}, page_content=\"6 \\nNon-parametric Methods \\nWe have seen that the Bayes rule is based on p(k I x) oc nkp(x I C = k). \\nWe usually estimate the nk from the proportions in the training set, \\nand we have considered parametric models for Pk(x) such as the \\nmultivariate normal distribution, In this chapter we consider non\\xad\\nparametric estimates of the class distributions. We have also seen that \\nclassification can be done by choosing the largest of fk(x) = p(k I x), and \\nthat these can be estimated by regression methods, so in this section we \\nalso consider, briefly, non-parametric regression methods. We end with \\na discussion of the use of mixture distributions which, while parametric, \\nis designed to approximate arbitrary class distributions. \\nThe methods of this chapter are illustrated on small problems. \\nThey can be applied to larger problems in many dimensions (and \\nnearest neighbour methods are often very successful) but by their very \\nnature there is nothing to illustrate except performance figures. Nearest \\nneighbour methods are within the diagnostic paradigm; other methods \\nwhich work within the sampling paradigm and aim to model the class\\xad\\nconditional densities in many dimensions need a very large training set \\nto be successful. \\n6.1 Non-parametric estimation of class densities \\nMost non-parametric methods are based on the idea that a function \\nis locally constant, and much of the difficulty in their use is deciding \\nwhat is meant by 'local' in the high-dimensional space q'. We will \\nstart by considering kernel methods. A kernel K is a bounded function \\non q' with integral one. Suitable examples include probability density \\nfunctions such as the multivariate normal. We assume that K is in \\nsome sense peaked about 0. We then use K(x-y) as a measure of the \\nproximity of x and y. (This suggests that we should take K ( -x) = K (x) \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 193}, page_content='182 6 Non-parametric Methods \\nand this requirement is commonly imposed.) The empirical distribution \\nof x within a group k gives mass 1/nk to each of the examples. This \\nsuggests that a local estimate of the density Pk(x) can be found by \\nsumming each of these contributions with weight K (x -x;), that is \\n(6.1) \\nand this can also be interpreted as an average of kernel functions \\ncentred on each example from the class. We have \\n~(k I ) = 1rkPJ(x) = ~ L[i]=k K(x-x;) \\np X \"\"\\' ~ ( ) ::!1 . ( 6.2) Ltk 1rJPJ X Li n1;1K(x- X;) \\nWhen the prior probabilities are estimated by nkfn, (6.2) simplifies to \\n\"\"\\' . K(x-x·) p(k I x) = Lt[z]=k z \\nL;K(x-x;) \\' (6.3) \\nthe weighted proportion of points around x which have class k. \\nThe difficulty with kernel methods is the choice of K. Suppose \\nwe make the very reasonable choice of a multivariate normal density. \\nClearly the mean should be zero. How do we choose the covariance \\nmatrix? We have seen in discriminant analysis the importance of \\nchoosing the right metric via the within-group covariance. The case \\nof univariate x has been studied in most detail (Silverman, 1986; \\nHardie, 1990; Wand & Jones, 1995), and Scott (1992) also considers \\nthe multivariate case. Even in one dimension it seems that adaptive \\nmethods are needed, that is those which change the spread of the kernel \\nover the space fl£. In our context we want to identify correctly those \\nregions in which p(j I x) is maximal, and near-equality will normally \\noccur in the tails of the densities, so it is particularly important to \\nestimate the tails correctly. This seems almost completely ignored in \\nthe density estimation literature. For example, plots of the estimated \\ndensities on log scale show how rough they are in the tails (e.g. Duda \\n& Hart, 1973, Figures 4.1 and 4.2), and discrimination is based on \\ndifferences in log densities. In the two-class case, Hall & Wand (1988) \\nsuggest estimating 7r2P2(x)-n1Pl(x) directly by a kernel estimate. (With \\nestimated prior probabilities this amounts to a kernel estimate for the \\nwhole sample, but counting samples from class 1 with a negative kernel.) \\nFigure 6.1 shows the estimated class densities for the data on Cush\\xad\\ning\\'s syndrome, using a normal kernel with bandwidth chosen by a \\nstandard reference (Venables & Ripley, 1994, Chapter 5). The decision \\nregions of the resulting classifier are shown in Figure 6.2. Remember that [i] is \\nthe group of training \\ncase i. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 194}, page_content=\"Figure 6.1: The class \\ndensities for the data on \\nCushing's syndrome, \\nestimated by kernel \\nmethods with a \\nbivariate normal kernel. \\nFigure 6.2: The decision \\nregions for the data on \\nCushing's syndrome, \\nusing the estimated \\nclass densities shown in \\nFigure 6.1. 6.1 Non-parametric estimation of class densities \\n8 \\n:;' \\n~ \\n0 \\n;:; \\n8 .,; \\n0 \\nci \\n:g \\nci ;.51 \\nOJ \\nu b b 10 50 \\nT etrahydrocortisone \\nb \\nb \\n10 50 \\nT etrahydrocortisone 183 \\nNote that if we consider (6.3) with a normal kernel with covariance \\nmatrix KL, as K --+ 0 the posterior probabilities concentrate on the \\nclass of the training-set example nearest to x in Mahalanobis distance. \\nOn the other hand, the tail behaviour of the kernels is critical in \\ndetermining the relative balance of the prior proportions of the classes \\nand the effect of the training data at points x well outside the training \\nset. It is quite common practice to use kernels with bounded support, \\nso the density estimate at x could be zero for one or even all classes. \\nKernel discriminant analysis is the subject of monographs by Hand \\n(1982) and Coomans & Broeckaert (1986). The ALLOC80 computer \\npackage (Hermans et al., 1982) for kernel discriminant analysis is widely \\nused. \\nKernel methods can be used to estimate regression surfaces, by \\naveraging the values of y attached to the nearby data points. We \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 195}, page_content='184 6 Non-parametric Methods \\nobtain the Nadaraya-Watson non-parametric kernel regression \\n~( ) 2::::; y;K(x- x;) y X = . l:;K(x- x;) (6.4) \\nNow suppose we use (6.4) to estimate fk(x) = p(k I x). Then y IS \\nthe indicator function for class k, and (6.4) reduces to (6.3). This \\nform of classification fits into the framework of Chapter 4. (The non\\xad\\nparametric kernel regression can also be derived by taking a kernel \\ndensity estimator in the space :1£ x 1R and evaluating E[Y I X= x].) \\nKernel methods are known in the pattern recognition literature as \\nParzen windows after Parzen (1962). There was earlier work on the \\nmethod by M. Rosenblatt (1956) and, in passing, Fix & Hodges (1951). \\nThe extension to more than one dimension is usually attributed to \\nCacoullos (1966) and Murthy (1966). \\nKernel methods are readily updated for use in leave-one-out cross\\xad\\nvalidation if we retain the numerator and denominator of (6.3). To find \\np(k I Xj, ff\\\\ {xj}) we only have to subtract K(O) from the denominator, \\nand from the numerator when k is the true class of Xj. \\nSpecht (1990a, b, 1991) has re-labelled these methods as neural \\nnetworks (without any apparent biological motivation); (6.3) he calls a \\nprobabilistic neural network and ( 6.4) a general regression neural network. \\nKernel methods require the whole training set to be retained. Meth\\xad\\nods which select a smaller set of centres are considered in Section 6.4. \\nAn alternative approach advocated by Specht (1967a, b) is to approxi\\xad\\nmate p(x) for Gaussian kernels as the product of a multivariate normal \\ndensity and a polynomial, using a Taylor expansion. (Details are given \\nby Duda & Hart, 1973, pp. 106-107, and Wasserman, 1993, pp. 46-51.) \\nThis is more in the spirit of of the next subsection. \\nWe stress in Section A.1 that a density is defined with respect to \\nan underlying measure, and can be changed by changing that measure. \\nThus if we know that Pj is near some density po, we should choose that \\ndensity as the underlying measure, and so estimate Pj by an estimate \\nof the ratio Pj/Po times po. For a kernel estimator this becomes \\npj(x) = Po(x)_!_ LK(x- x;)/po(x;). \\nnj i \\nWe call p0 a fixed start density; p j will not integrate to exactly one. \\nWe can apply the same procedure with a density Po estimated from a \\nparametric family (for example the family of normal distributions) by \\nusing a parametric start \\n(6.5) A closer connection to \\nneural networks is \\nmade for rotationally \\nsymmetric normal \\nkernels and features of \\nunit length, for then \\nK(x-xi)= f(xT x) \\nwhere \\nf(t) oc exp(l -t)/2a2. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 196}, page_content=\"6.1 Non-parametric estimation of class densities 185 \\nHowever, the standard theory no longer applies. Hjort & Glad (1995) \\nshow that the bias and variance are (asymptotically) essentially un\\xad\\nchanged from those of the fixed start estimator using the (unknown) \\nbest possible fixed start within the parametric class. \\nHjort & Jones (1996) reverse the roles of the parametric and non\\xad\\nparametric parts of ( 6.5) by estimating the parameters of the parametric \\nfamily locally (as defined by the kernel). More precisely, the density at \\nX is estimated by f(x; fl(x)) for a parametric family f(·; ()),where (}(x) \\nis chosen to maximize a local log-likelihood \\n~LK(x-xi)logf(xi;{})-I K(t-x)f(t ;())dt. \\nIn both versions the hope is that the parametric family will capture \\nthe broad features of the density, and allow a kernel with a much \\nlarger spread to be used. This may make the methods feasible in \\nmore dimensions than the simple kernel method, but the issues of the \\nchoice of kernel remain. These methods will be most effective in a \\nsmall number of dimensions . A more constrained form of correction \\nto a parametric start is discussed later under 'projection pursuit density \\nestimation'. \\nOrthogonal expansion estimators \\nA general approach towards non-parametric density estimation is via \\nexpansions in orthogonal basis functions, estimation of necessary co\\xad\\nefficients, and a rule to decide when to stop including terms in the \\nexpansion. The expansion approach has some advantages over ker\\xad\\nnel methods in statistical pattern recognition problems. This approach \\noften yields a compact representation of the estimates of class densi\\xad\\nties, with a low number of coefficients describing the estimate. Most \\ntexts on density estimation mention the approach , whereas Tarter & \\nLock (1993) consider only orthogonal expansions. \\nWe start with a general orthonormal set of basis functions 1/)k(x) \\non f!( with respect to a suitable weight function w, that is, \\nI 1pj(y)1pk(y)w(y)dy = I{j = k}. \\nExamples of such structures abound, see for example Abramowitz & \\nStegun (1965, Chapter 22) or Thisted (1988, §5.3.2). We will exemplify \\nthe method by Fourier series, for f!( = [0, 1] and 1/)k(x) = exp 2nikx. (It \\nis always worth bearing in mind that the feature space may profitably be \\ntransformed before density estimation; for example the transformation \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 197}, page_content='186 6 Non-parametric Methods \\n<l>(x) will map JR ~ [0, 1] and turns near standard normal densities into \\nnear uniform ones.) Other examples we shall meet in Section 9.1 are \\npolynomial expansions (Hermite polynomials) orthogonal with respect \\nto a uniform weight on [-1,1] and with respect to 4>(x) (the normal \\ndensity) on JR. \\nSuppose we wish to approximate a density function f by a series \\nexpansiOn f(x) = L:k CklJYk(x). The best choice of coefficients q IS \\ngiven by \\nck = j lJYk(Y)f(y)w(y) dy = E 1pj(X)w(X) \\nin the sense of minimizing the mean integrated squared error MISE = \\nE j(f -f)2w. The infinite series I:;:o q1pk(x) defined in this way \\nconverges pointwise to f(x) if f is continuous. We estimate Ck by \\nck = I: lJYk(Xi)w(Xi)/n. With estimated coefficients we do have to stop \\nthe expansion at some term m. More generally, we could use \\n00 \\nf(x) = L bkck1pk(x) \\nk=l \\nfor some suitable tapering sequence (bk). \\nLet us explore Fourier series a little. Since the Fourier coefficients \\nare defined for both positive and negative integers, our sums should \\nextend infinitely in both directions (or be truncated at ±m ). We can \\nalways write a Fourier series estimator as a kernel estimator on taking \\nK( ) L b 2 .k sinn(2m+ l)x x = k exp nz x = --.--\\xadsmnx \\nlkl~m \\nif we truncate at ±m. This is the Dirichlet kernel shown in Figure 6.3; \\nit has a narrow peak and slowly decaying oscillations about zero. Thus \\nwith this tapering 7 can take negative values, but for the tapering \\nsequence bk = max[O, 1-k/(m + 1)], the kernel is the Fejer kernel \\nK(x) = _1_ [sinn~m+ 1)x]2 \\nm + 1 smnx \\n~ \\nand so f is non-negative (Figure 6.3). \\nA number of stopping rules have been proposed. It is fairly easy to \\nshow that including term k will decrease MISE if jqj2 > 1/(n + 1) \\n(Tarter & Lock, 1993, §4.2) and with a bias-correction argument this \\nsuggests including term k only if lcki2 > 2/(n + 1). Unfortunately \\nthis will lead to the inclusion of an infinite number of terms, so it is '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 198}, page_content='Figure 6.3: Kernels for \\nFourier series estimator. \\nLeft: Dirichlet kernel. \\nRight: Fejer kernel. \\nRidge functions are \\nconstant orthogonally \\nto one direction. 6.1 Non-parametric estimation of class densities 187 \\nlll lll \\nN N \\n0 0 \\nN N \\n~ ~ \\n~ ~ \\nlll lll \\n0 0 \\nu;> u;> \\n·0.4 ·0.2 0.0 0.2 0.4 ·0.4 ·0.2 0.0 0.2 0.4 \\npreferable to include terms until this test is failed for 2-4 consecutive \\nterms. Diggle & Hall (1986) give a similar but more complex alternative. \\nAn alternative approach is to allow the data to choose the tapering \\nsequence. Tarter & Lock report good results with the choice \\nOrthogonal series estimators meet similar difficulties to kernel den\\xad\\nsity estimators once we move to IR d, d > 1. We can, for example, use \\na multidimensional Fourier series, but these are tied to a particular \\ncoordinate system, and the stopping rule has to be defined for a d\\xad\\ntuple index. For Hermite polynomials invariance to rotations can be \\nachieved by including all terms up to degree m, but there are many of \\nthese for moderate m and we will need moderate m to approximate all \\nbut the simplest functions. Series estimators are particularly useful for \\nlow-dimensional projections. \\nParametric starts can be used with orthogonal series as well as \\nkernel density estimates; for some univariate examples using a normal \\nstart and Hermite polynomials see Buckland (1992a, b). \\nProjection pursuit density estimation \\nProjection pursuit density estimation (Friedman et al., 1984) is the ap\\xad\\nplication of projection-pursuit ideas (Section 9.1) to density estimation, \\nand so is appropriate when the variation in the densities is concentrated \\nin a linear subspace of f!C. It estimates a density by the formula \\nM \\nPM(x) = Po(x) IJ qm(a~x) (6.6) \\nm=l \\nwhere Po is an initial density (perhaps an appropriate multivariate \\nnormal distribution) and the qm are multiplicative corrections which \\nare ridge functions. We will consider its application to n samples x;, \\nperhaps the training samples for a single class. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 199}, page_content=\"188 6 Non-parametric Methods \\nThe corrections in (6.6) are chosen recursively. At stage m we \\nhave Pm-1 and choose am and qm to maximize the goodness-of-fit of \\nPm as measured by the Kullback-Leibler divergence E log Pm(X) for X \\ndrawn from the true density. Given am = a it is easy to show that \\nthis is maximized by choosing qm to be the ratio of the densities p \\nand Pm-1 projected onto the direction a. This is estimated by the ratio \\nof a univariate density estimate for the data points (aT xi) projected \\non a to the projection of Pm-1 (the integration to find the marginal \\ndistribution along aT x being done by Monte Carlo methods). Rather \\nthan retain the full density estimates, Friedman et al. approximated the \\nestimate of qm by a cubic spline. \\nThis method gives us an estimate of Pm given a. Since \\nElogpm(X) = Elogqm(aTX) + Elogpm-1(X), \\nwe choose am = a to maximize E log qm(a rx) and estimate this by its \\nsample version ~ 2:: logqm(a T xi). This is maximized numerically. \\nIt remains to choose the number of terms M. Standard methods \\nsuch as cross-validation can be used. Often examining qm (which shows \\nthe ratio of the two densities) against the projected data (aT xi) will \\nindicate if a worthwhile improvement can be made. \\nThe density estimation strategy of Friedman (1987, §4) is 'backward' \\nrather than 'forward' in flavour. Exploratory projection pursuit is used \\nto find a direction a such that a rx is maximally non-normal. We then \\nremove the marginal structure in direction a by \\ncp(aTx) \\np(x) +-p(x) ( T )' Parx a x \\nthat is by adjusting the marginal density to be standard normal (see \\npage 297), and repeat the process. Eventually the exploratory process \\nwill be unable to find an interesting projection, and the remaining \\ndensity can be fitted by a normal distribution. Reversing the process \\nreveals a density for X which is a normal density times a series of \\ncorrection terms, and the corrections are ridge functions. The marginal \\ndensities can be fitted by any one-dimensional estimation method, in\\xad\\ncluding splines, kernel and orthogonal series methods, but the compact \\nrepresentation of orthogonal series will be especially useful. \\nIt would be possible to use q-dimensional correction terms, for \\nsmall q. \\nDiscrete distributions \\nThus far we have implicitly assumed that we have continuous features, \\nso !!{ c lR.P. We can consider non-parametric estimation of class Friedman et al. (1984) \\nuse crude histogram \\nestimators of the \\ndensities, but kernel \\nmethods could be used. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 200}, page_content='xc denotes the values \\nof X; for i E C \\nThe empty set 0 is \\nincluded to give a \\nconstant term. 6.1 Non-parametric estimation of class densities 189 \\ndistributions in the discrete case too. For simplicity let us assume that \\nwe have p discrete features. Then a distribution is specified by a p-way \\ntable of probabilities of all possible combinations, and the natural non\\xad\\nparametric estimator is to use the frequencies of the cells of the table \\nin the training set !Y. Of course, if p is large and !Y is not enormous, \\nmost of the cells will be given zero probability. When we come to \\nclassify future cases, we are likely to find that the estimated class \\nprobabilities are zero under all classes. Thus it is essential to smooth \\nthe observed frequencies before using them in a plug-in classifier. We \\nwill consider how to do so, but point out that it will normally be better \\nto use a logistic regression than the methods considered here, as that \\nuses the data to model the quantities of direct interest, the posterior \\nprobabilities. \\nThe natural idea for statisticians would be to build a contingency \\ntable model of the joint distribution of the features (McCullagh & \\nNelder, 1989). The most common choice would be a log-linear model, \\nin which the log probability \\nlogPr{Xt = Xt, ... ,Xp = xp} = LA.c(xc) \\nc (6.7) \\nis expanded over subsets C c {1, ... ,p}. When the terms of (6.7) are \\nrestricted, for example by omitting A.c(xc) for large C, the family \\nof probability distributions is restricted, but the coefficients A.c may \\nbe fitted by maximum likelihood. Choosing the appropriate restricted \\nmodel is an art, and can be considered within the graphical framework \\nof Chapter 8. \\nThe most extreme restriction of ( 6. 7) is to omit all sets C of two or \\nmore features, so \\nPr{Xt = Xt, ... ,Xp = Xp} = e;.0 IT e).;(x;) \\ni \\nwhich amounts to assuming independence of the features. Thus (6.7) \\ncan be seen as an expansion away from an independence model. \\nOther expansions have been used for binary data. Suppose each \\nfeature takes the values 0 and 1, and that feature i takes the value 1 \\nwith probability Pi· Let \\nand consider the 2P polynomials y~1 • • • Yl· These are orthogonal with \\nrespect to the independence model, so we can take an orthogonal series '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 201}, page_content='190 6 Non-parametric Methods \\nexpansion. If we estimate p; by the frequency Pi with which feature \\ni takes value 1 in the training set, we have the Bahadur-Lazarsfeld \\nexpansion (Bahadur, 1961a, b; Lazarsfeld, 1961) \\nPr{Xt = Xt, ... ,Xp = xp} \\np \\n= rrg1(1-p;)1-Xj[1+ L:cijYiYj+ I: CijkYiYjYk+ .. l (6.8) \\ni=l i<j i<j<k \\nThe estimates c... are just the sample moments of Yi for the appropriate \\nindices. Often just the first correction term is used, which is a correction \\nfor correlations only. \\nKernel methods \\nFor binary features, Aitchison & Aitken (1976) proposed the kernel \\nsmoothing \\nwhere K(x, y) = hP-d(l -h)d and d = llx-yll2 is the number of \\ndisagreements between x and y. Here 1/2 < h ~ 1 is a smoothing \\nconstant; for h = 1 there is no smoothing. The kernel gives weight hP \\nto cell x and weight hP-k(l-hl to cells that differ in k features. Note \\nthat the kernel is a product over the features, since d is a sum over \\nfeatures. \\nThis product kernel can be extended naturally to categorical data \\nwith ki ~ 2 possible outcomes (give probability h to the observed \\noutcome, (1 -h)/(k;- 1) to all others), to ordered categorical data \\n(spread the probability 1-h over adjacent outcomes) and to features \\nwhich are counts (Aitken, 1983). For mixed continuous and discrete \\nfeatures we can take a product of an appropriate kernel for each feature. \\nFor binary and categorical features the kernel estimator takes a convex \\ncombination of the frequencies with a uniform distribution. This has \\nbeen considered in its own right as a method of smoothing (for example, \\nFienberg & Holland, 1973). \\nThe smoothing constant h can be chosen separately for each fea\\xad\\nture. Choosing an appropriate degree of smoothing remains a difficult \\nproblem (P. Hall, 1981; Tutz, 1986, 1988, 1989). Averaging with a \\nuniform distribution gives a different perspective which suggests other \\nways to choose the degree of smoothing (Fienberg & Holland, 1973; \\nTitterington, 1980; Brown & Rundell, 1985). '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 202}, page_content=\"Choosing the \\nneighbourhood using \\nall the classes may not \\nbe a good idea if the \\nclass densities have very \\ndifferent structure. \\nSee the glossary. \\nThis was reproduced in \\nAgrawala (1977), \\nDasarathy (1991) and \\nwith a commentary by \\nSilverman & Jones \\n(1989). 6.2 Nearest neighbour methods 191 \\n6.2 Nearest neighbour methods \\nOne simple adaptive kernel method is to choose K to be constant over \\nthe nearest k examples and zero elsewhere. This does not in fact define a \\ndensity as the estimate of p(x) (its integral is infinite) but (6.2) suggests \\na simple estimate of the posterior distribution as the proportions of \\nthe classes amongst the nearest k data points. This is a piecewise \\nconstant function over f!l, and gives a classifier known as the k-nearest \\nneighbour rule. This differs from using the k-nearest neighbour density \\nestimate for each class, as we choose the neighbourhood from the k \\nnearest points of any class. If the prior probabilities are known and the \\nproportions of the classes in the training set are not proportional to nk, \\nthe proportions amongst the neighbours need to be weighted (Brown \\n& Koplowitz, 1979). (We ignore this in the theory below.) \\nThe version with k = 1 is often rather successful. This divides \\nthe space f!l into the cells of the Dirichlet tessellation of the data \\npoints, and labels each by the class of the data point it contains. We \\ncan also consider the analogue of (6.4) which gives a locally constant \\nnon-parametric regression surface, and once again corresponds to the \\nk-nearest neighbour estimate of the posterior probabilities. \\nBoth the k-nn method and kernel discrimination were first given \\nin an unpublished report by Fix & Hodges (1951). There is a very \\nextensive literature on nearest neighbour classifiers, much of which is \\nreviewed or reprinted in Dasarathy (1991). \\nTies in the distances can occur with finite-precision data (or if the \\nunderlying distribution has a discrete part). One solution is to include \\nall patterns as near as or nearer than the k-nearest neighbour, and take \\na majority vote amongst them. \\nNearest neighbour rules can readily be extended to allow a 'doubt' \\noption by the so-called (k, t)-rules (Hellman, 1970), called in this field \\na 'reject option'. These take a vote amongst the classes of the k nearest \\npatterns in f!l, but only declare the class with the majority if it has t \\nor more votes, otherwise declare 'doubt'. Indeed, if there are different \\nerror costs, we may want to allow the minimum majority to depend on \\nthe class to be declared. Properties of this class of rules are discussed \\nby Devijver & Kittler (1982) and Loizou & Maybank (1987), but for \\nlarge samples. \\nThe k-nn rule can be critically dependent on the distance used in the \\nspace f!l, especially if there are few examples or k is large (Figure 6.4). \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 203}, page_content=\"192 6 Non-parametric Methods \\n0 a 0 0 0 a \\n0 0 \\nc c \\nc c \\n0 0 0 0 .,; .,; \\nc \\nb b \\na b a b \\nb b \\na ubb \\n50 \\nT etrahydrocortisone \\nCover & Hart (1967) gave a famous result on the large-sample be\\xad\\nhaviour of the nearest neighbour rule. Note that the expected error rate \\nis always bounded below by E*, by the optimality of the Bayes rule. \\nProposition 6.1 Let E* denote the error rate of the Bayes rule in a K\\xad\\nclass problem. Then the error rate of the nearest neighbour rule averaged \\nover training sets converges in L1 as the size of the training set increases, \\nto a value E 1 bounded above by \\nE* (2-~E·) K-1 . \\nProof: Let X1 be the nearest neighbour to X, a randomly sampled \\npattern with class C. The (rather technical) arguments of C. J. Stone \\n(1977) and Devroye (1981a) show that \\nNow Eip(k I XI)-p(k I X) I ~ 0. \\nPr(C1 -=/= C I X= x) = L p(i I x) E [p(j I XI) I X= x] \\nif.j Figure 6.4: Decision \\nboundaries of the 1-nn \\nrule for the Cushing's \\nsyndrome data. The left \\nplot uses Euclidean \\ndistance on our usual \\nplot, the right \\nEuclidean distance on \\nlog10 excretion rates. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 204}, page_content='/\\' and It\\' denote \\nmonotone convergence \\nfrom below and above, \\nrespectively. and so 6.2 Nearest neighbour methods \\nEIPr(Ct =I= C I X)-L p(i I X) p(j I X) I ~ 0. \\ni#j 193 \\nThus E1 = Eet(X) where e1(x) = L:i-hp(iJx)p(jJx) = 1-\"L:;P(ilxf \\nThe conditional Bayes risk r*(x) is 1 -maxi p(i I x) = 1 -p(k I x), \\nsay, so by the Cauchy-Schwarz inequality \\nand (K-l)_Lp(ilx)2 ~ [_Lp(ilx)f =r·(x)2 \\ni=fk i=fk \\n(K-1) L p(i I x)2 ~ r*(x)2 + (K-1) (1-r*(x)2) \\ni \\n1-LP(i I x)2 ~ 2r*(x)- K ~ 1 r*(x)2. \\ni \\nOn taking expectations we obtain \\nE ~ 2E*-~E (r*(X)2] ~ 2E*-~(E*)2 \\n1\" K-1 \" K-1 \\nusing E(Y2) =VarY+ (E Y)2 ~ (E Y)2 0 \\nIt is easy to see that the upper bound is attained if the densities Pk(x) \\nare identical and so the conditional risks are independent of x. \\nFor the k-th nearest neighbour rule detailed results are only avail\\xad\\nable for two classes. Intuitively one would expect the 2-nn rule to be no \\nimprovement over the 1-nn rule, since it will achieve either a majority \\nof two or a tie, which we will suppose is broken at random. The \\nfollowing result supports that intuition. On the other hand, we could \\nreport \\'doubt\\' in the case of ties (the (2, 2)-rule). \\nProposition 6.2 Suppose there are two classes, and let Ek denote the \\nasymptotic error rate of the k-nn rule with ties broken at random and E£ \\nif ties are reported as \\'doubt\\'. Then \\nProof: We rely on L1 convergence results such as those quoted in the \\nproof of Proposition 6.1 to show the existence of the asymptotic results \\nand to replace p(k I Xr) by p(k I X) in deriving the asymptotic results. \\nLet rk(x) denote the (limit of the) probability of misclassifying x. We \\ncan think of this as taking k + 1 samples from p(·l x) and finding that \\nthe first belongs to the minority group. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 205}, page_content='194 6 Non-parametric Methods \\nTemporarily denote the posterior probabilities of the two classes \\nby p = p(11 x) and q = p(21 x), and let ~ = pq. Consider ak. the \\nprobability that the first of k samples is in a strict minority, and let \\nb; = (~;) ~; be the probability of exactly i examples from each class. \\nFirst consider k odd. On adding a point, we can create a tie but not a \\nminority, so \\nk-1 (2k-1) k-1 k k-1 (2k-1) k k-1 alk-1 -alk = P 2k -1 k p q + q 2k -1 k p q \\nk -1 (2k -1) k k -1 \\n= 22k -1 k ~ = 2k-1 bk. \\nIf k is even, adding a point can create a minority but not a tie so \\na2k+1 = a2k + !bk \\n1 k -1 1 \\na2k+1 = a2k-1 + 2bk-2k _ 1 bk = a2k-1 + 2(2k _ 1) bk. \\nClearly a1 = 0, so by induction \\nk 1 \\na2k+1 = L 2(2i-1) b;. \\n1=1 \\nIn terms of these quantities, we have \\nr~dx) = a2k+1 \\nr2k-1(x) = a2k + bk \\nr2k(x) = r~k(x) + !bk = a2k + !bk \\nFrom these formulae r2k+1-r2k-1 = (2~-!)bk ~ 0. Direct calculation \\nshows that r1(x) = 2~ and r~(x) = ~ = !r1(x). On taking expectations \\nthis establishes the hierarchy on each side. Now \\nNote that lim r2k-1 (x) = lim r~k(x) = ~ 2(2 .1 \\n1) b;. \\nk-HXJ k--+00 L.....t l -\\ni=1 \\nr(x) = min[p(11 x),p(21 x)] = ! [1-J1- 4~(x)] \\nhas the Taylor series expansion \\n• 00 \\n1 (2i -2) i 00 \\n1 \\nr (x) = L i i -1 ~ = L 2(2i -1)b;. \\n1=1 1=1 \\nThe proof is completed by taking expectations and using the monotone \\nconvergence theorem. D '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 206}, page_content='Figure 6.5: \\nLarge-sample risk rk \\n( k odd) or rk ( k even) \\nof k-nn rules against \\nthe Bayes risk r\\' in a \\ntwo-class problem. 6.2 Nearest neighbour methods 195 \\n\"\\' ci \\n... \\nci \\n\"\\' ci \\n~ \\n0 \\n~ \\n\"! \\n0 \\n;; \\n:; \\n0.0 0.1 0.2 0.3 0.4 0.5 \\nBayes risk \\nThis result is from Cover & Hart (1967); the formulae are given \\nwithout proof by Devijver & Kittler (1982) and Fukunaga (1990). \\nFigure 6.5 shows rk{x) as a function of r*(x); this shows the agreement \\nis excellent for moderate r*(x) even for small k (but not k = 1 ). \\nComparable results for the (k, t)-nn rule were first considered by \\nTomek (1976a). Later Loizou & Maybank (1987) showed that for \\nt > k/2 we have \\nwhere E*(d) refers to the Bayes rule (if any) with doubt cost d such \\nthat its doubt rate is equal to the asymptotic doubt rate of the (k, t)-nn \\nrule, and c(k,t,K) is a computable constant. Since E*(d) ~ E*, this can \\nbe used to give asymptotic lower bounds for E*. Clearly the left-hand \\nside increases with t. \\nAnother version of the asymptotics is to allow k to increase with \\nthe size n of the training set. C. J. Stone (1977) showed that provided \\nk --+ oo and kjn --+ 0, the risk for the k-nn rule (not averaged over \\ntraining sets) converges in probability to the Bayes risk. For k/ log n --+ \\noo, Devroye (1981b) strengthened this to almost sure convergence. The \\nsame methods allow the variance over training sets for fixed k to be \\nestimated. \\nAll these performance measures are asymptotic and they do not \\napply to finite samples; for example the error rate does not decrease \\nmonotonically with odd k. Notice that the results do not depend on the \\nmetric, whereas in practice the choice of metric is often very important. \\nThere are few finite-sample results. Cover (1968) is widely quoted as \\nshowing that the risk of the 1-nn rule converges to E1 at rate O(n-2), \\nbut his results only apply to the bias for one-dimensional f£. Fukunaga '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 207}, page_content=\"196 6 Non-parametric Methods \\n& Hummels (1987a, b; see also Fukunaga, 1990, §7.3) consider an \\nexpansion of the mean difference between the error rates of the k-nn \\nrule and its asymptotic version. They found a leading term of O(n-21P) \\nfor p-dimensional f!l', and for the 2-nn with ties reported as 'doubt', the \\nrate O(n-41P). These are very slow rates of convergence for moderate \\np, but since that to E~ is faster, they suggest that for two classes it is \\nbetter to estimate £1 by doubling the estimate of E~ (as suggested by \\nFukunaga & Flick, 1985). \\nThese results only concern the bias of the true error rate based on \\na training set of size n as an estimator of Ek. The variability must \\nalso be taken into account if these results are to be used to bound the \\nBayes risk. We have seen that for odd k, Ek-l,rk/21 = Ek-1 ~ E*, which r l is defined on \\nsuggests bounding the Bayes risk by the achieved performance of the page xii. \\n(k-1, fk/21)-nn rule. However, recall from Section 2.7 that we can \\nachieve lower variability by not using the class labels, but averaging \\n1 -max p(k I x) over a test or training set. If we observe ki neighbours \\nof class i, this suggests estimating the error rate by E, the average of \\nmin(k1,k2) jk. In Section 2.7 we viewed E as a biased estimate of Ek. \\nUnder our large-sample assumptions for the training set but averaging \\nover a test set, it can be shown that (for k odd) E E = Ek-l,rk/21, \\nbut that E has a lower variance than the test-set error rate of the \\n(k-1, fk/21)-nn rule (Devijver & Kittler, 1982, §10.8). \\nThese results are confined to two classes. For K ~ 2 we have \\nProposition 6.3 In the large-sample theory the means of the risk-averaged \\n(3, 2) -nn rule and the error rate of the (2, 2)-nn rule are equal and provide \\na lower bound for the Bayes risk. The risk-averaged estimator has smaller \\nvariance. \\nProof: Condition on x, and draw three samples from 1J = p(·l x), as \\nthe observed point and its two neighbours. Let ( be the probability \\nthat two are from one class, one from another; unless this occurs both \\nrules score zero. The (2, 2)-nn rule scores one if the observed point \\nis in the minority, so has conditional mean and mean square of ( /3. \\nThe risk-averaged 3-nn scores 1/3 under all assignments of the three \\nsamples, so its conditional mean and mean square are (/3 and (/9. \\nConditionally and hence unconditionally the means are the same and \\nthe risk-averaged estimator has smaller variance. \\nNow (/3 = 'L,k(IJ~-IJt) = Ll, say. We will show that Ll ~ 1-maXIJk \\nand average over x to bound the Bayes risk. Suppose IJl ~ IJk for \\nk > 1. Consider increasing IJi by [) and decreasing IJj by [) for i, j > 1. \\nIf IJi + IJj < 2/3 we can increase Ll by zeroing the smaller of IJi and Check the derivative . \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 208}, page_content=\"6.2 Nearest neighbour methods 197 \\nru; thus we can zero the smallest Y/j > 0 in turn, transferring the mass \\nto the next smallest. If rt1 > 1/3, we can make just one Y/j > 0, which \\nshows that d ~ ytf(l -ytt} + (1-rtd2111 = Y/1(1-ytt} < 1 -'11· If \\nyt1 ~ 1/3 we can make just two Y/i, Y/j > 0; if Y/i > 1/3 the case already \\nproved applied to (Y/i, Y/1, Y/j) shows d < 1-Y/i < 1-'11· We can show \\nthat the result holds for three Y/i = 1/3 by direct calculation. 0 \\nThis suggests estimating a lower bound for the Bayes risk by running \\nthe 3-nn classifier on the training set and reporting 1/3 the number of \\noccasions on which the neighbours are two of one class, one of another \\n(and of course one of the neighbours will be the training-set example \\nitself). If the distances are tied, we can average over ways of breaking \\nthe tie, since this will be equivalent to averaging over perturbations of \\nthe points. \\nChoice of metric \\nWe have not avoided the choice of a metric on !!l, and this can again \\ncause difficulties. In practice Euclidean distance is normally used, but \\nafter a suitable scaling of the variables. It may make sense to use \\nan (estimated) Mahalanobis distance if the within-class distributions \\nare roughly normal and of similar covariance matrix. For two classes, \\nShort & Fukunaga (1980, 1981) looked at a local metric with the aim \\nof minimizing the mean-square error between the finite-sample risk and \\nthe asymptotic risk (which we have seen does not depend on the metric \\nused). They show the metric should be of the form \\nd(x, y) = IP(11 x)-p(11 Y)l, \\nexpand this about x and estimate the coefficients of the expansion. \\nShort & Fukunaga (1980) and Myles & Hand (1990) experiment with \\nextensions to several classes. \\nFukunaga & Flick (1984) suggest choosing a global quadratic metric \\nwith the same aim (and again with two classes). Their metric is a \\nMahalanobis distance and so they choose the inverse covariance A to \\nobtain \\nd(x, z) = V(x-z)T A(x-z). \\nThey compute the value of A which approximately minimizes the error \\nrate, and then estimate the quantities involved (the underlying densities) \\nby k-nn density estimation. \\nOne of the most important steps in choosing a metric may be \\nto exclude features which have little or no relevance. The features \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 209}, page_content='198 6 Non-parametric Methods \\nselected by classification trees (Chapter 7) can be a very useful guide \\n(Ripley, 1993). \\nAn appealing idea is to combine the features of kernel methods \\nand k-nearest neighbour methods by distance-weighting the classes of \\nthe neighbours in reaching a decision. This has proved controversial \\n(Dudani, 1976; Bailey & Jain, 1978; Morin & Raeside, 1981; MacLeod \\net al., 1987; Parthasarthy & Chatterji, 1990) in that asymptotically for \\nfixed k the distance-weighting does not help. However, this is not the \\ncorrect basis for the comparison, since using distance-weighting may \\nallow a larger value of k to be used for a given size of training set. \\nData editing \\nOne common complaint about both kernel and k-nn methods is that \\nthey can take too long to compute and need too much storage for \\nthe whole training set. The difficulties are sometimes exaggerated, as \\nthere are fast ways to find near neighbours (for example Friedman et \\nal., 1975, 1977; Fukunaga & Narendra, 1975; Kalantari & McDonald, \\n1983; Kamgar-Parsi & Kanal, 1985; Preparata & Shamos, 1985; Ruiz, \\n1986; Kim & Park, 1986; Niemann & Goppert, 1988; Bryant, 1989; \\nJiang & Zhang, 1993) and fast approximate ways to find kernel density \\nestimates by binning (Hardie, 1991; Scott, 1992). However, in many \\nproblems it is only necessary to retain a small proportion of the training \\nset to approximate very well the decision boundary of the k-nn classifier. \\nThis concept is known as data editing. It can also be used to improve \\nthe performance of the classifier by removing apparent outliers. \\nThere are many editing algorithms: the literature on data editing is \\nextensive but contains few comparisons. (It is surveyed in Dasarathy, \\n1991.) The multiedit algorithm of Devijver & Kittler (1982) can be \\nspecified as follows (with parameters I and V): \\n1 Put all patterns in the current set. \\n2 Divide the current set more or less equally into V ~ 3 sets. Use \\npairs cyclically as test and training sets. \\n3 For each pair classify the test set using the k-nn rule from the \\ntraining set. \\n4 Delete from the current set all those patterns in the test set which \\nwere incorrectly classified. \\n5 If any patterns were deleted in the last I passes return to step 2. \\nThe edited set is then used with the 1-nn rule (not the original value of \\nk ). Devijver & Kittler indicate that (for two classes) asymptotically the '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 210}, page_content='Figure 6.6: Reduction \\nalgorithms applied to \\nFigure 1.3. The known \\ndecision boundary of \\nthe Bayes rule is shown \\nwith a solid line; the \\ndecision boundary for \\nthe 1-nn rule is shown \\ndashed. \\n(a) multiedit. \\n(b) The result of \\nretaining only those \\npoints whose posterior \\nprobability of the actual \\nclass exceeds 90% when \\nestimated from the \\nremaining points. \\n(c) condense after \\nmultiedit. \\n(d) reduced nn applied \\nafter condense to (a). 6.2 Nearest neighbour methods \\n0 \\nD 0 \\nD C D \\n\\\\ Ltl or:Z,fl 0~ 0 co \\nd\\'Bif.:>\\\\, a -o<»>loa ~ \\n0 ~0 I \\\\ df \\ne e 0\"\" D 0 0 ~ I e \\\\ ~D D ., ... \\' -... ,_ :!. .. / • •• ~.tl • \\n••• A. • ·~ .. ·-. ... -... . ... . . ., ·\\' ..•.. \\n(a) \\nI \\nD D I • I D \\n\\' I 0 \\n\\' \\n(c) \\' \\' ..... / .... ____ _ \\n(b) . .. , . . . • . \\nI I \\no I • I o \\n\\' -. \\n(d) 199 \\n1-nn rule on the edited set out-performs the k-nn rule on the original \\nset and approaches the performance of the Bayes rule. (The idea is that \\neach edit biases the retained points near x in favour of the class given \\nby the Bayes rule at x, so eventually this class dominates the nearest \\nneighbours. This applies to any number of classes.) \\nFigure 6.6(a) illustrates the multiedit algorithm applied to the syn\\xad\\nthetic dataset shown in Figure 1.3 on page 12. The Bayes rule is known \\nin this example (since it is synthetic). In practice multiediting can \\nperform much less well and drop whole classes when applied to mod\\xad\\nerately sized training sets with more dimensions and classes. Another \\nidea (Hand & Batchelor, 1978) is to retain only points whose likelihood \\nratio Py(x)/pi(x) against every class i -=/= y exceeds some threshold t. \\n(The densities are estimated non-parametrically.) It make more sense to \\nretain points for which p(y I x) is high, for example those which attain \\na majority t in a (k, t)-rule for a larger value of k. This is illustrated \\nin Figure 6.6(b) for the synthetic example using the (10,9)-nn. \\nEarlier editing algorithms were given by Wilson (1972), Wag\\xad\\nner (1973), Tomek (1976b) and Penrod & Wagner (1977). \\nThe multiedit algorithm aims to form homogeneous clusters in fl£. \\nHowever, only the points on the boundaries of the clusters are really \\neffective in defining the classifier boundaries. Condensing algorithms '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 211}, page_content='200 \\n\"\\' 0 \\n0 \\n0 \\n-1.0 6 Non-parametric Methods \\n-0.5 0.0 0 \\n• 0 \\n.r:J Cl [l \\nIt • r::. IJ • . \\'c .. \\n0.5 1.0 \\naim to retain only the crucial exterior points in the clusters. For \\nexample, Hart (1968) gives: \\n1 Divide the current patterns into a store and a grabbag. One possible \\npartition is to put the first point in the store, the rest in the grabbag. \\n2 Classify each sample in the grabbag by the 1-nn rule using the store \\nas training set. If the sample is incorrectly classified transfer it to \\nthe store. \\n3 Return to 2 unless no transfers occurred or the grabbag is empty. \\n4 Return the store. \\nThis is illustrated in Figure 6.6(c). \\nA refinement, the reduced nearest neighbour rule of Gates (1972), is \\nto go back over the condensed training set and drop any patterns (one \\nat a time) which are not needed to correctly classify the rest of the \\n(edited) training set. As Figure 6.6(d) shows, this can easily go too far \\nand drop whole regions of a class. Other attempts at condensing and \\nreducing are discussed by Swonger (1972), Ullmann (1974), Ritter et \\nal. (1975), Tomek (1976c), Chidananda Gowda & Krishna (1979) and \\nFukunaga & Mantock (1984). \\nNearest neighbour methods will give low apparent error rate (zero \\nfor 1-nn) so it is essential to use other forms of performance assessment. \\nFortunately the leave-one-out cross-validated error rate can be com\\xad\\nputed as easily as the apparent error rate by finding the k neighbours \\nof x excluding x itself. \\nEssentially the same ideas have been considered within the field \\nof machine learning, known as \\'memory-based learning\\' (for example, \\nStanfill & Waltz, 1986). In its simplest form this is just a 1-nn classifier \\nbased on storing all the examples. Editing and condensing techniques Figure 6.7: The result \\nof the reduced nearest \\nneighbour rule of \\nGates (1972) applied \\nafter condense to the \\nunedited data of \\nFigure 1.3. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 212}, page_content='6.3 Learning vector quantization 201 \\nhave also been considered (E. E. Smith & Medin, 1981; Kibler & Aha, \\n1987). These seem re-discoveries of the simplest such algorithms. \\nExamples \\nThe variables in the Pima Indians diabetes example were on very \\ndifferent scales, and so were each scaled to have range about one over \\nthe training set. The initial choice of k was made by both leave-one-out \\nand 10-fold cross validation. For the latter the error rates were 57, 65, \\n57, 54, 51 and 55 (out of 200) for k = 1, 3, 5, 7, 9, 11; for leave-one-out \\nwe obtained 58, 65, 60, 55, 49 and 41. On the test set the numbers of \\nerrors were 98 for k = 1 and 82 for k = 9, out of 332. This is an \\nexample in which there is considerable mixing between the classes, and \\nlocal methods will be unable to pick up the broad trends. Using the \\n3-nn rule on the training set suggested a lower bound of about 15% \\nfor the Bayes error in this problem; linear methods achieve about 20%. \\nFor the forensic glass data, there are many rational ways to choose \\nthe metric, since the features are eight proportions plus the refractive \\nindex. We chose to rescale the refractive index to about ±10 but not \\nto scale the compositional features. With this metric, risk-averaging \\nthe 3-nn rule suggests a lower bound of 14% for the Bayes risk. Each \\nof the 1-nn, 3-nn and 5-nn rules had a similar level of performance; \\nthe 1-nn rule had a cross-validated error rate of 23.4% and confusion \\nmatrix \\nWinF WinNF Veh Con Tabl Head \\nWinF 59 7 4 0 0 0 \\nWinNF 12 59 3 2 0 0 \\nVeh 2 5 10 0 0 0 \\nCon 0 2 0 8 1 2 \\nTabl 1 0 0 2 6 0 \\nHead 1 3 1 1 1 22 \\nComparing this with the confusion matrices for larger values of k \\nshows the effect of very different proportions for the six classes; as k \\nincreases fewer errors are made on the more abundant classes, whereas \\nmore are made on the rare classes. \\n6.3 Learning vector quantization \\nThe refinements of the k-nn rule aim to choose a subset of the training \\nset in such a way that the 1-nn rule based on this subset approximates \\nthe Bayes classifier. It is not necessary that the modified training set '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 213}, page_content=\"202 6 Non-parametric Methods \\nis a subset of the original and an early step to combine examples to \\nform prototypes was taken by Chang (1974). The approach taken in \\nKohonen's (1988a, b, 1990a, b, 1995) learning vector quantization is to \\nconstruct a modified training set iteratively. Following Kohonen, we \\ncall the modified training set the codebook. This procedure tries to \\nrepresent the decision boundaries rather than the class distributions. \\nOnce again the metric in the space P£ is crucial, so we assume the \\nvariables have been scaled in such a way that Euclidean distance is \\nappropriate (at least locally). \\nVector quantization \\nThe use of 'vector quantization' is potentially misleading, since it has a \\ndifferent aim, but as it motivated Kohonen's algorithm we will digress \\nfor a brief description. \\nVector quantization is a classical method in signal processing to \\nproduce an approximation to the distribution of a single class by a \\ncodebook. Each incoming signal is mapped to the nearest codebook \\nvector, and that vector sent instead of the original signal. Of course, \\nthis can be coded more compactly by first sending the codebook, then \\njust the indices in the codebook rather than the whole vectors. One \\nway to choose the codebook is to minimize some measure of the \\napproximation error averaged over the distribution of the signals (and \\nin practice over the training patterns of that class). Taking the measure \\nas the squared distance from the signal to the nearest codebook vector \\nleads to the k-means algorithm which aims to minimize the sum-of\\xad\\nsquares of distances within clusters (Section 9.3). An 'on-line' iterative \\nalgorithm for this criterion is to present each pattern x in turn, and \\nupdate the codebook by \\nme ~ me+ a(t)[x-me] if me is closest to x (6.9) \\nmi ~ mi for the rest of the codebook. \\nUpdate rule (6.9) motivated Kohonen's iterative algorithms. Note \\nthat this is not a good algorithm for k-means; better algorithms are \\ndiscussed in Section 9.3. \\nMax (1960) and Zador (1982) have pointed out that choosing the \\naverage r th power of the distance as the measure of approximation \\nerror amounted to choosing the density of the codebook vectors to \\napproximate the dj(d + r)th power of the true probability density \\nin d dimensions. Thus for d large the k-means procedure codes an \\napproximation to p(x). This is an asymptotic result for large codebooks, Gersho & Gray (1992) \\nis a reference text on \\nvector quantization; a \\nshort introduction is \\ngiven by Gray (1984). \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 214}, page_content='6.3 Learning vector quantization 203 \\nbut does indicate that a codebook produced by vector quantization for \\neach class might be a good initial reduction of a very large training set. \\nIterative algorithms \\nKohonen (1990a) advocated a series of iterative procedures which has \\nsince been modified; our description follows the implementation known \\nas LVQ_PAK documented in Kohonen et al. (1992). A initial set of \\ncodebook vectors is chosen from the training set. (We discuss later \\nprecisely how this might be done.) Each of the procedures moves \\ncodebook vectors to try to achieve better classification of the training \\nset by the 1-nn rule based on the codebook . The examples from the \\ntraining set are presented one at a time, and the codebook is updated \\nafter each presentation. In our experiments the examples were chosen \\nrandomly from the training set, but one might cycle through the training \\nset in some pre-specified order. \\nThe original procedure LVQ 1 uses the following update rule. A \\nexample x is presented. The nearest codebook vector to x, me, IS \\nupdated by \\nme -me + et(t)[x- me] \\nme -me-et(t)[x-me] if x is classified correctly by me \\nif x is classified incorrectly (6.10) \\nand all other codebook vectors are unchanged. Initially oc(t) is chosen \\nsmaller than 0.1 (0.03 by default in LVQ_PAK) and it is reduced linearly \\nto zero during the fixed number of iterations. The effect of the updating \\nrule is to move a codebook vector towards nearby examples of its own \\nclass, and away from ones of other classes. \\'Nearby\\' here can cover \\nquite large regions, as the codebook will typically be small and in any \\ncase will cover f!l rather sparsely. Kohonen (1990a) motivates this as \\napplying vector quantization to the function ln1P1(x)-n2P2(x)l for two \\nclasses (or the two classes which are locally most relevant). \\nA variant, OLVQ1, provides learning rates Cte(t) for each codebook \\nvector, with an updating rule for the learning rates of \\nCte(t-1) \\nCtc(t) = 1 + (-1)/(classification is incorrect) Ctc(t -1)\" (6.11) \\nThis decreases the learning rate if the example is correctly classified, \\nand· increases it otherwise. Thus code book vectors in the centre of \\nclasses will have rapidly decreasing learning rates, and those near class \\nboundaries will have increasing rates (and so be moved away from \\nthe boundary quite rapidly). As the learning rates may increase, they '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 215}, page_content=\"204 \\n0 \\nci \\n-1.0 6 Non-parametric Methods \\n+ \\n+ \\n+ + \\n-0.5 0.0 0.5 1.0 \\nare constrained not to exceed an upper bound, often 0.3. Practical \\nexperience shows that the convergence is usually rapid, and LVQ_PAK \\nuses 40 times as many iterations as codebook vectors. \\nAn explanation of this rule is given by Kohonen et al. (1992) and \\nKohonen (1995, p. 180) which we interpret as follows. At all times \\nthe codebook vectors are a linear combination of the training set \\nvectors (and their initializers, if these are not in the training set). Let \\ns(t) = (-1)/(classification is incorrect), so we can rewrite (6.10) as \\nmc(t + 1) = mc(t) + s(t)ex(t)[x(t)- me] \\n= [1 -s(t)ex(t)]mc(t) + s(t)ex(t)x(t) \\n= [1-s(t)ex(t)][1- s(t-1)ex(t-1)]mc(t-1) \\n+ [1-s(t)ex(t)]s(t -1)ex(t- 1)x(t -1) + s(t)ex(t)x(t). \\nNow suppose x(t-1) = x(t) and the same codebook vector is closest \\nat both times (so s(t-1) = s(t) ). If we ask that the multiplier of x(t) \\nis the same in both terms, we find \\n[ 1 -s( t )ex( t)] ex( t -1) = ex( t) \\nwhich gives (6.11). This adaptive choice of rate seems to work well, as \\nin our examples. \\nThe procedure LVQ2.1 (Kohonen, 1990b) tries harder to approxi\\xad\\nmate the Bayes rule by pairwise adjustments of the codebook vectors. \\nSuppose ms, IDt are the two nearest neighbours to x. They are updated \\nsimultaneously provided that ms is of the same class as x and the class \\nof mt is different, and x falls into a 'window' near the mid-point of \\nms and IDt. Specifically, we must have \\n. (d(x,ms) d(x,mt)) 1-w mm , >--d(x,mt) d(x,ms) 1+w Figure 6.8: Results of \\nlearning vector \\nquantization applied to \\nFigure 1.3. The initially \\nchosen codebook is \\nshown by small circles, \\nthe result of OLVQl by \\n+ and subsequently \\napplying 25,000 passes \\nof L VQ2.1 by triangles. \\nThe known decision \\nboundary of the Bayes \\nrule is also shown. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 216}, page_content=\"6.3 Learning vector quantization 205 \\nfor w ::::::: 0.25. (We can interpret this condition geometrically. If x \\nis projected onto the vector joining ms and mt, it must fall at least \\n(1 -w)/2 of the distance from each end.) If all these conditions are \\nsatisfied the two vectors are updated by \\nms ~ ms + or:(t)[x- ms], \\nmt ~ mt -or:(t)[x- mt]. ( 6.12) \\nThis rule may update the codebook only infrequently. It tends to over\\xad\\ncorrect, as can be seen in Figure 6.8, where the result of iterating LVQ2.1 \\nis to push the codebook vectors away from the decision boundary, and \\neventually off the figure. Thus it is recommended that LVQ2.1 only \\nbe used for a small number of iterations (30--200 times the number of \\ncodebook vectors). \\nThe rule LVQ3 tries to overcome over-correction by using LVQ2.1 \\nif the two closest codebook vectors to x are of different classes, and \\n(6.13) \\nfor E around 0.1-0.5, for each of the two nearest codebook vectors if \\nthey are of the same class as x. (The window is only used if the two \\ncodebook vectors are of different classes.) This introduces a second \\nelement into the iteration, of ensuring that the codebook vectors do \\nnot become too unrepresentative of their class distribution. It does still \\nallow the codebooks to drift to the centre of the class distributions and \\neven beyond, as Figure 6.9 shows. \\nThe recommended procedure is to run OLVQl until convergence \\n(usually rapid) and then a moderate number of further steps of LVQ1 \\nand/or LVQ3. \\nde Sa & Ballard (1993) motivate a variant of LVQ2.1 by applying \\nstochastic approximation to a kernel regression estimator of ln1P1 (x)-\\n7!2P2(x)l for two classes. This normalizes the step size (replacing [x-ms] \\nby [x-ms]/llx- msll) and reduces the window size as well as or:(t). \\nInitialization \\nThe package LVQ_PAK chooses the initial codebook vectors from \\namongst the training set vectors. It is desirable that the initial vec\\xad\\ntors lie inside the Bayes boundary for their class, as otherwise they end \\nup representing 'islands' in the classification induced by the 1-nn rule \\nbased on the training set which are merely noise. Thus the candidates \\nfor initialization are screened to ensure that they are correctly classified \\nby a k-nn rule for moderate k (default 7). \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 217}, page_content=\"206 6 Non-parametric Methods \\nq - . + \\n00 + \\n+ 0 0 \\n.. o+ \\nIt) \\nc) \\n+ + .. \\n+ 0 \\n+ + \\n0 \\nc) \\n-1.0 -0.5 0.0 0.5 1.0 \\nKohonen (1990a) advocates a more constructive approach to the \\ninitial codebook, for example using a vector quantization of the whole \\ntraining set (ignoring class) or his own 'self-organizing map' method \\n(Section 9.4). \\nA further question is how many codebook vectors to use, and how \\nthey should be partitioned amongst the classes. The literature seems to \\nassume that the larger number the better, as this will enable a better \\npiecewise linear approximation to the decision boundaries of the Bayes \\nclassifier. However, this argument assumes that the iterative algorithms \\ncan make a good job of distributing the codebook, and as Figures 6.9 \\nand 6.10 show, this is not necessarily so. We may do better to start with \\na better-designed codebook such as the result of editing procedures. \\nIt is unclear how many codebooks vectors should be selected for \\neach class, since the number needed depends as much on how well \\nthey are employed as on the proportions of the class. Kohonen et \\nal. (1992) suggest using equal numbers per class initially, and altering Figure 6.9: Further \\nresults of LVQ with a \\nlarger codebook. This \\ntime the triangles show \\nthe results from LVQ3. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 218}, page_content='6.4 Mixture representations 207 \\nthe proportions so that the median of the nearest neighbour distances \\nbetween vectors within a class is roughly similar across classes. \\nExamples \\nFor the Pima Indians data we used OLVQ1 plus LVQ2 or LVQ3 with \\n2-4 vectors per class, and found a test-set error rate of around 70/332. \\nIncreasing this to 10 vectors per class increased the error rate to about \\n75/332. \\nA procedure consisting of initializing, running OLVQ1 to conver\\xad\\ngence then 10,000 iterations of LVQ3 was run under the standard \\n10-fold cross-validation for the forensic glass data. The estimated error \\nrate was 30.4%. Assigning equal numbers of vectors to each class \\n(4 each) reduced this slightly, to 29.9%, making less errors in the \\ncontainers and tableware groups, and more for non-float window glass. \\n6.4 Mixture representations \\nAnother way of looking at kernel density estimation with a non-negative \\nkernel is that it represents a probability density by \\np(x) = L wif;(x) \\ni \\nwhere the densities f;(x) = K (x -x;) and the weights are uniform (or \\nmr[i]/n[i] for known class probabilities). Then pj(x) is of the same form, \\nbut giving weights 1/nj to points from class j and zero to the others. \\nVector quantization uses a somewhat more general uniformly-weighted \\nmixture. This suggests representing densities by a mixture of a fixed set \\nof densities, so \\np(x) = ~ [ ~ 1tjWij] f;(x) \\nI } (6.14) \\nand \\n\"( .\\n1 ) n j 2::::; Wijj;(x) p} X = . L; [I:j 1tjWij]f;(x) (6.15) \\nWe may also want to allow parameters within J;, for example the \\nmeans and in the covariance matrix of a normal density, so we will \\nwrite f;(x; 8;). We considered the case of a two-component normal \\nmixture in Chapter 2 on pages 41-42. \\nMixture densities of this sort have been considered occasionally as \\ngeneral models for density estimation (for example by Roeder, 1990). '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 219}, page_content=\"208 6 Non-parametric Methods \\nWe could estimate the class-conditional densities PJ by choosing the \\nWiJ on the basis of the training samples labelled by class j and then \\nuse (6.15) for classifying future samples. It is also possible to use \\nunclassified cases to assist in the estimation process, as proposed in \\nthis context by Traven (1991). Sebestyen (1962) proposed an iterative \\nprocess of approximating the class-conditional densities by Gaussian \\nmixtures; two recent accounts also using Gaussian mixtures are Chou \\n& Chen (1992) and Streit & Luginbuhl (1994), both of which regard \\nthis as a 'neural network' method. \\nThere is an extensive literature on the estimation of mixture distri\\xad\\nbutions surveyed by Redner & Walker (1984), Titterington et al. (1985) \\nand McLachlan & Basford (1988). It is straightforward to write down \\nthe likelihood for the observed training patterns (xP, yP) and any un\\xad\\nclassified patterns zu as \\nII 1tyP L WjyP fi(xP; (}i) II L n jWijfi(zu; ei) \\np i u i,j \\nbut finding a maximum may be another matter. Indeed, it is possible \\nthat the maximum may occur with the components f;(x;8i) degen\\xad\\nerating around observed points. This can be avoided by suitably \\nconstraining the parameters ei. One favourite device for numerically \\nfinding a maximum is the EM algorithm (Section A.2 has the details). \\nThis pretends the example really did come from one of the compo\\xad\\nnents fi(x; (}i), say I, but this is unobserved. The posterior probabilities \\nof I = i given xP are then used to weight the various components \\nin the mixture; for example for a general Gaussian mixture we use \\nweighted mean and covariance estimators for each mixture component. \\nOf course, the posterior probabilities depend on the parameters , so the \\nprocess must be iterated. \\nThe convergence of the EM algorithm is notoriously slow (Redner \\n& Walker, 1984), and it may be better to use a conventional numerical \\noptimization technique. It is unclear whether this reputation is justified, \\nas it may be much easier to find a nearly-optimal solution (in the \\nsense of high log-likelihood) than to find the maximizing parameters \\nprecisely. (For pattern recognition purposes, only a good approximation \\nto the mixture density is needed.) There are a number of ways to \\nfind good starting points for the optimization , for example by ad hoc \\npartition of the space !!{ and fitting a component to the patterns \\nfalling in each partition. To emphasize the importance, note that if \\nthe EM algorithm is applied to each class of the synthetic data of \\nFigure 1.3 for a two-component mixture with equal covariance matrices \\n(the truth), it becomes trapped in a poor local minimum from starting \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 220}, page_content=\"This update formula is \\nnot quite correct, as fi \\ndepends on the new Iii· 6.4 Mixture representations 209 \\npoints which do not separate the group means sufficiently on the x axis. \\nIngrassia (1992) demonstrates that both the standard EM algorithm and \\nhis implementation of simulated annealing can find an inappropriate \\nlocal minimum in univariate multimodal normal mixture problems with \\nquite high probability. \\nA Bayesian approach will have a prior on the parameters (8;) \\nand on the mixing proportions wij. These are generally taken to \\nbe independent, and the proportions for each class given a Dirichlet \\ndistribution. The EM algorithm can be used to find a posterior mode \\nfor the parameters, but the predictive distributions p(j I x) can only \\nbe found by the iterative simulation methods discussed in Section A.3 \\n(Diebolt & Robert, 1994; Gelman et al., 1995). The simplest way \\nto apply the Gibbs sampler will be in its 'blocked' form, simulating \\nalternately all the components JP for the examples given the component \\nparameters, then the component parameters given (JP). \\nIn the spirit of neural networks, an 'on-line' approach has been \\nsought, in which the parameter estimates are adjusted whenever an \\nexample is presented (and the training set will be presented many times). \\nTraven (1991) considers an on-line approximation to the EM algorithm \\nfor general Gaussian mixtures (with separate covariance matrices). The \\ncurrent estimates of the means and variance are found as weighted \\nmeans and variances. Those weights depend on the current parameter \\nestimates, so the estimates cannot be updated exactly when a new \\nexample is presented unless the data are retained. When a new example \\nx is observed, Traven uses \\n/i; +---/i; + 1'f;(x-/i;) \\n~ ~ T \\n~i +---~i + q;(x-/i;)(x-ji;) \\nfor each component, where (q;) = p(i I x)/[p(i I x)+ L.f=l p(i I xP)]. This is \\nunavailable, and is approximated by p(i I x)/(N+1)p; on the assumption \\nthat the xP were a random sample from the class and N is large. Using \\na constant rather than N will allow 'forgetting' of examples. The only \\nadvantage of this procedure seems to be to avoid storing the data, and \\nif the dataset is very large it may well be sufficient to use a smaller \\nsample to estimate the parameters. \\nAn alternative way to estimate p(j I x) would be to regress the \\nclass indicator on the variables f;(x), which will differ from (6.15) and \\nwhich for radially symmetric component functions has been discussed \\nin Section 4.2. As there, we have avoided the question of choosing \\nthe number of component densities which can in general only be done \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 221}, page_content=\"210 6 Non-parametric Methods \\nusing knowledge of the complexity of the densities p;(x) or by cross\\xad\\nvalidation . \\nVector quantization can be seen as a special case of a finite mixture, \\nin which the components are constant densities over the tiles of the \\nDirichlet tessellation formed by the codebook. There have also been \\nmethods to design modified training sets for use with kernel methods, \\nand these can be seen as using finite normal mixtures (or normalized \\nGaussian radial basis functions) with equally weighted components. \\nFor example, Specht (1991) describes a simple clumping method to \\nselect cluster centres, and Burrascano (1991) uses LVQ to select a set \\nof centres for the normals. \\nHastie & Tibshirani (1996) explore normal mixtures with a common \\ncovariance matrix :E for all components in all classes. This is a rather \\nrestrictive assumption, but does allow all the components to be rescaled \\nsimultaneously to Np{J.l,l} as in linear discriminant analysis. Each class \\nis then represented by a distribution over component means rather than \\na single mean, but the between-groups covariance matrix can still be \\ndecomposed to find 'canonical variates' on which the data may be \\ndisplayed . This model is similar to using LVQ with the Mahalanobis \\ndistance for :E, as the latter can be considered to be using equally\\xad\\nweighted mixtures. The LVQ model has the advantage of choosing the \\ncodebook vectors for good discrimination, and mixture models for the \\nclasses suffer from modelling the class populations accurately in regions \\nwhere this is not needed. \\nChoosing the number of mixture components is notoriously difficult \\n(McLachlan & Basford, 1988; Peck et al., 1989; Furman & Lindsay, \\n1994). \\nExamples \\nFinding a good local maximum for maximum likelihood fitting of a \\nmixture of normals is difficult in practice, and we found a wide range of \\nfits from different starting points. The k-means algorithm of Section 9.3 \\nwas used to initialize the means, with the covariance matrices started \\nat the within-cluster covariance matrix. However, k-means is itself a \\nrandom algorithm subject to local minima, so the whole procedure was \\nrun several times and the best fit selected. The k-means procedure \\ndepends on the distance used in f!l; Euclidean distance was used after \\ncareful scaling of the features. \\nFigure 6.11 shows the plug-in classifiers for normal mixtures fitted \\nto the synthetic dataset (which was generated from a normal mixture). \\nIn this case unequal mixtures were used (to avoid biasing the fit too \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 222}, page_content='Figure 6.11: The \\ndecision boundaries for \\nthe plug-in Bayes \\nclassifier for mixture \\nmodels fitted to the \\nsynthetic dataset. The \\ndashed line corresponds \\nto fitting two normals \\nwith unequal \\ncovariances to each \\nclass; the dotted line to \\nfitting five components \\nto each class, whose \\nmeans are shown by +. 6.4 Mixture representations 211 \\n-1.0 -<l.S 0.0 0.5 1.0 \\nmuch to the correct model). Fitting five rather than two components per \\nclass decreased the deviance by about 20 but used 30 extra parameters . \\nThe use of AIC strongly suggested two components per class, but the \\npresence of many local minima makes this theoretically dubious (and \\nwe know that the global maximum has infinite log-likelihood). \\nFor the Pima Indians data we saw a suspicion of bimodal distri\\xad\\nbutions on page 99. Using two normal components per group with a \\ncommon covariance matrix (between as well as within groups) gave a \\ntest-set error of 64/332, a negligible improvement over linear discrim\\xad\\ninant analysis. Allowing different common covariance matrices within \\nclasses achieved 84/332, comparable with quadratic discriminant anal\\xad\\nysts. \\nFor the forensic glass data some of the classes are too small to fit \\neven a single normal density, so we need to use a common covariance \\nmatrix over all the classes and components. Some trials suggested that \\nthree components per class was a reasonable compromise , for which \\nthe cross-validated error rate was 30.8%, more than for LVQ. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 223}, page_content=''),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 224}, page_content='7 \\nTree-structured Classifiers \\nThe use of tree-based methods for classification is relatively unfamiliar \\nin both statistics and pattern recognition, yet they are widely used in \\nsome application s such as botany (Figure 7.1) and medical diagnosis as \\nbeing extremely easy to comprehend (and hence have confidence in). \\nThe automatic construction of decision trees dates from work in the \\nsocial sciences by Morgan & Sonquist (1963) and Morgan & Messenger \\n(1973). (Later work such as Doyle, 1973, and Doyle & Fenwick, 1975, \\ncommented on the pitfalls of such automated procedures .) In statistics \\nBreiman et al. (1984) had a seminal influence both in bringing the work \\nto the attention of statisticians and in proposing new algorithms for \\nconstructing trees. At around the same time decision tree induction was \\nbeginning to be used in the field of machine learning, which we review \\nin Section 7.4, and in engineering (for example, Sethi & Sarvarayudu , \\n1982). \\nThe terminology of trees is graphic, although conventionally trees \\nsuch as Figure 7.2 are shown growing down the page. The root is \\nthe top node, and examples are passed down the tree, with decisions \\nbeing made at each node until a terminal node or leaf is reached. Each \\nnon-terminal node contains a question on which a split is based. Each \\nleaf contains the label of a classification. A subtree of T is a tree with \\nroot a node of T; it is a rooted subtree if its root is the root of T. \\nA classification tree partitions the space f£ of possible observations \\ninto sub-regions corresponding to the leaves, since each example will be \\nclassified by the label of the leaf it reaches. Thus decision trees can be \\nseen as a hierarchical way to describe a partition of f£. We could give \\nthe botanist a description of each species and ask for the description \\nwhich matches the current specimen. Even in small domains this can \\nbe too difficult, and a decision tree provides a structured description of \\nthe knowledge base. Often the same information can be structured in '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 225}, page_content='214 7 Tree-structured Classifiers \\n1. Leaves subterete to slightly flattened, plant with bulb 2. \\nLeaves flat, plant with rhizome 4. \\n2. Perianth-tube > 10mm I. x hoUandica \\nPerianth-tube < 10mm 3. \\n3. Leaves evergreen I. xiphium \\nLeaves dying in winter I. latifolia \\n4. Outer tepals bearded I. germanica \\nOuter tepals not bearded 5. \\n5. Tepals predominately yellow 6. \\nTepals blue, purple, mauve or violet 8. \\n6. Leaves evergreen I. foetidissima \\nLeaves dying in winter 7. \\n7. Inner tepals white I. orientalis \\nTepals yellow all over I. pseudocorus \\n8. Leaves evergreen I. foetidissima \\nLeaves dying in winter 9. \\n9. Stems hollow, perianth-tube 4-7mm I. sibirica \\nStems solid, perianth-tube 7-20mm 10. \\n10. Upper part of ovary sterile 11. \\nOvary without sterile apical part 12. \\n11. Capsule beak 5-8mm, 1 rib I. enstata \\nCapsule beak 8-16mm, 2 ridges I. spuria \\n12. Outer tepals glabrous, many seeds I. versicolor \\nOuter tepals pubescent, CHew seeds I. x robusta \\nother ways. Many botanical trees amount to a set of rules describing \\none class, so each class is eliminated in turn. Another research area in \\nmachine learning has been to induce sets of rules from a training set, \\neither directly or via an induced tree (e.g. Michalski, 1980; Quinlan, \\n1987a, b, 1993). \\nThe idea of tree induction is to construct a decision tree from a \\nset of examples, which is how humans construct trees. It is usual to \\ndo so by growing the tree, that is by successively splitting leaves. Tree \\nconstruction is easiest when there is an exact partition of !!l, that is one \\nwhich classifies every example correctly. The alternative, in which the \\ndistributions of observations from the classes overlap, is often called a \\nnoisy classification problem. For the exact case, we need to continue \\nto grow the tree until every example is classified correctly. In a noisy \\nproblem to do so would over-fit the examples at hand, and the two \\npossible strategies are to stop growing the tree early, or to prune the \\ntree after constructing, closely analogous to forwards and backwards \\nselection in regression. Figure 7.1: Key to \\nBritish species of the \\ngenus Iris. Simplified \\nfrom Stace (1991) \\np. 1140, by omitting \\nparts of his \\ndescriptions. \\nConfusingly, these \\nstrategies are sometimes \\ncalled pre-and \\npost-pruning. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 226}, page_content='Table 7.1: Example \\ndecisions for the space \\nshuttle autolander \\nproblem, from Michie \\n(1989). \\nFigure 7.2: Decision \\ntree for shuttle \\nautolander problem. \\nThe numbers m/n \\ndenote the proportion \\nof training examples \\nreaching that node \\nwhich are misclassified. 7 Tree-structured Classifiers \\nstability error sign wind magnitude visibility decision \\nany any any any any no auto \\nxstab any any any any yes noauto \\nstab LX any any any yes noauto \\nstab XL any any any yes noauto \\nstab MM nn tail any yes no auto \\nany any any any Out of range yes noauto \\nstab ss any any Light yes auto \\nstab ss any any Medium yes auto \\nstab ss any any Strong yes auto \\nstab MM pp head Light yes auto \\nstab MM pp head Medium yes auto \\nstab MM PP tail Light yes auto \\nstab MM pp tail Medium yes auto \\nstab MM pp head Strong yes noauto \\nstab MM pp tail Strong yes auto \\n~ ~moo \"~ \\n0 \\n0/128 17/125 \\nstability:stab \\nstability:xstab \\n17/61 \\nerror:MM,SS \\n~ error:LX,XL \\nI nMuto I \\n0/32 \\nmagn:Light,Medium,Strong ~ ma~ \\n~ 0/8 \\nerror:MM ~ err[fu \\n~ 0/12 \\n~·· \\n0/3 ~ magn:Light,Medium ~ mag~ \\n0/4 ~ \\nwind:head \\n~ wi~d:~to I \\n0/1 0/1 I nMuto I \\n0/64 215 '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 227}, page_content=\"216 7 Tree-structured Classifiers \\nThe main differences between algorithms for tree construction are \\nthe pruning strategy used and the exact rule for splitting nodes. Many \\nalgorithms only allow binary splits, that is to divide a node into two; \\na few allow multi-way splits (for example by flower colour). Note that \\nthese are just algorithms; there are only very simple models and no \\ndeep theorems in this field. \\nThere are two types of optimality to be considered. One is opti\\xad\\nmality of the partition of fl£, which can be judged by the error rate \\nachieved. In principle we could seek an optimal partition amongst all \\nprescribed partitions of fl£, for example those representable by a set of \\ndecision rules splitting on a single feature. This is a computationally \\ninfeasible procedure for all but the smallest problems, but the step\\xad\\nwise construction of a partition by a decision tree can be seen as an \\napproximation to finding the optimal partition. \\nThe other sense of optimality is to represent a partition by a tree in \\nthe best possible way. The most obvious criterion is to use the minimal \\nexpected number of tests. Hyafil & Rivest (1976) showed this particular \\nproblem to be NP-complete; Payne & Meisel (1977) give an algorithm \\nto construct optimal trees with respect to fairly general cost functions. \\nThere are a number of partial surveys of the literature . Diet\\xad\\nterich (1990) covers 'recent developments in practical learning algo\\xad\\nrithms'. Safavian & Landgrebe (1991) is wide-.ranging but shallow. \\nQuinlan (1986, 1990, 1993) surveys the machine-learning approaches \\nwithin his own school. \\n7.1 Splitting rules \\nIn this section and the next we consider the component pieces of \\ncurrently favoured tree-construction algorithms. Some historical alter\\xad\\nnatives are mentioned in Section 7.4. Note that the number of possible \\ntrees is vast, so there is no question of an exhaustive search over trees. \\nConsider first splitting a leaf. There is a set of features from which \\nto construct splitting attributes. For binary features we will clearly \\nconsider the binary split on that feature. For categorical features with \\nL > 2 levels we can either consider an L-way split, or consider binary \\nsplits dividing the levels into two groups. (There will be 2L-l -1 non\\xad\\nempty pairs of groups, so this generates many attributes for large L.) \\nFor ordered features the natural splits are binary of the form x ~ Xc; \\nthis applies both to continuous measurements and to ordered categories. \\nSome systems also consider linear combinations of continuous features \\nand Boolean combinations of logical ones. (See Section 7.5.) \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 228}, page_content=\"7.1 Splitting rules 217 \\nEach leaf will have a set of attributes A on which it might be \\nsplit. How should we consider the value of the split? There have \\nbeen many suggestions from several different viewpoints. Consider \\nfirst a population viewpoint. That is, there is a known probability \\nrri is the set of classes. distribution over f!£ x '(} of examples which would reach that leaf. \\nThis gives a marginal probability distribution Pk over '(}. Consider \\nsplitting on attribute A which has levels a1, ... , am. There is then a \\nprobability distribution Pik over attributes and classes, and the child leaf \\nA · denotes summation corresponding to A= ai would have probability distribution p(k I ai) = \\nover that index. Pik/Pi· over classes k. \\nWe can then ask if the child nodes are on average 'purer' than \\ntheir parent. A measure of impurity should according to Breiman et al. \\n(1984, p. 24) be zero if Pj is concentrated on one class, and maximal \\nif Pj is uniform. Two commonly used measures of impurity are the \\nentropy \\ni(p) =-L Pj logpj \\nj \\n(where 0 log 0 = 0) and the Gini index \\ni(p) = LPiPj = 1-LPJ. \\nii=j j \\nOne interpretation of the Gini index is the expected error rate if the \\nlabel is chosen randomly from the class distribution at the node. (It \\nmay be better to use this than the error rate from the Bayes rule at the \\nnode since it gives an element of 'look ahead'. Quite often no feasible \\nsplit reduces the error rate, yet after two or three splits large reductions \\nin error rate emerge; see the right-hand branch of Figure 7.2.) \\nThe decrease in average impurity on splitting by attribute A is then \\nm \\ni(pc)-L Pi· X i(p(c I ai)). \\ni=l \\nA common approach is to choose the split that maximizes this. Since \\nthis will in general favour many-valued attributes, Breiman et al. and \\nmany others confine attention to binary attributes. (See Section 7.4 for \\nadjustments for multi-way splits.) \\nBreiman et al. preferred the Gini index. The entropy index has been \\nused widely, for example by Sethi & Sarvarayudu (1982) and Quinlan \\n(1983) in the engineering and machine learning literature respectively. \\nThe premise of the following proposition holds for both the entropy \\nand Gini measures of impurity. Part (ii) reduces the number of attributes \\nwhich need consideration for two classes from 2L-l-1 to L-1, but \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 229}, page_content=\"218 7 Tree-structured Classifiers \\nit has no simple extension to three or more classes. (The result is due \\nto Breiman et al., but the very much shorter proof is original.) \\nProposition 7.1 Suppose i(p) is strictly concave. \\n(i) The decrease in impurity is non-negative, and zero if and only if the \\nthe distributions are the same in all children. \\n(ii) Suppose there are two classes. For a categorical feature, order \\nthe levels in increasing p(11 x = x;). Then a split of the form \\n{ x1, ... xt}, { Xt+l• ... , xL} maximizes the reduction in average impu\\xad\\nrity. \\nProof: (i) We have by Jensen's inequality \\nL p;.i(p(c I a;)) ~ i(L p;.p(c I ai)) = i(pc) \\ni \\nwith equality if and only if p(c I a;)= p(c) for all i and c. \\n(ii) With just two classes we can regard i(p) as a function of p1 \\nonly; it remains strictly concave. Consider dividing into two groups by \\nallocating to group 1 with probability a; when x = x;. Then \\n(a) the average impurity of the two groups is minimized by taking \\na;= 0 or 1 by concavity, and \\n(b) the partial right derivative of the average impurity with respect to \\na; (which exists by concavity) at a; = 0 is of the form \\np(X = x;)[Ap(11 x = x;) + B] \\nfor constants A and B, and so is positive (when the optimal solution \\nis to allocate x; to group 1) for all i ~ t or all i > t for some t. \\nand both examples lead to the postulated form of split since which \\ngroup is labelled 1 is arbitrary. D \\nAnother way to look at this approach IS to define the average \\nimpurity of the tree as \\nI(T) = L qti(p(c It)) \\nleaves t \\nwhere qt is the probability an example reaches node t. The decrease in \\nI on splitting the node is then qt times the decrease in node impurity \\nwe considered before, so that strategy is equivalent to splitting the node \\nto minimize the average tree impurity. See the glossary. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 230}, page_content='This paragraph is \\ntechnical and not \\nneeded elsewhere. 7.1 Splitting rules 219 \\nOf course, to use this population approach we estimate all the \\nprobabilities by frequencies in the training-set examples reaching the \\nnode. Ciampi et al. (1987) and Clark & Pregibon (1992) take another \\napproach, viewing the tree as a probability model for the training set. \\nFor each node t there is a probability 1tte that an example reaches \\nthat node and is of class c, which can in principle be computed from \\nthe distribution over f£ x C(} by partitioning. Suppose we condition on \\nthe features for all the examples in the training set. We then know \\nthe number nt of examples which will reach leaf t, and the numbers \\nnte of each class at that node will have a multinomial distribution \\nwith probabilities nelt = ntclnt·· The conditional likelihood is then \\nproportional to \\nII II n,c nelt \\nleaves t classes e \\nand this allows us to write a deviance for the tree probability model as \\nD(T) = L Dt\\' Dt = -2 L nte log 1telt· \\nclasses e leaves t \\n(This is a deviance, since in the perfect model nelt = 1 whenever nte > 0 \\nat a leaf t.) If we estimate 1tte by the maximum likelihood estimate \\nnelt = ntclnt, the maximized deviance is \\nD(T) = 2[Lntlognt- Lntelognte]· \\nt t,e \\nThe splitting strategy is to choose the attribute which maximizes the \\ndeviance. \\nNow consider the average tree impurity for the entropy measure. \\nWhen the probabilities are estimated this is \\n\"\\'\"\\' nt . \"\\'\"\\' nt net net l(T) = ~ -l(nte!nt) =-~--log-n n nt nt t t,e \\n= -L net lognelt = D(T)/2n \\nn t,e \\nso the splitting strategies for deviances and for entropy-based impurity \\nare identical. \\nWe can take this duality of approaches further. Many impurity \\nmeasures can be written as a sum over examples: for example the Gini \\nindex is the sum of (1 -Pe)/n where c is the class of the example. \\nSuppose there are nc examples of class c. Then \\ne '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 231}, page_content=\"220 7 Tree-structured Classifiers \\nand this is minimized over (pc) by taking Pc: = nc/n. Thus the use \\nof the Gini index can be considered as a probability model with a \\ndifferent measure of goodness-of-fit. Chou (1991) considers a larger \\nclass of measures of the form \\nI(T) = L i(t) = L ~E [t(Y,p(t)) It] \\nleaves t t \\nwhere Y is the class of an example, t is a loss function and p(t) is \\nchosen to minimize the conditional expectation. The Gini index then \\narises from t(Y, p) = llind(Y)-pll2 = 1 + llpf-2py (and ind(Y) is the \\nK -tuple of indicators Y = k) and the entropy from t( Y, p) = -log py. \\nClearly l(T) is always reduced by a split (since there is a p(t) for each \\nchild to minimize over). Further, let \\nd(t,p(t)) = E [t(Y,p(t)) It]-i(t). \\nThen if we consider a binary split over values of a categorical split, \\nit is optimal only if for each category x assigned to the left child tL \\nd(x, p(tL)) ~ d(x, p(tR)), and conversely for the right child tR (Chou, \\n1991). This extends Proposition 7.1, at least for impurity indices of \\nChou's form. \\nPriors, weights and costs \\nThere are several assumptions made so far which we may wish to \\nrelax. Quite often the training set is not a random sample from \\nthe whole population , but chosen to disproportionally represent the \\nclasses, especially to over-represent rare classes. Suppose that the classes \\nare known to have probabilities (nk) in the population, but have nk \\nrepresentatives out of n in the training set. The population approach \\nworks with (Pk), the population distribution of classes within the node \\nt. Clearly n1k/n1 is no longer an appropriate estimate of Pk. and we \\nwould use the probability vector proportional to n1k/n1 x mrk!nk. \\nAnother extension is to allow weights to be attached to the examples. \\nThe most obvious reason is that we have an integer number wi of \\nexamples like this one, and wish to avoid the overhead of computing \\nwith many copies. This suggests interpreting nck and n1 as the sum of \\nweights, not merely counts of examples. Note that we can incorporate \\npriors for the classes via weights, by giving all examples in class k a \\nweight nnk!nk (or multiplying the current weight by this factor). \\nSuppose we wish to attach costs to different misclassifications, say \\nthe cost Cij of misclassifying examples of class i as class j. One \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 232}, page_content=\"There are l1.5028369 t J \\nrooted subtrees of a \\nbinary tree with t \\nleaves; Breiman et \\na/. (1984, p. 284). 7.2 Pruning rules 221 \\napproach is to say that the tree construction is merely modelling the \\nposterior probabilities p(k I x), and the costs should be used to choose \\nthe classification at each node, but not otherwise. However, differential \\ncosts suggest that we would like a more accurate model for some \\nclasses than for others. Breiman et al. (1984, §4.4) suggest that this can \\nsometimes be incorporated into the impurity index. For example, the \\ninterpretation given for the Gini index suggests the modified form \\ni(p) = L Cij PiPj\\xad\\nifj \\nUnfortunately, this effectively symmetrizes the costs (since the coefficient \\nof PiPj is Cij + Cji) and so is completely ineffective in two-class \\nproblems. It can also fail to be concave, and so give rise to splits with \\nnegative 'decreases' in impurity. \\nFor two classes there is a simple approach to misclassification costs. \\nEach example in class 2 costs a factor C2I!C12 more to misclassify \\nthan an example in class 1, which suggests weighting the examples in \\nclass i by Cij for j '/= i. This will also be appropriate for more classes \\nif the misclassification costs depends only on i and not on j '/= i. \\nHow about the deviance approach? We use the weights for each \\nexample to weight the log-likelihood ; the deviance becomes \\nD(T) = L De, \\nleaves t D1 = -2 L nee log 1tcle· \\nclasses c \\nwhere nee now represents the sum of the weights of examples reaching \\nleaf t of class c. (This is certainly appropriate if weights represent \\nmultiple examples.) We will once again estimate nclt by neclne, but \\nthis is an estimate of the biased posteriors, and will be adjusted to \\nbe proportional to n1c/nc x nnkfnk to estimate the posteriors in the \\npopulation. Note that the latter is what we get if we weight examples \\nin class k by nnkfnk. \\n7.2 Pruning rules \\nThe number of rooted subtrees of a binary tree is very large so we need \\na way to navigate this family efficiently. \\nCost-complexity pruning \\nThe best-known procedure for tree pruning is that proposed by Breiman \\net al. (1984). Let R(T) be a measure of a tree formed by adding the \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 233}, page_content='222 7 Tree-structured Classifiers \\ncontributions from the leaves. One obvious candidate is the number of \\nmisclassifications on the training set or a test set; another is the entropy \\nor deviance of the partition. Let the size of a tree be the number of \\nleaves. (For a binary tree the total number of nodes is twice the size \\nminus one.) Then Breiman et al. (1984) proposed choosing a rooted \\nsubtree T of the full tree To which minimizes \\nRa(T) = R(T) + e~:size(T). \\nWe can also consider Ra(T) as the sum of R(t) + tX over the leaves of \\nT. This can be seen as using a Lagrange multiplier for size, so finding \\nthe minimizing trees for all tX is equivalent to finding the trees with \\nminimum R(T) for each size. (Our results are equally valid for other \\nmeasures of size such as the total costs of the tests at the nodes.) \\nWhen using the apparent error rate on the training set we will want \\nto choose a positive e~: to penalize size, but our results also apply to \\ne~: = 0 which would be appropriate with the error rate on a test set. \\nCiampi et al. (1987) consider pruning with the Akaike Information \\nCriterion which corresponds to taking R(T) as the deviance and tX = \\n2(K -1). (The AIC penalizes minus the log-likelihood by the number \\nof parameters. Estimating the probability distribution within a leaf \\ntakes K -1 parameters. This count ignores parameters in the splitting \\nattribute and the selection of the attribute itself; it is unclear how these \\nshould be counted.) \\nBreiman et al. showed that there is a nested family of subtrees Tk \\nof To = T such that each is optimal for a range of e~:, and so there are \\nvalues \\n-00 = CI:Q < (X t < ... 00 \\nsuch that Ti is an optimal tree for e~: E [e~:i, tXi+t). Further, they gave \\nan algorithm to construct the tree sequence (Tk). Often tXt ~ 0, for \\nexample if R(T) is the measure used to grow the tree (such as deviance \\nor Gini) or the error rate on the training set (from Proposition 7.5). \\nHowever, tXt = 0 is quite common. \\nWe will now prove these results. There can be a number of trees \\nwith the same value of 14( T); we will consider only one which is a \\nsubtree of all to be optimal, and if this exists we call it T(e~:). Consider \\na non-trivial tree T, and for any non-terminal node t let Tr be the \\nsubtree rooted at that node. Let \\n( T) = R(t)-R(T1) \\ng t, size (T1)-size (t) If we have weights, we \\nwould use these in \\ncalculating R(T). To \\nhandle missing values \\nwe will need a modest \\nextension, in which \\nR(T) also contains \\ncontributions from all \\nnodes. Precisely, R(T) \\nis assumed to be a sum \\nover leaves plus a sum \\nover non-leaves, the \\nsummands being \\ndifferent in the two \\ncases. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 234}, page_content='This was proposed as a \\nnew algorithm by \\nGelfand & Delp (1991), \\nGelfand et al. (1991) \\nand Guo & Gelfand \\n(1992), the latter \\nincluding a more \\ncomplex proof. \\nHowever, the algorithm \\nis implicit in earlier \\nwork, and explicit, \\nwithout proof, in \\nQuinlan (1987a), under \\nthe name of reduced \\nerror pruning. It follows \\nimmediately from \\nTheorems 10.7 and \\n10.10 of Breiman et \\nal. (1984). 7.2 Pruning rules 223 \\nwhich compares the reduction in R by including the subtree with the \\nincrease in size. Note that g(t, T) > rx if and only if Ra(t) > Ra(Tt). \\nThe effect of pruning at node t is to replace T1 by t. \\nProposition 7.2 Suppose we number the nodes of a tree T so that each \\nnode precedes its parent. If we visit the nodes in this order (bottom-up) \\nand prune at node t if Ra(t) ~ Ra(T!) for the current tree T\\', the result \\nis T(rx). \\nProof: We will establish by induction that when node t is considered \\nall the branches at t are optimally pruned. This is clearly true for \\nthe leaves. At node t we either prune with value Ra(t) or not with \\nvalue Ra(T{) = Lbranches B Ra(T~) if this is strictly smaller. If there \\nis a subtree T\" rooted at t with a smaller value of Ra it must be \\nnon-trivial , and there must be a branch B with Ra(T~) < Ra(T~) \\nand so T~ is not optimally pruned, a contradiction. Now suppose \\nthere is another subtree with the same value of Ra. Then each of its \\nbranches (it must have some) will have the same value of Ra as the \\ncorresponding branch of r: and so include that branch. Thus after \\nnode t is considered , the current r: is optimally pruned. When the \\nroot is reached the current tree is optimally pruned, so is T(rx). D \\nThis gives an algorithm to find T(rx) for a single rx. We now show \\nhow to find (rxk) and the tree sequence Tk. From now on we assume \\nthat size is increasing, that is adding nodes increases (we.akly) the size. \\nProposition 7.3 Let rx1 be the smallest value of g(t, T) for any non\\xad\\nterminal node t of T. The optimally pruned tree is T for rx < cq, and \\nT1 = T(rxt) is obtained by pruning at all nodes t with g(t, T) = rx1. \\nFurther, g(t, Tt) > rx1 for all non-terminal nodes of Tt. \\nProof: The optimality of T for rx < rx1 is immediate from Ra(t) > \\nRa(Tt) and Proposition 7.2. Consider rx = rx1, and pruning by Propo\\xad\\nsition 7.2. Whenever the tree is pruned, Ra(Ts) is unchanged for all \\nnodes s of the new tree. Thus Ra(t) ~ Ra(T!) for the current tree T\\' \\nif and only if Ra(t) ~ Ra(T1) if and only if g(t, T) ~ rx1. Then for a \\nretained node t, \\nRa1(t)-Ra1(Ttt) = Ra1(t)-Ra1(Tc) + [Ra1(Tr)-Ra1((Tt)t)] \\n= Ra1 (t)-Ra1 (Tr) = g(t, T)[ size (Tt)-size (t)] \\n> rx1 [size(Tt)- size(t)] ~ rx1 [size((Tt)c)- size(t)] \\nso g(t, Tt) > rx1. D '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 235}, page_content=\"224 7 Tree-structured Classifiers \\nProposition 7.4 For f3 > a T(/3) is a subtree of T(a) and is the result \\nof {3-pruning of T(a). \\nProof: We will show by induction that Tt(f3) is a subtree of Tt(a) \\nand conclude that T(/3) is a subtree of T(a). This is true at the leaves. \\nAt node t we compare Ra(t) to Ra(Tt(a)) and Rp(t) to Rp(Tc(f3)) and \\nin each example prune if the first is (weakly) smaller. We must show \\nthat if Ra(t) ~ Ra(Tc(a)) then Rp(t) ~ Rp(Tt(f3)). Now since Tc(/3) is a \\ncandidate for a-pruning of the tree rooted at t, we have \\nRp(t) = Ra(t) + ({3-a)size(t) ~ Ra(Tt(a)) + ({3-a)size(t) \\n~ Ra(Tt(f3)) + ({3 -a)size(t) \\n= Rp(Tc(f3))- ({3-a)[size (Tc(/3))- size (t)] \\n~ Rp(Tc(f3)). \\nSince T(/3) minimizes Rp(T') over all rooted subtrees T' of T and \\nis a subtree of T(a), it also minimizes Rp(T') over rooted subtrees of \\nT(a). 0 \\nThe algorithm of Proposition 7.3 can be applied to the new tree \\nT1 = T(al) to find a2 > a1 (since g(t, Tl) > a1 for all non-terminal \\nnodes of T1) and T2 = T(rx2) and so on until Tk is the trivial tree, \\nthe root of To = T. From Propositions 7.3 and 7.4, T(rx) = T1 for \\nrx1 ~ rx < rx2 and T(rx2) = T2. Repeating the process gives T(rx) for all \\nrx ~ rxk. and Proposition 7.4 shows that the trivial tree is optimal for \\nrx ~ rxk. This completes the algorithm to find the tree sequence: \\n1 Set k = 0 and write out To= T. \\n2 Set a= oo. \\n3 Visit the non-terminal nodes t in bottom-up order and calculate \\nR(Tt) and size(Tc) by summing over the descendants (and including \\nany contribution at t ). Set \\n( ) = R(t)-R(Tt) \\ng t size (Tt)-size (t) \\nand rx =min( a, g(t)). \\n4 Visit the nodes in top-down order and prune whenever g(t) =a. \\n5 Set k = k + 1 and write out ak = rx and Tk = T. \\n6 If T is non-trivial go to 2. \\nWe could visit the nodes in any order at step 4, but a top-down order \\navoids considering nodes which themselves will be pruned away. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 236}, page_content=\"In fact Breiman et al. \\naveraged only at the \\nvalues Cjct.krxk+ 1 ) for \\nthe sequence rxk for the \\noriginal tree, but this \\nsaves but little effort. 7.2 Pruning rules 225 \\nIt remains to choose the particular tree within this sequence. If we \\nhave a validation set we can use its error rate with rx = 0. Otherwise \\nBreiman et al. propose selecting the value of rx using cross-validation. \\nThe training set is split into V parts; Breiman et al. (1984, pp. 11-\\n12) seem to prefer 3 but later users (e.g. Clark & Pregibon, 1992) \\nrecommend 10. For each of the parts, a tree sequence is constructed \\nfrom the remaining V -1 parts of the training set, and its measures \\nR(Tk) calculated, to give a piecewise constant function for R(T(rx)). \\nThis is averaged over all V parts, and rx chosen to minimize the \\nfunction. \\nBecause the training set is disjoint from the test set in each of the \\nV cross-validation experiments, we can expect to form a reasonably \\nunbiased estimate of R(T(rx)). If V is small we have used a considerably \\nsmaller training set, and so might expect to overestimate the error rates, \\nbut this does not necessarily mean that the relative values of R(T(rx)) for \\ndifferent rx are seriously biased. In practice the estimates of R(T(rx)) \\nare highly variable over the choice of parts of the training set, and \\nthe estimated function may have no minimum within the range of rx \\nconsidered, or a very broad minimum. Breiman et al. suggest choosing \\nthe largest value of rx with the cross-validated R(T(rx)) just above the \\nminimum (the 'one SE' rule). The standard error can be estimated \\nfrom a binomial distribution for error-count pruning, or a chi-squared \\ndistribution for deviance pruning. \\nThere is a difficulty with cross-validating deviance measures R(T) \\nnot found with error rates nor the Gini measure. Suppose that at \\nsome leaf t a class c occurs in the test set but not in the training set. \\nThen the fitted probability lite = 0 and so the deviance at that leaf \\nis infinite. (The other measures give a unit penalty.) This might be \\nthought appropriate, and will certainly lead to that leaf being pruned, \\nbut makes it difficult to average R(T(rx)). There are several ad hoc \\nsolutions, all of which involve altering the fitted probabilities. One we \\nhave used successfully is to give a prior of one example per class at \\neach node so lite = (ntc + 1)/(n1 + K) for K classes, which is never \\nzero, but can approach zero if a class does not occur in a large number \\nof examples. \\nThis is one of a family of shrinking approaches. Bahl et al. (1989), \\nChou (1991) and Buntine (1992) each smooth at all splits, not just the \\nleaves, taking the fitted probabilities to be a convex combination of \\nthose of the parent and the frequencies in the child node. (Clark & \\nPregibon, 1992, also propose this.) It remains to choose the convex \\ncombination, and indeed to decide if it should be the same at each \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 237}, page_content=\"226 7 Tree-structured Classifiers \\nnode. Chou uses leave-one-out cross-validation at each node; Bun tine \\nuses a combination which depends on the sample size (see Section 7.7). \\nClark & Pregibon use a constant factor over the tree, chosen by cross\\xad\\nvalidation, and see this as an alternative to pruning. \\nAnother approach to pruning \\nGelfand et al. (1991) and Gelfand & Delp (1991) point out that the \\noptimally pruned tree with respect to the true misclassification rate \\nR(T), were this available, need not be within the family T(cx) pruned \\nwith respect to the apparent error rate. We have already seen how to \\nprune with respect to an honest measure of error rate. Gelfand et al. \\npropose a pruning algorithm based on dividing the training set into \\ntwo and alternating the role of the halves. Initially the tree is grown \\n(and nodes labelled) using one half and pruned using the error rate on \\nthe other half. The tree is then re-grown from the pruned tree using \\nthe previous test half and pruned using the previous training half to \\nestimate the error rate. This is repeated until the tree size is unchanged. \\nThe pruned subtrees are nested and increasing, and if a node is terminal \\nat two successive steps growth from that node can be stopped. \\nLong formal proofs are given in Gelfand et al. (1991) for the Gini \\nmeasure of impurity. We can give a short and general argument. It is \\nimportant here that ties are broken consistently when labelling leaves; \\nwe need to choose the class of the parent node if this is a contender. \\nProposition 7.5 Suppose a training set is partitioned, and the whole set \\nand each cell of the partition are labelled by a class with the highest \\nfrequency within it. Then the apparent error rate is decreased by parti\\xad\\ntioning, strictly so unless the whole partition is given the same class. \\nProof: Let the frequency of class c within cell i be nci· Then the \\nsuccess count before division is maxc nc., maximized by k, say, and \\nafter division is 2:::::; maxc nci ~ maxc 2:::::; nci = nk· with equality only if \\nk maximizes nci for each class. If the error rate is the same, the class \\nof the parent is a contender for the class of each cell. D \\nSuppose the two halves of the training set are .'T 1 and .'T 2, and let \\nR(il(T) denote the number of errors for test set .'T;, using the labels \\nassigned when the tree was grown. Let T* denote the tree grown using \\n.'T 1 and optimally pruned using R(2l. \\nProposition 7.6 The tree T* is unchanged under optimal pruning using \\nR(1l. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 238}, page_content=\"7.2 Pruning rules 227 \\nProof: From Proposition 7.2 it suffices to show that R(ll(Tt) < R(1l(t) \\nfor each interior node. Now Tt corresponds to a partition of the \\nexamples of .r-1 reaching node t, so by Proposition 7.5 R(ll(Tt) ~ \\nR(ll(t) with equality only if Tt gives the same partition as t and hence \\nR(2l(Tt) = R(2l(t) which would contradict the optimality of the pruning \\nofT*. 0 \\nNow suppose a tree S is grown starting from T* using §' 2 and \\noptimally pruned to s· using R(1l. Since S contains T*, Proposition 7.6 \\nshows that s· contains T* (since pruning is monotone on trees). If a \\nleaf in T* remains a leaf in s· the growing and pruning process to \\nform T* will be repeated at the next step, and so that node will always \\nremain a leaf. As there are only a finite number of examples and empty \\nleaves will never be generated, the process must stop. \\nGelfand et al. (1991) propose reporting an error rate based on \\nclassifying at each leaf the examples from the half of the training set \\nother than the one on which that leaf was labelled (when it was grown). \\nThey recommend this procedure only for large datasets (since it works \\nwith half the data at a time), and we have found it unsatisfactory for \\nmoderately sized datasets, in which T* is often just the root subtree. \\n'Pessimistic' and 'error-based' pruning \\nQuinlan (1987a, 1993) introduced two much cruder ideas for pruning. \\nIn the first approach, he proposes a continuity correction for cost\\xad\\ncomplexity pruning, so that the number of errors on the training set at \\neach node is increased by one half. The idea was to better estimate the \\ntrue rather than apparent error rate. (This idea is exactly equivalent \\nto taking rx = 0.5, since R(t) is increased by one half.) He compares \\nthe error rate of the tree Tt with the error rate at node t (after a \\n'continuity correction' adding one half to each error count). Rather \\nthan prune only if the adjusted error rate for node t is smaller, he \\nproposes to prune unless it is somewhat larger, specifically when \\nerror rate for t < error rate for Tt + std. dev.( error rate for Tt ). \\n(Quinlan is vague about how to calculate the last term; his example \\nappears to use a binomial formula with a common probability, but it \\nwould be better to calculate the standard deviation within each leaf.) \\nThis looks like an approximation to a significance test, except that the \\nvariability of the left-hand side is not taken into account. Again the \\ndetails are vague, but Quinlan states that all subtrees are considered as \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 239}, page_content=\"228 7 Tree-structured Classifiers \\ncandidates for pruning (unlike Proposition 7.2). As there are very many \\nsuch subtrees, this seems unlikely. \\nA much larger adjustment of the apparent error rate is proposed \\nin his 1993 book. Suppose a leaf t covers N examples, J of which \\nare misclassified. Then R(t) is taken to be the 87.5% point of a \\nbinomial (N, J / N) distribution . This could be calculated exactly (and \\nis in the C4.5 program) but given the approximate nature of the justi\\xad\\nfication, using a normal approximation to the binomial to give \\nR(t) = J + 1.15 X J J(1-J IN) \\nseems perfectly adequate . Our understanding is that the algorithm of \\nProposition 7.2 is used. \\nExamples \\nThe data on diabetes amongst Pima Indians have provided a difficult \\nexample for many methods, and is typical of the difficulty of using tree\\xad\\nbased methods. It is easy to grow a tree (using the deviance/entropy \\napproach) with many nodes: our initial tree has 22 nodes, shown in \\nFigure 7.3. (One of the splits has another attribute with exactly the same \\nsplit.) Four nodes can be pruned without changing the classifications at \\nall; the error rate on the test set is 81/332. This is just about significantly \\nworse than the logistic regression with 66/332, as the McNemar statistic \\nis 1.96. \\nFigure 7.4 shows the difficulty in choosing the size by 10-fold cross\\xad\\nvalidation of error-rate pruning. There is little variation with the size \\nof tree down to size 3, which suggests the latter should be adopted. \\nThis gives the rule that diabetes should be predicted if the plasma \\nglucose level exceeds 123.5 and the diabetes pedigree function exceeds \\n0.31. This rule has a test-set error rate of 90/332, worse than the \\nunpruned tree. However , the difference is not statistically significant, \\nfor McNemar's test statistic is [129-201 -1]/ ,J29 + 20 ~ 1.14. In this \\ncircumstance we should not use risk averaging, as most examples are \\npredicted with maxp(c I x) = 1, and so the true test-set error rate is \\ndramatically underestimated (11% instead of 24.4%). \\nQuinlan's 'pessimistic' pruning removes just 3 nodes, and AIC re\\xad\\nmoves just one. Indeed, if we grow an even larger tree by allowing \\nsmaller populations within the leaves, both Quinlan and AIC select a \\ntree with about 30 nodes. Such large trees have a test-set error rate of \\nabout 99/332. \\nUsing the Gini index rather than entropy to grow the tree produced a \\nsimilar but not identical tree, with splits occurring in a slightly different \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 240}, page_content='Figure 7.3: The \\nclassification tree grown \\nfor the Pima Indians \\ndiabetes data based on \\na training set of size \\n200. Growth was \\nstopped at leaves with \\n10 or fewer examples. \\nFigure 7.4: Number of \\nerrors vs size for \\nerror-rate pruning of \\nthe tree of Figure 7.3. 7.2 Pruning rules 229 \\n~c,-~ / .81200 \\nglu<123.5 \\n/ glu>123.5 \\n/ ~ s;o\\\\ ~Yes\\\\ I .5/YO I 38791 \\n/age<2B.5 \\nage>28.5 ~~pad-<O.S095 \\nped>0.3095. \\n-- \\\\__ ;~· No\\\\ No\\\\ No~ Ye\\\\ ;-4tl4 ?73 2.!3 15756 \\nnpreg<.2.5 glu<90 glu<166 bmi<28.65 1 npre~5 ~ glu~: 1 g~ ! bml>2\\\\: \\nb1T5B~ npl(:~ ~9 \\nbmi<l~\\\\ pedl:\\\\ ~8 \\nageE~~ ped<E:5\\n\\\\ \\n,11 ~L I \"1-;t ~,~ ~\\'~~; ~~~-rr; ~I ~J~pedi \\n0/53 1/5 316 0/10 /2/10 /7/16 /6/19 0/8 0/6 215 /7/28 0/17 \\n771 bp>71 bmir·::,,35:_85 bmr::,,_r_· r I_ \\n@ ck ~ Jo~ @ ~N~ ~ rr;e\\' ~5 015 0/6 Ts1ro 016 I fiils ~\" 17,,-\\nJ4.5 ped<0.1 8 age<40 \\nnpre >4 5 I ped>0.18 J age>40 \\n~l L~ ~ C,Ni~ ~ 01s 21s 11s 3/B /4110 1n \\nped<0.393 \\nckpe~3 \\n2/5 1/5 \\nalpha \\n-lnf 0.00 0.67 0.75 1.00 1.50 5.00 15.00 \\n,----\\n0 ,._ \\nIll rD \\n\"\\' \"\\' \"\\' ~ 0 ·e rD \\nIll Ill \\nI \\n0 Ill \\n20 15 10 5 \\nsize '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 241}, page_content=\"230 7 Tree-structured Classifiers \\norder. The Gelfand et al. (1991) procedure depends on the (random) \\ndivision into two sets, but normally produced a tree with around 15 \\nnodes, and a test-set error rate of around 85/332. \\nFor the forensic glass data, growing an initial tree using the en\\xad\\ntropy/deviance measure and using cost-complexity pruning on the \\ncross-validated error rate gives the plot shown in Figure 7.5. This \\nsuggests choosing a tree of size 12, or 9 using the '1 SE' rule as these \\ncounts are approximately Poisson and so have a standard error of \\nabout 8. \\nalpha \\n-lnf 0.0 0.5 1.0 2.5 4.7 8.0 27.0 \\n20 15 10 5 \\nsize \\nIt is tedious to cross-validate this choice of tree size within a cross\\xad\\nvalidatory assessment of performance, and we introduce a slight bias \\nby not doing so here. (The cross-validation partition chosen for cost\\xad\\ncomplexity pruning was not the same as that used for assessment.) The \\nperformance of the trees pruned to size 9 and 12 were almost identical; \\nsize 9 gave the estimated confusion matrix \\nWinF WinNF Veh Con Tabl Head \\nWinF 55 13 2 0 0 0 \\nWinNF 14 50 6 3 2 1 \\nVeh 5 8 4 0 0 0 \\nCon 0 3 0 9 0 1 \\nTabl 0 1 0 1 5 2 \\nHead 2 2 0 2 1 22 \\nand an error rate of 32.2%. \\nThe whole Gelfand et al. (1991) procedure was cross-validated. On \\nthe whole dataset it grew a tree with 6 nodes that did not classify \\nas container or tableware at all, just using the refractive index and \\nmagnesium and calcium oxides. Under cross-validation the tree size Figure 7.5: \\nCross-validated error \\ncount us tree size for \\nthe forensic glass data. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 242}, page_content='Figure 7.6: Pruned \\nclassification tree for \\nthe forensic glass data. 7.3 Missing values \\nMg<2.695 \\nMg>2.695 \\nAl<1.42 \\nAl>1.42 \\n~~~~;I \\nRI<·O \\ncb \";~; \\nK<Or:\\\\ I K>0.29 \\nc;J~ \\nMg<f~\\\\ I Mg>3.75 \\nc;J Jw~~;J 231 \\nvaried widely between subsets; the assessment of the error rate was \\n42%. \\nThe Quinlan pruning procedure tended to prune lightly; its cross\\xad\\nvalidated error rate was 31%. \\n7.3 Missing values \\nOne attraction of tree-based methods is the ease with which missing \\nvalues can be handled. Consider the botanical key of Figure 7.1. We \\nonly need to know about a small subset of the 10 observations to \\nclassify any example, and part of the art of constructing such trees \\nis to avoid observations which will be difficult or missing in some of \\nthe species (or, as in the case of capsules, for some of the examples). \\nHowever, missing values may be unavoidable , and there are several \\napproaches to handling them. \\n1 A general strategy is to \\'drop\\' an example down the tree as far as \\nit will go. If it reaches a leaf we can predict y for it. Otherwise we \\nuse the distribution at the node reached to predict y, as shown in '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 243}, page_content=\"232 7 Tree-structured Classifiers \\nFigure 7.2, which has predictions at all nodes. However, examples \\nwhich are stopped high up the tree have little of the available \\ninformation used. \\nThe pruning algorithms will have to be interpreted carefully, since \\nR(T1) must include examples which reach node t but do not reach \\nthe leaves. This can easily be achieved by summing over both \\nchildren and the examples which are not passed on. \\nThis strategy can also be used in tree growth. The deviance approach \\nwill automatically weight the value of a split by the proportion of \\nnon-missing values in assessing which attribute to use. \\n2 An alternative strategy is used by many botanical keys and can be \\nseen at nodes 9 and 12 of Figure 7.1. A list of characteristics is \\ngiven, the most important first, and a decision made from those \\nobservations which are available. This is formalized in the method \\nof surrogate splits in which surrogate rules are available at non\\xad\\nterminal nodes to be used if the splitting attribute is unobserved. \\nBreiman et al. (1984, §5.3) choose the attribute which maximizes the \\nprobability of making the same decision at the node as the primary \\nsplit. \\n3 It is possible to split examples with missing values between the \\nbranches. In principle this should be done using the conditional \\nprobabilities of left and right splits given all the observed infor\\xad\\nmation. In general that probability is unavailable. What we can \\nestimate easily is the probability of going left or right given the \\nattributes used in earlier splits, from the frequencies of complete \\nexamples at the node. \\nIn this approach each example is split into a probability distribution \\nover leaves; each time a missing value is encountered the current \\nfractional example is subdivided. (There is a potential problem \\nhere, especially if it is the same feature under consideration as a \\nhigher split, since different conditional distributions will be used \\neach time the example is split. If an example has already been \\nsplit, the imputed values at earlier splits also have to go into the \\nconditioning.) \\nThe obvious way to produce a classification for a split example is \\nto combine the posterior probabilities in the leaves reached by its \\nfractions using the probabilities assigned to leaves, and then assign \\nthe class with the highest overall posterior probability. However, \\nQuinlan's (1993) C4.5 system takes the simpler but less rational \\napproach of weighting the leaf classifications, and choosing the \\nclass with the overall highest probability. Quinlan (1986) \\nattributes to Alen \\nShapiro the idea of \\nbuilding a tree to \\nestimate the conditional \\ndistribution of the \\nmissing value. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 244}, page_content=\"7.3 Missing values 233 \\n4 Another possibility is to take 'missing' as a further level of the \\nattribute (e.g. Kass, 1980). For methods which allow multi-way \\nsplits this has the disadvantage of increasing the number of levels, \\nso that making some values missing can increase the gain in impurity \\n(Quinlan, 1986). This can be circumvented by allowing only binary \\nsplits, or by penalizing multi-way splits. \\nIn most approaches tree construction is based on the examples \\nwithout any missing observations. Where missing values are very \\nfrequent this may be unacceptable or even impossible. Quinlan (1986) \\nsuggests replacing missing values by the distribution within the class \\nat that node when computing the expected value of a split. On the \\nother hand, Quinlan (1993) multiplies the impurity gain calculated on \\nknown examples by the proportion of missing values (as implied by \\nthe deviance approach) and his C4.5 system uses fractional examples \\nthroughout tree construction . However, note that in the deviance \\napproach if example splitting is used the partitioning is no longer \\nrecursive (as the fitted probability for such examples depends on each \\nbranch). \\nAll of these ideas have merits and demerits, depending on how \\ncommon missing values are and whether they are missing at random. \\nFor example, in medical diagnosis the absence of a test might well \\ncarry information, and examples with the value of an attribute missing \\ncould be very different from those with a recorded value. On the other \\nhand, if missing values are rare, there will not be enough information \\nto usefully treat 'missing' as a separate attribute value. \\nSometimes features are not missing but also not known exactly; \\nfor example a continuous feature may only be known to lie within an \\ninterval, or a test may indicate a 80% chance of being positive. Such \\ninformation is best handled by splitting examples. \\nExample \\nThe Pima Indians diabetes data has many missing values, so we tried \\nout the value of splitting examples, with a training set that had 200 \\ncomplete examples and 100 partially missing examples. Growing and \\npruning a tree on this augmented training set led to a slightly larger \\ntree shown in Figure 7.7, which makes 74/332 errors on the test set. \\nThis highlights the role of the plasma glucose level and the body \\nmass index. Figure 7.8 shows the training-set examples on those two \\nfeatures, which indicates that the separation is rather weak. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 245}, page_content='234 \\n>< Q) \\n\"0 .5 \\nIll Ill al E \\n>-\"0 0 .0 0 I!) \\n0 \"It \\n0 (\\') \\n0 \\nC\\\\1 7 Tree-structured Classifiers \\n~ glu<127.5 ~ glu>127.5 \\n~ Yes \\n321177 49/123 \\nbmk28.75 \\n@ \\n7/28 \\n12132 \\n+ glu>166 \\n~ \\n2112 \\n+ \\n+-++ +it-\\n+ ---\\n---+ + \\n- -*+ \\n; t + + \\n* + \\n+ + + + \\n--+ ---+ + + + + -+! +-_--+ + --++ +:-+ \\n+ +\" \\n-+ - +:~ -:.. :_ ;+--+-+ + + + \\n+ ... t. + + + ++ : T ~ \\ni-+ - + ++ \\n+ \"\\'\"+ -\\n:.+ - ;r : --\\n+-\\n+ + \\n60 80 100 120 140 160 \\nplasma glucose + \\n- + + + \\n+ \\n• + \\n+ \\n180 200 Figure 7.7: The \\nclassification tree grown \\nand pruned for the \\nPima Indians diabetes \\ndata based on a \\ntraining set of size 300. \\nFigure 7.8: The \\npresence or absence of \\ndiabetes against plasma \\nglucose and body mass \\nindex for the training \\nset of the Pima Indians \\ndata. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 246}, page_content=\"7.4 Earlier approache s 235 \\n7.4 Earlier approaches \\nQuinlan (1986) provides an historical overview of developments in the \\nfield of machine learning. He considers the TDIDT family of algorithms \\n(Top Down Induction of Decision Trees) to stem from Hunt's Concept \\nLearning System (Hunt et al., 1966), via his own ID3 (Quinlan, 1979) \\nand the ACLS system of Patterson & Niblett (1983). Another branch \\nis the ASSISTANT systems of Kononenko et al. (1984) and Cestnik et \\nal. (1987). Quinlan calls his own descendant of ID3 C4.5. Many of the \\nlater systems are commercial and so not documented in the scientific \\nliterature. There was a family of CLS systems; the last, CLS-9, chose \\nthe split which maximized the number of examples correctly classified \\nover the new leaves. \\nAll of these systems (except CLS) are based on the entropy measure \\nof impurity. ID3 examines all candidate attributes and chooses that with \\nthe largest 'information gain', which is what we called the reduction in \\naverage impurity at the node. All the probabilities are estimated from \\nfrequencies in the training set. Most of these systems allowed only two \\nclasses. \\nWhat attributes are allowed? In ID3 only categorical features were \\nconsidered and the split is into all levels of the feature. Both ACLS and \\nASSISTANT use a binary division of the feature. Continuous features \\ncould in principle be divided into ranges or split as in Figure 7.1. \\nIf multi-way splits are allowed, they would be expected to have \\ngreater information gain; indeed they may have a large information \\ngain even if the attribute has no predictive power. Quinlan (1986, 1988) \\nsuggests guarding against this by comparing the information gain to \\nwhat he terms the 'information value' IV of the attribute A, that is the \\nentropy of the distribution of attribute values at the node. He suggests \\nchoosing the attribute which maximizes \\ngain (A)/IV \\nover attributes 'with average-or-better gain amongst all tests examined' \\n(itself a size-biased selection criterion). \\nOne feature of the original ID3 was that it works with a 'window', \\nthat is a subset of the training data. This is initially chosen as a random \\nsubset and the tree grown on it. The rest of the training set is tested, and \\na selection of incorrectly classified examples is added to the window. \\nThe tree is extended and the process repeated. In a logical domain this \\ncan reduce the computation, but has not been used in the descendants \\nof ID3 (except C4.5). \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 247}, page_content=\"236 7 Tree-structured Classifiers \\nThe main differences between these algorithms come in their stop\\xad\\nping rules. The original ID3 had no stopping rule. Quinlan (1983) \\nproposed a chi-square test for the value of the split on the chosen A, \\nthat is a test of independence in the' A cross class' table. Niblett (1987) \\nsuggested Fisher's exact test for the same purpose. The ASSISTANT \\nsystem compared a cross-validation estimate of the error after splitting \\nat the node with the apparent error at the node, and stopped if the \\nformer was worse. Its successor, ASSISTANT86, computed the node \\nsize times the information gain divided by the entropy and stopped if \\nthis was smaller than a preset threshold (for example 4%). \\nSo far we have assumed that no attributes have missing values. \\nASSISTANT either used the proportions of the attribute amongst \\nexamples of the same class at that node to fill in the most frequent \\nvalue (as used by CN2, Clark & Niblett, 1989) or used fractional \\nexamples to express the distribution over values. Many of the general \\nschemes discussed above have been used. \\nSuppose there are K classes. The Laplacian error estimate replaces \\nthe error rate ed ni at a leaf by [ei + K -1] I [ni + K], in a crude attempt \\nto compensate for the optimistic bias of the re-substitution estimator \\nof error. Niblett & Bratko (1986) pruned the tree at node t if the naive \\nLaplacian error estimate at that node is less than that for the subtree \\nrooted at t. (This is described in detail in Niblett, 1987, and also used \\nby Casey & Nagy, 1984.) \\nBratko & Kononenko (1987) give a number of comparisons for \\ndomains of medical diagnosis. Their results show that binary trees \\nare generally smaller and have slightly lower error rates than multi\\xad\\nway trees, and that stopping early slightly improves the error rate but \\nmarkedly improves comprehension. \\nOne early strand of work in statistics was given by Kendall & Stuart \\n(1966, §44.30-32) and Richards (1972). They consider continuous\\xad\\nvalued attributes and two classes. Suppose for a feature X that one \\nclass, say class 1, has generally smaller values than the other. The split \\nis then ( -oo, min2 Xi) to class 1, (max1 Xi, oo) to class 2 and the overlap \\n[min2 Xi, max1 Xd is passed to the next level. The attribute is chosen \\nfor which this rule decides the most examples, and the process repeated \\nat the next level. \\nThe system THAID of Morgan & Messenger (1973) was one of \\nthe first statistical applications of decision trees. Its splitting criterion \\nwas the error rate. A descendant, CHAID (Kass, 1980) chooses the \\nsplit with the highest significance in the A cross class table. However, \\nit found the (approximately) most significant table including amalga-\"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 248}, page_content='See Section 9.3. 7.5 Refinements 237 \\nmating categories of multi-level attributes. The stopping rule was again \\nbased on significance, with some allowance for selection. Mingers (1987) \\nalso based the choice of split on the highest statistical significance of \\nthe contingency table of A cross classes. \\nCiampi et al. (1987) merge leaves in a post-processing step if their \\npopulations are sufficiently similar. An agglomerative clustering algo\\xad\\nrithm is applied to the leaves with a dissimilarity measure computed \\nfrom a log-likelihood-ratio test of the difference in within-leaf class \\ndistributions. We do not see this as preferable to pruning methods. \\nThe work in the engineering literature is diverse and wide-ranging; \\nwe will only highlight a few ideas. (Safavian & Landgrebe, 1991, \\ncatalogue many more.) Henrichon & Fu (1969) set up a tree with \\na linear combination at each node whose range was partitioned into \\npositive, negative and undecided; the undecided examples are passed \\nto the next layer. The partitioning criterion was to approximately \\nminimize the error rate. Swain & Hauska (1977) suggested minimizing \\nthe sum of measurement and error costs using a 1-step lookahead. \\nAgain for a two-class problem, Friedman (1977) and Rounds (1980) \\nused the Kolmogorov-Smirnov distance between the distributions of \\nthe two classes to choose the feature, and split at a maximum of the \\ndistance. Multi-class problems can be considered by building a tree \\ncontrasting each class with the first, and combining information in the \\nleaves of the K -1 trees to decide between the K classes. Sethi & \\nSarvarayudu (1982) took an information-based approach identical to \\nthat which was emerging in machine learning. \\nEngineers have continued to be active in this field, for example \\nArgentiero et al. (1982), Casey & Nagy (1984), Dattatreya & Sarma \\n(1981, 1985), Goodman & Smyth (1988), Kurzynski (1983a, b), Li & \\nDubes (1986), Schuermann & Doster (1984) and Wang & Suen (1984, \\n1987). \\n7.5 Refinements \\nA modest amount of progress towards more efficient algorithms has \\nbeen made since Breiman et al. (1984). \\nChan & Bao (1991) and Fayyad & Irani (1992) noticed that we \\ncan restrict the set of cut-points tried for a continuously-valued at\\xad\\ntribute A with the entropy measure of impurity or equivalently using \\ndeviances. The empirical distribution of A jumps only at observed val\\xad\\nues, so clearly the optimal cut-point will be one of the observed values. \\nWhat these authors proved is that the cut-point always occurs on the '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 249}, page_content=\"238 7 Tree-structured Classifiers \\nboundary between two classes, so we do not need to consider observed \\nvalues if those to the immediate left and right correspond to examples \\nof the same class. How much of a saving this produces depends on the \\nnumber of classes (clearly it is best if this is small) and on the degree of \\noverlap of class-conditional distributions of A. The results of Fayyad \\n& Irani showed a very modest speed-up (less than two overall), and \\nour experiments showed even less. \\nChou (1991) provided a partial extension to part (ii) of Proposi\\xad\\ntion 7.1 based on the ideas discussed earlier. This result corresponds to \\nfinding a locally optimal partition of an attribute with L levels in the \\nsense of reduction in average impurity or deviance, in linear expected \\ntime in the number of examples. It is only locally optimal, a point \\nChou glosses over in his title and description. \\nCrawford (1989) considered alternative estimators of the error rate \\nR(T) to be used in cost-complexity pruning, based on the bootstrap \\n(Section 2.7). The idea of the bootstrap is to resample with replacement \\na sample of size n (the original size) from the training set. (Clearly \\neach of the original examples will occur an integer number of times \\nin the bootstrap sample.) A tree sequence can be grown from each \\nbootstrap sample, and the bias in the error rates for the bootstrap \\nsamples used to estimate the bias of R(T(a)). That is, for each of B \\nbootstrap samples we grow and prune a tree to find Tb(a) and evaluate \\nthe difference between the error rate for the real training set and the \\nbootstrap sample. The average of this quantity over the B samples, \\n@(a), is the bootstrap estimate of the bias of R(T(a)), so finally a IS \\nchosen to minimize \\nR(T(a)) +@(a). \\nBreiman et al. (1984, p. 312) give some calculations which sug\\xad\\ngest that the bootstrap estimator of the bias R(T) will systematically \\nunderestimate the bias. This property is not shared by Efron's (1983) \\n.632 bootstrap (Section 2.7). Crawford (1989) reports experiments on \\npruning via both bootstrapped and the .632 bootstrap estimators of the \\nerror rate, generally preferring the .632 bootstrap to both the ordinary \\nbootstrap and cross-validation. \\nIncremental learning \\nThus far we have assumed that the whole training set is available \\ninitially. It is easy to envisage situations in which the training set \\nbecomes available from an on-line process, and it is desired to maintain \\nan up-to-date decision tree. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 250}, page_content=\"7.5 Refinements 239 \\nQuinlan (1979) originally envisaged building a decision tree by ID3 \\nincrementally, but this was as a computational shortcut in a noisefree \\nproblem where it might be hoped that a small subset of examples would \\ninduce the broad shape of a suitable tree. \\nIncremental tree induction has been taken up by Schlimmer & \\nFisher (1986), Utgoff (1988a, 1989, 1990), Utgoff & Brodley (1990) and \\nVan de Weide (1989, 1990). The difficulty with the incremental growth \\nof trees is that early decisions on which attribute to split were based \\non few examples and so are likely to be wrong. Utgoff (1988a) allows \\nhis procedure ID5 to recover by testing the current optimality of a \\nsplit, and if it is sub-optimal to re-order the subtree if the optimal split \\noccurs within the sub-tree. Utgoff (1990) gives a modification which is \\nguaranteed to recover the tree grown by ID3. Van de Weide's objective \\nis to grow the smallest possible tree. \\nAll this work was for noiseless problems. Crawford (1989) considers \\nincremental tree growth for noisy problems, carrying out the whole \\nprocedure (growth and pruning) on a subtree when a new example \\nshows that the split at the root of that subtree is sub-optimal and \\nwill affect the path of the new example through the existing subtree. \\nSubsequently he used bootstrap resampling to estimate if the gain by \\nre-growing the subtree was significant. \\nHybrid methods \\nWe have mentioned that some systems allow linear combinations of \\ncontinuous variates or Boolean combinations of binary ones at each \\nnode. Some of these have been termed hybrid by Utgoff (1988b), and \\nare discussed by Dietterich (1990). \\nThe STAGGER system of Schlimmer & Granger (1986) combines a \\n'Bayesian weight-learning algorithm with a method for constructing \\nBoolean expressions'. The FRINGE algorithm of Pagallo (1989), Pagallo \\n& Haussler (1989, 1990) builds on STAGGER, and post-processes trees \\nconstructed by ID3 to include new attributes constructed as Boolean \\ncombinations of existing ones. \\nSoft splits \\nA classification tree makes hard splits; for example in Figure 7.1 \\ncompletely different paths are taken if we measure the perianth tube as \\nlonger or shorter than 10 mm. We might be worried if we measured \\n9.9 mm, and test both possibilities. An automated system will not do \\nthat unless it is enhanced by soft splits, of the form 'branch right with \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 251}, page_content=\"240 7 Tree-structured Classifiers \\nprobability a(x)'. Hitherto a(x) = l(x > xo), but we can envisage a \\nsmoother transition, and average the predictions by splitting examples \\nas for missing values. Carter & Catlett (1987) used a piecewise linear \\na, linearly interpolating between (xL, 0), (xo, 0.5) and (xR, 1) for XL < \\nx0 < XR. They and Quinlan (1993, §8.1.2) suggested choosing XL and \\nXR (in various ad hoc ways) after the main cut-point xo has been \\nchosen in the usual way. Training set examples can then be divided \\nif their value of x falls in the range [xL, XR], and the tree growth \\ncontinued. \\nIt would not be difficult to choose XL and XR as well as xo to \\nmaximize the reduction in impurity at the split. We could use a logistic \\na, but an asymmetric smoothing of the split may be desirable. \\n7.6 Relationships to neural networks \\nTwo distinct relationships between neural networks and decision trees \\nhave been pointed out. The first is that the splitting mechanism invoked \\nat a node is a way to split optimally the examples reaching that node \\ninto two (or more), and a neural network could be used to select \\nthe attribute to be used. The simplest network would split a linear \\ncombination of the variates, and this gives the perceptron trees of U tgoff \\n(1988b) and neural trees ofSankar & Mammone (1993) (and Stromberg \\net al., 1991). However, Breiman et al. (1984) had already considered \\nallowing linear combinations of variates when setting out the list of \\nattributes at each node, so this gives no added generality. (The idea \\ngoes back to at least Henrichon & Fu, 1969.) The growth procedures \\nare different, in that both Utgoff and Sankar & Mammone use the \\nperceptron learning rule (Section 3.6); Utgoff also used incremental \\ninduction for a noiseless problem. Breiman et al. used a gradient \\ndescent method to find a local maximum in the change in average \\nimpurity. \\nAn obvious extension is to allow a non-linear discrimination rule \\nat each node. Indeed, we can avoid the combinatorial search over the \\nset of attribute splits at a node by seeking a non-linear combination of \\nthe features as a new feature and splitting on that. Many smooth and \\nnon-linear regression techniques could be used, including feed-forward \\nneural networks as considered by Guo & Gelfand (1992). They found \\ndifficulty in extending minimizing the impurity to non-linear functions, \\nand instead used standard least-squares neural network methods to \\ntrain the function to discriminate between two groups of classes. With \\nmore than two classes they need a rule to choose the partition of the \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 252}, page_content=\"7. 7 Bayesian trees 241 \\nclasses; one idea for a moderate number of classes is to compare the \\nimpurity change for each partition of the classes. \\nThe other relationship which has been explored is to use the decision \\ntree to guide the design of a neural network. Brent (1991) considers \\nperceptron trees with t splits and so t + 1 leaves dividing f£ into \\nt + 1 regions with piecewise linear boundaries. There is an immediate \\ncorrespondence between such trees and a neural network with threshold \\nunits and two hidden layers of sizes t and t + 1, the first hidden layer \\ncorresponding to non-terminal nodes of the tree and the second hidden \\nlayer to a path from the root to a leaf with weights zero or ±1. This \\ncan be used as a starting point for optimizing the neural network, \\nperhaps with sigmoidal units. Brent minimizes the deviance of a split \\n(in fact using a slightly different form similar to Fisher's exact test), \\nbut also uses the fact that the threshold units can be approximated \\nby sigmoidal units of high gain to allow optimization methods on the \\nneural network to approximate finding an optimal split in the sense of \\ndeviance reduction. \\nSethi's (1990, 1991) entropy nets have essentially the same idea. The \\nfirst hidden layer again computes the splitting functions at the nodes. \\nThe second and third layers are of AND and OR nodes respectively, \\nand are wired to produce the same partition as the decision tree. (Note \\nthat AND and OR can both be produced by a perceptron.) The second \\nlayer corresponds to ANDing conditions down each path, and the OR \\nlayer collects paths with the same terminal class. Again, the threshold \\nnodes may be relaxed to have sigmoidal response functions. Notice \\nthat although standard neural-network training algorithms could be \\nused, Sethi points out that the derived neural network will be sparsely \\nconnected and of a special form. He proposes the use of a simple \\nreinforcement learning rule. Each node in the AND and OR layers is \\nassociated with identifying a single class, since it is part of one path \\nthrough the tree, and only the weights to these layers are trained, \\nreinforcing the strongest signals amongst nodes associated with the \\nsame class. \\n7.7 Bayesian trees \\nA full Bayesian approach to tree construction will be stymied by the \\nvast number of possible trees, each of which should appear in the \\nposterior average. The best that is possible is to average over a few \\ngood trees, which is the approach taken by Buntine (1992). \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 253}, page_content=\"242 7 Tree-structured Classifiers \\nThe prior has to be given in several steps. A tree is fully specified by \\nits topology (hitherto what we have meant by T ) and the specification \\nof the conditional probabilities. Buntine chooses independent and \\nidentical Dirichlet priors at each node, principally for computational \\nconvenience. For topologies, one can put a uniform distribution over \\nall possible trees, or over tree shapes (ignoring the choice of attributes), \\nor code for the complexity of the tree. \\nGiven the prior and a dataset, it is in principle possible to find \\nthe posterior distribution over trees, and sum out over the topologies, \\ngiving a posterior distribution over classes for any future example. In \\npractice this is impossible, and Buntine uses a variety of heuristics to \\ngrow trees which look to be of high posterior probability, and then \\naverages over those trees. Let B(tiJ, ... ,tiK) = IJr(tij)/r(l::tij) be the \\nnormalizing constant in the density of the Dirichlet distribution. Then \\nP(T I . . ) P(T) IT B(ntl + tiJ, ... , ntk + tiK) trammg set oc ( ) \\nI B tiJ, ... ,tiK eaves t (7.1) \\nP((ntc) IT, training set) oc IT ( 1 ) IT n~~c+IXc-1\\n. B tiJ, ... ,tiK leaves t classes c \\nThen (ntc) has mode at (ntc + tic)/(nt + 2::: tic), which is (for tic= 1) the \\nsmoothing we proposed earlier. \\nBuntine uses (7.1) to suggest an heuristic for measuring the quality \\nof a split, \\nP( ) IT B(ntl+tiJ, ... ,ntk+tiK) test x . \\n. d B(tiJ, ... ,tiK) chil ren \\nNote that this would be appropriate if (7.1) was a product over paths, \\nnot leaves. Similarly the shrinking proposed is towards the parent node, \\nwhereas (7.1) suggests a shrinking towards tic/ 2::: tij, and this is set from \\nthe overall distribution of classes in the training set. \\nPerhaps the most interesting heuristic is the use of 'option trees', \\nin which more than one attribute at each node can be considered, but \\nnot simultaneously. Thus not only the best one-step lookahead split \\nis chosen, but the best few are kept in play, allowing a range of trees \\nwith high posterior probability to be generated. An earlier idea along \\nsimilar lines is that of Kwok & Carter (1990). \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 254}, page_content='8 \\nBelief Networks \\nThe supervised methods considered so far have learnt both the structure \\nof the probability distributions and the numerical values from the train\\xad\\ning set, or in the case of parametric methods, imposed a conventional \\nstructure for convenience. Other methods incorporate non-numerical \\n\\'real-world\\' knowledge about the subject domain into the structure of \\nthe probability distributions. Such knowledge is often about causal re\\xad\\nlationships, or perhaps the lack of causality as expressed by conditional \\nindependence. \\nThese ideas have been most explored within the field of expert \\nsystems. This is a loosely defined area, and definitions vary: \\n\\'The label \"expert system\" is, broadly speaking, a program \\nintended to make reasoned judgements or to give assistance in \\na complex area in which human skills are fallible or scarce ... .\\' \\n(Lauritzen & Spiegelhalter, 1988, p. 157) \\n\\'A program designed to solve problems at a level comparable \\nto that of a human expert in a given domain.\\' (Cooper, 1989) \\n\\'An expert system has two parts. The first one is the knowl\\xad\\nedge base. It usually makes up most of the system. In its simplest \\nform it is a list of IF ... THEN rules: each specifies what to \\ndo, or what conclusions to draw, under a set of well-defined \\ncircumstances. \\nThe second part of the expert system often goes under the \\nname of \"shell\". As the name implies, it acts as a receptacle for \\nthe knowledge base and contains instruments for making effi\\xad\\ncient use of it. These include a short-term memory, tree-searching \\nmachinery and a user interface.\\' (Crevier, 1993, pp. 156-7) \\nThe last definition is the traditional one in AI, but excludes expert \\nsystems based on probabilistic knowledge by assuming that the knowl\\xad\\nedge base is made up of \\'if ... then rules\\'. Another aspect of expert '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 255}, page_content=\"244 8 Belief Networks \\nsystems which is often stressed is the availability of facilities to provide \\nexplanations, usually in the form of a chain of deductions which lead \\nto the conclusion. \\nIn the development of probabilistic reasoning by Pearl (1986, 1988, \\n1993a), Lauritzen & Spiegelhalter (1988) and co-workers, the structural \\ndivision is slightly different. The knowledge base is represented by a \\nqualitative description of the dependencies (more accurately, lack of \\ndependence) between the variables in the system, and the quantita\\xad\\ntive description of the numerical values of those dependencies. The \\nrole of the 'shell' is taken by the set of algorithms which manipulate \\nthe probabilities in an automatic way to present conclusions, such as \\nthe posterior probabilities of the various classes. Systems based on \\nthese ideas have many names: they have been called Bayesian expert \\nsystems, Bayes(ian) net(work)s, belief net(work)s, causal (probabilistic) \\nnetworks, probabilistic expert systems and probabilistic reasoning on \\ncausal graphs. \\nA very simple example may help to fix ideas. Suppose the input x \\nis a set of m features x1, ... , Xm. As usual, we wish to find the posterior \\nprobabilities p(k I x) to classify a future case. The rule called naive or \\nidiot's Bayes (Warner et al., 1961; Titterington et al., 1981) takes \\nm \\np(k I x) oc nk II p(xi I k). \\ni=l (8.1) \\nOne derivation of (8.1) is to assume p(x I k) = f1 p(xi I k ), that is that \\nthe features are conditionally independent given the class, from which \\nit is immediate that \\nm \\np(k I x)p(x) = p(x, k) = p(x I k )nk = nk II p(xi I k ). \\ni=l \\nThe qualitative part of the knowledge base is then the assumption of \\nconditional independence, and the quantitative part is the specification \\nof the probabilities nk and Pk(xi) = p(xi I k). The shell is the set of \\nrules for manipulating probabilities (essentially Bayes' formula) plus a Figure 8.1: The causal \\ngraph for the idiot's \\nBayes rule. The arrows \\nrepresent causal \\ninfluence. \\nThis list of names is not \\nexhaustive. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 256}, page_content=\"Recent books include \\nAlmond (1995), Jensen \\n(1996) and Shafer \\n(1996). 8 Belief Networks 245 \\nuser interface. The qualitative part of naive Bayes can be expressed \\ngraphically as in Figure 8.1. \\nIn the rest of this chapter we will develop more complex networks \\nthan Figure 8.1 and corresponding stylized ways to apply Bayes' formula \\nto derive the posterior probabilities, and to modify them as more \\ninformation becomes available. \\nOverviews of this area with various applications are provided by \\nSpiegelhalter et al. (1993), Andreassen et al. (1991), Charniak (1991) and \\nNeapolitan (1990), as well as by papers within the collections edited by \\nOliver & Smith (1990), Shafer & Pearl (1990) and Gammerman (1995). \\nApplications in computer vision are described by Agosta (1990), Binford \\net al. (1989), Levitt et al. (1990) and Rimey & Brown (1992). Various \\ncommercial and free shells are available, including BAlES (Cowell, \\n1992, 1995), Hugin (Andersen et al., 1989), IDEAL (Srinvas & Breese, \\n1990) and PRESS (Gammerman et al., 1995). Although we work with \\nprobabilities, the same calculations can be applied to other measures of \\nbelief which satisfy certain axioms (see Section A.4)-see Pearl ( 1988), \\nDempster & Kong (1988), Shafer & Shenoy (1986), Shenoy et al. (1988), \\nShenoy (1989) and Shenoy & Shafer (1990)-and also to consistency \\ncalculations in computer databases (Fagin, 1977). \\nThe methods of this chapter are more complicated than, say, clas\\xad\\nsification trees, so it is worth asking if the ability to feed in qualitative \\nknowledge actually improves the accuracy of classification. Several of \\nthe discussants of Spiegelhalter et al. (1993) asked this, specifically in \\nthe context of medical diagnosis. Their answer (page 280) is equivocal. \\nBelief networks are designed and trained to answer more than just \\nthe question of classifying future cases. They are able to give a much \\nhigher level of explanation, including exploring what were important \\ninput features in reaching the conclusion and whether the input data \\nwere in some sense in conflict. To do so they model the whole joint \\ndistribution. Although there will be an advantage in using qualitative \\nknowledge (at least if it is a reasonable approximation to reality), the \\nneed to model the whole distribution makes more demands on limited \\ndata resources. \\nTwo restrictions need to be noted. Most of the development of belief \\nnetworks has been restricted to categorical variables, that is discrete \\nrandom variables with a finite (and usually small) number of levels; \\nsome developments using continuous variables are being made (such \\nas Lauritzen, 1992, and Gammerman et al., 1995). Second, the problem \\nof conditioning belief networks on observations is in general NP-hard \\n(Cooper, 1990) and so the methods described here are potentially \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 257}, page_content='246 8 Belief Networks \\nprohibitively slow. Fortunately, in many real systems the networks are \\nsparsely connected, and the shells do seem to work quite fast enough. \\nThere has been a parallel (and until recently completely separate) \\ndevelopment of methods within the field of pedigree analysis in genetics \\n(Spiegelhalter, 1990; Cannings et al., 1978; Cannings & Thompson, \\n1981; Thompson 1985). \\nBelief networks are often associated with notions of causality. Opin\\xad\\nions on the usefulness of this vary from complete scepticism (Speed, \\n1990) to enthusiasm (Pearl, 1993b, 1995). Since our purpose is predic\\xad\\ntion of pattern classes, we will avoid discussion of causality except to \\nuse known causality to help us specify probability models. \\nIn the past directed graphs were widely used because there was \\nperceived to be a problem with zero probabilities in graphical models \\non undirected graphs. This is inaccurate; these problems disappear \\nwhen special distributions or (especially) special graphs are considered. \\nThus we derive most of the methodology in the context of decomposable \\n(undirected) graphs after considering the simple case of a directed tree. \\n8.1 Graphical models and networks \\nGraphs such as Figure 8.1 are used to represent conditional inde\\xad\\npendence properties on a collection of random variables. It will be \\nimportant to keep a clear distinction between directed graphs which \\ncan represent causality, and undirected graphs without arrows which \\nrepresent dependence without specifying a causal direction; some terms \\nare used for both with subtly different meanings. \\nThroughout this section we will assume we are given a finite col\\xad\\nlection of random variables Xv, v E V, and we wish to describe qual\\xad\\nitatively the dependencies between these random variables. In our \\napplications these random variables will include the features in the \\npattern x and the class C. However, they may also include unobserved \\nfeatures. For example, an important extension to Figure 8.1 is given in \\nFigure 8.2 where the class is not assumed to be reported accurately, as \\nmay be common where diagnosis is difficult. \\nTo describe dependence we will use the language of graph theory. \\nThis is usually self-explanatory, but more formal treatments can be \\nfound in many basic accounts of theoretical computer science, including \\nKnuth (1968), Cormen et al. (1990) and Sedgewick (1990), as well as the \\nspecialist books by Berge (1973) and Golombic (1980). Maier (1983) \\ngives a different perspective, that of database theory. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 258}, page_content=\"Figure 8.2: The causal \\ngraph for the idiot's \\nBayes rule with \\ninaccurate reporting of \\nclasses. \\nAnother convention is \\nto call complete \\nsubgraphs cliques, when \\na clique is maximal if \\nno vertex can be added \\nwithout making the \\nsubgraph incomplete. \\nPedants will call these \\nacyclic directed graphs. 8.1 Graphical models and networks 247 \\nA graph is a collection of vertices and edges. The vertices will \\nrepresent the set of random variables (hence the use of V to denote \\nthe set). The set of edges is a set of unordered pairs of distinct vertices; \\nif an edge is present it is indicated on a diagram by a line (without an \\narrow) joining the pair of vertices. A path on a graph is list of vertices \\nfor which each successive pair is joined by an edge. A subgraph is a \\nsubset of vertices together with those edges both of whose vertices are \\nin the subset. A subgraph is said to be connected if there is a path \\njoining every pair of vertices, and complete if every possible edge is \\npresent. The maximal complete subgraphs of a graph are called its \\ncliques. A cycle is a path which returns to its origin and visits no vertex \\nmore than once. A connected graph with no cycles is called a tree. \\nWhere necessary, we will refer to graphs as undirected graphs. Di\\xad\\nrected graphs also have a set of vertices and edges, but the edges are \\nordered pairs of vertices, and are represented on a figure (such as Fig\\xad\\nure 8.2) by lines with arrows from the first vertex to the second vertex. \\nThe first vertex is often called the parent and the second (marked by the \\narrow) the child. The notions of paths and cycles extend immediately to \\ndirected graphs. We will make frequent use of directed acyclic graphs, \\nDAGs, that is directed graphs without cycles. A directed tree has the \\nproperties that it has one vertex, the root, such that a (directed) path \\nleads from the root to any vertex, and any other vertex has precisely \\none incoming arrow. An ancestral subgraph of a directed graph contains \\nall the ancestors of its vertices; for a directed tree ancestral subgraphs \\nare rooted subtrees. \\nA polytree is a singly-connected DAG, that is a DAG in which at \\nmost one path exists between any two vertices (see Figure 8.3). \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 259}, page_content='248 8 Belief Networks \\nMarkov networks \\nThe usual way to interpret the (in)dependencies represented by an \\nundirected graph f§ is what Pearl (1988) calls an !-map. Given three \\nsubsets A, B, C of vertices, we say C separates A and B in f§ if every \\npath from (a vertex in) A to (a vertex in) B goes through (a vertex \\nin) C. For any subset A of V let XA denote the collection of random \\nvariables associated with the vertices in A. Then we consider whether \\nXA and XB are conditionally independent given Xc, which we write \\nas \\nXA ...JL XB I Xc or sometimes A ...JL B I C. \\nWe say the graph f§ is an I-map of the distribution if this is true \\nwhenever A and B are separated by C. We say the distribution \\nis global Markov with respect to f§ if separation implies conditional \\nindependence. \\nNote that the complete graph on V will be an· I-map of any \\nprobability distribution on X v, since then there will never be any \\nseparating C. This shows that there may be subsets A, B and C \\nwith the conditional independence property A ...JL B I C, for which not \\nevery path from A to B goes though C. If it is also true that all \\nconditional independencies are represented by separation, Pearl calls \\nthe representation by this graph a perfect map. An I-map f§ is called \\nminimal if removing any edge from f§ makes it no longer an I-map. If \\nthere is a unique minimal I-map, it is called the Markov network of the \\ndistribution. \\nMarkov properties of distributions on graphs have been studied in \\nthe areas of random fields (Preston, 1974, 1976) and image analysis \\n(Geman & Geman, 1984; Geman, 1990; Isham, 1981). Several Markov Figure 8.3: An example \\nof a polytree. \\nThe two concepts are \\nthe same, but the graph \\nvaries for an 1-map and \\nthe distribution for \\nglobal Markov. \\nDistributions for which \\nthere is a Markov \\nnetwork that is perfect \\nare often called \\ngraphical models, \\nalthough this term is \\nalso used more loosely. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 260}, page_content=\"This example is from \\nPearl (1988, p. 135). \\nThe set V \\\\ {a,b} can \\nbe any pair of vertices, \\nand knowledge of any \\npair of random \\nvariables determines \\nwhich of the three \\noutcomes occurs. Thus \\nconditionally Xa and \\nXb are constant. 8.1 Graphical models and networks 249 \\nproperties have been defined. Denote by oA the boundary of A, the \\nset of vertices in Ac which have a neighbour in A (so all neighbours \\nof points in A are in A U oA ). Then the most important Markov \\nproperties for a given graph r'§ are \\nglobal For any disjoint subsets A, B and C such that C separates A \\nand B (all paths from A to B contain a member of C) we have \\nXA ...JLXB I Xc. \\nlocal The conditional distribution of Xa given X V\\\\{a} depends only \\non Xa{a}• or equivalently X a ...JL X V\\\\[{a}uo{a}J I Xa{a}· This is prob\\xad\\nably easier to describe in words: the random variables at a and \\nthose at vertices not connected to a by an edge are conditionally \\nindependent given those which are so connected. \\npairwise Xa and Xb are conditionally independent given all the other \\nrandom variables if there is no edge from a to b. \\nThese properties allow us to read off successively weaker conditional \\nindependence statements from the graph; the global Markov property \\nis equivalent to the graph being an I-map. The three properties can \\nbe strictly different. Consider the four discrete random variables with \\njoint distribution \\na b c d Pr \\n0 0 0 0 1/3 \\n0 1 1 1 1/3 \\n1 1 0 2 1/3 \\nThese are taken as the random variables at the vertices of a graph. \\nThe pairwise Markov property is satisfied by any graph on the vertices, \\neven that with no edges. On the other hand, the four random variables \\nare far from independent, and the conditional distribution of Xd given \\nXa, Xb and Xc depends on at least two of the conditioning random \\nvariables. For the local Markov property to hold we have two possible \\nminimal graphs: \\nb c \\nand for both the global Markov property holds. Thus in this example \\nthere are two distinct minimal I-maps. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 261}, page_content=\"250 8 Belief Networks \\nAn example of a local but not global Markov distribution is given \\nby taking the graph a-b c-d and the same non-constant random \\nvariable at the four vertices. This is trivially local Markov, but { b} and \\n{ c} are separated by f/J, and Xb .lL Xc is false. \\nOur main example demonstrates the inadequacy of the 'obvious' \\nway to construct a minimal I-map, that is to include the edge {a,b} in \\nthe graph if and only if X a .f.-xb I X V\\\\{a,b} holds. This is clearly the \\nminimal graph to satisfy the pairwise Markov property, but need not \\nbe global Markov. By the following result, if we confine attention to \\ndiscrete random variables and strictly positive probability distributions \\nthis will be an I-map, and therefore the unique minimal I-map. This \\nresult has a confusing history; it is often attributed to Hammersley & \\nClifford in 1971, although they did not publish for nearly twenty years \\nand gave one of a series of increasingly more general statements. \\nProposition 8.1 Suppose we have a collection of discrete random variables \\ndefined on the vertices of a graph. \\n(i) Suppose the joint distribution is strictly positive. Then the pairwise \\nMarkov property implies that there are positive functions <Pc, sym\\xad\\nmetric in their arguments, such that \\nPr{Xv = xv} oc II </Jc(xc) \\nc \\nthe product being over cliques of the graph. (8.2) \\n(ii) A potential representation (8.2) implies the global Markov property \\nfor any distribution. \\nProof: (i) We may assume (by re-labelling if necessary) that each \\nrandom variable Xs can take the value 0. The proof proceeds by \\ninduction on the size of A= {s I Xs =/= 0}, and we prove the existence \\nof functions <Pc (with <Pc = 1 for non-complete C) such that \\nPr{X = x} = Pr{X = 0} II </Jc(xc). (8.3) \\nCcA \\nThis can be reduced to a product over cliques by assigning <Pc to a \\nclique which contains it, and multiplying the original clique function by \\n</Jc. \\nDefine the functions <Pc recursively by \\n</Jc(xc) = Pr{Xc = xc,Xcc = 0} / Pr{X = 0} II </Jv(xv) \\nD~C The \\nHammersley-Clifford \\nresult relates the local \\nMarkov property to the \\nglobal property and to \\na potential \\nrepresentation. See \\nClifford (1990) for the \\npublished version and \\nhistorical comment. \\nThe rest of this section \\nis rather technical and \\nmay be skipped at first \\nreading. \\nDefining </Jc = 1 for \\nnon-complete C allows \\nus to take products \\nover all subsets. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 262}, page_content='8.1 Graphical models and networks 251 \\nwhere the product is over strict subsets, and cpc(O) = 1. (Note that \\n<Pc > 0 which avoids having 0 x p/0 in the manipulations that follow.) \\nClearly (8.3) holds if A is complete, and so holds if A is empty or has \\none element. Now suppose it holds if A has k or fewer elements. Split a \\nnon-complete A with k + 1 members as B U { s} U { t} where B has k -1 \\nelements and s and t are not neighbours. Then Xs .lL Xt I XB,XAc, so \\nPr{Xv = Xv} = Pr{Xs = X8,Xt = Xt,XB = XB,XAc = 0} \\n= Pr{XB = XB,Xs = X8,Xt = O,XAc = 0} \\nPr{Xt = Xt I XB = XB,Xs = X8,XAc = 0} \\nX--~------------------------~ Pr{Xt = 0 I XB = XB,Xs = X8,XAc = 0} \\n= Pr{XB = XB,Xs = X8,Xt = O,XAc = 0} \\nPr{Xt = Xt I XB = XB,Xs = O,XAc = 0} \\nX--7---~~----------~----~ Pr{Xt = 0 I XB = XB,Xs = O,XAc = 0} \\n= Pr{XB = XB,Xs = X8,Xt = O,XAc = 0} \\nPr{Xt = Xt,XB = XB,Xs = O,XAc = 0} \\nX--~----------------------~ Pr{Xt = O,XB = XB,Xs = O,XAc = 0} \\n= Pr{X = 0} II cpc(xc)ITccBu{t} cpc(xc) \\nCcBU{s} ITccB cpc(xc) \\n= Pr{X = 0} II cpc(xc) \\nCcBU{s}U{t} \\nwhere we use conditional independence at step 3, (8.3) for sets of size \\nat most k at step 5 and the fact that a complete subset C c A cannot \\ncontain both s and t at the last step. This establishes the result for \\nany set A of size k + 1 and completes the inductive step of the proof. \\n(ii) Suppose a potential representation (8.2) is given. Then \\nPr{Xv} II /\"\" II Pr{XA I XAc} = Pr{XAc} = cpc(Xc) ~ c/Jc(Xc) \\ncliques C XA cliques C \\ndi!! c </>c(Xc) / t di!! c </Jc(Xc) \\nCnA# CnA# \\nwhere we cancel terms for cliques C disjoint from A. A clique with \\nC n A=/= 0 is contained in Au 8A, so the right-hand side is a function \\nof XAuoA and Pr{XA I XAc} = Pr{XA I XaA}· Some of the potentials \\n¢c may take zero values, but these calculations still hold if we take \\n0/0 = 0. \\nNow suppose A and B are separated by C. Let B\\' be the set of \\nvertices which can be reached by a path from B which does not meet C '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 263}, page_content=\"252 8 Belief Networks \\nand let D = (B' U C)c => A; by construction D, B' and C are disjoint \\nand C separates D and B', so no neighbour of D is in B'. Then \\nPr{Xv I XB',Xc} = Pr{Xv I Xvc} = Pr{Xv I Xav} does not depend on \\nXB'· Thus D JL B' I C and hence A JL B I C as A c D,B c B'. 0 \\nSome partial relaxation of the positivity condition is possible: see \\nMoussouris (1974), Averintsev (1975) and Ripley & Kelly (1977). Our \\ncounter-example on page 249 also shows (with the minimal I-map \\nshown as the right-hand graph) that a distribution can be global Markov \\nbut not have a potential representation. \\nAn alternative to imposing strict positivity on the distribution is to \\nimpose further conditions on the graph. Matus (1992) shows that all \\nthree Markov properties are equivalent for any distribution if and only \\nif every sub graph on three vertices contains two or three edges. A graph \\nis said to be triangulated or chordal if every cycle of length four or more \\nhas a chord (an edge joining two non-consecutive vertices), and we will \\nsee in Proposition 8.2 that we can construct a potential representation \\nfor a triangulated I-map. Conversely, if a graph is not triangulated, it \\nhas a chordless cycle of length four or more, and our counter-example \\n(extended if necessary by copies of Xd along the cycle, and constant \\nvariables elsewhere) shows a distribution on the vertices of the graph \\nthat is global Markov but does not have a potential representation. \\nThus being global Markov and having a potential representation are \\nequivalent for all distributions on a graph if and only if it is triangulated. \\nWe could ask if all conditional independence properties entailed by \\nbeing global Markov can be read from the graph by separation . Geiger \\n& Pearl (1993) show that this is so, by constructing a strictly positive \\ndistribution such that XA JL XB I Xc if and only if C separates A and \\nB on the graph. (The random variables used in this construction do \\ntake a finite set of values, but not one that can be specified in advance.) \\nMarkov trees \\nIf an undirected graph is a tree, any vertex can be declared as the root, \\nand a directed tree formed by assigning arrows to point away from the \\nroot (along the unique path from the root to a vertex). The simplest \\npossible tree is one with no branches, that is a chain of vertices. The \\nsimplest Markov network is a Markov chain, for which the Markov \\nproperty is usually stated symmetrically \\n'past and future are independent given the present' \\nbut the usual calculations on a Markov chain depend heavily on the \\ntime ordering of the vertices to work either forwards or backwards in If there were a potential \\nrepresentation, \\nPr{a = O,b = 1, \\nc = O,d = 2} = \\n</>{a,bj(O, 1)</>{a,cj(O, 0) X \\n</>{b,dj(1, 2)</>{c,dj(O, 2) > 0. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 264}, page_content=\"Here Sv is any set of \\nvalues, possibly all \\npossible values. We are \\nexcluding evidence such \\nas x. = xb. \\nThe rest of this \\nsubsection is technical \\nand not needed \\nelsewhere. 8.1 Graphical models and networks 253 \\ntime. Most of these methods can be extended to trees, with calculations \\nproceeding up towards the root or downwards away from the root. \\nAlthough some of the ideas logically belong in the next section, trees \\nare so important an idea that we treat this special case here. \\nMarkov chains are usually specified by the transition probabilities \\nPr{Xt = j I Xt-l = i}, and Markov trees are also commonly specified \\nby giving Pr{Xv I Xa, a the parent of v }. This does specify the joint \\ndistribution, since we can label the vertices in increasing order away \\nfrom the root of the tree so that a vertex is preceded by its parent, and \\nthen \\nPr{Xv}= IlPr{Xi iXj,j<i}= ITPr{XiiXa,a theparentofi} \\ni i \\n(8.4) \\nwhere at the second step we use the separation of v from the rest of \\nthe graph by its parent. The root has a special role in (8.4): it has no \\nparent and Pr{Xt} appears without conditioning. In a tree the cliques \\neach contain just one edge, so (8.4) is a potential representation of the \\ndistribution (without any positivity condition). \\nFigure 8.2 provides a natural example of a directed tree, with the \\ntrue class as the root. The calculation we would wish to perform is \\nto condition the distribution by restrictions E = n{ Xv E Sv} on the \\nvalues at some or all of the vertices (which we think of as the 'evidence'), \\nspecifically to find Pr{Xv IE} for one or more individual vertices. There \\nare simple ways to do so making use of the tree structure (Pearl, 1982, \\n1988). Conditioning on the value taken by Xv has two effects. One is \\nto break the tree at v so we can find the probability distribution of the \\ndescendants of v independently of all the rest of the tree. The other \\neffect is to change the distribution of the ancestors of v and all their \\ndescendants (that are not direct descendants of v ). We can consider \\nboth effects by first propagating messages up the tree, then down from \\nthe root. In the following a vertex is considered to be a descendant of \\nitself, but not its own ancestor. \\nWe suppose that we have calculated the marginal distributions Pv \\nat each vertex: this can be done by \\nX a \\nwhere a is the parent of v (and we are given the marginal distribution \\nof the root vertex). We want p~(xv) = Pr{Xv = Xv IE}. For any vertex \\nv let E; denote the conditioning event (if any) on the descendants of \\nv, and let E;; denote the condition on the remaining random variables. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 265}, page_content='254 8 Belief Networks \\nThen by Bayes\\' formula \\nusing the separation by vertex v. \\nThe first term is only non-trivial if v has some children ui. Partition \\nE;; into events concerning the descendants of each child, which are \\nconditionally independent given Xv so \\nPr{E;; I Xv} = l(Xv E Sv) II Pr{E;; I Xv} \\nchild u \\nand \\nXu \\nwhich can be computed by a pass over the tree towards the root. Now \\nconsider the second term of (8.5). Variables in E: can only influence \\nXv through its parent Xa, so \\nPr{Xv I Et} = L Pr{Xv I X a= Xa} Pr{Xa = Xa I Et}. (8.7) \\nX a \\nNow E: includes the restriction on Xa and E;i, plus Eb\" for any \\nother children b of a. We have \\nPr{Xa I Et} = Pr{Xa I Xa E Sa,Ed\",Eb\" for other children b} \\noc I(Xa E Sa) Pr{Xa I Ed\"} II Pr{Eb\" I Xa}· (8.8) \\nb \\nThe terms in the product will have been found at (8.6) in the inwards \\npass, and so (8.7) and (8.8) can be computed on a subsequent outwards \\npass. \\nThese computations may also be organized as asynchronous \\nmessage-passing. Each vertex keeps a current version of A.(xv) = \\nITu Pr{E~ I Xv} as a product over its children (empty products being \\none) and v(xv) = Pr{Xv = Xv IE:}. Then p~(xv) is I(xv E Sv)A.(xv)v(xv) \\nnormalized to unit sum. When information becomes available at ver\\xad\\ntex v, it passes Pr{ E;; I Xv E Sv} to its parent a, which is converted \\nto Pr{ E;; I X a} by (8.6) and used to update A.(xa). It also passes to \\neach child Pr{ Xv I Xv E Sv, E:, Eb\"} where that child is excluded from \\nthe b considered. This message is found from (8.8) by re-normalizing. \\nWhenever another vertex receives a message it passes on messages to \\nits other neighbours . \\nIt will be helpful to consider an example. The graph is shown \\nin Figure 8.4. There are 12 binary random variables which we label '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 266}, page_content=\"Figure 8.4: An example \\nof a directed tree. \\nTable 8.1: Specifications \\nof conditional \\nprobabilities for the \\ndirected tree of \\nFigure 8.4. \\nThe two values of the \\nprobability refer to the \\nvalues zero and one of \\nthe unspecified variable, \\nin that order. 8.1 Graphical models and networks \\nA ;l\\\\' \\n0 g 8 \\nPr{A = 1} = 0.3 \\nPr{B = 11 A= 0} = 0.9 \\nPr{B=1IA=1}=0.1 \\nPr{ C = 11 A = 0} = 0.2 \\nPr{ C = 11 A = 1} = 0.8 \\nPr{D = 11 B = 0} = 0.5 \\nPr{D = 11 B = 1} = 0.4 \\nPr{ G = 11 B = 0} = 0.7 \\nPr{ G = 11 B = 1} = 0.2 \\nPr{K = 11 B = 0} = 0.3 \\nPr{K = 11 B = 1} = 0.9 Pr{L = 11 C = 0} = 0.1 \\nPr{L=11C=1}=1.0 \\nPr{M = 11 C = 0} = 0.0 \\nPr{M = 11 C = 1} = 0.7 \\nPr{N = 11 G = 0} = 0.5 \\nPr{N = 11G= 1} =0.1 \\nPr{P = 11 G = 0} = 0.1 \\nPr{P = 11 G = 1} = 0.6 \\nPr{S =11M= 0} = 0.8 \\nPr{S =11M= 1} = 0.5 \\nPr{T =11M= 0} = 0.3 \\nPr{T =11M= 1} = 0.7 255 \\nby capital letters. Their joint distribution can be specified via the \\nprobabilities of Table 8.1. \\nWe first calculate the marginal probabilities by a downwards pass \\nin alphabetical order of the vertices. We then are told that C = P = \\nS = 1 and N = T = 0, and asked for the conditional probability that \\nB = 1 (it was 0.66 before conditioning) . The first step is to pass the \\ninformation up to the root, then down to B. Knowledge of S and \\nT is actually irrelevant, as we know C which separates B from S, T. \\nPropagating N = O,P = 1 to g gives Pr{Eg-I G} = (0.05,0.54), and \\npropagating this to b gives Pr{E; I B} = (0.393,0.148). Propagating \\nC = 1,S = 1, T = 0 to a gives a term Pr{A I Et} = Pr{A I C = 1,S = \\n1, T = 0} = Pr{A I C = 1} = (0.14, 0.24)/0.38. This is passed down to \\nb, giving Pr{B I Et} = (0.23,0.15)/0 .38 using (8.7). From (8.4) we have \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 267}, page_content=\"256 8 Belief Networks \\nPr{B I C = P = S = 1,N = T = 0} oc (0.393 x 0.23,0.148 x 0.15), and \\nfinally the desired probability is 0.1972. \\nA further question we can ask is 'what is the most likely explanation \\nof B = 1 ', that is what values of the remaining variables are most \\nlikely to have occurred with B = 1. This can be achieved by the same \\nupdating procedures, merely replacing averaging over children by a \\nmaximum operation (Pearl, 1988, Chapter 5). We can find the most \\nprobable configuration with B = 1 by finding both the most probable \\npattern of descendants and the most probable pattern of ancestors and \\ntheir other descendants. In our example we are given B = C = 1, so \\nPr{A I B = C = 1} oc Pr{A}Pr{B = 11 A}Pr{C = 11 A}= (0.126,0.024), \\nso A = 0 is the most probable ancestor. Clearly D = 0 and K = 1 \\nare most probable. Finally Pr{ G I B = 1, N = 0, P = 1} oc Pr{ G I B = \\n1} Pr{N = 0 I G} Pr{P = 11 G} = (0.040,0.108) and the most plausible \\nexplanation is (A= O,C = 1,D = O,G = 1,K = 1,N = O,P = 1). \\nAnother method to organize these calculations is via the joint \\nmarginal distributions of each adjacent pair of vertices, which is just \\nthe marginal distribution of the parent times the transition probability \\nto the child. When a condition is ·imposed at a vertex, this alters the \\njoint distributions of that vertex and each neighbour. The evidence can \\nthen be propagated to each neighbour of that neighbour (and hence all \\nover the tree) by updating the joint density of Xa,Xb as \\n{ } { } Prnew{Xa} Prnew Xa,Xb = Prold Xa,Xb X { } . Prold Xa \\nWe illustrate this for our example, considering only the vertices abcgnp \\nfor compactness. (The distributions of the remaining vertices can be \\nfound conditionally on this set quite easily.) Initially the marginal \\nprobabilities that Xv = 1 are \\na b c g n p \\n0.3 0.66 0.38 0.37 0.352 0.285 \\nand the joint probabilities on the five edges are \\n00 0 1 1 0 1 1 \\na-b 0.07 0.63 0.27 0.03 \\na-c 0.56 0.14 0.06 0.24 \\nb-g 0.102 0.238 0.528 0.132 \\ng-n 0.315 0.315 0.333 0.037 \\ng-p 0.567 0.063 0.148 0.222 This mode of reasoning \\nis called abductive \\ninference. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 268}, page_content='8.1 Graphical models and networks 257 \\nNow we condition on N = 0. This changes the marginal probabilities \\nto \\n00 0 1 1 0 1 1 \\ng-n 0.4861 0 0.5139 0 \\nVertex g then sends the message Prnew{G}/Proid{G} = (0.7716, 1.3889) \\nto its neighbours b and p and updates Pr{ G = 1} = 0.5139. Let us \\nconsider the effect on p, which is to update the edge marginal to \\n00 01 10 11 \\ng-p 0.4375 0.0486 0.2056 0.3083 \\nIf we now condition on P = 1, we find Pr{ G = 1} = 0.8638. \\nWe now update the b-g edge by rescaling by the G marginal. We \\nhave not updated it since Pr{ G} = 0.37, so the result is \\n00 01 10 11 \\nb-g 0.0221 0.5556 0.1141 0.3082 \\nwhich gives Pr{B = 1} = 0.4223. Updating the a-b edge we find \\nPr{A = 1} = 0.4780 and \\n00 0 1 1 0 1 1 \\na-b 0.1189 0.4031 0.4588 0.0192 \\nFinally, consider conditioning on C = 1. First update the a-c \\nedge to produce \\n00 01 10 11 \\na-c 0.4176 0.1044 0.0956 0.3824 \\nthen set the entries for C = 0 to zero and renormalize to give Pr{A = \\n1} = 0.7855. Now we update the a-b edge again to get \\n00 01 10 11 \\na-b 0.0489 0.1656 0.7539 0.0316 \\nThis gives Pr{B = 1} = 0.1972; which is the conditional probability we \\nrequire. Propagating this ends up with marginal probabilities of \\na b c g n p \\n0.7855 0.1972 1 0.916 0 1 '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 269}, page_content='258 8 Belief Networks \\nand the joint probabilities of the five edges are \\n00 0 1 1 0 1 1 \\na-b 0.0489 0.1656 0.7539 0.0316 \\na-c 0 0.2145 0 0.7855 \\nb-g 0.0307 0.7721 0.0533 0.1439 \\ng-n 0.084 0 0.916 0 \\ng-p 0 0.084 0 0.916 \\nThe details of such calculations are tedious to do by hand, but lend \\nthemselves to automated recursive calculations, and have been used by \\nBinford et al. (1989). Kim & Pearl (1983) (see also Pearl, 1988) extend \\nthe principles to polytrees, where a vertex may have more than one \\nparent as well as more than one child. The method fails if applied to \\nDAGs which would have loops if viewed as undirected graphs. There \\nare a few ways to convert such DAGs into polytrees (Pearl, 1988, §4.4): \\n1 clustering methods, in which vertices are joined into a composite \\nnode. For example, if in Figure 8.5 we combine vertices b and c the \\nDAG is converted to a chain (which is of course a tree). We will see \\na systematic version of this idea in the next section. \\nIncreased total \\nserum calcium b Metastatic cancer \\n2 conditioning, in which the values at one or more vertices are fixed, \\nand the results averaged over the conditioned variables. If in \\nFigure 8.5 we condition on Xa, we have a polytree. \\n3 simulation methods which we discuss in the next section. \\nDecomposable models \\nThe Markov network of a strictly positive distribution has a \\nparametrization via the clique functions cf>c of (8.2). Computations \\nare very much simplified for graphs that have no cycle of four or \\nmore vertices without a chord (an edge joining two non-consecutive \\nvertices). Such graphs are called chordal or triangulated. A probability \\ndistribution is said to be decomposable with respect to a graph t§ if t§ Figure 8.5: A \\nhypothetical medical \\nbelief network (after \\nCooper, 1984, and \\nPearl, 1988). \\nThis is equivalent to \\nanother graph property \\ncalled decomposability, \\nso these graphs are also \\ncalled decomposable. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 270}, page_content=\"A join tree is also called \\na junction tree. Blair & \\nPeyton (1993) give an \\nintroduction to these \\nideas, and Lauritzen \\n(1996, Chapter 2) gives \\na self-contained account \\nof the graph-theoretical \\nproperties based on \\ndecomposability. \\nsi c cj(i) follows from \\nthe definition of a join \\ntree:ifvEC1nC for \\nj < i then v E CJ(i)· 8.1 Graphical models and networks 259 \\nis an I-map and triangulated. If 'fJ is omitted, the minimal I-map is \\nassumed (if it is unique). The undirected version of the directed graph \\nin Figure 8.5 is not decomposable, and neither is the right-hand I-map \\nfor our counter-example on page 249. \\nFor decomposable distributions there is a natural way to specify \\nthe joint distribution by 'local' pieces, which we will call a marginal \\nrepresentation. These are the marginal distributions over the cliques, and \\nare much more easily interpreted than general potential representations. \\nProposition 8.2 A distribution which is decomposable with respect to 'fJ \\ncan be written as a product of the distributions on the cliques of 'fJ divided \\nby the product of the distributions on their intersections. \\nTo prove this we need a few more concepts. A join tree T of cliques \\nis a tree with the cliques as its vertices, such that if we remove all the \\ncliques containing a vertex of V, the tree stays connected. Thus if two \\ncliques contain a common vertex v E V, so does every clique on the \\npath in the tree between them. Every triangulated graph has a join tree \\n(Beeri et al., 1983), and triangulatedness can be tested and a join tree \\nfound by the following procedure (Tarjan & Yannakakis, 1984): \\n1 Order the vertices by maximal cardinality search. Start anywhere, \\nand number next a vertex with the largest number of already \\nnumbered neighbours. \\n2 Starting with the highest numbered vertex, check for each vertex that \\nall its lower-numbered neighbours are themselves neighbours. (By \\nadding missing edges here, a triangulated graph will be produced.) \\n3 Identify all the cliques, and order them by the highest numbered \\nvertex in the clique. \\n4 Form the join tree by connecting each clique to a predecessor (in \\nthe ordering of step 3) which shares the most vertices. \\nIt can be useful to use the freedom to start anywhere in constructing \\nthe join tree, and subsequently to take any clique as its root. Incidentally, \\nsince this algorithm labels cliques by a subset of the vertices, there can \\nbe no more cliques than vertices in a triangulated graph. \\nThis construction orders the join tree so that the unique path from \\nCt to CJ has cliques in increasing order. For i ~ 2 let j(i) be the \\nnumber of the predecessor of clique i on the unique path, and let \\nSi = Ci n (Ct U · · · u Ci-d· Then Si c CJ(i), which is called the running \\nintersection property (Beeri et al., 1983). Let Hi = C1 U · · · U C-t and \\nRi = C \\\\ Si. The clique sequence is also perfect, which means that \\noRin Hi is a complete subgraph for all i ~ 2. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 271}, page_content='260 8 Belief Networks \\nProposition 8.3 The separator Si separates Ri from Hi\\\\ Si. \\nProof: Fix a path from Ri to Hi\\\\ Si, and suppose it has a vertex v \\nin RJ for some j > i but none in Uk> J Rt-. Let a and b be the first \\nvertices before and after v which are not in R1. Then a and b are \\nboth in oR1 n H1 and hence neighbours. We can construct a connected \\nsubset of the path within H1 by omitting the vertices between a and \\nb. By repeating this process we find a connected subset of the original \\npath wholly in Hi+l, and this path will have an edge from a E Ri to \\nbE Hi. We will show that bE Si. Since a and b are neighbours, they \\nboth belong to some clique Ck, and since a E Ri, k ~ i. Suppose k > i. \\nThen a, b E Ck n Hk c Cs for some s < k, and by repeating if necessary \\nwe find a,b E Cs for some s ~ i. Thus bE Ci n Hi= Si. D \\nProof of Proposition 8.2: \\nThe sets Ri form an ordered partition of V, with UJ<iRi = UJ<i Ci = \\nHi. so \\nPr{Xv} = IIPr{XR; IXRp···,XR;_1} = IIPr{XR; IXHJ \\ni i \\nThe separation property shows that XR; _jl_ XH; I Xs;, so Pr{XR; I XH;} = \\nPr{XR; I Xs;} and \\nPr{X v} =II Pr{XR; I Xs;} =II Pr{Xc;} / Pr{Xs;} (8.9) \\nknown as the set-chain and marginal representations. If any denomina\\xad\\ntor is zero, the expression is taken as zero. D \\nNote that we have not used a positivity condition here, but (8.9) pro\\xad\\nvides a potential representation. Thus for triangulated graphs we have \\na potential representation if and only if the global Markov property \\nholds, by Proposition 8.1(ii). \\nThe set-chain representation provides a minimal way to specify \\nthe joint distribution. It is not the same as specifying the transition \\nprobabilities Pr{Ci I CJ(i)} = Pr{Ri I CJ(i)} of the join tree as a Markov \\ntree, since Si = Ci n CJ(i) is strictly smaller than CJ(i)· \\nThe fill-in produced by the Tarjan-Yannakakis algorithm is fast to \\ncompute, but it may add more edges than are necessary. Algorithms \\nto produce minimal fill-ins are known (Rose et al., 1976), but are \\npotentially very slow (this is an NP-complete problem: Yannakakis, This is corollary A.8 of \\nDawid & Lauritzen \\n(1993), but their proof \\nis skeletal. \\n1981; Wen, 1990). An approximate algorithm using simulated annealing See the glossary. \\nis described by Kjcerulff (1992). '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 272}, page_content='We hope the sets are \\nsmall. \\nThis graph bas more \\nedges, so all separation \\nproperties on ~6 also \\nhold on ~; thus ~6 is \\nanI-map. 8.1 Graphical models and networks 261 \\nThe next proposition shows that we can read off conditional inde\\xad\\npendence properties from the join tree. \\nProposition 8.4 Suppose CA and CB are sets of cliques in the join tree of \\na decomposable distribution separated by Cc. Then the sets of variables \\nA = U CA and B = U CB are conditionally independent given the set \\nC = UCc. \\nProof: It will suffice to show that C separates A\\\\ C and B \\\\ C on \\nthe original graph and then use the global Markov property. We prove \\nthis by contradiction. Suppose there is a path in V \\\\ C from Yo E A to \\nYm E B via Yt, ... , Ym-1· Associate Yo with a clique Co E CA and for \\ns ~ 1 associate Ys with a clique Cs that contains both it and Ys-1; as \\nthe vertices are not in C, these cliques cannot belong to Cc. The cliques \\nC5_1 and Cs need not be neighbours in the join tree, but because they \\nhave Ys in common, all cliques on the unique path from Cs-1 to Cs \\nalso contain Ys and so are not members of Cc. In this way we can \\nassemble a path in the join tree from Co E CA to Cm avoiding the \\ncollection Cc. It is not necessarily the case that Cm E CB, but if it is \\nnot there is another clique in CB containing Ym, and we can adjoin the \\npath from Cm to that clique (which again avoids Cc ). This contradicts \\nthe separation of CA and CB by Cc, so there is no such path (y). 0 \\nThese results show that for decomposable distributions we can work \\nwith the sets of variables in the join tree, and for trees the local specifi\\xad\\ncation of the distribution is easy. Further, we can update the marginal \\ndistributions in cliques by message-passing when conditioning infor\\xad\\nmation becomes available. This makes it attractive to work not with \\na minimal I-map C§ for a distribution, but with a triangulated I-map \\nC§t. formed by triangulating C§. The distribution is decomposable with \\nrespect to C§t., and this gives us potential and marginal representations \\nand allows us to compute using the join tree of C§t.. Of course, some of \\nthe interpretation is lost, but this does not affect the calculations and \\nthe original graph may be used for interpretations. What may make \\nthis procedure unattractive is that C§t. may be very different from C§; \\nit could even be complete. \\nConditioning on evidence \\nSuppose we wish to introduce the evidence E = { Xv E Sv} which \\nis a restriction on the variable at a single vertex v. We will do so \\nby finding the marginal representation of Pr{·, E}. Then summing \\nover any clique gives us Pr{E} and we can divide by this to give the '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 273}, page_content='262 8 Belief Networks \\nmarginal representation of Pr{-I E}. Select a clique Ci that contains \\nv. Suppose we have a potential representation (c/Jc) (for example, the \\nset-chain representation). By setting cpc(xc) = 0 whenever Xv rf. Sv we \\nobtain a potential representation of Pr { ·, E}, and we employ the general \\nprocedure below to turn a potential representation into a marginal one. \\nSuppose we have a potential representation (c/Jc). We can allow a \\nlittle more generality by having functions lps on the separators S and \\nasking that \\nPr{Xv = xv} =II c/Jc(xc) j II lps(xs) (8.10) \\nc s \\nwhere 1/0 is taken as 0 (and so if lps(xs) = 0 we can adjust cpc(xc) to \\nbe zero). For any neighbouring pair of cliques Ci. Cj with S = Ci n Cj \\nconsider the operation of replacing lps by the marginal lp~ of c/Jc; \\n(formed by summing over the variables in Ci \\\\ S) and replacing c/Jci \\nby c/Jci x lJ)s/lJ)s. This step maintains a potential representation. We \\ndescribe lp~ /lJ)s as the message passed over the edge of the join tree. \\n(When conditioning a marginal representation, this can be thought of \\nas finding the revised marginal distribution of S, and adjusting the \\nmarginal distribution of the adjoining clique to Pr{ Cj I S}Pr{ S}.) \\nNow suppose this step is performed for multiple conditioning events \\nand each edge in the join tree until no further progress can be made. \\nThe steps can be organized in many ways (Jensen et al., 1990; Shenoy \\n& Shafer, 1990; Dawid, 1992): an attractive order is to pass messages \\nin to the root and then out to the leaves. When this has been done, \\nwe will have the marginal representation. (Formal proofs are given by \\nDawid, 1992.) \\nBy replacing averaging by maximizing in forming lp~ we can find the \\nmost plausible explanation just as for Markov trees (Dawid, 1992). A \\nmodified averaging (omitting evidence on S ) computes the distribution \\nat each variable conditional on the evidence everywhere else (Cowell & \\nDawid, 1992) which is useful in monitoring consistency of information \\n(Spiegelhalter et al., 1993). \\n8.2 Causal networks \\nWe now turn to causal networks, which are defined on DAGs (directed \\ngraphs without cycles). Such graphs can always be numbered so that \\nthe parent(s) of a vertex have a lower number than the vertex itself. \\n(This is called a topological sort of the DAG.) It is always true (for '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 274}, page_content='To avoid any \\nmeasure-theoretic \\ndifficulties on the \\nexistence of conditional \\nprobabilities, we will \\nformally define \\nrecursive to mean \\nXv JLXa<v I pa(v). \\nPearl (1986, p. 245) \\nincorrectly claims \\nuniqueness. 8.2 Causal networks 263 \\ndiscrete random variables) that \\nPr{Xv} =II Pr{Xv I Xa,a < v} (8.11) \\nv \\nbut to relate the distribution to the graph we ask that for a recursive \\nmodel \\nPr{X v} =II Pr{Xv I X a, a a parent of v }. (8.12) \\nv \\n(We will use pa(v) as a shorthand for this condition.) By summing over \\nXv in reverse order of the vertices we find from (8.12) that \\nPr{Xv, v ~ j} =II Pr{Xv I pa(v)} \\nv~j \\nand so being recursive is equivalent to \\nPr{Xv IXa,a < v} = Pr{Xv lpa(v)}. \\nConversely, if we are given the conditional probabilities of each variable \\ngiven its parents, (8.12) can be used to define the joint distribution. This \\nis the main attraction of a causal representation, as the conditional \\ndistributions given the parents are often easier to supply than clique \\npotentials. \\nGiven a distribution on a set of vertices, we can ask for what \\nDAGs it is a recursive model. It is certainly recursive for the DAG \\nwhich makes each vertex a child of all lower-numbered vertices, from \\n(8.11), and we can form a smaller DAG by declaring as parents of \\nvertex v only a subset of earlier vertices on which Pr{Xv I X a, a < v} \\nactually depends. If there are zeroes in the probability distribution \\nthen this procedure may not be unique. Consider the counter-example \\nto a potential representation on page 249 in order (a, b, c, d). We \\nfind a to be the parent of b, (a, b) to be the parents of c, but for \\nd we have the choice of any two from (a, b, c). If the distribution is \\nstrictly positive we can select as parents those vertices b for which \\nXv ~ XbiXa, a =/= b, a < v (using the equivalence of pairwise and local \\nMarkov on {a: a~ v} ). \\nNote that this construction of a DAG depends on the ordering of \\nthe vertices, whereas the notion of a recursive model does not: some \\norderings may produce much simpler DAGs than others. It is here that \\ncausality is used to select a beneficial ordering. (Todd, 1995, illustrates \\nthis for the example of Lauritzen & Spiegelhalter, 1988, by choosing a \\ndifferent set of causalities.) '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 275}, page_content=\"264 8 Belief Networks \\na \\n2 \\nTo make use of the machinery for Markov networks (particularly \\non decomposable graphs) we would like to turn a recursive model \\non a DAG into a Markov network. It does not in general suffice \\nto use the graph formed by dropping directions; Figure 8.6 will help \\nillustrate why. Suppose all the variables are binary, with Pr{Xa = 1} = \\nPr{Xb = 1} = 0.3 + 0.4Xt, Pr{X2 = 1} = 0.3 + 0.4/(Xa = Xb) and \\nPr{X3 = 1} = 0.3 + 0.4X2. Then Xa _jl_ xb I xl, but Xa .t xb I X!,X2, \\nwhereas on the undirected graph {1,2} separates a and b. Clearly two \\nvertices that have a common child need to be joined in the undirected \\ngraph. This is reflected in the idea of the moral graph (Lauritzen \\n& Spiegelhalter, 1988). The steps to convert the DAG to its moral \\n(undirected) graph are \\n1 replace all the directed edges by undirected ones and \\n2 add edges joining the parents (in the DAG) of each vertex if neces-\\nsary. \\nThe neighbours of a vertex in the moral graph are its parents and \\nchildren in the original DAG, plus the other parents of those children. \\n(Pearl calls the other parents of a vertex's children its mates.) \\nProposition 8.5 A recursive model on a DAG is global Markov and has \\na potential representation on the moral graph of its DAG. \\nProof: We will show the existence of a potential representation; by \\nProposition 8.1(ii) the distribution must be global Markov. \\nStart by setting the potential for each clique to one. For each vertex \\nv, select a clique which contains it and all its parents and multiply its \\npotential by Pr{Xv I Xa, a a parent of v }. (There may be more than Figure 8.6: An example \\nto illustrate conditional \\nindependence on a \\nDAG. \\nThere must be such a \\nclique: the graph is \\nmoral. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 276}, page_content='This subsection is \\ntechnical and not used \\nelsewhere. \\nSpirtes et a/. (1993) call \\nthis causal Markov. We \\ncan replace nd( v) by \\nnd(v) \\\\ pa(v) to give \\ndisjoint sets of \\nvariables. 8.2 Causal networks 265 \\none choice of clique, in which case any will do.) This converts (8.12) to \\na potential representation by grouping terms into cliques. D \\nNote once again that no positivity condition is required. \\nThe moral graph is then triangulated by filling in, if necessary, and \\nconverted to a join tree of its cliques, to which the methods of Markov \\ntrees can be applied. \\nBy Proposition 8.2 the joint distribution can be specified by giving \\nthe marginal probability distributions for each clique. Originally we had \\n(8.12), which may seem to be the easiest way to specify the joint distri\\xad\\nbution. However, having the clique marginals allows us to specify the \\nmarginal distribution of each random variable. Further, the distribution \\nwill change as information becomes available. Consider Figure 8.2 on \\npage 247. Initially the distribution can be specified conditionally on the \\ntrue class. However, once some of the variables and/or the reported \\nclass are known, we will wish to find the conditional probability of \\nthe true class. In this case the moral graph comes from dropping the \\narrows, and any ordering that has the true class first or second may \\nbe used. The cliques are all the edges individually, and can be in any \\norder. In this case the DAG is already a tree and can be used directly. \\nMarkov properties on DAGs \\nWe have seen that a recursive model induces a global Markov dis\\xad\\ntribution on the moral graph. Because there can be some in-filling \\nat moralization, the conditional independencies implied by the moral \\ngraph may not include all those implied by the original recursive model. \\nConsider Figure 8.6; its moral graph is the same as that of the DAG \\nwith a link from a to b, and on the latter DAG Xa ..fl.-Xb I X1 is \\nallowed. Of course, the conditional independencies still hold but are \\nencoded in the numerical values of the particular representation used. \\nIn-filling to triangulate the moral graph can produce further mask\\xad\\ning of conditional independence properties; this is tolerated for the \\nsimplification in computation that results. \\nThis has aroused interest in looking directly at Markov properties \\non a DAG. Several definitions of directed Markov are in use. That of \\nDawid & Lauritzen (1993) is \\nXv .JL nd(v) I pa(v) for all v E V \\nwhere nd( v) is the set of variables whose vertices are not descendants \\nof v. (This is called directed local Markov by Lauritzen et al., 1990.) \\nThe definition of Lauritzen (1989) and Lauritzen et al. (1990) (directed '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 277}, page_content='266 8 Belief Networks \\nglobal Markov) is that XA JL XB I Xc whenever C separates A from \\nB in the moral graph of the smallest ancestral DAG containing A, \\nB and C. This resolves the difficulty with Figure 8.6. We know that \\nXa JL Xb I X1, but a and b are joined in the moral graph. They are \\nnot however joined in the moral graph of the subgraph on vertices \\n{a,b, 1}, which is ancestral. That we have to take an ancestral subgraph \\nis shown by noting that Xa _j._ Xb I X3; the dependence arises through \\nthe ancestor X2 of X3. \\nLauritzen et al. (1990) show that their two definitions of directed \\nMarkov are equivalent to each other and to being a recursive model. It \\nis obvious that directed global Markov implies directed Markov which \\nimplies recursive. For any moralized ancestral DAG (8.12) provides \\na potential factorization, which by Proposition 8.l(ii) implies the dis\\xad\\ntribution is global Markov on the moral graph hence XA JL XB I Xc. \\nAnother proof is given in Propositions 8.6 and 8. 7 which also applies \\nto continuous random variables (without assuming a joint density). \\nPearl (1986, 1988) finds conditional independence properties via a \\nmore complicated notion of separation for DAGs, called d-separation. \\nA DAG is said to be an I-map of a distribution if XA JL XB I Xc \\nwhenever C d-separates A and B. To define d-separation, consider a \\ntrail from A to B on the DAG, following edges in either direction. At \\neach vertex inside the trail there will be two arrows. A trail is blocked \\nby C at a vertex v if the two edges at v either \\n(i) do not have converging arrows and v E C, or \\n(ii) do have converging arrows and neither v nor any of its descendants \\nare in C, \\nand A and B are said to be d-separated by C if every trail from A \\nto B is blocked by C at some internal vertex. Consider once again \\nthe example of Figure 8.6. Then Xa JL Xb I Xt follows from the d\\xad\\nseparation of {a} and {b} by {1}, but {a} and {b} are not d-separated \\nby {1, 2} or {1, 3}, as the trail a-2-b is no longer blocked. \\nThe d-separation condition for independence (empty C) is that A \\nand B have disjoint ancestral sets, which is as we would expect. \\nFor a DAG to be an I-map for a distribution is equivalent to the \\ndistribution being directed global Markov by the following proposition, \\nand so is equivalent to being a recursive model. \\nProposition 8.6 (Lauritzen et al., 1990) \\nFor sets A, B and C of vertices, d-separation of A and B by C is \\nequivalent to the separation of A and B by C in the moral graph of the \\nsubgraph of ancestors of A U B U C. Kiiveri et al. (1984) \\nhave other definitions \\nof local and global \\nMarkov properties on a \\nDAG. \\nSpirtes et al. (1993) call \\na vertex with \\nconverging arrows a \\ncollider. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 278}, page_content=\"8.2 Causal networks 267 \\nProof: Let d be the moralized ancestral subgraph . Suppose A and B \\nare separated by C in d. We will show that A and B are d-separated \\nby C. Consider a trail from A to B in the DAG. \\nFirst we show that any trail which is not wholly in d is blocked. \\nFollow the trail from A, and let a be the first vertex not in d. By the \\nancestral property, the arrow on the edge leaving d must point to a. \\nOnly descendants of a can be reached by continuing the trail without \\nconverging arrows, and these cannot be in d (or their ancestor a \\nwould also be). To reach B the trail must have a pair of converging \\narrows at a vertex outside d, and will be blocked at that vertex since \\nit is not an ancestor of any vertex in C (as all such vertices are in d). \\nSecond, consider a trail from A to B on the DAG with all vertices \\nin d. If this contains a vertex with converging arrows, the predecessor \\nand successor will have been 'married' in the moral graph, and so by \\nreplacing the converging arrows by the added edge we can find a path \\nin the moral graph d which avoids vertices with converging arrows. \\nThis path must contain a vertex from C, and hence the trail on the \\nDAG must be blocked at a vertex without converging arrows. \\nConversely, suppose A and B are not separated by C in d. Take \\na path from A to B which does not meet C, and form a trail on the \\nDAG by replacing any edges formed at moralization by a diversion \\nto their common child (which is in d). Suppose this trail is blocked; \\nthis can only happen at a vertex a with converging arrows and no \\ndescendants in C. Since the vertex is in d, it must have a descendant \\nin either A or B. Thus we can follow the trail from A to a and then \\nfollow its descendants to B, or from B to a and then via descendants \\nto A. This gives us a trail with strictly fewer blocking vertices. By \\nrepeating the process we can find an unblocked trail in the DAG from \\nA to B. 0 \\nThese results show that for a recursive model on a DAG we can \\nread off conditional independence properties by d-separation in the \\nDAG or separation in the ancestral moral graph. Can all conditional \\nindependencies of sets of variables be found in this way? The general \\nanswer must be 'no', as conditional independencies could be encoded \\nin the numerical values of the distribution, but Geiger & Pearl (1990) \\nconstruct a recursive model for which any sets of variables which are \\nnot d-separated are not conditionally independent. Thus no other \\nconditional independencies can be found from the DAG alone. If \\nwe know that some relations between variables are deterministic (occur \\nwith probability one) we can extract more information by D-separation, \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 279}, page_content=\"268 8 Belief Networks \\nwhich differs from d-separation by also blocking a trail at a vertex \\nwhose variable is determined by Xc (Geiger et al., 1990). \\nProposition 8.7 (Verma & Pearl, 1990) \\nIf a distribution is a recursive model on a DAG, that DAG is an !-map \\nfor the distribution. \\nProof: The proof is by induction on the size of the DAG ~ : the result \\nis trivial if the DAG has only one element. Let w be the last vertex in \\nthe DAG (and hence has no children), and suppose the result is true \\nfor the subgraph ~' omitting w. Our distribution is still recursive on \\n~'. Let A, B, C be disjoint sets of vertices such that A and B are \\nd-separated by C in ~; we will show A .JL B 1 C. Any trail with w \\nas an internal vertex necessarily has converging arrows at w. Consider \\nthree exhaustive cases. \\n(a) The vertex w f. AU B U C. Every trail including w is blocked, so \\nA and B are d-separated by C in ~' and A .JL B I C. \\n(b) Suppose that vertex w is in either A or B; we will assume w E A, \\nand let A' = A \\\\ { w }. Note that A' is d-separated from B by C in \\n~'. Now w has no parents in B (or there would be a one-step trail \\nfrom A to B). Let P be the set of parents of w which are not in \\nC; P is also d-separated from B by C in ~'. (Any trail from P to \\nB either goes through w and is blocked, or can be extended to a trail \\nfrom w to B which is not blocked at the added internal vertex in P.) \\nThus A' U P .JL B I C and for a recursive model { w} .JL B I A' U C U P \\n(since the conditioning set includes all the parents of w ). These imply \\nAU P .JL B I C (see (A.4)) hence A .JL B I C. \\n(c) We have w E C, so no trail can be blocked at w. It follows that A \\nand B are d-separated by C' = C \\\\ { w}. Now w must be d-separated \\nfrom either A or B by C' or there would be an unblocked trail from \\nA to B via w. Suppose this is true for B, so A u { w} and B are \\nd-separated by C'. We have AU{w} .JL B I C' by case (b), which implies \\nA .JL B I C' U {w} = C (see (A.3)). D \\nCalculations on moral graphs \\nThere are several calculations we may wish to perform on the joint \\ndistribution, but the most common are to condition on observed vari\\xad\\nables and to marginalize , especially to find the distribution over the true \\nclass. As we have seen, it will be normal to start with the conditional \\nprobability tables (8.12), which give a potential representation (see the \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 280}, page_content=\"8.2 Causal networks 269 \\nproof of Proposition 8.5). This may be converted to a marginal repre\\xad\\nsentation by the standard message-passing algorithm (page 261), which \\nalso allows us to condition a marginal representation on evidence of \\nthe form E = n{xv E Sv}. \\nIn general finding a marginal distribution of a subset of random \\nvariables is difficult, but it is immediate from the marginal represen\\xad\\ntation if the subset is contained in some clique. Fortunately we will \\nmostly be interested in marginals of a single variable, and then will \\nhave at least one clique to choose from. There is a trick (Pearl, 1988, \\n§3.5.3; Jensen, 1991) to find the probability of a specific event of the \\nform E = n{ Xv E Sv }, as the conditioning procedure in the join tree on \\nE without normalization finds Pr{', E}, and this can be summed over \\nany clique to find Pr{E}. (This is done in the example which follows.) \\nAn example \\nWe give an entirely fictitious example from medical diagnosis with eight \\nvertices presented in Figure 8. 7 and Table 8.2. The specification of the \\ndistribution via the conditional probability tables is \\np(A) p(B) p(C) p(DIA,B) p(EIA) p(FIC) p(GID,E) p(HID,F). \\nTo form the moral graph (Figure 8.8) we have to join a to b, d to \\ne and d to f; this is already a triangulated graph. The cliques are \\nabd, ade, cf, deg and dfh. One ordering by maximum cardinality \\nsearch, starting from a, for the vertices is abdegfhc and for the cliques \\nis C1 = abd, C2 = ade, C3 = deg, C4 = dfh, Cs = cf. The separators are \\nthen S2 = ad, S3 = de, S4 = d, Ss = f and so the marginal representation \\nlS \\np(A,B,D) p(A,D,E) p(D,E, G) p(D,F,H) p(C, F) \\np(A, D) p(D, E) p(D) p(F) \\nIn forming the join tree we have considerable freedom, as C4 can \\nbe linked to any of its predecessors. One choice (joining C4 to C3 ) \\nwould make the join tree into a chain, but we chose the tree shown in \\nFigure 8.9. \\nThe marginal probabilities may be found from p( CJ) = \\np(D I A, B)p(A)p(B), p(C2) = p(E I A)p(S2), p(C3) = p(G I D, E)p(S3), \\np(Cs) = p(F I C)p(C) and p(C4) = p(H I F,D)p(Ss)p(S4). We find, writ\\xad\\ning tables in lexicographic order (false before true, last index varies \\nfastest). \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 281}, page_content='270 8 Belief Networks \\nSmoker Unfit Figure 8.7: The DAG \\na for an artificial medical \\nBreathless diagnosis problem. \\ng \\nFamily history \\nof heart disease \\nb \\nChest pains \\nPoor diet Indigestion \\n~ \\nf \\nPr{A} = 0.5 Pr{B} = 0.2 Pr{C} = 0.3 \\nTable 8.2: The Pr{ D IF, F} = 0.05 Pr{D IF, T} = 0.3 Pr{D I T,F} = 0.2 Pr{D IT, T} = 0.5 conditional probability Pr{E IF}= 0.3 Pr{E IT} = 0.5 tables for the DAG of \\nPr{F IF} = 0.1 Pr{F IT} = 0.4 Figure 8.7. For each \\nPr{ G IF, F} = 0.01 Pr{ G IF, T} = 0.5 Pr{ G I D = T} = 1 vertex the condition is \\nPr{H I F,F} = 0 Pr{H IF, T} = 1 Pr{H IT, F} = 0.5 Pr{H IT, T} = 1 on its parents in \\nalphabetical order. \\nSmoker Unfit Figure 8.8: The moral \\na graph associated with \\nBreathless Figure 8.7. \\ng \\nFamily history \\nof heart disease \\nb \\nChest pains \\nh \\nPoor diet Indigestion \\nf \\nabd ad ade de Figure 8.9: The clique \\nCi d c; tree associated with \\n<:; Figure 8.8. The \\nseparators S; are \\nd.fh marked on the edges. \\nC4 '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 282}, page_content='8.2 Causal networks 271 \\nABO 0.38 0.02 0.07 0.03 0.32 0.08 0.05 0.05 \\nAD 0.45 0.05 0.37 0.13 \\nADE 0.315 0.135 0.035 0.015 0.185 0.185 0.065 0.065 \\nDE 0.50 0.32 0.10 0.08 \\nDEG 0.495 0.005 0.16 0.16 0 0.1 0 0.08 \\nD 0.82 0.18 \\nCF 0.63 0.07 0.18 0.12 \\nF 0.81 0.19 \\nDFH 0.6642 0 0 0.1558 0.0729 0.0729 0 0.0342 \\nNote the treatment of clique C5 = cf; we need the marginal for f, and \\nthis demands that we process C5 before C4. It is natural to think of \\nC4 depending on C5, but to produce a tree we have to label the edge \\nin the opposite direction. \\nWe can also find the initial marginal probabilities via a potential rep\\xad\\nresentation and message-passing . Suppose we take initial potentials as \\np(A)p(B)p(D I A, B), p(E I A), p(G I D, E), p(H I F,D) and p(F I C)p(C). \\nMessage-passing then multiplies <Pc2 by p(A, D), <Pc3 by p(D, E) and \\n<f>c4 by p(D) and by p(F) to form the marginal representation. \\nSuppose a patient presents symptoms of breathlessness and chest \\npains, and is a smoker. What is the probability of heart disease? We \\ncondition on the evidence A = G = H = T. We iilustrate the message\\xad\\npassing approach by sending messages to C1 = abd at the root of the \\ntree. We enter G = H = T at cliques C3 and C4. For A = T we have a \\nchoice of cliques, and choose Ct. We start at C4 with a message over \\nS4 = d to C1, the new-to-old ratio of the separator distributions. \\nDFH 0 0 0 0.1558 0 0.0729 0 0.0342 \\nD 0 .1558 0.1071 \\nmsg 0.1558/0.82 = 0.19 0.1071/0.18 = 0.595 \\nClique C3 sends a message over S3 = de to C2 : \\nDEG 0 0.005 0 0.16 0 0.1 0 0.08 \\nDE 0.005 0.16 0.1 0.08 \\nmsg 0.005/0.5 = 0.01 0.16/0.32 = 0.5 1 \\nThis is then incorporated into clique c2 and a message sent over \\nS2 =ad to Ct: \\nADE 0.00315 0.0675 0.035 0.015 0.00185 0.0925 0.065 0.065 \\nAD 0.07065 0.05 0.09435 0.13 \\nmsg 0.157 1 0.255 1 '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 283}, page_content=\"272 8 Belief Networks \\nClique C1 then incorporates two messages and its own constraint. We \\nneed only give the results for A = T : \\nBD 0.32 · 0.19 · 0.255 0.08 · 0.595 · 1 0.05 . 0.19 . 0.255 0.05 . 0.595. 1 \\n0.015504 0.0476 0.002423 0.02975 \\nD 0.017927 0.07735 \\nso Pr{Evidence} = 0.095277 and Pr{D I Evidence}= 0.812. \\nAs C 1 is the root, we should then pass messages back down the \\ntree, but our question has already been answered from the marginal in \\nC1. It will be convenient at this stage to normalize, that is to divide the \\nmarginal distribution C1 by Pr{Evidence}. The messages sent to C2 \\nand C4 are then approximately \\nAD 0 0 0.188/0.09435 0.812/0.13 \\nD 0.188/0.1558 0.812/0.1071 \\nThis modifies the clique marginals to (approximately) \\nADE 0 0 0 0 0.0037 0.1843 0.406 0.406 \\nDFH 0 0 0 0.188 0 0.553 0 0.259 \\nThese send messages to C3 and Cs of \\nDE 0.00037/0.005 0.1843/0.16 0.406/0.1 0.406/0.08 \\nF 0.553/0.81 0.447/0.19 \\nand those cliques can be updated to \\nDEG 0 0.00037 0 0.1843 0 0.406 0 0.406 \\nCF 0.430 0.165 0.123 0.282 \\nAfter these calculations it is often easy to answer further questions. \\nSuppose we discover that the patient's family has a history of heart \\ndisease. This amounts to new . evidence that B = T. Rather than go \\nthrough the full procedure of propagating messages, we can just examine \\nthe marginal distribution of C1 to see that the conditional probability \\nof heart disease is now 0.02975/(0.002423 + 0.02975) ~ 0.925. Similar \\ncalculations will often allow us to evaluate the value of 'buying' various \\nitems of new evidence, and so decide which to obtain (Lauritzen & \\nSpiegelhalter, 1988, §5.5; Jensen & Liang, 1995). \\nMixed models \\nLauritzen (1989, 1992) and Olesen (1993) consider analogues of these \\nprocedures for conditional Gaussian distributions (page 41). The dis\\xad\\ntinction between discrete and continuous variables leads to consider\\xad\\nation of marked graphs (with the marks differentiating discrete and \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 284}, page_content=\"The connection to the \\nGibbs sampler was \\nmade by Hrycej (1990), \\nChavez & \\nCooper (1990), \\nShachter & Peot (1990), \\nand, more elegantly, by \\nYork (1992). \\nThis too has been \\nconsidered in genetics: \\nOtt (1989), Ploughman \\n& Boehnke (1990) and \\nKong (1991). 8.2 Causal networks 273 \\ncontinuous random variables) and stronger concepts of decomposabil\\xad\\nity and moralization. \\nThere is a considerable literature about multivariate normal belief \\nnets, but for pattern recognition we need at least one discrete variable, \\nthe true class. \\nSimulation-based calculations \\nThe calculations of the previous subsection work well for sparsely\\xad\\nconnected graphs with discrete random variables which take a small \\nnumber of values. The approach can be extended to conditional Gaus\\xad\\nsian distributions (page 41) when the evidence on continuous variables \\nis a precise value (Gammerman et al., 1995), but most attempted ex\\xad\\ntensions run into overwhelming computational complexity. \\nHenrion (1988) (and Henrion et al., 1991) used a stochastic simu\\xad\\nlation method he called (probabilistic) logic sampling. This uses (8.12) \\nfor unconditional simulation of the whole collection of random vari\\xad\\nables. This is easy; we move through the DAG in vertex order, at each \\nstage sampling conditionally on the values at the parents of the current \\nvertex (and these values must be already known). To condition, run \\nmany simulations and only keep those which are consistent with the \\nconditions, using these to compute frequencies of any events of interest. \\nProvided the evidence has positive probability this will work, although \\nif the probability of the evidence is low, only a small proportion of the \\nruns will be retained, and so the process may be slow. If we need to \\ncondition on point values of continuous variables, this approach will \\nbe impossible (or if we replace the point value with a small interval, \\npossible but impracticably slow). \\nPearl (1987) suggested the use of iterative simulation methods such \\nas the Gibbs sampler (see Section A.3). This approach had previously \\nbeen used in image analysis and has since become popular in main\\xad\\nstream Bayesian statistics. These methods are now often called MCMC \\nfor 'Markov chain Monte Carlo'. We have a collection of random \\nvariables on a graph; as in the join tree each random variable might \\nitself be a collection. For the Gibbs sampler we pick a vertex, and \\nsample from Xv conditional on all the random variables at all the \\nother vertices. For a local Markov distribution (to which we restrict \\nattention) the conditional distribution depends only on the values of \\nthe random variables at neighbouring vertices. (We have already seen \\nthat the neighbours in the moral graph consist of the parents, children, \\nand other parents of those children.) If the graph is sparsely connected, \\nthis may be a small enough set for the conditional simulation to be \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 285}, page_content=\"274 8 Belief Networks \\nperformed quite easily. To actually run the Gibbs sampler, vertices are \\npicked sequentially, either at random or in some pre-assigned order. \\nConditioning is easy for the type of evidence we are considering; \\neach random variable is simulated conditionally on the event(s) con\\xad\\ncerning it. \\nThis seems a very attractive method. If we want some aspect of \\nthe distribution of a subset of random variables, all we need to do is \\nto simulate the whole set, and compute the frequency of the desired \\nevent(s). By taking a large enough sample we can compute the desired \\nprobabilities to any accuracy required. The difficulties are \\n• we have to run the Gibbs sampler long enough to ensure that we \\nhave a close enough approximation to the asymptotic distribution, \\nand \\n• we need independent or close-to-independent samples from the \\ndistribution, which we can achieve by taking samples sufficiently far \\napart in the run of the Gibbs sampler. \\nThese can be achieved, but the convergence can be extremely slow; \\nRipley & Kirkland (1990) give a dramatic example in which equilibrium \\nhas not been approached after each random variable in the system has \\nbeen sampled 10,000 times. In fact the second point is unnecessary in \\nour application if we just count to obtain the frequency of events, since \\nwe are estimating an expectation, and expectations are additive. Thus \\nN \\n~ LI[X~) E E] \\nt=l \\nwill be an unbiased estimator of Pr{ E} provided the process is started \\nin the equilibrium state (by a sample from the correct distribution). \\nHowever, it remains true that the estimator will be very variable unless \\nthe process has been run many times longer than the interval between \\napproximately independent samples, and that we will have to discard \\nan initial part of the run to combat the first point. \\nThe Gibbs sampler step applied to the moral graph obtained from \\na recursive model needs a method to sample from the distribution at \\na vertex conditional on the parents, children and 'mates' of the vertex, \\nplus any evidence restriction on the random variable. It is easy to see \\nthat the conditional density (ignoring any evidence) is of the form \\np(Xv I Xa{v}) oc p(Xv I X a, a a parent of v) \\nX IT p(Xc I Xa, a a parent of c). (8.13) \\nchildren c For this problem it is \\nknown that there are \\nbetter MCMC samplers \\nthan Gibbs (Peskun, \\n1973), but the simplicity \\nof Gibbs sampling may \\nprevail. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 286}, page_content='8.3 Learning the network structure 275 \\n(Pearl, 1988, p. 218, gives a formal proof, but this is immediate.) This can \\nbe an awkward distribution to sample by standard rejection sampling \\n(Ripley, 1987), but it is easy to sample from the first term, then use \\nrejection sampling on each of the child terms separately. (Gammerman \\net al., 1995, derive this by a very indirect route involving auxiliary \\nvariables; Besag & Green, 1993.) If evidence is involved at the node \\nthis can be incorporated by conditioning in the simulation given the \\nparents. \\nAn attraction of the Gibbs sampler to computer scientists is its \\nintrinsic parallelism. We can update the sample at vertices which are not \\nneighbours simultaneously; samples can also be updated by different \\nprocessors asynchronously, at least if we ensure that messages are \\nreceived from neighbours before processing begins. There are message\\xad\\npassing policies which prevent simultaneous updating of neighbours \\nwithout needing a central controller; some are sketched in Pearl (1988, \\npp. 219-222). \\nSimulated annealing (Ripley, 1987; Aarts & Korst, 1989) can be \\nused to find the most plausible explanation (the most probable combi\\xad\\nnation of other variables given those observed) but is likely to be very \\nslow to do so. (This method was suggested by Hrycej, 1992, Theorem \\n11.2.3.) \\nThere is no reason why Gibbs sampling should be applied to a \\nsingle variable at a time, and blocked variants have been explored \\nin image analysis, in which a group of variables is sampled given \\nits neighbours; the idea being that although sampling may be more \\ndifficult, the process may traverse the sample space more rapidly and \\nso converge to equilibrium faster. Choosing suitably sized groups of \\nvariables is a black art at present. \\n8.3 Learning the network structure \\nSo far we have assumed that the graph or DAG was given by the \\nexperts. Is there any hope of inducing the network from examples? \\nSeveral approaches have been pursued. \\nFrom the traditional statistical viewpoint an undirected graphical \\nmodel represents a restriction of a full dependence, for example (and \\nmost commonly) a log-linear model for a contingency table. The model\\xad\\nbuilding strategy is to move around the space of possible models, and \\neventually to select a small number of simplified models which are \\ncomplex enough to explain the patterns in the examples, yet have '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 287}, page_content='276 8 Belief Networks \\ncomprehensible interpretations. (It is this last point which often leads \\nto a consideration of only subsets of graphical models; Edwards & \\nHavarnek, 1985; Upton, 1991.) The strategy can involve either a series \\nof hypothesis tests or searching to minimize a penalized fit criterion such \\nas AI C. (Lauritzen et al., 1994, give a case study of both for the CHILD \\nnetwork of Spiegelhalter et al., 1993.) This approach usually involves \\na great deal of user control, although it is beginning to be partially \\nautomated. Since interpretability is paramount, a domain-expert will \\nalways be needed to monitor the process. \\nA variant on the testing approach is to use a series of tests for \\nconditional independence to find the Markov boundary of each vertex \\n(those vertices on which Pr{ Xv I X V\\\\{v}} depends), and then construct \\na graph with respect to which the distribution is local Markov (Fung \\n& Crawford , 1990). There will be very many tests, some of which will \\ngive the wrong answer by chance. One way to combat this problem of \\nmultiple comparisons (suggested by Fung & Crawford, 1990) is to use \\nthe significance level a of the tests as a parameter of the procedure, and \\nselect it by cross-validating a relevant measure of performance. Many \\nfewer tests would be needed to establish a graph with respect to which \\nthe distribution is pairwise Markov, since we need only test for each \\npair of vertices. Each test will be a test of conditional independence in a \\nmany-way contingency table, so there will difficulties with sparse tables \\nunless the training set is large. The Fung-Crawford approach has the \\nadvantage of allowing small sets of neighbours to be considered first. \\nAnother tradition , from the social sciences, for inferring a causal \\nnetwork from data (often in a regression setting) is represented by \\nGlymour et al. (1987) and Spirtes et al. (1993). Finding a DAG makes \\nthe problem considerably harder, although it does allow the specifi\\xad\\ncation of smaller conditional probability tables, and the deduction of \\ncausal relationships. An edge between a and b will be absent in the \\nDAG if there is a set S such that X a ...JL Xb I Xs, so a search over subsets \\nis needed. Since we know that the undirected version of the DAG is \\na subgraph of the moral graph, finding a graph with respect to which \\nthe distribution is local Markov provides a good starting point. The \\nmain result of Spirtes et al. (1993), their Theorem 3.4, applies only to \\na subclass of possible distributions (those with some DAG as a perfect \\nmap) and for the example on page 249 their procedures induce a graph \\nwith no edges. (They make no attempt to check their condition in \\ntheir examples, but it would be straightforward to check that the data \\ndistribution is recursive on the induced DAG.) There are proposals to \\nuse these methods in \\ninsurance, where a \\ndatabase of millions of \\ncases is available. \\nIn the regression setting \\nthis is called path \\nanalysis. \\nIf a follows b in the \\norder then we can take \\nS to be the parents of \\na. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 288}, page_content=\"8.3 Learning the network structure 277 \\nMarkov trees \\nChow & Liu (1968) (see also Pearl, 1988, Chapter 8) extend the \\ncontingency-table approach by seeking the belief tree that best approx\\xad\\nimates (in Kullback-Leibler divergence) a general multi-way discrete \\ndistribution. They show that: \\n• given the tree topology, the conditional distribution of each node \\ngiven its parent in the best approximation is that in the original \\ndistribution, and \\n• the best tree is any minimal spanning tree with distance between a \\nand b given by the mutual information of (Xa,Xb), \\nSince there are efficient algorithms for finding minimum spanning trees \\n(Cormen et al., 1990, Chapter 24; Sedgewick, 1990, Chapter 31) this \\nallows a best approximating Markov tree to be constructed. \\nThe proof of these two properties is easy. We assume for simplicity \\nthat we have discrete random variables. Given a tree, let p(i) denote \\nthe parent of vertex i (empty for the root). Any recursive model P' \\ncan be expressed as \\nP'{X} =II P'{XiiXp(i)} \\ni \\nfrom (8.12) on specializing to a tree. Now consider the Kullback-Leibler \\ndivergence between the true distribution P and a recursive model P' : \\nd(P,P') = I:P{xv}log [P{xv}/P'{xv}] \\nxv \\n= I: P{xv} logP{xv}- I:P{xv} I: logP'{xi I Xp(i)} \\nxv xv i \\nxv j X;, Xp(i) \\n= I:P{xv}logP{xv} \\nxv \\n-I: I: P { Xp(i)} [I: P {xi I Xp(i)} log P' {Xi I Xp(i)} J. \\ni Xp(i) X; \\nThis can be minimized by maximizing each term in square brackets; \\nthe maximum occurs at P'{xi I Xp(i)} = P{xi I Xp(i)} and the minimized \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 289}, page_content=\"278 8 Belief Networks \\ndivergence is \\nd(P,P') = LP{xv}logP{xv}- LP{xv} L::logP{x;IXp(i)} \\nxv xv i \\n= Ll(X;,Xp(i)) + LP{xv}logP{xv}- LLP{xi}logP{x;}. \\ni XV i Xi \\nA tree can always be turned into a causal graph by choosing a root \\nand directing edges away from the root. This can be convenient in \\nspecifying the conditional probabilities, especially if the root is chosen \\njudiciously. However, if we are inducing structure from data, the \\nconditional probabilities will normally be estimated from the same \\ndataset. In that case the unknown true distribution is replaced by \\nfrequencies in the training set, and the simplest way to specify the \\nMarkov tree is via a marginal representation , with Pr{Xa, Xb} for \\nadjacent vertices estimated by the frequency in the training set. (The \\nconsistency conditions are automatically satisfied.) This can also be seen \\nas fitting an unrestricted Markov tree to the training set by maximum \\nlikelihood . \\nPriors for belief networks \\nThe full Bayesian procedure is to put a prior on the topology of belief \\nnetworks, and then average the results over belief networks weighted \\nby their posterior probabilities. This principle has been pursued by \\nCooper & Herskovits (1992), in tandem with learning the conditional \\nprobability tables from data. There is an enormous number of different \\nnetworks; for example Cooper & Herskovits (1992, p. 319) quote \\napproximately 4.2 x 1018 for ten vertices. Cooper & Herskovits make \\nsome very strong assumptions on the prior on topologies (such as a \\nuniform distribution) to simplify computation. All such assumptions \\nare unrealistic, as considering that set of vertices implies a belief in a \\nsparsely connected network. (However, the prior may be swamped by \\nthe data and so be practically irrelevant.) My own prior might often \\nbe approximated by one on the number of edges. \\nThe full Bayesian approach is normally computationally impossible \\nas we cannot average over all topologies. Full averaging can be replaced \\nby averaging over the few most plausible topologies , maybe even just the \\nmost plausible. This can be considered as the approach of traditional \\nmodel selection and of Chow & Liu (1968), with slightly different \\nmeasures of plausibility . (Cooper & Herskovits suggest how to calculate \\na topology with close to highest posterior probability, given an ordering \\non the vertices and asking that parents precede children.) The combinatoric s of \\nDAGs are considered \\nby Robinson (1977). \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 290}, page_content='8.4 Boltzmann machines 279 \\nTo make use of Bayesian methods we need prior distributions over \\nthe parameters (in the conditional probability tables for recursive mod\\xad\\nels or clique marginals for decomposable models) as well as efficient \\nmeans to integrate out those parameters to find the posterior proba\\xad\\nbilities of models. This is possible for Dirichlet priors for conditional \\nprobability tables and the hyper-Dirichlet priors introduced by Dawid & \\nLauritzen (1993) for clique marginals (which have a consistency condi\\xad\\ntion). Indeed, with these priors the posterior distribution of the random \\nvariables is Markov after integrating out the current distribution of the \\nparameters. \\nMadigan & Raftery (1994) confine their model averaging, as we \\nsaw in Chapter 2. They employ a stepwise search procedure through \\nthe space of models, adding or deleting an edge at a time (and, for \\nundirected graphs, staying within the class of decomposable models by \\nremoving an edge only if it is a member of just one clique, and only \\nadding an edge if it does not create a chordless cycle). \\nIt is possible to use simulation methods, but they will also only \\naverage over a small subset of the topologies, so the method will \\nneed to be carefully constructed to give useful results. Madigan & \\nYork (1995) illustrate the use ofMCMC methods to traverse the model \\nspace. \\nHidden variables \\nA Markov or belief network can have one or more vertices representing \\nunobserved latent variables. This device is widely used in medical \\napplications, for example to represent the true (rather than reported) test \\nresult (as in Figure 8.2). They will cause observations to have missing \\nvalues, and so complicate the learning of conditional probability tables. \\nUnsurprisingly, the EM algorithm and variations (see Section A.2) have \\nbeen used (Spiegelhalter et al., 1993, Section 7). \\nHidden variables can also be allowed within topologies inferred \\nfrom data, in which case their interpretation is not specified in advance. \\nPearl (1988, Section 8.3) considers hidden vertices in trees, and other \\nmethods have been developed (Liu et al., 1991; Verma & Pearl, 1991). \\n8.4 Boltzmann machines \\nOne very specific case of our networks, the Boltzmann machine (Hinton \\n& Sejnowski, 1983; Ackley et al., 1985; Rumelhart & McClelland, 1986, \\nChapter 7), has a place in the history of neural networks. A Boltzmann \\nmachine has binary random variables at a finite set of vertices V which '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 291}, page_content=\"280 8 Belief Networks \\nare completely connected, and the conditional distribution at each \\nvertex given all the other random variables is Bernoulli with 'success' \\nparameter ev given by a logistic regression on the other vertices, so \\nlogit (e;) = w;o + 'L.N=i WijXj. As the network is completely connected, \\nthe graph properties are trivial; the expressive power comes from the \\nrestriction on the conditional distributions. The 'connection weights' \\nW;j are restricted to be symmetric ( W;j = w ji) and without loss of \\ngenerality we can take w;; = 0. The joint distribution is then \\nwhere Z = L exp L W;j x;x j· \\nxv i<j \\nBoltzmann machines are used to 'learn' an input-output distribu\\xad\\ntion, that is the joint distribution of a set of binary random variables, \\nsome of which are designated inputs I and some outputs 0. There will \\nnormally also be further units (designated 'hidden', H). Let S =I U 0, \\nthe variables which are 'visible'. Once the parameters (all the W;j) \\nare given, we have a joint distribution over X v, which gives a joint \\ndistribution over Xs, and hence the conditional distribution Xo I XJ. \\nThus a Boltzmann machine models the full joint distribution of inputs \\nand outputs (the latter indicating classes). \\nSo far the weights have been unspecified; the issue is to choose them \\nto best approximate a given joint distribution of inputs and outputs, \\nspecifically the empirical distribution of a training set fl. This is done \\nby maximum likelihood fitting of the parameters. However, since the \\njoint distribution is unknown as a function of the weights, the necessary \\nquantities are estimated by simulation, by Gibbs sampling. \\nThe precise procedure used is gradient ascent, which only entails es\\xad\\ntimating the derivative of the log-likelihood with respect to the weights. \\nTo avoid handling w;o separately, we assume a vertex 0 and implicitly \\ncondition on Xo = 1. The log-likelihood is \\nL = LlogPrw{Xs = xs} = L[log I:exp LWijXiXj -logZ J. \\n:T :T XH i<j \\nConsider just one summand L1 of the log-likelihood. We have \\nand \\naLl 'L-xH X;X j exp 'L.i<j Wij X;X j a log z \\nawij 'L-xH exp 'L.i<j Wij x;x j aw;j \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 292}, page_content=\"8.4 Boltzmann machines 281 \\n= E[X;Xjl(Xs = xs)] _ Pr{X; =X·= 1} \\nPr{Xs = xs} 1 \\n= Pr{X; = Xj = 11Xs = xs}-Pr{X; = Xj = 1}. \\nThus \\naa~ . = L [Pr{X; = Xj = 11 Xs = xs}-Pr{X; = Xj = 1}]. (8.14) \\nIJ xsEff \\nEach summand in (8.14) is estimated by simulation. Two runs of \\nthe Gibbs sampler are needed, one for the unconditioned network, \\nand one conditioned on Xs. (In the terminology of the field, the \\ninputs and outputs are 'clamped'.) A small step is made uphill, the \\ngradient re-estimated and so on. Once an approximation to a (local) \\nmaximum of the likelihood is found, future cases can be 'presented' \\nby conditioning on X1 and running the Gibbs sampler to find the \\nconditional distribution of Xo. Note that missing or partially observed \\nvalues are easily accommodated by not (fully) conditioning, both during \\ntraining and during prediction . \\nUnfortunately, the convergence of the Gibbs sampler has proved \\nto be problematic even in toy problems, since at every step of steepest \\nascent the Gibbs sampler has to run to convergence for each example \\nin the training set. For example, Kohonen et al. (1988) found that \\nBoltzmann machines out-performed feed-forward neural networks on a \\ntoy problem, but were too slow to use on their real example. \\nThe mean field approximation (Peterson & Anderson, 1987; Hay kin, \\n1994, §8.13) avoids the simulation in performing (8.14) by approximating \\nthe probabilities. Specifically, the random variables X; are replaced by \\ntheir means, so Pr{X; = Xj = 1} is replaced by the products of the \\nmeans of X; and Xj which, since they are binary, is 8;8j. (This is \\nsuggested by a saddle-point approximation given in the references.) \\nThis reduces the problem to calculating (Bv). We also replace the actual \\ninput variables by their means, so logit (8;) = w;o + ~Ni w;jBj in this \\napproximation, and ( Bv) is the solution to this non-linear system of \\nequations. If we want the conditional distribution, we know Xs and \\nthus solve the mean-field equations with the constraint Bs = Xs. As \\nall simulation is avoided, mean-field Boltzmann learning is much faster \\nthan using Gibbs sampling. It appears to work well even for small \\nsystems (Peterson & Anderson , 1987; Hinton, 1989b). \\nAttempts have been made to improve the performance of the Boltz\\xad\\nmann machine by abandoning its symmetry; it has every unit connected \\nto every other, and each can influence the other. If we consider a DAG \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 293}, page_content='282 8 Belief Networks \\nthere is no immediate feedback of influences. Logistic units have been \\nconsidered in analogues of Boltzmann machines by Apolloni & de \\nFalco (1991) and Neal (1992a, b). Assume that the vertices of the DAG \\nare labelled so that parents precede children. Then we assume that \\nII II exp X; 2: .<i wijXJ Pr{Xv=xv}= Pr{X11XJ,j<i}= 1 i ... (8.15) \\n1 + exp J<i w11X1 \\nand w;1 = 0 unless j is a parent of i. As we saw for Henrion\\'s logic \\nsampler, unconditional simulation of a recursive model is very easy, but \\nas the joint distribution is known explicitly unconditional simulation is \\nnot needed. From (8.15) we have \\n-0-log Pr{X v = xv} = X1X1-_j_ log [1 + exp\"\"\" wijx1] ow·· ow·· ~ v v ~~ \\nexp L:J<i WijXJ \\n=~~-~ =~~-~~ 1 + exp LJ<i WijXJ \\nwhere 81 is the \\'success\\' probability for X; conditional on its parents. \\nHence \\na a \\naw .. Pr{Xs = Xs} = L Pr{X v = xv} ow·. log Pr{X v = xv} \\nI] XH I] \\n= E[(X;-8i}XJI(Xs =Xs)] \\n0~iJ log Pr{Xs = xs} = E [(X;-8;)XJ I Xs = xs J \\nso the gradient of the log-likelihood is \\n:~. = L E[(X;-8;)X 1 I Xs =xs] \\n] XsEff (8.16) \\nwhere 81 depends on the parents of X;. This is evaluated by Gibbs \\nsampling (and we have already seen the form of the Gibbs sampler for \\na belief net). \\nA special case of this belief-net Boltzmann machine was considered \\nearlier by Yair & Gersho ( 1990a, b), under the name of a Boltzmann \\nperceptron network. They have general inputs X1, binary hidden units \\nX v and binary outputs X1. The hidden units depend on the output \\nunits via individual logistic regressions, and the output units depend \\non the inputs and hidden units via a multiple logistic regression (so \\nthe outputs are mutually exclusive). The architecture is then very \\nsimilar to a single-hidden-layer neural network, except that the hidden '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 294}, page_content=\"Note that \\nlog(l + exp f3h1Jh) R; \\nf3hiJJh when lf3hkBhl is \\nsmall. 8.5 Hierarchical mixtures of experts 283 \\nunits are randomly on or off with the probability that their output \\nvalue would be in the corresponding neural network. This makes the \\nposterior probabilities encoded by a set of weights slightly different, \\nfor although the average output of a hidden unit is its probability \\n(}h, its effect enters non-linearly into the output probabilities via the \\nsoftmax output stage. (This difference disappears in the mean-field \\napproximation; Hopfield, 1987.) However, we can average correctly by \\nreplacing f3hk(}h by log(1 + exp f3hk(}h) in the output multiple logistic \\nregression. This gives a slightly different output layer for the neural \\nnetwork, but back-propagation can still be applied to find oLjowij \\nwithout simulation. \\nOther variants of Boltzmann machines have been proposed. The \\nradial basis Boltzmann machines of Kappen (1995) have binary or con\\xad\\ntinuous input and output units and continuous hidden units with fixed \\ninhibiting connections between the hidden units. A stochastic diffusion \\n(Langevin) simulation system is used. The inhibition between hidden \\nunits essentially allows only one of them to be on at a time, and so \\nrestricts the solution space. This variant appears to be able to solve \\nrealistically sized problems. \\n8.5 Hierarchical mixtures of experts \\nHierarchical 'mixtures of experts' (HMEs; Jordan & Jacobs, 1994) are \\na way to specify the conditional distribution of class c given features x \\nthat has connections to several topics, and has already been touched on \\nin Chapter 2. It is most closely related to belief networks. The mixture \\nof experts idea was introduced by Jacobs et al. (1991), and hierarchies \\nof experts by Jordan & Jacobs (1992). \\nThe idea is that there are a number of classifiers Ci, each of which \\nproduces for an input x a posterior distribution over classes. Each of \\nthese can be thought of as appropriate for a particular subpopulation \\nS of cases, and another 'gating' classifier G tells us the proportions of \\nthose subpopulations at x. Since the subpopulation is unknown, the \\nposterior probabilities over classes is \\np(c I x) = LP(c I x,S = s)Pr{S =sIx}. (8.17) \\nNote that this is a model-based approach to stacked generalization. \\nIn the hierarchical form, (8.17) is used to define a classifier, and the \\nprocess repeated to combine classifiers of this form. Normally only a \\nfew levels of the hierarchy are used, but Waterhouse & Robinson (1994) \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 295}, page_content=\"284 8 Belief Networks \\nuse up to ten levels, combining a pair of networks at each stage. The \\nterminology is perhaps a little imprecise: there is a single layer of \\n'experts' but the mixture is defined hierarchically. Thus (8.17) still holds \\nfor an HME, but Pr{S =sIx} is parametrized hierarchically. \\nThe classifiers used in HMEs could be quite general, but in all the \\nexamples presented in the references they are logistic discriminants. \\nWe can view a 'mixture of experts' as a belief network. The features \\nx provide a set of vertices I which are numbered first and connected \\nto all expert and gating vertices. Each 'expert' is represented by a \\nvertex which has as inputs all the feature vectors XI and has as its \\nstate variable a class. The 'gating' classifier has as inputs the features, \\nand state variable one of the experts. The output vertex 0 has state \\nthe actual class, and inputs the states of the experts and the gating \\nclassifier. This is shown in Figure 8.10. The extension to a hierarchy is \\nimmediate: at each stage we combine two or more subnets by adding a \\ngating classifier with state the label of a subnet connected to the feature \\nvectors, and an output node connected to the gate and the outputs of \\nthe subnets. Despite this representation, the marginal distribution of \\nXI is never specified, as we always work conditionally on the feature \\nvectors. \\nWe can then use the methods of belief nets to find the posterior \\nprobabilities Pr{ SIx, y} given an example and its true class. So far \\nwe have implicitly assumed that all the classifiers are fully specified, \\nbut in fact they are logistic discriminants with a vector of parameters. \\nThese parameter s can be chosen by maximum likelihood by a variety \\nof algorithms. If parameters (} specify the 'experts' and <P the gating \\nprocess, the log-likelihood is \\nL((}, </>; ff) = L log L Ps(Y I x; (}s)Pr{S = s I x; </> }. \\n(x,y)Eff s \\nIn many problems this is simple enough to maximize directly. Alter\\xad\\nnatively we can use the EM algorithm (Section A.2), by viewing S, Figure 8.10: The belief \\nnetwork for a 'mixture \\nof experts'. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 296}, page_content='Figure 8.11 : The belief \\nnetwork for Bayesian \\ninference on a \\'mixture \\nof experts\\'. 8.5 Hierarchical mixtures of experts 285 \\nthe true subpopulation, as a missing value. The log-likelihood for the \\ncomplete data is \\nL(8, ¢; {(x, y, s)}) = 2: logps(Y I x; Bs) +log Pr{S = s I x; 4>} \\n(x,y,s) \\nand at step i we have to maximize the expectation of this over S \\nevaluated at the current parameter estimates, \\nQ((8, ¢), (8, ¢)(i)) = \\nl:l:Pr{S =s I y,x,¢Ul}[logps(Yix;8s)+logPr{S =sIx;¢}]. \\n:T s \\nThe maximization over Q then splits into separate maximizations over \\nthe parameters 85 in each expert and over ¢. For Bs we have a \\nmaximum likelihood estimate with case weights Pr{ S = s 1 y, x; ¢Ul}. \\nFor 4> we have to maximize the mutual information between Pr{S = \\ns I y, x; ¢Ul} and Pr{ S = s I x; 4> }. For a hierarchically specified gate \\nthis further divides into maximal mutual information problems at each \\nlevel. \\nBoth Jordan & Jacobs (1994) and Waterhouse & Robinson (1994) \\nuse variants of this EM algorithm, for example not maximizing fully \\nat the M step, and finding \\'on-line\\' versions. Waterhouse & Robin\\xad\\nson (1994) found a number of difficulties with their version, which \\nappeared quite prone to reach a local maximum of the log-likelihood. \\nThey suggest alleviating this by having a large pool of experts, some of \\nwhich are then effectively ignored. \\nAn alternative (Peng et al., 1994) is to consider Bayesian inference. \\nAs in Spiegelhalter & Lauritzen (1990), we can add a parent to each \\nclassifier which contains its parameter vector. Thus Figure 8.10 becomes \\nFigure 8.11. Gibbs sampling can then be used to find the posterior dis\\xad\\ntribution of the parameters given the training set 5\"\\', and so to integrate \\nout the posterior distribution of the parameters to find the predictive '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 297}, page_content=\"286 8 Belief Networks \\nposterior distribution. Estimating the parameters by maximum likeli\\xad\\nhood is essentially the 'plug-in' version, since the maximum likelihood \\nparameter values will give the posterior mode if fiat priors are used for \\nthe parameters (as Peng et al., 1994, did). \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 298}, page_content=\"There are differences \\nrelated to sex rather \\nthan colour, for \\nexample, but sex is also \\nrecorded. 9 \\nUnsupervised Methods \\nUnsupervised methods are used when no classes are defined a priori, or \\nwhen they are but the data are to be used to confirm that these are suit\\xad\\nable classes. Examples of the latter type are quite common in biology, \\nwhere species are often defined by physical characteristics, and datasets \\nof biochemical measurements become available. The interesting ques\\xad\\ntion is then whether the physical and biochemical measurements define \\nthe same classification. A variant of this occurs with our Leptograpsus \\ncrabs data. There the division into species was based on colour, and \\nthe interesting question is whether this is supported by morphological \\ndifferences . Our analyses hitherto have been to find supporting morpho\\xad\\nlogical differences, but this begs the question of whether there might be \\neven more striking differences unrelated to colour. \\nUnsupervised methods are generally designed for visualization, either \\nto show views of the data which indicate groups, or to show affinities \\nbetween the examples by displaying similar examples close together. \\nDendrograms are a one-dimensional display of similarity, with the height \\nof the join indicating (dis)similarity. For example, Figure 9.1 shows a \\ndendrogram of the Cushing's syndrome data. Each pair is joined in the \\ntree, and the height at which they are joined is an indication of their \\ndissimilarity. This plot shows clearly that one point (labelled u) is very \\ndifferent from the rest, and does tend to group the diseases together, \\nimperfectly. However, this is two-dimensional data, and the data can \\nbe plotted as in Figure 1.2 on page 11. This shows that we too would \\nhave difficulty separating the groups. \\nGroupings found by unsupervised methods are usually referred to \\nas clusters. They are usually taken to be disjoint (we do not allow an \\nanimal to belong to two species) but sometimes it is helpful to allow \\nsome overlap (botanical populations may contain hybrids). Finding \\nclusters is one of the uses of the word 'classification', and the book \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 299}, page_content='288 \\n0 \\nN \\n\"\\' 0 \\n0 \\n0 9 Unsupervised Methods \\nby Gordon (1981) entitled Classification is entirely concerned with \\nunsupervised methods, mainly clustering. \\nUnsupervised methods are sometimes used to classify. For example, \\nwe could use Figure 9.1 to select the closest grouping, and take a \\nmajority vote amongst its true classes. This has been advocated (Fuzzy \\nARTMAP: Carpenter et al., 1992; Carpenter & Grossberg, 1994), but \\nis dangerous as the unsupervised groupings may reflect a completely \\ndifferent classification of the data (colour vs sex for our Leptograpsus \\ncrabs, or, as in fact occurs, overall size). \\nOur exposition will move from visualization towards finding struc\\xad\\nture in the data. We start with methods to show linear or smooth \\nnon-linear transformations of the dataset which will reveal interesting \\nstructure in low-dimensional plots (usually two-dimensional scatter\\xad\\nplots). We then consider the class of methods sometimes known as \\nmultidimensional scaling which produce low-dimensional plots (again, \\nusually in one or two dimensions) in which similar data points are \\nplotted close together. \\nThe last two sections are concerned with clustering. The first covers \\nmethods to produce a few large clusters, and to produce taxonomic \\nhierarchies such as Figure 9.1. The last section concerns methods which \\nproduce many clusters, but link them in a one-or two-dimensional \\nlayout wherein nearby clusters are more similar than distant clusters. \\n9.1 Projection methods \\nProjection methods choose one or more linear combinations of the origi\\xad\\nnal features to maximize some measure of \\'interestingness\\'. Equivalently, \\nthe space of features is rotated in IR.P, and the first few dimensions of \\nthe rotated space are retained. Figure 9.1: Dendrogram \\nof the Cushing\\'s \\nsyndrome data by the \\n\\'single-link\\' method. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 300}, page_content='A is a p x q matrix, \\neach column of which \\ngives the coefficients of \\na linear combination. \\nProofs are given at the \\nend of this section. \\nThis is the usual \\ndefinition of principal \\ncomponents . 9.1 Projection methods 289 \\nPrincipal components \\nPrincipal components occur in a number of problems and by differ\\xad\\nent names: they are same thing as the Karhunen-Loeve expansion \\nof Watanabe (1969) and Devijver & Kittler (1982), for example. Jol\\xad\\nliffe (1986) and Jackson (1991) devote whole books to this topic. \\nSuppose the data are n ~ p vectors Xi E JRP forming the rows \\nof an n x p data matrix X. We will assume that the column means \\nare zero, that is that each feature has mean zero in the given sample. \\nThe idea is to take q < p linear combination XA E 1R q which in \\none of a number of senses best represent the original data. This is \\ndone by taking the singular value decomposition of the data matrix \\nX (Golub & Van Loan, 1989) X = UAVT, where A is a diagonal \\nmatrix with decreasing non-negative entries (Jci), U is an n x p matrix \\nwith orthonormal columns, and V is a p x p orthogonal matrix. \\nThen the principal components are the columns of XV. Since X and \\nA must have the same rank, at most p of the singular values (the \\ndiagonal elements of A) will be non-zero. Then we have the following \\nproperties : \\n1 The first singular value (the first column of XV) is the linear \\ncombination aT x for a of unit length with the largest variance, the \\nsecond is the combination of largest variance which is uncorrelated \\nwith the first, and so on. \\n2 The first q < p columns of XV are the linear projection of X into \\nq dimensions with the largest variance. (The covariance matrix of a \\nq-dimensional projection is a q x q matrix, and this one is largest \\nas measured by the trace and also by the determinant.) \\n3 Let X = U Aq vT be the matrix formed by setting all but the first q \\nsingular values (diagonal elements of A) to zero. Then X is the best \\npossible rank-q approximation to X in several senses, including \\nthe Frobenius norm, the square root of the sum of squares of the \\nelements. \\n4 Another way to express this is that if we project onto the first q \\nprincipal components we have the most accurate rank-q recon\\xad\\nstruction of the original data points. \\n5 Yet another way to express this is to say that the points of the \\nq-dimensional projection onto the first q principal components lie \\nin a q-dimensional space, and this is the best-fitting q-dimensional \\nspace as measured by the sum of the squares of the distances from \\nthe data points to their projections into the space. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 301}, page_content=\"290 9 Unsupervised Methods \\nIn summary, if you want a reduction to q < p dimensions by linear \\ncombinations of the features, the principal components have many \\noptimality properties. Note that the first two properties show that the \\nmeasure of 'interestingness' that is maximized is the variance. \\nThe emphasis on variance reveals the Achilles' heel of principal \\ncomponents: they depend on the units in which the features are mea\\xad\\nsured. In a biological problem in which we might have lengths, volumes \\nand weights, the principal components will depend critically on the \\nunits used. Even when all the measurements are lengths, do we want \\nto regard variation in the length of a small part as equivalent to vari\\xad\\nation in the length of the whole organism? Usually not; in biological \\nproblems the first principal component will normally be a measure of \\noverall size, and be of little interest. So unless we have good a priori \\nreasons to regard the variances of the features to be comparable, we \\nwould normally make them equal by rescaling all the features to have \\nvanance one. \\nThere is an older approach to principal components which is better \\nknown but numerically less stable. This is to form the covariance \\nmatrix S of the observations, and take its eigenvalue decomposition \\nCDCT. As S is a covariance matrix, hence non-negative definite, the \\neigenvalues will be real and non-negative. Now (for centred data) \\n(n-1)S = XTX = VAUTUAVT = VA2VT, so D = A2j(n-1) \\nand C = V. Thus the principal components may be found from \\nthe eigendecomposition of S. It is customary to advocate using the \\neigendecomposition of the correlation matrix rather than the covariance \\nmatrix, which is the same procedure as rescaling the features to unit \\nvariance before calculating the covariance matrix. \\nViruses example \\nWe consider the Tobamovirus group of the viruses example, which has \\nn = 38 examples with p = 18 features. Figure 9.2 shows plots of the \\nfirst two principal components with 'raw' and scaled variables. As the \\ndata here are counts, there are arguments for both, but principally for \\nscaling as the counts vary in range quite considerably between variables. \\nVirus 11 (Sammon's opuntia virus) stands out on both plots: this is \\nthe one virus with a much lower total (122 rather than 157-161). Both \\nplots suggest three subgroupings. \\nIn both cases the first two principal components have about equal \\nvariances, and together contain about 69% and 52% of the variance in \\nthe raw and scaled plots respectively. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 302}, page_content='Figure 9.2: Principal \\ncomponent (top row) \\nand Sammon mapping \\n(bottom row) plots of \\nthe Tobamovirus group \\nof the viruses example. \\nThe plots in the left \\ncolumn are of the raw \\ndata, those in the right \\ncolumn with variables \\nrescaled to have unit \\nvariance. The points are \\nlabelled by the index \\nnumber of the virus. \\nFigure 9.3: Pairwise \\nscatterplots of the first \\nthree principal \\ncomponents of the \\nLeptograpsus crabs \\ndata. Males are coded \\nas capitals, females as \\nlower case, colours as \\nthe initial letter of blue \\nor orange. Ill \\n0 \\nu;> \\n0 \\nIll \\n0 \\nu;> \\n0 \\n\\'7 \\n~ \\n0 \\n0 \\n0 9.1 Projection methods \\n.. \\nC\\\\J \\n30 \\n,,]{:\\'t ,;!\\'\" \\n\" \" 0 \\n:Js\" \\'7 \\n15 , .. <)I ..,., \\'it\\'\" \\n\\'i\\' l\\'l6 \\n-15 -10 -5 0 5 10 -4 \\n(a) \\n\"., .. \\n\" <D .. \" \" -.t \\n\" 37 \\n35 C\\\\J \\n\" 38\" \\n2~1239 \":Ia 0 \\n,,.,, \\n<)I \\n30\" \\n/133 \"i \\n-20 -10 0 10 -10 \\n(c) \\n010 00 005 010 015 \\no~oo oo ooo 0 \\n8~ 8&\\'i~ 8 8 o boo 8 0 o 8 0 a \\'\\\\\\\\\\\\0 e 0 \\no\\'i! f o8 +8 b \\no~W~\\\\; ~ ( PC1 <lit ,.,0·~ \\'\" !l \\'6 \\'t BIIJ b \\nB o \\'!, o~ lb \\n<jleOB o: b \\n0 B 8 • \\n0 • b b b 0 \\nb b b 0 0 \\nlb B~ ~.,g!f\\'~?l,0 \\nb \\\\• ft\" ~bo ooJo ~o \\nID b bB \"\\' o B o PC2 • B b ~o o \\nB o Bo B ~ B \\no ecft.R\\'!ts o \\n8 !b llo ~ oj \\no eol:lg V• \\n0 B Boo ~IP o \\nB B \\n• ~. 8~~~\" B~ Bl \\ne e ~B ~~B b u b \\nII B B h ~Bto ~ ~ \\'1{1. 8!1\\'~ • ~ ••• ~~>-.; \\n\"• •• • b 0 ~0 ~ o~c:S a II> \"t, 8 • \\no 0e~~00 ~ b 0 0 0 0 B \\n6!P~ocflgo oia~. o \\n0 0 0 0 \\n~ 0 0 0 o& 0 0 \\n0 0 0 0 \\n-1.5 -1.0 ·0.5 0.0 0.5 1.0 291 \\n.... .. , \\n4f . ., 30 \\n2~, 32 \" \" \\n\" ..... •s 45 \\n4342 ., \\n-2 0 2 4 \\n(b) \\na \\n2~0 37 ., \\n\"\"\\'\" 35 .. \\n36 14!:5 \\n\" \\n.. \\'~\\' \" l!l\"\" 30 \\noi,\\', \\n-5 0 5 \\n(d) \\n~~ \\' b ~ 8 \\n0 00 ~ 8 8 \\n0 ~t 8 \\noCD of,8~1!~ \\noO> o~Bb \\ngoO bb b b II \\n0 \"\\'lb\\\\, • \\n0 8 • \\n0 \\n0 rJ> ••• \\n0 ~~ \\no• VJ 0° \"\\'ij \\n0 40 \\' •• 0 \\nff\\' 0 \\n0 II \\'\\\\, %0 ~B ~ \\no 8 B \\no o 0o\\'O • 11. \"tt 8 \\n0~ ~0 \"!! B \\n0 ~0 8 ~ \\n0 ~ \"\"\" 8 cfo 0 1188 \\nPC3 \\n-0.10 -0.05 0.0 0.05 0.10 10 \\n\"\\' 0 \\n0 \\n0 \\n\"\\' 9 \\n0 \\n0 \\n0 \\n9 '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 303}, page_content=\"292 9 Unsupervised Methods \\nCrabs example \\nThe crabs data are displayed on their first three principal components \\nin Figure 9.3. The data were transformed to log scale but not further \\nscaled (although that is arguable) . Since the groupings (by colour and \\nsex) are known, the points in the different groups are distinguished \\non the plots. The figure shows that the first principal component is \\nlargely unrelated to colour and sex: it is almost an average of the \\nmeasurements on log scale, and so is displaying size. The second \\nprincipal component tends to distinguish sex and the third colour, the \\nmost interesting grouping. \\nIterative methods \\nA consequence of property 4 of the principal components is that the \\nbest we can do by taking p x q and q x p matrices A and B and \\nforming XAB to approximate X in sum-of-squares is to take XA as \\nthe first q principal components. Now there are many other equally \\ngood solutions, for XAcc-1 B will give the same fit for any invertible \\nq x q matrix C. We can express this by saying that we can only \\noptimize over the subspace spanned by the q linear combinations. \\nIt is obvious that XAB is the outcome of a feed-forward neural net\\xad\\nwork with no bias unit, all linear units and q units in the hidden layer. \\nThus the best possible fit by least squares of such a network trained \\nwith output equal to input (an 'auto-encoder ' or 'auto-associator ') is \\ngiven by the subspace spanned by the first q principal components. \\nThis is a much-rediscovered fact (Bourlard & Kamp, 1988, and Baldi \\n& Hornik, 1989, were amongst the first) and every 'on-line' algorithm \\nto fit the neural network leads to an iterative algorithm to find the \\nsubspace spanned by the principal components . This has led to around \\n100 papers on such algorithms. Well-known versions are those of \\nBrockett (1991), Oja (1982, 1989, 1992), Oja & Karhunen (1985) and \\nSanger (1989). One of the simplest methods that extracts principal \\ncomponents (rather than just the subspace) is the APEX algorithm of \\nKung & Diamantaras (1990). (See Haykin, 1994, §9.8.) \\nAlthough the idea of these algorithms is interesting, they seem \\nunnecessary in practice. Singular value decomposition routines can \\nhandle quite large matrices X, and when they cannot cope, we can \\nalways find the covariance matrix on a single pass through the data \\nand find its eigenvalues . \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 304}, page_content='9.1 Projection methods 293 \\nRobustness \\nThe extraction of principal components is based on variances, and so \\nis sensitive to the presence of outliers. Outliers in high-dimensional \\ndata are notoriously difficult to find, although they often emerge as a \\nside-effect from some projection pursuit methods. \\nIt is highly desirable to use a method of extracting principal com\\xad\\nponents which is less sensitive to outliers. This can be achieved by \\ntaking an eigendecomposition of a robustly estimated covariance or \\ncorrelation matrix. There is a slight catch, in that it is essential to \\nrobustly estimate the means, and that this must be done by estimating \\nthe means of all features simultaneously (Rousseeuw & Leroy, 1987, \\np. 250). So the real task is to estimate the vector of means and the \\ncovariance matrix. There have been many attempts to do this; see \\nfor example Devlin et al. (1981). Modern approaches are discussed in \\nSection 2.5. \\nAn alternative approach is to find a projection maximizing a robust \\nmeasure of variance in q dimensions. This would have to be done \\niteratively, as for the projection pursuit methods described below. \\nProofs \\nAs our results depend on various properties of principal components, \\nthese are proved here for those who are interested in the details. \\nProposition 9.1 Consider an n x p matrix X with singular value decom\\xad\\nposition X = U AVT. The best approximation in Frobenius norm to X \\nby a matrix of rank k ~ min( n, p) is given by \\n- . T X= Udmg(Al, ... ,Ak, ... ,O)V . \\nThis is also the best approximation by a projection onto a subspace of \\ndimension at most k, the projection onto the space spanned by the first k \\ncolumns of U, and maximizes the Frobenius norm of a projection of X \\nonto a subspace of dimension at most k. \\nProof: We have IIX-xf = IIA-Ak112 = L~~~(n,p) AT-Now X \\ncorresponds to a projection onto the space spanned by the first k \\ncolumns of U, say Uk> since that projection gives \\nConsider any approximation Y of rank at most k. This can be \\nwritten as Y = AB where A is n x k and B is k x p (for example, via '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 305}, page_content=\"294 9 Unsupervised Methods \\nthe SVD of Y ). Now consider the best approximation of the form AC \\nfor any k x p matrix C. Since the squared Frobenius norm is the sum \\nof the squared lengths of the columns, this is solved by regressing each \\ncolumn of X in turn on A; the optimal choice is C = (AT A)-1 AT X \\nand \\nwhere P A = A(A T A)-1 AT is the projection matrix onto span( A). Now \\nwe choose PA to maximize IIPAXII2: \\nmin(n,p) min(n,p) \\nIIPAXII2 = IIPAUAf = L AJIIPAujll2 = L AJPJ \\n1 1 \\nand Pj ~ 1 (it is the length of a projection of a unit-length vector), \\n~ PJ = liP AUf = liP A 112 = k. It is then obvious that the maximum is \\nattained if and only if the first k Pj 's are one, the rest zero, so \\nk min(n,p) \\nIIX-y f?: IIXII2-IIPAXII2?: IIXII2-L AT= L AT= IIX-Xll2. \\n1 k+l \\nAny projection of X onto a subspace of k dimensions has rank at \\nmost k. D \\nProposition 9.2 Consider n observations of p features forming a matrix \\nX. Then the projection of Proposition 9.1 \\n(a) minimizes the sum of squared lengths from points to their projections \\nonto any subspace of dimension at most k, \\n(b) maximizes the trace of the covariance matrix of the projected vari\\xad\\nables onto any subspace of dimension at most k, and \\n(c) maximizes the sum of squared inter-point distances of the projections \\nonto any subspace of dimension at most k. \\nProof: Without loss of generality we can centre the observations, \\nso each variable has mean zero. Part (a) follows from the squared \\nFrobenius norm of X -PAX being the sum of squared lengths of its \\nrows. For (b) the squared Frobenius norm of PAX is the sum of squares \\nof the projected variables, that is n -1 times the sum of the variances \\nof the variables, which is the trace of the covariance matrix (and is \\ninvariant to the choice of a basis for that subspace). For (c) consider \\nany projection PAX. Let d,5 be the distance between observations r \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 306}, page_content=\"9.1 Projection methods 295 \\nand s, and drs the distance under projection (which is smaller, as it \\nis a projection). Let Yr be the-r th projected observation. Then, using \\nL::: Yr = 0 (since the projections are still centred), \\nr,s r,s r,s \\nwhich is maximized according to Proposition 9.1. 0 \\nProposition 9.3 The principal components defined by property I are \\ngiven, in order, by columns of V. The first k principal components span \\na subspace with the properties of Proposition 9.2. \\nProof: Consider a linear combination y =aT x with II all = 1. Then \\nwhere a' = vr a also has unit length (and this corresponds to rotating \\nto a new basis for the feature variables). It is clear that the maximum \\noccurs when a' is the first coordinate vector, or a the first column \\nof V. Now consider the second principal component bT x. It must be \\nuncorrelated with the first, so \\nand it is obvious that the maximum variance under this constraint \\nis given by taking b' as the second coordinate vector. An inductive \\nargument gives the remaining principal components. \\nUsing the principal component variables we have X = U A, so it \\nclear that the subspace spanned by the first k columns is the approxi\\xad\\nmation of Propositions 9.1 and 9.2. 0 \\nProposition 9.4 Consider an orthogonal change X B to k new variables. \\nAmongst such transformations the first k principal components have max\\xad\\nimal variance, both in the sense of the trace and of the determinant of \\nthe covariance matrix. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 307}, page_content=\"296 9 Unsupervised Methods \\nProof: The trace statement is Proposition 9.2(b ), but we will give an \\nalternative proof. Consider the SVD of X B, and let its singular values \\nbe !J.l, ... ,IJ.k· We will show /.lj ~ Aj,j = 1, ... ,k, which suffices as the \\ntrace of the variance matrix is proportional to the sum of the squared \\nsingular values, and the determinant is proportional to their product. \\nConsider a variable y = xT a which is a unit-length linear combina\\xad\\ntion of the first j principal components of the B set, but is orthogonal \\nto the first j -1 original principal components. (A dimension argument \\nshows that such a variable exists. Since B is orthogonal y is also a \\nunit-length combination of the original variables and of their principal \\ncomponents.) Thus y has variance at least llJ and at most Jc], hence \\n/.lj ~ Aj. D \\nProjection pursuit methods \\nProjection pursuit methods seek a q-dimensional projection of the data \\nthat maximizes some measure of 'interestingness', usually for q = 1 or \\n2 so that it can be visualized. This measure would not be the variance, \\nand would normally be scale-free. Indeed, most proposals are also \\naffine invariant, so they do not depend on the correlations in the data \\neither. \\nThe methodology was named by Friedman & Tukey (1974), who \\nsought a measure which would reveal groupings in the data. Later \\nreviews (Huber, 1985; Friedman, 1987; Jones & Sibson, 1987) have \\nused the result ofDiaconis & Freedman (1984) that a randomly selected \\nprojection of a high-dimensional dataset will appear similar to a sample \\nfrom a multivariate normal distribution to stress that 'interestingness' \\nhas to mean departures from multivariate normality. Another argument \\nis that the multivariate normal distribution is elliptically symmetrical, \\nand cannot show clustering or non-linear structure, so all elliptically \\nsymmetrical distributions should be uninteresting. \\nThe simplest way to achieve affine invariance is to 'sphere' the data \\nbefore computing the index of 'interestingness'. Since a spherically \\nsymmetric point distribution has covariance matrix proportional to the \\nidentity, we transform the data to have identity covariance matrix. This \\ncan be done by transforming to principal components, discarding any \\ncomponents of zero variance (hence constant) and then rescaling each \\ncomponent to unit variance. As principal components are uncorrelated, \\nthe data are sphered. Of course, this process is susceptible to outliers \\nand it may be wise to use a robust version of principal components. The idea goes back to \\nKruskal (1969, 1972). \\nKruskal (1969) needed \\na snappier title! \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 308}, page_content=\"Here <I> is the \\ncumulative distribution \\nfunction of a standard \\nnormal. 9.1 Projection methods 297 \\nThe discussion of Jones & Sibson (1987) included several powerful \\narguments against sphering, but as in principal component analysis \\nsomething of this sort is needed unless a particular common scale for \\nthe features can be justified: \\nSpecific examples of projection pursuit indices are given below. Once \\nan index is chosen, a projection is chosen by numerically maximizing \\nthe index over the choice of projection. A q-dimensional projection is \\ndetermined by a p x q orthogonal matrix and q will be small, so this \\nmay seem like a simple optimization task. One difficulty is that the \\nindex is often very sensitive to the projection directions, and good views \\nmay occur within sharp and well-separated peaks in the optimization \\nspace. Another is that the index may be very sensitive to small changes \\nin the data configuration and so have very many local maxima. Rather \\nthan use a method which optimizes locally (such as quasi-Newton \\nmethods) it will be better to use a method which is designed to search \\nfor isolated peaks and so makes large steps. In the discussion of Jones \\n& Sibson (1987), Friedman says \\n'It has been my experience that finding the substantive minima of a \\nprojection index is a difficult problem, and that simple gradient-guided \\nmethods (such as steepest ascent) are generally inadequate . The power \\nof a projection pursuit procedure depends crucially on the reliability \\nand thoroughness of the numerical optimizer.' \\nand our experience supports Friedman's wisdom. It will normally \\nbe necessary to try many different starting points, some of which may \\nreveal projections with large values of the projection index. Posse (1990, \\n1995b) considers an almost random search which Posse (1995a) finds \\nto be superior to his implementation of the optimization methods of \\nJones & Sibson and of Friedman . \\nOnce an interesting projection is found, it is important to remove \\nthe structure it reveals to allow other interesting views to be found more \\neasily. If clusters (or outliers) are revealed, these can be removed, and \\nboth the clusters and the remainder investigated for further structure. If \\nnon-linear structures are found, Friedman (1987) suggests non-linearly \\ntransf-orming the current view towards joint normality, but leaving the \\northogonal subspace unchanged. This is easy for q = 1; any random \\nvariable with cumulative distribution function F can be transformed \\nto a normal distribution by $-1(F(X)). For q = 2 Friedman suggests \\ndoing this for randomly selected directions until the two-dimensional \\nprojection index is small. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 309}, page_content=\"298 9 Unsupervised Methods \\nProjection indices \\nA very wide variety of indices have been proposed, as might be expected \\nfrom the many ways a distribution can look non-normal. A projection \\nindex will be called repeatedly, so needs to be fast to compute. Recent \\nattention has shifted towards indices which are rather crude approxi\\xad\\nmations to desirable ones, but very fast to compute (being based on \\nmoments). \\nFor simplicity, most of our discussion will be for one-dimensional \\nprojections; we return to two-dimensional versions at the end. Thus we \\nseek a measure of the non-normality of a univariate random variable \\nX. Our discussion will be in terms of the density f even though the \\nindex will have to be estimated from a finite sample. (This can be done \\nby replacing population moments by sample moments or using some \\ndensity estimate for f such as the kernel methods of Section 6.1.) \\nThe original Friedman- Tukey index had two parts, a 'spread' term \\nand a 'local density' term. Once a scale has been established for X \\n(including protecting against outliers), the local density term can be \\nseen as a kernel estimator of J f2(x) dx. The choice of bandwidth is \\ncrucial in any kernel estimation problem; as Friedman & Tukey were \\nlooking for compact non-linear features (cross-sections of 'rods'-see \\nTukey's contribution to the discussion of Jones & Sibson, 1987) they \\nchose a small bandwidth. Even with efficient approximate methods \\nto compute kernel estimates, this index remains one of the slowest to \\ncompute. \\nJones & Sibson (1987) introduced an entropy index J f logf (which \\nis also very slow to compute) and indices based on moments such \\nas [K~ + 1/4K~]/12, where the K's are cumulants, the skewness and \\nkurtosis here. These are fast to compute but sensitive to outliers (Best \\n& Rayner, 1988). \\nFriedman (1987) motivated an index by first transforming normality \\nto uniformity on [-1,1] by Y = 2<I>(X) -1 and using a moment \\nmeasure of non-uniformity, specifically j(fy -1/2)2. This can be \\ntransformed back to the original scale to give the index \\nIL = j [f(x)-¢(x))2 d \\n2¢(x) x. \\nThis has to be estimated from a sample, and lends itself naturally to an \\northogonal series expansion, the Legendre series for the transformed \\ndensity. \\nThe index I L has the unfortunate effect of giving large weight to \\nfluctuations in the density f in its tails (where 4> is small), and so Here cf> is the standard \\nnormal density. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 310}, page_content=\"9.1 Projection methods 299 \\nwill display sensitivity to outliers and the precise scaling used for the \\ndensity. This motivated P. Hall (1989) to propose the index \\nIH = j [f(x) -l/J(x)]2 dx \\nand Cook et al. (1993) to propose \\nIN= j[f(x)-l/J(x)]24J(x)dx. \\nBoth of these are naturally computed via an orthogonal series estimator \\nof f using Hermite polynomials (Thisted, 1988, §5.3.2). Note that all \\nthree indices reduce to 2::~ wi(ai-bi)2, where ai are the coefficients in \\nthe orthogonal series estimator, and bi are constants arising from the \\nexpansion for a standard normal distribution. \\nTo make use of these indices, the series expansions have to be \\ntruncated, and possibly tapered as well (see Section 6.1). Cook et \\nal. (1993) make the much more extreme suggestion of keeping only a \\nvery few terms, maybe the first one or two. These still give indices \\nwhich are zero for the normal distribution, but which are much more \\nattuned to large-scale departures from normality. For example, If: is \\nformed by keeping the first term of the expansion of IN, (ao -1/2jii)2 \\nwhere ao = J l/J(x)f(x) dx = El/J(X), and this is maximized when ao is \\nmaximal. In this case the most 'interesting' distribution has all its mass \\nat 0. The minimal value of ao gives a local maximum, attained by \\ngiving equal weight to ±1. Now of course a point mass at the origin \\nwill not meet our scaling conditions, but this indicates that If: is likely \\nto respond to distributions with a central clump or a central hole. To \\ndistinguish between them we can maximize ao (for a clump) or -a0 \\n(for a central hole). These heuristics are borne out by experiment. \\nIn principle the extension of these indices to two dimensions is \\nsimple. Those indices based on density estimation just need a two\\xad\\ndimensional density estimate and integration (and so are likely to be \\neven slower to compute). Those based on moments use bivariate \\nmoments. For example, the index IN becomes \\nIN= J J [f(x,y) -ljJ(x)ljJ(y)]24J(x)ljJ(y)dxdy \\nand bivariate Hermite polynomials are used. To maintain rotational \\ninvariance in the index, the truncation has to include all terms up to a \\ngiven degree of polynomial. \\nAll the indices described so far are implemented for q = 2 in XGobi, \\na freely available data visualization tool from Bellcore (Swayne et al., \\n1991). \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 311}, page_content=\"300 9 Unsupervised Methods \\nEslava & Marriott (1994) defined two indices for q = 2 specifically \\ndesigned to display all clusters; conventional indices have a tendency \\nto superimpose clusters in the projection. Suppose the projected points \\n(or those not very near the origin) have ordered polar angles ei. \\nThe polar nearest neighbour index (to be minimized) is the average \\nof min( lei-ei-ll, i8i+l-eil), the angular separation from the remaining \\npoints. Their other criterion maximizes the mean radial distance, or \\nequivalently for sphered data, minimizes the variance of the radial \\ndistance. Posse (1995a, b) has another two-dimensional index also based \\non ideas of radial symmetry, using a chi-squared index of departure \\nfrom normality averaged over univariate projections. \\nOne of the very few examples of a method which is both bio\\xad\\nlogically motivated and practically useful is the projection index of \\nIntrator (1990, 1992) and Intrator & Cooper (1992) based on the BCM \\nmodel of neuron selectivity put forward by Bienenstock et al. (1982). \\nThe BCM model is based on a one-dimensional projection c = aT x of \\na signal x, which is chosen to maximize \\n(9.1) \\nwhere the 'threshold' e = Ec2 is adjusted according to the distribution \\nof the population of examples (in practice the training sample). Notice \\nthat (9.1) is not scale-free, and will be negative for large c and hence a. \\nThus there is no need to normalize a to unit length. There is a natural \\non-line algorithm to minimize (9.1), namely \\nwhere e will also be updated from time to time. (Intrator & \\nCooper, 1992, discuss the stability of the differential equation limit \\nof this update.) We can also consider several BCM neurons with lateral \\ninhibition, in which case c is replaced by ck -I} L_Hk Cj for neuron k. \\nThe BCM neuron is itself a projection index, but as it is based \\non moments it will be sensitive to outliers. Intrator replaces c = aT x \\nby t(c) for the usual logistic function t; this effectively transforms \\nto [0, 1] by the inverse of the logistic cumulative distribution function \\nbefore computing the index. Not only does this give a one-dimensional \\nprojection index but the lateral inhibition BCM network may be used \\nto project onto q > 1 dimensions. Applications are shown by Intra\\xad\\ntor (1991, 1992) and Intrator & Gold (1993). \\nThere is no unanimity over the merits of these indices (except the \\nmoment index, which seems universally poor). Some workers have \\nreported that the Legendre index is very sensitive to outliers, and this The BCM model \\nappears to be very well \\nsupported by \\nexperiment. \\nThe details, especially \\nthe constants , differ \\nfrom paper to paper. In \\nthe original BCM paper \\n0 = (Ecf. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 312}, page_content=\"9.1 Projection methods 301 \\nis our experience. Yet Posse (1995a) found it to work well, in a study \\nthat appears to contain no ·outliers. The natural Hermite index is \\nparticularly sensitive to a central hole and hence clustering. The best \\nadvice is to try a variety of indices. \\nViruses example \\nFigure 9.4 shows six views of the main group of the viruses dataset \\nobtained by (locally) optimizing various projection indices; this is a \\nsmall subset of hundreds of views obtained in interactive experimen\\xad\\ntation in XGobi. With only 38 points in 18 dimensions, there is a \\nlot of scope for finding a view in which an arbitrarily selected point \\nappears as an outlier, and there is no clear sense in which this dataset \\ncontains outliers (except point 11, whose total residue is very much \\nless than the others). When viewing rotating views of multidimensional \\ndatasets (a grand tour in the terminology of Asimov, 1985) true outliers \\nare sometimes revealed by the differences in speed and direction which \\nthey display-certainly point 11 stands out in this dataset. \\nNot many views showed clear groupings rather than isolated points. \\nThe Friedman- Tukey index was most successful in this example. Eslava\\xad\\nG6mez (1989) studied all three groups (which violates the principle of \\nremoving known structure). \\nThis example illustrates a point made by Huber (1985, §21); we \\nneed a very large number of points in 18 dimensions to be sure that we \\nare not finding quirks of the sample but real features of the generating \\nproc~ss. Thus projection pursuit may be used for hypothesis formation, \\nbut we will need independent evidence of the validity of the structure \\nsuggested by the plots. \\nCrabs example \\nVarious projections of the crabs data are shown in Figure 9.5. This is a \\nrather different example, with 200 examples on only five variables, and \\nwith four groups suspected in advance. The first term of the natural \\nHermite expansion does find other local maxima, but the view shown \\nin Figure 9.5(b) is the most commonly found. The other indices are less \\nsuccessful. View (b) is close to a local maximum for Friedman's index, \\nbut more often just the colour forms are separated as shown in view \\n(d). The Friedman-Tukey index does not recognize these clusters, and \\ninstead finds views such as (c) which seem to have no interpretation. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 313}, page_content=\"302 9 Unsupervised Methods \\nFigure 9.4: Projections \\nof the Tobamovirus \\ngroup of the viruses \\ndata found by \\n,36 ,36 projection pursuit. \\n!• •• .. \\n:·;,}~. Views (a) and (b) were ·_.:.~;~ \\nfound using the natural \\n)7 ,17 Hermite index, view (c) ,46 \\n,46 by minimizing ao, and \\nviews ( d, e, f) were \\n,45 found by the \\nFriedman- Tukey index J j2 with different \\n(a) (b) choices of bandwidth \\nfor the kernel density \\nestimator. \\n_.39 .,41 ~ .. \\nJl2 \\n,. \\n~-\\n'11 \\n;·: \\n,14 ,45 \\n-~h ,14 \\n./ \\n(c) (d) \\n-~· \\n,·II.· \\n)0 \\n(e) (f) \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 314}, page_content='Figure. 9.5: Projection s \\nof the uprQgrapsus \\ncrabs data found by \\nprojection pursuit. View \\n(a) is a random \\nprojection. View (b) was \\nfound using the natural \\nHennite index, view (c) \\nby the Friedman -Tukey \\nindex and view (d) by \\nthe Friedman (1987) \\nindex. 9.1 Projection methods \\n. .. \\n···~· ...... ·.·: .. ..... :.;; ...... . ... .... ... ,. \\'· .,~. .. \\' \"\\'\\'{:c.·..;. ~ .: .. ~.,.: .. _ .:. \\n• •• • ..-. t •• • • . . . . , .. \\n:\\' ~: ... = .. \\n(a) \\n\\' . . . . . . ·: .:·. ·. \\n: .......... -:.· .. : \\n• • I • ,• . ~ . .;.J.::.·.t: .. ,·.· . . ..... s-.......... . . .. . ., .... , ... ... ~·.. .. \\n.... t :. • • .. : \\n(c) . ·~· . .·:...:t,. \\n·<~\\'\" . .. ··· \\n~:-:~· . \\'\"\\'(~,,. , ·. . \\n(b) ·-, .. , :,:\\\\ ... .. : ... \\'• \\n: ... :.. ·.;:: -~.: .. . .. ··:-.:....-: :·. . \\' ,-l\\'tt. \\\\ .... :. \\n.·· ~-·: .. • • .c;.,. .., \\n• I • • ~~ ,\"\\'o • .... : .. ·· ·;:r ... ·· .. · ... .. \\n(d) 303 \\nIt is worth noting that the successful projections have ignored the \\nmost variable direction, size, in favour of a view with more structure, \\nunlike all the other methods we illustrate on this example. \\nNon-linear feature extraction \\nOne of the characterizations of the principal components was that if we \\ntook a linear map F:RP-+ Rq and another linear map G:Rq-+ RP, \\nthe most accurate reconstruction G(F(x)) in the sense of least squares \\nis given by using tbe first q columns of V to form the first q principal \\ncomponents and for the reconstruction map G. \\nCan we do better with non-linear mappings F and G ? The answer \\nmust be yes, for the diagonal Cantor construction can map invertibly \\nRP into R ! (Write each of the components Xt, .•. , Xp in a binary \\nexpansion and interleave the expansions to obtain the binary expansion '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 315}, page_content=\"304 9 Unsupervised Methods \\nof a number in IR. This F is continuous.) In practice we want F and \\nG to be not too far from linear, and in particular very smooth. \\nThe most common suggestion is to use feed-forward neural net\\xad\\nworks to fit F and G (Kramer, 1991; Usui et al., 1991; Cottrell & \\nMetcalfe, 1991; DeMers & Cottrell, 1993). If we model both F and \\nG by networks with a single hidden layer, we end up with a five-layer \\nnetwork. The input and output layers have linear units, as does the \\nmiddle layer (with q units). The second and fourth layers have sig\\xad\\nmoidal units. Multi-layer networks are notoriously difficult to train, and \\nthese methods have shown limited success. None appears to have used \\nskip-layer connections nor weight decay, and it is not clear whether the \\ncurrent lack of success is intrinsic or due to inefficient methods of train\\xad\\ning the network. Such networks are often referred to as 'bottlenecks', \\n'auto-encoders' or 'auto-associators'. \\nKambhatla & Leen (1994) take another approach, which in their \\nexamples (and that of DeMers & Cottrell) is at least competitive in \\nfit with an auto-encoder, but very much faster to train. Whereas an \\nauto-encoding is trained globally, Kambhatla & Leen use principal \\ncomponents locally within the partitioning of JRP defined by some \\nform of vector quantization of the dataset. This can be seen as an \\napproximation to defining a q-dimensional manifold in JRP. Clearly the \\nvector quantization should be performed with an eye to the approxima\\xad\\ntion error, and this is done by measuring the distances from a codebook \\nvector to a vector which might be assigned to its cluster orthogonal \\nto the local approximation, that is in the space of the omitted p -q \\nprincipal components. \\nPrincipal curves and surfaces \\nPrincipal curves are defined by Hastie & Stuetzle (1989) as a mapping \\nof a dataset in JRP to a one-dimensional manifold in JRP. Let f(A.) \\nbe a smooth curve in JRP parametrized by A. E IR. Then for any data \\npoint x E JRP we seek the nearest point A.(x) on the curve in Euclidean \\ndistance. The curve is called a principal curve for a distribution on JRP \\nif E[X I A.(x) = A.] = f(A.), that is the mean of those points that project \\nto a point on the curve is that point. \\nThere are many possible parametrizations of a one-dimensional \\ncurve; the most natural is in terms of arc length A. from a fixed point \\non the curve. \\nWe have defined a principal curve for a distribution, and the natural \\nway to find such a curve is to project a distribution onto a candidate \\ncurve f(A.), and to take as the next iteration the conditional expectation There could be more \\nthan one nearest point, \\nbut this will be \\nexceptional. For \\ndefiniteness, we choose \\nthe nearest point with \\nlargest )., \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 316}, page_content=\"These are the distances \\nallowed for expense \\nclaims in bureaucracies. 9.2 Multidimensional scaling 305 \\nE{X I A.(x) = A.} and re-parametrize this in terms of arc length. There \\nare no known guarantees that this algorithm will converge. For a set \\nof data, the points only project to a discrete set of values of A., and the \\nconditional expectation must be replaced by a smoothing operation. \\nWe have a set of values (A.i, x;). Whereas we considered scatterplot \\nsmoothers in Chapter 4 for univariate x, these methods extend readily \\nto multidimensional x. In most methods we smooth each coordinate of \\nx separately. \\nWe can think of principal curves as an algorithm to map A. = F(x) \\nby projecting to the nearest point and then projecting back by x =((A.). \\nAs such it is very similar to the method of Kambhatla & Leen (1994), \\nbut handles the projection step in a smoother way. Tibshirani (1992) \\nproposed a variant on the original principal curves idea. \\nIn principle this technique can be extended to manifolds of q > 1 \\ndimensions, called principal surfaces, although q-dimensional smooth\\xad\\ning is much harder unless data are abundant, even for q = 2. \\nAs with all projection techniques, principal curves and surfaces \\ndepend critically on the scaling of the features; current algorithms also \\ndepend on choosing well the degree of smoothing. \\n9.2 Multidimensional scaling \\nIn multidimensional scaling we are given the distances drs between \\nevery pair of observations. These could be genuine distances in some \\nhigh-dimensional space, or they could be surrogates for the Euclidean \\ndistances. For example, some favourite examples use 'official' road \\ndistances between major towns and the scheduled flight times between \\ncities. The latter need not even be symmetric, but we will confine \\nattention to symmetric distances. Thus we suppose we are given non\\xad\\nnegative symmetric numbers drs which we will call dissimilarities to \\nindicate that they need not be genuine distances. In particular, they \\nneed not satisfy the triangle inequality \\nsatisfied if they were produced by a metric. (Gower & Legendre, 1986, \\nexplore when dissimilarities are metric.) \\nMost of the work on multidimensional scaling has been developed \\nin the psychological literature, but has also been discussed in ecology \\nunder the name of ordination. The recent short book by Cox & \\nCox (1994) has considerable detail on the various methods and their \\nhistory. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 317}, page_content=\"306 9 Unsupervised Methods \\nDissimilarities \\nIf we are given an n x p matrix X of data to be considered as n \\np-variate continuous observations, there are several ways to measure \\nthe distance between the pairs of observations. If these observations \\nare categorical, there are even more ways. Since many of them produce \\nmeasures of distance which do not satisfy the triangle inequality, the \\nmore general term dissimilarity is used. A dissimilarity is just a non\\xad\\nnegative symmetric function on pairs of objects; we will usually assume \\nthat the self-dissimilarities are zero. Kaufman & Rousseeuw (1990, §1.2) \\nreview many definitions of dissimilarities. \\nExactly the same choices of a distance measure occur when k\\xad\\nnearest neighbour methods are used in supervised classification. \\nFor continuous data, the most obvious dissimilarity is Euclidean \\ndistance computed from d2 = xxr. This does however depend crit\\xad\\nically on the scales in which the features are measured. One way out \\nwe saw for principal component analysis is to rescale the features to \\nunit variance, and in projection pursuit we saw the idea of 'sphering' \\nthe daJa. In this context sphering implies using Mahalanobis distances \\nwith respect to a covariance matrix I:, which could be the covariance \\nmatrix of these observations if n > p. Another idea is the Manhattan \\nor L1 distance, that sums the absolute differences in features. \\nFor categorical data, the most commonly used dissimilarity is based \\non the simple matching coefficient, that is the proportion Crs of features \\nwhich are common to the two observations r and s. As this is \\nbetween zero and one, the dissimilarity is found by drs = 1 -Crs· For \\nbinary features, it might be thought that having a feature present in \\nboth observations should be considered a more important indication \\nof similarity than having it absent in both. (Think of types of pottery \\nfound in neolithic graves.) The Jaccard coefficient Crs considers the \\nproportion of features which are present in one or other observation \\nwhich are found in both. Once again, drs = 1-Crs· Coefficients between \\nzero and one which are high for similar observations are quite common, \\nand called similarity coefficients. \\nFor ordinal data, the most appropriate treatment seems to be to \\nuse the ranks as if they were continuous data, probably after rescaling \\nto the range [0, 1] so that every ordinal feature is given equal weight. \\nThere then arises the question of how to handle mixtures of continuous, \\ncategorical and ordinal features. The definition of Gower (1971) has \\nbeen widely adopted. For each feature f we define a dissimilarity d{s, \\nand an indicator I fs which is one only if feature f is recorded for both \\nobservations. Further, I/s = 0 if we have a categorical feature and an \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 318}, page_content=\"9.2 Multidimensional scaling 307 \\nabsence-absence match. Then \\n(9.2) \\nThe classical or metric method \\nIn the classical or metric method of multidimensional scaling, often \\nknown as principal coordinate analysis (Gower, 1966) but going back to \\nSchoenberg (1935), Young & Householder (1938) and Torgerson (1952, \\n1958), we assume that the dissimilarities were derived as Euclidean \\ndistances between n points in p dimensions, for unknown p. Given \\nthe distances, we obviously cannot recover the observations themselves, \\nsince the distances are invariant to rigid motions (translations, rotation \\nand reflections) of JRP. It transpires that this is the only freedom \\nallowed. \\nProposition 9.5 For any symmetric matrix T, define the matrix \\nT' = _! [r _ (Tl)l T _ l(Tl)T + 1 TTl] \\n2 n n n2 \\nby subtracting row and column means and adding back the overall mean, \\nor, equivalently, by removing row means then column means. \\n(a) Given any configuration X of n points in JRP, the matrix T = (d;s = \\nllxr-Xsf) gives a non-negative definite T' = xxT. Such a set of \\ndistances is called Euclidean. \\n(b) Given a symmetric n x n matrix T with non-negative definite T', we \\ncan find a configuration of points in JR(n-l) such that T = (d;5). \\n(c) A necessary and sufficient condition for an n x n matrix T to be a \\nsquared distance matrix is that w T Tw ~ 0 for all w with w T 1 = 0. \\n(d) Any two configurations of n points with the same (d;5) differ only by \\na shift and a rigid motion of JRP, so lie in (shifted) subs paces of the \\nsame minimal dimension, the rank of T'. \\nProof: (a) Without loss of generality, centre the data so every column \\nof X has zero mean. Then T = (llxr-Xsll2) = (llxrll2+11xsll2-2x,!xs) = \\nElT +lET -2XXT where E = (llxrll2). Let e = ET1 so Tl = nE+el \\nand 1 TEl= 2ne. Thus \\n-2T' = ElT +lET-2XXT-ElT-e11T /n \\n-lET-ell T /n + 2ne11 T jn2 \\n= -2XXT \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 319}, page_content=\"308 9 Unsupervised Methods \\nwhich is non-positive definite. \\n(b) Let T' = CD2CT be the eigendecomposition of T', noting that \\nthe eigenvalues are non-negative, and by construction T' has zero \\ncolumn sums and so has rank r at most (n-1). Take X as the first \\nr columns of CD, so T' = xxr. This configuration is centred, since \\nIIX1112 = lTT'l = 0. Note that (llxrll2) = diag(XXT) = diag(T'), so \\nT' determines T = (d;s) and (under zero means) this gives the same \\nT' by result (a). \\n(c) Note that [(/ -llT /n)w]TT[(I -llT /n)w] = -2wTT'w which is \\nnegative if T' is non-negative definite. \\n(d) The procedure of (b) constructs a canonical configuration which \\nis obtained by a shift (to zero mean) and a rigid motion from either \\nconfiguration. D \\nGiven a Euclideap dissimilarity on n points, this proposition pro\\xad\\nduces a data matrix in r ~ n-1 dimensions with distances equal to \\nthe dissimilarity, and part (d) shows that this is the minimal number \\nof dimensions needed. If we want a lower-dimensional view, Proposi\\xad\\ntion 9.2 tells us to take the first q principal components of X, and \\nthis corresponds to taking only the q largest eigenvalues of T' and \\nthe first q columns of CD. This is the optimal approximation in the \\nsense of minimizing the sum of squared dissimilarities minus squared \\ndistances over projections, and hence gives most weight to representing \\nlarge dissimilarities accurately. \\nIf the set of dissimilarities is not Euclidean, we can seek an ap\\xad\\nproximation by a Euclidean set in JRk for small k. We know that \\nT' = CDCT cannot be non-negative definite, but we can set all the \\nnegative elements and the small positive elements of D to zero and use \\nthe columns of CD corresponding to the large positive eigenvalues. If \\nthe dissimilarities are close approximations to Euclidean distances in a \\nsmall number of dimensions, we expect to find a small number of large \\npositive eigenvalues, the rest being near zero. If this is not the case, \\none of the other techniques may be preferable (but is likely to be much \\nmore computationally intensive). \\nOne common mistake with classical scaling is to supply squared \\ndistances: these are not likely to be simply representable by distances. \\nSammon mapping \\nSammon mapping is a multidimensional scaling technique introduced \\nby Sammon (1969) and widely known even where other methods of \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 320}, page_content=\"9.2 Multidimensional scaling 309 \\nmultidimensional scaling are unheard of. Given a dissimilarity d ~n n \\npoints it constructs a k-dimensional configuration with distances d to \\n(locally) minimize \\nNote that this is undefined if there are pairs with zero dissimilarity. In \\ncontrast to principal coordinate analysis, this gives weight to represent\\xad\\ning small dissimilarities accurately, which may be desirable if the plot \\nis being used to detect clusters. \\nSammon used a diagonal Newton method to locally optimize E; \\nthis is a Newton method in which the off-diagonal part of the Hessian \\nis ignored, and the step length reduced by a 'magic' factor of 0.3-0.4. \\nDetails of the algorithm are given in his paper. We have found that \\nit is quite often necessary to use a smaller step-length factor (or even \\nto use a crude search over step length) to avoid divergence . Most \\nimplementations seem to use a random starting point, but starting \\nfrom a classical solution can save much CPU time, if it is a good \\napproximation. \\nThe Sammon mapping for the main group of the viruses example \\nis shown in Figure 9.2 on page 291. This shows much less compact \\ngroupings than the principal components plots. As Sammon mapping \\nis a more accurate representation of small distances, this should caution \\nagainst over-interpretation of those groups. \\nIn the viruses example the Sammon algorithm does not converge \\nat all unless the 'magic' factor is reduced to around 10-3. This is not \\nuncommon behaviour when some points have to move very close to each \\nother in the optimization run. Using a random starting configuration \\n(as is common practice) produced very much worse local minima, with \\nE around 0.3--0.5 rather than 0.07 for the configuration shown. Virus 22 \\n(sunn-hemp mosaic virus) is clearly separated in the mapping of the \\nscaled data. \\nOrdinal methods \\nWhat are known as non-metric or ordinal methods of scaling do not \\nattempt to match the dissimilarity by a distance, but to choose a \\nconfiguration whose distances have similar order properties, that is \\nthat points which have larger dissimilarity from a given point should \\nbe farther away. For such a method it is immaterial whether we \\nsupply (approximate) distances or squared distances, and the fit will be \\ninvariant to overall scale of the dataset, as well as to rigid motions. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 321}, page_content='310 9 Unsupervised Methods \\nA configuration X gives Euclidean distances brs between pairs of \\npoints. We choose an increasing function e so that O(drs) is close to brs· \\nThe sum of squares of the differences is used, for then e can be found \\nby isotonic regression, for which there are simple algorithms (Barlow et \\nal., 1972). This is then minimized over the configuration (standardized \\nto have unit sum of squares from the origin) by a gradient descent \\nalgorithm. Equivalently we minimize \\nover e and the configuration of points giving rise to distances (brs). \\nThis is differentiable with respect to the configuration points (Kruskal, \\n1971; de Leeuw, 1984). \\nOne detail in the implementation of ordinal methods is the treatment \\nof tied dissimilarities. Clearly if dij < dk1 we want O(dij) ::::; O(dkl), but \\nhow should we consider dij ::::; dkl? If we insist that O(dij) ::::; O(dkl), we \\nare attempting to preserve the equality of tied dissimilarities, which can \\nbe a considerable constraint on the solution. It is normal practice to \\nallow such ties to be broken. \\nThe idea of ordinal methods is due to Shepard (1962a, b) and was \\ndeveloped into an objective method by Kruskal (1964a, b). \\n\"\\' .. ., \\n20 .. \\n\" \" .... ,. \\n0 \\n\"\" \\':.t • \\nC)l \\n\"\\'\" ~g111 33 \\n\"\\'t \\n-5 0 5 \\nFigure 9.6 shows a local mtmmum for ordinal multidimensional \\nscaling for the scaled viruses data. This fit is similar to that by Sammon \\nmapping in Figure 9.2, but the subgroups are more clearly separated, \\nand viruses 10 (frangipani mosaic virus), 17 (cucumber green mottle \\nmosaic virus) and 31 (pepper mild mottle virus) have been clearly Isotonic regression is \\nthe name for fitting an \\nincreasing or decreasing \\nfunction by least \\nsquares. The solution is \\npiecewise constant. \\nFigure 9.6: Non-metric \\nmultidimensional \\nscaling plot of the \\nTobamovirus group of \\nthe viruses example. \\nThe variables were \\nscaled before Euclidean \\ndistance was used. The \\npoints are labelled by \\nthe index number of the \\nvirus. \\nThe fit is poor, with \\nSTRESS~ 17%, and \\nwe found several local \\nminima differing in \\nwhere the outliers were \\nplaced. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 322}, page_content='Figure 9.7: Distortion \\nplots of Sammon \\nmapping and \\nnon-metric \\nmultidimensional \\nscaling for the viruses \\ndata. For the right-hand \\nplot the fitted isotonic \\nregression is shown as a \\nstep function. 9.3 Clustering algorithms 311 \\nseparated. Figure 9.7 shows the distortions of the distances produced \\nby the Sammon and ordinal scaling methods. Both show a tendency to \\nincrease large distances relative to short ones for this dataset, and both \\nhave considerable scatter. \\nSammon mapping Non-metric scaling \\n0 2 4 6 8 10 0 2 4 6 8 10 \\nobserved distances observed distances \\nFigure 9.6 shows some interpretable groupings. That on the upper \\nleft is the cucumber green mottle virus, the upper right group is the \\nribgrass mosaic virus and two others, and a group at bottom centre\\xad\\nright (16, 18, 19, 30, 33, 34) are the tobacco mild green mosaic and \\nodontoglossum ringspot viruses. \\n9.3 Clustering algorithms \\nClustering algorithms are methods to divide a set of n observations \\ninto g groups so that members of the same group are more alike than \\nmembers of different groups. If this is successful, the groups are called \\nclusters. The number of groups g may be pre-assigned, or it may \\nbe decided by the algorithm. Formally, a cluster algorithm produces \\na mapping c: {1, ... , n} ~ {1, ... , g} associating a group with every \\nexample. Some (but not all) clustering algorithms work by representing \\neach group by a representative point (not necessarily an example), and \\nthese have close links with vector quantization (Section 6.3). \\nAll of these methods are just algorithms: even those which aim to \\noptimize a criterion are not guaranteed to find the global optimum. Like \\nall unsupervised methods they are judged by their results; a successful \\nclustering produces groups which can be interpreted by domain experts. \\nOf the many books on clustering, Kaufman & Rousseeuw (1990) is \\none of the most practically oriented and has example FoRTRAN programs '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 323}, page_content=\"312 9 Unsupervised Methods \\nwhich can be obtained from file servers. Older and more comprehensive \\nreferences are Anderberg (1973), Hartigan (1975), Spath (1985) and Jain \\n& Dubes (1988). \\nPartitioning methods \\nPartitioning methods divide the examples into a pre-assigned number of \\ngroups. For data in a Euclidean space .JRP we can assign a cluster centre \\nm to each group, and then choose the cluster centres and the groups \\nso as to minimize the sum of squared distances from each example to \\nits cluster centre. Formally, we minimize \\nThe minimization over the cluster centres is easy; we choose the centre \\nof cluster j to be the mean of the examples assigned to cluster j. (Thus \\nknowledge of the clustering c is sufficient to define the cluster centres.) \\nThe hard part is the combinatorial task of minimizing over clusterings. \\nThis method is sometimes called k-means or c-means, although \\nthose terms are also used to refer to specific algorithms. Early refer\\xad\\nences are Forgy (1965), Jancey (1966) and MacQueen (1967), but the \\nISODATA algorithm of Ball (1965) and Hall & Ball (1965) (and Hall \\n& Khanna, 1977) is closely related). All algorithms start with some \\ndivision of the examples into k groups or a set of k cluster centres. In \\nForgy's algorithm all examples are re-assigned simultaneously to their \\nnearest cluster centre, each cluster centre moved to the group's mean \\nand this process repeated. A group can become empty in this algorithm, \\nso it may choose less than k groups. MacQueen 's algorithm differs in \\nthat each example is considered in turn, and the cluster centres are \\nupdated whenever an example is assigned to a group. Both variants \\nalways reduce the sum of squared distances, and so must converge. \\nThe ISODATA algorithm is a variant of Forgy's in which groups are \\nsplit or merged (so k changes dynamically); MacQueen also considered \\nsplitting and merging. \\nA specific algorithm for k-means is given (including FORTRAN code) \\nby Hartigan & Wong (1979). This is based on transferring observations \\nfrom one group to another; other algorithms also allow the exchange of \\nobservations between clusters. Koontz et al. (1975) give a branch-and\\xad\\nbound algorithm to find the global minimum of the k-means criterion, \\nwhich is feasible for small sets of examples. There are also random \\nalgorithms based on the idea of simulated annealing (Flanagan et al., \\n1989; Zeger et al., 1992). This algorithm goes \\nback to Lloyd (1957), \\nwhich was unpublished \\nuntil 1982. \\nJancey's algorithm \\ndoubles the size of the \\nmoves. \\nSee the glossary. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 324}, page_content=\"See the glossary. \\nFigure 9.8: The clusters \\nsuggested by k-means \\nfor k = 6 for the virus \\ndata displayed on the \\nordinal \\nmultidimensional \\nscaling plot. 9.3 Clustering algorithms 313 \\nNote that k-means can assign any future example to one of the \\nk clusters, since it defines a partition of the whole feature space by \\nthe Dirichlet tessellation of the cluster representatives. Most cluster \\nmethods do not have this predictive aspect. \\nFigure 9.8 shows the clusters for 6-means for the virus data. The \\niterative process has to be started somewhere, and in this case was \\ninitialized from a hierarchical clustering discussed below. The choice of \\n6 clusters was by inspection of the visualization plots discussed above \\nand the dendrograms shown in Figure 9.9 (on page 320). \\n0 \\nk-medoids 3 \\n3 \\n3 \\n·5 1 1 \\n\\\\ ~ ' \\n1 1 2 2 \\n2 \\n2 2 \\nThe k-means algorithm must choose centres in IR.P, and so can only be \\nused when the dataset is available and consists of continuous features. \\nWe can overcome these restrictions by insisting that the cluster centres \\nbe examples. Then we seek a clustering c and cluster centres Xmi which \\nminimize \\nsince this squared dissimilarity is the squared Euclidean distance for \\n(scaled) continuous measurements. This may be dominated by outliers, \\nso it is usual to use dissimilarities without squaring. This is known \\nas k-median or k-medoid clustering (Vinod, 1969). Once again there \\nis a local minimization algorithm which reduces the criterion and so \\nmust converge (Kaufman & Rousseeuw, 1990, §2.4). This first selects k \\ncentres, then considers swapping a centre with an example which is not \\na centre and selects the most advantageous such swap. The process is \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 325}, page_content=\"314 9 Unsupervised Methods \\nrepeated until convergence. In general this finds a local minimum, but \\nfor k = 2 it finds the global minimum. For k > 2 Massart et al. (1983) \\ngive a branch-and-bound algorithm to find the global minimum which \\nis only practicable for small sets of examples. \\nClusters of different size and shape \\nA different extension of k-means is to allow the distance measure to \\nvary between clusters, that is to allow the size and/or shape of clusters \\nto vary. This can be motivated by assuming that the examples from \\neach of the groups are drawn independently from densities fj(x;()j), \\nbut that the labels Si which determined which group was appropriate \\nhave been lost. Then both the group density parameters ()j and the \\nlabels are regarded as parameters. (This is hard to justify theoretically, \\nand will normally give inconsistent estimates of ()j, as pointed out by \\nMarriott, 1975.) Then the likelihood is of the form \\nt((()j),(si);Y) = IT!s;(xi;()s;) \\ni \\nand the 'maximum likelihood' assignment of labels gives the clustering \\nof the examples. \\nNow specialize to normal distributions for the classes. Once the \\nclustering is known we can estimate the means as the sample means for \\neach group. Let Wk be the sum of (xi-x)(xi-x)T within group k. \\nThen the profile log-likelihood becomes \\nL((si);Y) = const-L trace(Wi~::j1\\n) + nj log l~jl \\nj \\nwhere nj is the number of observations assigned to group j. \\nNext we make some assumptions about the covariance matrices ~j· \\nIf these are assumed equal to the identity (or to a common multiple \\nof the identity) we recover the k-means criterion (since trace Wk is the \\nsum of squares to the cluster centre for group j ). If we assume that \\nthe variances are equal but otherwise unknown, we find ~ = 2::: Wj/n \\nand the clustering is chosen to minimize 1~1 or equivalently I WI for \\nW = 2::: Wj. (Up to a scale factor, W is the within-group covariance \\nmatrix we used for linear discriminant analysis in Chapter 3.) This \\ncriterion was proposed by H. P. Friedman & Rubin (1967). It can be \\nthought of as applying k-means while allowing the 'sphering' of the \\ndata to be adjusted. Thus its view of clusters is as ellipsoids of the same \\nsize, shape and orientation. Any of these restrictions can be relaxed. \\nScott & Symons (1971) relaxed all, and Banfield & Raftery (1993) \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 326}, page_content=\"9.3 Clustering algorithms 315 \\ndiscuss intermediate cases. The latter also allow a 'uniform' cluster to \\npick up outliers. \\nThese criteria can be optimized by algorithms of the types used for \\nk-means and k-medoids. \\nAdaptive resonance theory \\nThe adaptive resonance theory of Carpenter & Grossberg (1987a, b, \\n1990) and Carpenter et al. (1991a, b, 1992) (see also Moore, 1989; \\nGeorgiopoulos et al., 1990, 1991; Huang et al., 1995) is closely related \\nto adaptive versions of k-means such as ISODATA and MacQueen's \\nalgorithm, but was expressed in a pseudo-biological language that \\nclouds its simplicity. There are a variety of on-line algorithms that \\ngroup the input examples in up to a pre-specified number k of clusters. \\nThe first algorithm, ART 1, works with binary inputs. Let llxll = \\n2:: lxd be the Lt norm, for binary vectors the number of non-zero \\nelements. For each of k groups there is a prototype w j which is \\ninitially set to the vector of all ones (and is called 'uncommitted'). \\nWhen an example x is presented, it is compared in turn with each w j \\nin order of decreasing wJ xj(€ + llwjll) until a prototype is found with \\nwJ x > plixll. If such a Wj is found, the example is assigned to cluster \\nj and Wj is updated by the bitwise operation \\nWj +--Wj AND X. \\nThis algorithm has two parameters. The tolerance € is infinitesimal, \\nserving to break ties in favour of prototypes with more positive ele\\xad\\nments. The parameter p < 1 is called the vigilance, and controls the \\ndiffuseness of the clusters. Note that the first uncommitted prototype \\nto be considered will be selected. Once all prototypes are committed it \\nis possible that none will be selected and the input is then rejected. \\nART 1 is restricted to binary inputs and is highly sensitive to noise, \\nsince Wj can only be made smaller during updating. We can extend the \\nprocess to inputs in [0, 1] by replacing the AND operation by a bitwise \\nminimum. Adding 'momentum' (Moore, 1989) changes the update rule \\nto \\nWj +--(1-,B)min(wj,X) + ,Bwj \\nfor ,8 E [0, 1 ). Complement coding includes both a feature y and \\nits 'complement' 1 -y, so that llxll = p for all examples . With all \\nthese changes we have 'fuzzy ART'. It is often assumed that ,B = 1 if \\nthe prototype is uncommitted: this is called 'fast-commit slow-recode' \\nlearning. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 327}, page_content='316 9 Unsupervised Methods \\nAdaptive resonance theory provides a large family of algorithms, \\nbut only a little analysis has been performed on their properties. It \\nis unclear if they have any advantages over the earlier adaptive k\\xad\\nmeans algorithms. It is clear that they have a major disadvantage \\noriginally pointed out by Moore (1989), of sensitivity to noise. The \\nupdate rule can only reduce the coordinates of the prototypes, so if \\na large number of examples are presented, each having added noise, \\nthe prototypes will shrink towards the zero vector. Prototypes which \\nare close to the zero vector will fail the vigilance test, since wJ x = \\n2:::::: WjiXi ~ max[wj;] llxll-Thus for large enough training sets true clusters \\nwill be divided repeatedly into groups which depend on the order of \\npresentation of the examples. \\nMethods based on mixtures \\nSuppose we believe that the examples come from a mixture of sources, \\nand each has a parametrized density fi(x; 8i). The proportions Wi of \\nthe mixtures are also unknown. The fitting of such mixture densities \\nto data is discussed in Section 6.4. Once the mixture density has been \\nfitted, we can ask for any future observation x what is the posterior \\nprobability that it belongs to component i; this is \\nNow if we view this as a classification problem, we would assign \\nthe observation to the component with highest posterior probability. \\nThis can be applied to the training examples to produce a clustering \\nmethod, which partitions the data into a group (possibly empty) for \\neach component (Wolfe, 1970). \\nThis method is often confused with the likelihood-based partitioning \\nmethod. Both employ models which are mixtures of components. How\\xad\\never, the maximum likelihood method estimates the parameters in the \\ncomponents from classified data, then optimizes over the classifications, \\nwhereas the mixture method fits the parameters in the components and \\nthe mixing proportions from unclassified data. \\nFuzzy clustering \\nIn partitioning methods, each example is definitely assigned to one \\ncluster. Fuzzy logic allows degrees of membership of sets, so would \\nallow us to divide the membership of example i into proportions Uiv \\nfor group v. These membership proportions must be non-negative We assume that the \\nnoise allows each \\ncoordinate to take \\nvalues smaller than p. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 328}, page_content=\"9.3 Clustering algorithms 317 \\nand sum to one. From the perspective of probability theory, Uiv can \\nbe interpreted as a posterior probability of having been generated \\nby component v of a mixture, although in the interpretation below, \\nperhaps Ufv is closer to a posterior probability. \\nThe earliest and best-known fuzzy clustering technique is the fuzzy \\nk-means method of Dunn (1974) and Bezdek (1974). This minimizes \\nmin L L Ufv II xi-mv 112. \\n(U;v) . . \\nl 1 \\nHere the cluster centre is found as the weighted mean of the whole set \\nof examples with weights Ufv· \\nThis method has the same disadvantages as k-means of being \\nrestricted to continuous data with X available. Kaufman & Rousseeuw \\n(1990, §4.4) construct a fuzzy equivalent of k-medoids, to minimize \\nIt can be shown that the variant of this with squared dissimilarities \\nreduces to fuzzy k-means. \\nAuto Class \\nAutoClass (Cheeseman et al., 1988a, b) is a widely-used 'Bayesian clas\\xad\\nsification system' which is based on mixtures. There are J unknown \\nclasses. The major simplifying assumption made is that called idiot's \\nBayes in Chapter 8, that within each class the features are independent. \\n(For a multivariate normal distribution this corresponds to assuming a \\ndiagonal covariance matrix.) A normal distribution is used for contin\\xad\\nuous features, and a general discrete distribution for discrete features. \\nConjugate priors are used for the parameters in the component models. \\nUnder this assumption it claims a full Bayesian solution, including \\na random number J of classes. In practice the integration over the \\nparameters for each class density is too difficult, and the usual approx\\xad\\nimations (expansions about MAP estimators) are used. The value of J \\nis set to a large quantity by trial-and-error, and classes with negligible \\nposterior estimates of proportions are omitted. \\nMode separation \\nEarlier methods of partitioning were based on the idea of separating \\nthe modes of a multimodal density, implicitly assumed to be a mixture. \\nFor example, Henrichon & Fu (1968) considered projecting onto the \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 329}, page_content='318 9 Unsupervised Methods \\nfirst principal component, forming a density estimate (by a kernel \\nestimator or just a histogram), and splitting at each local minimum of \\nthe density estimate. Such a procedure is highly sensitive to the precise \\ndensity estimate used, much more so than would have been realized in \\n1968. Further procedures are described by Devijver & Kittler (1982, \\nChapter 11), but all suffer from the need to estimate densities in a \\nhigh-dimensional space. \\nHierarchical clustering \\nBiologists are used to taxonomic hierarchies: species are grouped into \\ngenera which are grouped into families and so on. Thus we can think \\nof clusters of clusters. Hierarchical methods of clustering produce a \\ntree, usually known as a dendrogram, such as Figure 9.1. This can be \\nread in two directions. From the bottom up, we start with n clusters \\nand the clustering changes at each level as two existing clusters are \\njoined. (This is the agglomerative view.) In the divisive view, we start \\nwith one cluster and successively split clusters into two parts until this \\nis no longer possible. These two views represent different families of \\nalgorithms. It is not necessary to split into two parts or to combine just \\ntwo clusters, but this is easier to compute and so normally done. \\nHierarchical methods avoid specifying how many clusters are appro\\xad\\npriate by providing the user with many different partitions by cutting \\nthe tree at some level (and normally this will achieve a partition into \\nany specified number of clusters). Sometimes this can help to choose \\nan appropriate number, but users should be warned that none of these \\npartitions may be particularly good, even under the criterion used in \\nthe hierarchical algorithm. \\nThe levels on Figure 9.1 represen~ a dissimilarity between examples; \\nwe can define the tree-dissimilarity d,5 as the minimum height in the \\ntree at which examples r and s belong to the same cluster. Such \\ndissimilarities obey not just the triangle inequality but the stronger \\nultrametric property \\ndrt ::::;; min(d, 5, dst)· \\nThus we can think of hierarchical clustering as approximating a given \\ndissimilarity by an ultrametric dissimilarity. \\nAgglomerative algorithms \\nThe essence of an agglomerative algorithm is very simple: pick the two \\nclusters with smallest dissimilarity and merge them. Starting is easy \\n(use each example as a cluster), but we are then faced with defining the '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 330}, page_content=\"9.3 Clustering algorithms 319 \\ndissimilarity between our merged cluster and all other clusters. There \\nare many methods to do so, and no consensus as to which is best. \\nTwo simple ideas are to define the dissimilarity between two clusters \\nto be the minimum and the maximum dissimilarity between pairs, one \\nfrom each cluster, and these give rise to clustering algorithms known \\nas single-link and complete-link clustering respectively. \\nIn single-link clustering, two examples will be joined at level A if \\nand only if we can find a chain of links of pairs of examples with \\ndissimilarity less than A. Thus the tree-dissimilarity drs ~ drs, and it \\ncan be shown that single-link gives the largest ultrametric dissimilarity \\nwith this property. It will tend to produce long and loosely connected \\nclusters, since only a single link is required. \\nIn contrast, complete-link clustering joins two clusters if and only if \\nall members of one cluster are close to the other cluster, and so tends \\nto produce 'compact' clusters, and relatively similar objects can remain \\nseparated up to quite high levels in the tree. \\nThere are many other rules for combining clusters. The only other \\none that is widely considered is group-average clustering, in which the \\ncombined dissimilarity of two groups is the average of all dissimilarities \\nbetween members of each group. Unlike single-and complete-link, this \\ndepends on the scale of the dissimilarities; the other two are equivariant \\nto increasing transformations (such as the square) of the dissimilarities. \\nWe also note that using the increase in the k-means criterion on merging \\nthe clusters is often attributed to Ward (1963). By standard analysis of \\nvariance computations, this attributes a squared dissimilarity of \\nto clusters A and B. \\nFigure 9.9 shows dendrograms produced by single-link, complete\\xad\\nlink and group-average clustering for the viruses data. All identify \\nviruses 10, 11, 17, 22 and 31 as loosely connected to the rest, and \\nsingle-link also highlights virus 46. (We note that 10, 11, 17, 31, 46 and \\n48 are called 'miscellaneous' in the original source.) Nevertheless, each \\ngraph gives the impression of three or four major groupings of viruses. \\nDivisive algorithms \\nDivisive algorithms are much less known (and so much less used). They \\ndo have the advantage that if most interest is on the upper levels of \\nthe dendrogram (for example to produce a partition into k clusters for \\nsmall k) they are much more likely to produce rational clusterings. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 331}, page_content=\"320 9 Unsupervised Methods \\nsingle-link complete-link group average \\nAt the first step, a divisive method has to consider the 2n-l -1 par\\xad\\ntitions of n examples into two non-empty sets. This is computationally \\ninfeasible, so of course only a small proportion of those partitions are \\nactually considered. By analogy to agglomerative methods, we might \\nseek a division into two clusters A and B such that the dissimilarity \\nbetween A and B is maximized. This is infeasible, but we can attempt \\nto approximate it by an iterative method. We could, for example, use \\nany of the partitioning methods into k = 2 clusters that we have dis\\xad\\ncussed earlier. For example, Ward's method could be used divisively \\nby applying 2-means recursively. (This seems not to appear in the \\nclustering literature, but is known as an algorithm for tree-structured \\nvector quantization; Gersho & Gray, 1992, §12.4.) \\nMacnaughton-Smith et al. (1964) proposed a method for general dis\\xad\\nsimilarities which is discussed in detail by Kaufman & Rousseeuw (1990, \\nChapter 5). We first select a single example whose average dissimilarity \\nto the remaining examples is greatest, and transfer that example to \\ncluster B. For all remaining examples of cluster A we compare the \\naverage dissimilarity to B with that to the remainder of A. If any \\nexamples in A are on average nearer to B, we transfer to B that for \\nwhich the difference in average dissimilarity is greatest. If there are no \\nsuch examples the process stops. This process splits a single cluster. \\nWe can then split each of the clusters that are created (unless one is Figure 9.9: \\nDendrograms from \\nthree common \\nhierarchical clustering \\ntechniques applied to \\nthe scaled viruses data. \\nSuch plots show the \\ndissimilarity at which \\nclusters are merged on \\nthe vertical scale and so \\nshow the construction \\nprocess from bottom to \\ntop. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 332}, page_content=\"9.3 Clustering algorithms 321 \\na singleton or all its members have zero dissimilarity from each other) \\nand repeat the process as far as is required. The splits are not uniquely \\nordered; Kaufman & Rousseeuw suggest splitting first the cluster with \\nthe largest diameter (maximum dissimilarity between members), which \\nwill be biased towards clusters with many members. \\nDivisive hierarchical clustering is reminiscent of the classification \\ntree methods discussed in Chapter 7. As there, we can restrict the com\\xad\\nbinatorial explosion by confining attention to splits which involve just \\none of the features; such methods are called monothetic. In Chapter 7 \\nthe value of a split was computed from the distributions of the class \\nvariable in the two daughters; here it must be expressed by the differ\\xad\\nence in the clusters on the feature variables themselves. The obvious \\nidea is to use the dissimilarity between the two daughters, calculated \\nfor example by group-averaging. \\nFor binary variables we can interpret monothetic methods a little \\nfurther. A split on a binary variable will generate clusters that differ \\nonly on the remaining variables, and we want these clusters to be as \\ndifferent as possible. Thus we seek one variable whose difference most \\naccurately reflects the difference of all. This is the aim of association \\nanalysis (Williams & Lambert, 1959). \\nExamples \\nWe will apply clustering methods to the crabs example. Since we saw \\nin Figure 9.3 that the variation was dominated by crab size, the data \\nwere adjusted to crabs of common size, effectively by dividing each \\nmeasurement by the geometric mean of all five measurements on that \\ncrab. Figure 9.10 shows some partitions into four clusters, which we \\nknow in advance to be the correct number. The k-means algorithm \\ndoes rather well (but the clusters are near to spherical here). It is not \\nsurprising that the hierarchical clustering does badly; it has merged \\n200 examples and past groupings will tend to dominate at the last \\nstages of agglomeration. The 'maximum likelihood' clustering with \\nellipsoidal clusters of the same size and shape should do well but does \\nnot, probably because the optimizer used seems less effective. \\nUsing mixtures of four normals with either a common covariance \\n(which in this problem is close to the truth) or separate covariances \\ndid slightly better than k-means, but took considerably longer, the \\nEM algorithm converging in about 10 iterations when started from the \\ncentres of the k-means solution but about 50-100 iterations from a \\nrandom start. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 333}, page_content='322 9 Unsupervised Methods \\n:g \\n0 -0.2 -o.1 0.0 0.1 0.2 \\n(a) \\n~ 9 L-~--------------~ \\n-0.2 -o.1 0.0 0.1 0.2 \\n(c) \\n~ \\n0 \\n\"\\' ~ L-~----------------~ \\n-0.2 -o.1 0.0 0.1 0.2 \\n(e) :g \\n9 \\n:g \\n9 -0.2 \\n-0.2 -o.1 0.0 0.1 0.2 \\n(b) \\n-o.1 0.0 0.1 0.2 \\n(d) \\n~ L_~--------------~ 9 -0.2 -o.1 0.0 0.1 0.2 \\n(f) \\nWe also tried all the applicable programs of Kaufman & Rousseeuw \\n(1990). The results for divisive clustering and fuzzy clustering are \\nshown in Figure 9.10. In this example k-medoids does well, as well \\nas k-means. On the other hand, fuzzy clustering shows little discrim\\xad\\nination, allocating most objects around 40--50% to one cluster, with \\nappreciable proportions to at least two others. The clustering shown \\nis \\'hardened\\' by taking the cluster with the largest membership co\\xad\\nefficient. Macnaughton-Smith et al.\\'s divisive method started with a \\n117/83 division, approximately by sex, then split the sexes by colour \\nform. Group-average clustering had all the females in one group, apart \\nfrom a small group of 5 outliers on the far left of the plot. \\n9.4 Self-organizing maps \\nThe self organizing map is an algorithm developed by Teuvo Kohonen \\n(1982a, b, 1989, 1990a, 1995). This is usually described in the language \\nof neural networks (involving \\'weights\\') and had a biological motivation Figure 9.10: Sammon \\nmapping plots of the \\nLeptograpsus crabs data \\nadjusted for overall size \\nof the example. Plot (a) \\nshows the true \\nclassification . The other \\nfive plots show a \\ndivision into four \\nclusters. Plot (b) shows \\nk-means, initialized by \\nthe (c), complete-link \\nhierarchical clustering . \\nPlot (d) shows \\n\\'maximum likelihood\\' \\nclustering with \\nellipsoidal clusters. Plot \\n(e) shows the \\nclassification by \\nMacnaughton-Smith et \\nal.\\'s divisive method, \\nand (f) shows the \\n\\'hardened\\' classification \\nfrom fuzzy clustering. \\nFuzzy clustering was \\nslow, at least 10 times \\nslower than any other \\nmethod considered. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 334}, page_content=\"9.4 Self-organizing maps 323 \\ndiscussed in these references, Kohonen (1993) and Ritter et al. (1992). \\nIt is, however, just a specific type of clustering algorithm. \\nIn the k-means method we saw that an example was assigned to \\nthe cluster whose representative mj is nearest to the example. This is \\nprecisely what happens in SOM, but the training algorithm attempts \\nto assign some structure to the representatives mi. A large number \\nof clusters are chosen, and arranged on a regular grid in one or \\ntwo dimensions. (Both square and hexagonal grids have been used.) \\nThe idea is that the representatives (called 'weights' by Kohonen) are \\nspatially correlated, so that representatives at nearby points on the grid \\nare more similar that those which are widely separated. \\nThis process is conceptually similar to multidimensional scaling. \\nThat maps similar examples to nearby points in a q-dimensional space. \\nIf we were to discretize the q-dimensional space, for example by dividing \\nit into a grid of square bins, we would have a mapping from the space \\nof possible examples into a discrete space that provides a clustering. \\nFurther, we could average the examples which are mapped to each bin to \\nprovide a representative for each non-empty bin, and the representatives \\nin nearby bins would be similar. This is precisely the spirit of SOM, \\nand it is often used to provide a crude version of multidimensional \\nscaling. Indeed Kohonen says \\n'I just wanted an algorithm that would effectively map similar patterns \\n(pattern vectors close to each other in the input signal space) onto \\ncontiguous locations in the output space.' (Kohonen, 1995, p. VI.) \\nWe have a spatial smoothness property of the cluster representa\\xad\\ntives which Kohonen refers to as topological ordering. Cherkassky \\n& Mulier (1994) draw analogies with principal curves, but those with \\nmultidimensional scaling seem closer. \\nKohonen defined an 'on-line' algorithm, so examples are presented \\nin some order (possibly random) until convergence. The cluster repre\\xad\\nsentatives are initially assigned at random in some suitable distribution. \\nWhenever an example x is presented, the closest representative mj is \\nfound. Then \\nm; +--m; + a[x-m;] for all neighbours i (9.3) \\nfor all representatives i which are neighbours of j on the grid. Both \\nthe constant a and the definition of 'neighbour' are allowed to change \\nwith time. A typical specification is that a might decline linearly from \\n1.0 to 0.04 over 1000 examples, then linearly to zero over the second \\nthousand, while the definition of a 'neighbour' is a grid point i within \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 335}, page_content=\"324 9 Unsupervised Methods \\ndistance r of j, where r declines linearly from 6 to 1 over the first \\n1000 examples. \\nThis defines just an algorithm , and the result will depend on the \\nrandom initialization , the order of presentation of the examples and \\nthe tuning of the constants. Clearly it will be necessary to start with \\nfairly large neighbourhoods, or no global order will emerge. Rather \\nthan update all clusters within the neighbourhood equally, it is natural \\nto have a distance-weighted factor within the update, so \\nfor all i (9.4) \\nwhere hiJ depends on the proximity of i to j, for example hiJ(t) = \\na1 exp -[d(i,j)/a1f. It is possible that some representatives may never \\nget updated unless the initial neighbourhoods are very large. On the \\nother hand, if the neighbourhoods are large, the representatives get \\nupdated in blocks, and it is wasteful to have so fine a grid. It is clearly \\nbetter to refine the grid rather than shrink the neighbourhoods , an idea \\nHaykin (1994) attributes to Luttrell (cf Luttrell, 1989). \\nIt is helpful to note what happens if we take neighbourhoods so \\nsmall that they only contain one point. Then there will never be any \\nconnection between representative points, and we might expect SOM to \\nreduce to k-means clustering. It does. Although the algorithm appears \\nto update only the representative for the cluster that x joins and not \\nthe one it leaves, the latter is achieved by the continual presentation \\nof examples and the 'forgetting' property for a > 0. Thus mj is an \\nexponentially-weighted average of all examples which have ever been \\nassigned to cluster j, and will eventually become the average of a stable \\nset. This suggests that we can regard SOMas a spatially smooth version \\nof k-means, and assess the degree of fit of a particular solution by the \\nquantization error, the sum of squared differences between examples \\nand the corresponding cluster centres. \\nAnalysis of the algorithm has been hampered by the lack of an \\n'energy' function that the algorithm can be considered to minimize, \\nand Erwin et al. (1992) showed that in general no such function exists. \\nHowever, if we restrict attention to randomly sampling from a training \\nset and take fixed neighbourhoods, clearly a suitable energy function is \\nV = ELhijiiX-md2 \\nj \\nwhere I is the cluster to which the randomly chosen input X is assigned \\n(Ruzicka, 1993). For randomly sampled inputs from a population, few \\nresults are known except for the special case of just one feature on [0, 1] \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 336}, page_content=\"Figure 9.11: SOM \\nmapping of the crabs \\ndata to a 6 x 6 grid. \\nThe labels of those \\nexamples mapped to \\neach cluster are \\ndistributed randomly \\nwithin the circle \\nrepresenting the cluster. \\nAs before the coding is \\nupper case for males, \\nlower case for females, \\n'B' for blue and '0' for \\norange. 9.4 Self-organizing maps 325 \\nCD @ @Q 0~ ® ~ \\n® 0 8 0 Q 0 ~ b \\n© 0 0 8 ~ b (;) b ® ~ \\n~ 0 0 00@ \\n~ C) 0 G0 Q \\n~ G ~ \\\\ @E!) Q \\nand a linear grid of representations (Cottrell & Fort, 1987; Bouton & \\nPages, 1993, 1994; Fort & Pages, 1993). \\nA batch version of SOM has been proposed much more recently \\n(Kohonen, 1995, §3.14). This is a simple adaptation of Forgy's algo\\xad\\nrithm for k-means; simultaneously for all clusters the representative is \\nupdated to the (weighted) means of examples which are mapped to a \\nneighbour of the cluster. This step is iterated, slowly decreasing the size \\nof the neighbourhoods. \\nThe results of SOM mapping of the crabs data to a 6 x 6 grid are \\nshown in Figure 9.11. (The grid size was chosen to allow a reasonable \\nnumber of the 200 examples to be mapped to each representative.) \\nThis figure illustrates the difficulty of displaying an SOM map. We \\nhave five-dimensional data, so cannot show the representatives directly, \\nneither on the grid nor as points in the feature space. What we can \\ndo is map each example to its nearest representative (its cluster centre) \\nand display the clustering, as we show in the figure. \\nContiguity-constrained clustering \\nKohonen 's SOM produces a grid of clusters. Often we wish to group \\nthose clusters into super-clusters, preserving the spatial smoothing. \\n(This is pertinent for Figure 9.11.) One way to do so is to use \\nsegmentation methods from image analysis. Many of these reduce \\nto agglomerative hierarchical clustering methods with contiguity con\\xad\\nstraints. Suppose we consider the group-average method of clustering, \\nbut only allow clusters to be merged if they are neighbours (that is \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 337}, page_content='326 9 Unsupervised Methods \\nthat there are members a E A and b E B which are neighbours on \\nthe grid). Then by construction clusters will always be connected sub\\xad\\ngraphs of the grid. Such algorithms are discussed by Gordon (1981), \\nMurtagh (1985, 1995a) and Beaulieu & Goldberg (1989) and applied \\nto a grid of SOM clusters by Murtagh (1995b). \\nOther visualization strategies for SOMs are given by Ultsch (1993a, \\nb), which display the similarity of the representatives mj by showing \\nthe magnitude of the gradient, viewing the representatives as a vector \\nfield (Figure 9.12). Figure 9.12: Ultsch \\nrepresentation of the \\nSOM representatives \\nfor Figure 9.11. The \\nrows and columns \\nbetween the units \\nrepresent the magnitude \\nof the gradient (black \\nbeing high); the \\ngreylevel at each unit \\nrepresents the median \\nof the surrounding \\ngradients. The label is \\nthe most common class \\nmapped to that \\nrepresentative. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 338}, page_content='10 \\nFinding Good Pattern \\nFeatures \\nIn this chapter we consider the problem of what features should be \\nincluded when designing our classifier. We should make clear at the \\noutset that this is an impossible problem; there may be no substitute \\nfor trying them all and seeing how well the resulting classifier works. \\nHowever, this may be computationally impracticable, and unless a large \\ntest set is available it may be impossible to avoid selection effects, of \\nchoosing the best of a large class of classifiers on that particular test \\nset and not for the population. \\nTo illustrate the difficulty, consider a battery of diagnostic tests \\nT1. ... , T m for a fairly rare disease, which perhaps around 5% of all \\npatients tested actually have. Suppose test T1 correctly picks up 99% \\nof the real cases and has a very low false positive rate. However, there \\nis a rare special form of the disease that T1 cannot detect, but T2 \\ncan, yet T2 is inaccurate on the normal disease form. If we test the \\ndiagnostic tests one at a time, we will never even think of including \\nT2, yet Tt and T2 together may give a nearly perfect classifier by \\ndeclaring a patient diseased if Tt is positive or Tt is negative and T2 \\nis positive. This illustrates that considering features one at a time may \\nnot be sufficient. \\nOur aim in this chapter is to indicate single features which are \\nlikely to have good discriminatory power (feature selection) or linear \\ncombinations of features with the same aim (feature extraction). Unfor\\xad\\ntunately the methods described can be quite effective with conventional \\nstatistical methods (linear and quadratic classifiers) but rather ineffec\\xad\\ntive with modern non-linear classifiers. One reason that this is the last \\nchapter of the book is that its methods are being supplanted by the \\nmodel selection methods discussed in earlier chapters. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 339}, page_content='328 10 Finding Good Pattern Features \\nThroughout this chapter we will work with the conventional 0--1 \\nloss although some of the ideas can be extended to situations with \\ngenuine costs for erroneous classifications. Thus here the Bayes risk is \\nthe error rate of the Bayes rule. We will also concentrate on K = 2 \\nclasses, as this suffices to illustrate the principles involved. \\n10.1 Bounds for the Bayes error \\nThe \\'gold standard\\' for a classifier was seen in Chapter 2 to be the \\nBayes error, the risk of the Bayes classifier. The Bayes classifier does \\nhowever depend on the information available, and the Bayes error will \\nbe higher if only some of the features are measured. Thus it is of interest \\nto estimate the Bayes error as a function of the variables included in \\nthe classifier design. Exact calculations are impossible (except in trivial \\nproblems) but· we can obtain reasonable upper and lower bounds. \\nThinking in terms of the Bayes error tells us immediately which \\nfeatures we ideally need, p(c I x) for K -1 of the classes. Of course this \\nis unattainable in practice, although it is one view of the derivation of \\nlinear and quadratic discriminants for normal class distributions. \\nIn a two-class problem the Bayes error is \\nE. = j min[p( 11 x), p(2! x)] p(x) dx = j min[ntPl (x), n2P2(x)] dx \\nand since min( a, b) ~ a5b1-s for any s E [0, 1], we have \\nE* ~ j[ntPt(xW[n2P2(x)]1-sdx = nfnl-sexp-Jc (10.1) \\nsay, where \\n(10.2) \\nThis is known as the Chernoff bound on the Bayes error (Chernoff, \\n1952, 1973). The special case of s = 1/2 was derived earlier by \\nBhattacharyya (1943) and is therefore known as the Bhattacharyya \\nbound. Because of its greater simplicity it is much more widely used. \\nWe will evaluate the Bhattacharyya bound for two normal distribu\\xad\\ntions. It becomes \\nE* ~ fo1Ci. exp-JB \\nJB = ~(Jlt- Jl2f[i(~l + ~2))-1(Jlt-Jl2) \\n+ 110 l!(~t + ~2)! 2 g J1~1\"\\'~2\\' (10.3) '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 340}, page_content=\"1-2El ~ \\n1-4£'(1-E') \\n= (1-2£')2, and \\nE~. E' ~ 1/2, so \\n.J1-2El ~ 1-2E'. 10.2 Normal class distributions 329 \\nThe second term of J B disappears if the two covariance matrices are \\nequal; in that case the Chernoff bound is tightest for s = 1/2. \\nDevijver & Kittler (1982, p. 58) point out that we can also obtain a \\nlower bound on the Bayes risk in terms of the Bhattacharyya coefficient. \\nThe 1-nn rule has asymptotic risk \\nE1 = j 2p(11 x)p(21 x) p(x) dx ~ j Jp(11 x)p(21 x) p(x) dx \\nsince p(11 x)p(21 x) ~ 1/4. Thus E* ~ E1 ~ jif11f2 exp -JB. However, \\nProposition 6.1 gives E1 ~ 2£*(1-E*) which we can invert to give \\n! [1-j1-4n1n2exp-2JBJ ~! [1-)1-2£1] ~ E* (10.4) \\nOf course, these bounds are only of any use if we know (or can estimate \\naccurately) the Bhattacharyya coefficient. \\nThe Chernoff and Bhattacharyya coefficients are only two of a large \\nclass of separation measures which indicate how dissimilar two proba\\xad\\nbility distributions are, in our case applied to the two class densities. \\nOther measures are the divergence \\n!{ } P1(x) \\nJv = 1t1P1(x)- 1t2P2(x) log P2(x) dx (10.5) \\nand the Patrick-Fisher coefficient (Patrick & Fisher, 1969) \\n[ ] 1/2 \\nJ p = j { n1P1 (x) -n2p(2(x)} 2 dx (10.6) \\nThe idea is to use one of the J coefficients to indicate how good a set \\nof features is likely to be; large values of the coefficient indicate that \\nit is likely that a classifier with low error rate can be found (although \\nthis is only guaranteed for Jc and JB ). The divergence Jv is signed, \\nso we would look for large absolute value. \\n10.2 Normal class distributions \\nIn practice the class probability densities Pi(x) are unknown, but \\nprogress can be made if we assume that they are normal distribu\\xad\\ntions. We have already seen at (10.3) the expression for JB. There is a \\nsimilar expression for Jc, and we have \\nJv = !(1'1 -p2)T[:E11 + :E21HI'1 -p2) \\n+!trace [:E!1:E2 + :E21:E1-21] (10.7) \\n1 1 2 )p = + - X \\nj(2n)PI2:E11 j(2n)PI2:E21 j(2n)PI:E1 + :E2I \\nexp -!(1'1 -1'2f [:E1 + :E2r1(1'1 -1'2). (10.8) \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 341}, page_content='330 10 Finding Good Pattern Features \\nNote that for equal covariance matrices (~1 = ~2 = ~), Jv = 8JB = \\n(p1 -p2f~-1(p1 -p2) is the Mahalanobis distance between the class \\nmeans. We saw on page 22 that the expected error rate for the linear \\nclassifier depended only on the Mahalanobis distance, so maximizing \\nJ B or J v is equivalent to minimizing the expected error rate of the \\nlinear classifier. \\nOther class separation measures commonly used are trace(w-1 B) \\nand IBI/ITI for the between-class B, within-class W and total T \\ncovariance matrices defined in Section 3.1. These too are obviously \\nclosely related to linear discrimination, and for K = 2 the trace measure \\nreduces to the Mahalanobis separation of the means. \\nWhy should we use these measures rather than fit a linear or \\nquadratic classifier and measure its performance directly? If compu\\xad\\ntation permits, there is no reason not to assess performance directly, \\nespecially if the performance can be assessed on the actual distri\\xad\\nbutions rather than normal distributions. Even for assumed normal \\ndistributions we can compute the expected error rate, using numerical \\nintegration for K > 2 classes or for quadratic classifiers. So separation \\nmeasures are best seen as a computational short cut for suboptimal \\nfeature selection. (Feature extraction in linear classifiers is simple: use \\nthe linear discriminants.) \\nMost feature selection methods such as forwards and backwards \\nselection and branch-and-bound (discussed in the next section) change \\nthe set of features under consideration by adding or deleting a single \\nfeature at a time. The various measures depend on the means Jli \\nand variance matrices ~i· These can readily be found by taking the \\nappropriate subsets of the mean vector and covariance matrix for all \\nfeatures, but it is worth noting that updating formulae for the inverses, \\ndeterminants and traces that occur in the separation measures are \\navailable (for example in Devijver & Kittler, 1982, pp. 266-267). \\nWe have acted as if the class means and variances are known. \\nIn practice they are estimated from data, and we may bias-correct \\nthe formulae for separation measure by similar ideas to those used in \\nSection 2.4. The correction for JB is given in Hjort (1986, §10.3). \\n10.3 Branch-and-bound techniques \\nThe simplest feature selection strategies are stepwise ones. Suppose we \\nwish to choose that combination of k < p features which maximizes \\nsome measure J of class separation or classifier performance. We will \\nassume that J is monotone, so that adding features is guaranteed not For example, using a \\ntest set or \\ncross-validation. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 342}, page_content=\"10.4 Feature extraction 331 \\nto decrease J. Forwards selection adds a feature at a time, at each stage \\nchoosing the addition that most increases J. Backward elimination starts \\nwith all p features and at each step drops the feature whose presence \\nleast increases J. The backward and forward procedures are optimal at \\neach stage, but are unable to anticipate interactions between features \\nof the sort we considered at the beginning of the chapter. Exchange \\nstrategies would start with a subset of size k, perhaps found by forward \\nor backward methods, then try exchanging a feature in the set with any \\noutside it. \\nThese strategies are all heuristics to avoid considering all of the very \\nlarge number of subsets of size k of p variables for even moderate \\np. We can often find a subset which is guaranteed to be best of \\nsize k without considering all subsets by the technique of branch-and\\xad\\nbound which is well known in combinatorial optimization and artificial \\nintelligence (Winston, 1992, Chapter 5), and was considered in this \\ncontext by Narendra & Fukunaga (1977). In choosing subsets of a \\nregression, the procedure is best known from the algorithm of Furnival \\n& Wilson (1974). \\nBranch-and-bound allows us to eliminate subsets A from consid\\xad\\neration if we know that a larger subset A' has a value of J which is \\nbelow that of our current best estimate a of the maximum value of \\nJ(A) over subsets of size k, for by monotonicity, necessarily J(A) < a. \\nAn initial estimate of a is found by one of the heuristic searches (or \\nit is set to -oo ). We start by considering the set A of all the features, \\nand search the tree of subsets of size at least k found by dropping \\none feature at a time. Whenever we find a subset A with J(A) < a \\nwe prune the tree at that point, to ensure that we do not consider \\nany subsets of A (which can also occur elsewhere on the tree with the \\nvariables in a different order, and should also be pruned). Whenever \\nwe find a subset of size k with J(A) >a, we remember the subset and \\nincrease a to this J(A). \\nThere are a number of strategies for the actual search of the tree. \\nWe would like to consider 'good' subsets first, and we need to reach the \\nleaves (the subsets of size k) to be able to increase a. So the search \\nneeds to be depth-first in 'good' subsets, and is aided by having a good \\ninitial estimate of a. \\n10.4 Feature extraction \\nFeature extraction is generally used to mean the construction of linear \\ncombinations aT x of continuous features which have good discrim-\"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 343}, page_content=\"332 10 Finding Good Pattern Features \\ninatory power between classes. It is naturally part of finding linear \\nclassifiers, and it is also often used as a data reduction technique , to \\nreduce the number of features to be input to a non-linear classifier. \\nThe simplest (and by far the most commonly used) method of \\nfeature extraction is to take the principal components of x. This \\nwas done, for example, by Candela & Chellappa (1993) in studies \\nof fingerprint images, and by Grother & Candela (1993) in studies \\nof hand-written zip codes. Apparently principal components have \\nnothing to do with discriminatory power (they are an unsupervised \\ntechnique) and it is easy to envisage (and find) examples where they \\nhave little discriminatory power. In problems where the features have \\nbeen carefully scaled and are highly correlated (like images), large \\nvariance of a linear combination may imply that it varies across classes. \\nIt is possible, at least in principle, to maximize a measure of class \\nseparation over one or a few linear combinations of the features. This \\ncan be seen as a supervised version of projection pursuit (Section 9.1), \\nin which the measure of 'interestingness' of the projection is related to \\nhow well it separates the known classes. Of course, we have to know the \\nclass-conditional densities on the projection, but they can be estimated \\nby the methods of Chapter 6, especially by kernel methods. Devijver \\n& Kittler (1982, §8.2.2) suggest that the Patrick-Fisher measure is most \\nsuitable for feature extraction with Gaussian kernel estimation since \\nits derivative with respect to the projection direction a can be found \\nanalytically. Blue et al. (1994) is the \\njournal version of these \\nreports. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 344}, page_content=\"A \\nStatistical Side I i nes \\nThis appendix explains more of the background of some statistical \\nideas which are used at several points in the main text, but may not be \\nwell-known even to statistical readers. \\nA.1 Maximum likelihood and MAP estimation \\nA prototypical statistical problem is to estimate the value of some \\nparameter 8 from a finite set !T = (Xi) of data. (In the parlance of \\npattern recognition, we will refer to this as the training set.) Since 8 \\nis described as a parameter, this implies the existence of a family of \\nprobability densities p(x; 8) for 8 E 0, and we will assume that the \\nobservations Xi are independent samples from an unknown density po, \\nwhich might be p( ·; 8) for some 8, but need not be. \\nTwo technical asides. The assumption of independence is easily \\ncircumvented by taking all the observations as X1. Readers not used \\nto measure-theoretic treatments of probability theory will associate \\ndensities with continuous distributions and probability mass functions \\nwith discrete ones. As probability mass functions are densities in the \\nrigorous theory (with respect to counting measure) it is permissible to \\ncall both 'densities' and we do so. \\nThe likelihood is a function of 8 defined by \\nt(8; !T) = p(!T; 8) = IT p(xi; 8). \\ni \\nAlthough it is another expression of the joint density, the notation \\nreflects the change in emphasis to fixed data and varying parameter. \\nThe maximum likelihood estimator (MLE) then associates with each \\ntraining set a value of 8 which maximizes t( 8; !T), or \\n8(!T) = argmax t(8; !T). \\ne \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 345}, page_content=\"334 A Statistical Sidelines \\nWe will almost always drop the dependence on f7 and regard (j as a \\nrandom variable. \\nFor sufficiently regular problems (for which the likelihood is differ\\xad\\nentiable and the maximum occurs in the interior of 0) the maximum \\nlikelihood will occur at a stationary point of the log-likelihood, and \\nthis is the most common way to find e. Beware though that the MLE \\nneed not occur at a stationary point, even a local maximum, but could \\noccur at the boundary of 0. \\nIn the Bayesian paradigm, the parameter vector e is random, and \\nso itself has a distribution. The posterior density of 8 can be found by \\nBayes' formula as \\np(81f7) cx:p(f718)p(8) =t'(8;f/)p(8). \\nA MAP (maximum a posteriori) estimator of e maximizes p(8 If/) \\nor, equivalently, t( e; fl) p( 8). Thus the maximum likelihood estimator \\nis a MAP estimator for the 'flat' prior over 0, the possibly improper \\ndistribution with uniform density. This highlights the problem with \\na MAP estimator for a continuous parameter; it finds the mode of \\na density. Densities are with respect to an underlying measure, and \\nthe MAP will depend on that measure. This implies that it will not \\ntransform in a sensible way. Suppose e is a parameter expressing \\na variance. Do we want the MAP of the variance. e, the standard \\ndeviation J(J, the precision K = 1/8 or the log-variance loge? The \\nmaximum likelihood estimator will transform in the way you would \\nexpect (we say it is equivariant to 1-1 transformations) but a MAP \\nestimator will not. Only if the posterior density is highly concentrated \\nabout its mode and we allow only smooth transformations is the \\nMAP estimator approximately equivariant. Thus MAP estimators are \\nmost useful as a simple summary of a highly concentrated posterior \\ndistribution. \\nA.2 The EM algorithm \\nThe Expectation-Maximization (EM) algorithm is a device to help find \\nmaximum likelihood estimators in a problem with unobserved data. \\nSuppose we have data X which have been observed and data Y \\nwhich have not, and a vector of parameters e. The goal is to find the \\nmaximum likelihood estimator of e given the observed data X in a \\nsituation in which the joint density p(x, y; 8) is known explicitly, but \\nthe marginal density of X, p(x; 8), can only be found by numerical \\nsummation or integration from the joint density. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 346}, page_content=\"A.2 The EM algorithm 335 \\nOne application is to problems where we have a series of pairs \\n(Xi, Yi) of which only Xi is observed (and Yi is often the label for a \\ncomponent of a mixture). Then the log-likelihood is \\nL(8;(Xi)) = Llog J p(xi,y;8)dy \\nl \\nand the presence of the logarithm inhibits any simplification. Another \\napplication is to missing observations of a few of the features. The \\nidea has many precursors, including the Baum-Welch algorithm in \\nspeech recognition (see Baum et al., 1970), but was developed in some \\ngenerality by Dempster et al. (1977). \\nLet Q(8,8') = E[logp(X, Y;8) I X;8'] which is also a function of \\nthe observed data X; the conditional expectation is over the values of \\nY, and is evaluated as if 8' were the true parameter. The EM algorithm \\nstarts at some value 8(0) and alternates two steps: \\nE Find Q(8,8U-1l) = E[logp(X, Y;8) 1 X;8U-1l]. \\nM Choose 8(il to maximize Q(8, 8U-1l). \\nEach iteration increases the log-likelihood L( 8; X) = log p(X; 8). Write \\nL(8;X) = logp(X, Y ;8) -logp(Y I X;8) \\nand take expectations using the density p( Y I X; 8') to obtain \\nL(8;X) = Q(8, 8')-E [logp(Y I X; 8) I X; 8']. \\nNow consider expectations E' with respect to p(Y I X; 8') and let \\nh(Y) = p(Y IX;8)/p(Y IX;8'). Then E'logh(Y) ~ E'h(Y)- 1 = 0 \\nfrom log x ~ x -1. Thus \\nE'logp(Y IX;8) ~ E'logp(Y IX;8'). \\nNow suppose Q(8, 8') > Q(8', 8'), so \\nL(8;X) = Q(8,8')-E'logp(Y IX;8) \\n> Q(8', 8')-E'logp(Y I X; 8') \\n= L(8' ;X). \\nThe increase in likelihood at an EM iteration will be positive provided \\nQ(8, 8U-1l) > Q(8U-1l, 8U-1l), and this is so unless 8(i-l) is already the \\nmaximizer. These arguments still apply to what is often called a GEM \\n(generalized EM) algorithm in which Q(8, 8U-ll) is not fully maximized, \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 347}, page_content=\"336 A Statistical Sidelines \\nbut eUl is chosen to increase its value (except at a global maximum, \\nand perhaps at a local maximum). \\nThe convergence properties of ( G )EM algorithms are often stated \\nrather loosely. At each step the log-likelihood is increased. In a problem \\nwhich does have a finite maximum to the likelihood (and by no means \\nall mixture problems do) the sequence L( eUl; X) is bounded above, and \\nso has a limit. That limit need not be a local maximum, but it will be \\nunder mild regularity conditions. (Convergence properties are discussed \\nby Boyles, 1983, and Wu, 1983; the proofs in Dempster et al., 1977, \\nare flawed.) Under further regularity conditions we can show the less \\nimportant condition that the sequence (8Ul) itself converges to a local \\nmaximizer of the likelihood. \\nIt is often useful to know the Hessian at the (local) maximum \\nlikelihood solution, for example to find asymptotic standard errors. \\nLouis (1982) gives an algorithm to do so, based on the complete-data \\nlikelihood. \\nThere is also a Bayesian view of the EM algorithm as a way to \\nfind posterior modes for a subset of the parameters. Write 8 = ( cp, 1p) \\nwhich in the Bayesian paradigm is a random vector, and suppose we \\nwish to find a mode of the posterior density p( cp I X). Now take 1p as \\nthe unobserved data. Since \\nlogp(c/J I X)= logp(c/J, 1p I X) -logp(1p I c/J,X) \\nthe same arguments apply to Q( cp, cp') = E [log p( cp, 1p I X) I c/J', X], so \\nthe EM algorithm can be used to help find a MAP estimator of cp. \\nThere are 'on-line' versions of the EM algorithm, given for exam\\xad\\nple by Titterington (1984), Celeux & Diebolt (1992) and Jordan & \\nJacobs (1994). \\nMixture distributions \\nMost applications of the EM algorithm are either to missing data or \\nmixture distributions. The latter are often particularly simple, and were \\ndiscussed by Dempster et al. (1977). Suppose we have a density of the \\nform \\np(x) = L wif;(x; cp;) \\nwhere the parameters cp of the densities may have common components \\n(for example a common covariance matrix in a Gaussian mixture), \\nand the mixing weights (w;) are unrestricted (apart from forming a \\ndiscrete distribution for the component I ). The parameter vector () \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 348}, page_content='A.3 Markov chain Monte Carlo 337 \\nencompasses the weight distribution (w;) and all qy;. We regard the \\ncomponent I as the missing data: Then the E step gives \\nQ(8, 8\\') = L L n;(x) [logf;(x; qy;) +log wd \\nxEY i \\nwhere \\n( ) [ ( .) I 8,J wif;(x; 4J;) \\nn; x = E I I = l X= x; = \"\\' ·f·( . A.·)· wj WJ 1 X,\\'+\\'] \\nTo maximize this over (w;) we need only consider the second term of \\nQ(8, 8\\'), so we are maximizing 2:: n; log w; where (n;) is the average of \\nthe n;(x) over the training set. We have \\nso 2:: n; log w; :::; 2:: n; log n; and w; = n;. For the parameters qy; we \\nmaximize the weighted log-likelihood \\nL L n;(x)logj;(x;qy;). \\ni xEY \\nIf there are no common parameters, each 4J can be found separately. \\nA.3 Markov chain Monte Carlo \\nMarkov chain Monte Carlo methods are iterative methods to simulate \\nfrom distributions that are not easily simulated by more direct meth\\xad\\nods. They have been used to simulate stochastic processes for many \\nyears (Metropolis et al., 1953; Ripley, 1977, 1979; Geman & Geman, \\n1984; Ripley, 1987), but have recently become popular in mainstream \\nBayesian statistics following their espousal by Gelfand & Smith (1990) \\nand Gelfand et al. (1990). (See Geyer, 1992; Smith & Roberts, 1993; \\nBesag & Green, 1993; Tierney, 1994; Besag et al., 1995; and Gelman \\net al., 1995, for recent reviews.) \\nWe will consider a finite collection Xv, v E V for random variables, \\nand use the notation of Chapter 8, that XA denotes the collection \\nX a, a E A c V. We are interested in sampling from the whole collection \\nX v or from the conditional distribution of XA given XAc, often with \\nthe aim of finding aspects of the marginal distribution of some subset A. \\nWe can in principle sample successively from the marginal distribution \\nof X1, then from X2l X1, X3l X1,X2 and so on, but these distributions \\nmay not be known sufficiently explicitly to sample from. Suppose we \\ndo know how to sample from the conditional distribution of Xv given '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 349}, page_content=\"338 A Statistical Sidelines \\nX V\\\\{v} for each random variable (as is required at the last step of \\nsuccessive sampling). Then starting with some set of values for X v, we \\ncan pick a variable, and sample it conditionally on the rest. This is \\nrepeated for all the variables in some order, and is what is known as \\nthe Gibbs sampler. This term is due to Geman & Geman (1984), who \\nshowed that for discrete random variables with a finite state space and \\nno zero-probability configurations the joint distribution of X v after n \\nsamples converges to the required joint distribution provided only that \\neach random variable is visited infinitely often. Thus we can visit the \\nvariables in random or systematic order, and visit some more often \\nthan others. \\nThe restriction to a strictly positive joint distribution is an essential \\none, as without it the Gibbs sampler may fail. Consider three binary \\nrandom variables A, B, C such that B and C are independent given \\nA = 0, but A = 1 implies B = C = 1. The Gibbs sampler will \\neventually reach the state A = B = C = 1 and be unable to escape. \\nAssuming irreducibility (that we can move with positive probability \\nfrom any configuration with positive probability to any other in a finite \\nnumber of steps) saves convergence (Ripley, 1987, §4.7). Sometimes \\nirreducibility can be retrieved by grouping vertices or by taking zero \\nprobabilities to be the limit of very small probabilities (Sheehan & \\nThomas, 1993), but there are practical limits to the value of these \\n'tricks'. It is widely assumed that this result still holds for continuous \\nrandom variables, although the theory is much more complicated and \\nfurther mild conditions are required (Chan, 1993; Smith & Roberts, \\n1993, Appendix A; Tierney, 1994). \\nIn the discrete case there are many other MCMC schemes. We \\nconfine attention to configurations with positive probability (without \\nany loss of generality). Suppose we have a transition kernel q(xv, x~) \\nwhich gives the conditional probability of moving from configuration \\nxv to configuration x'v· A Metropolis algorithm generates a move \\naccording to this conditional probability, and accepts it with probability \\nmin{1,p(x~)/p(xv)}. This converges to the joint distribution provided \\n(a) the kernel q is symmetric, \\n(b) the process is irreducible, and \\n(c) the process is aperiodic. (This says that the feasible return times to \\na state have period one, and precludes only returning with positive \\nprobability at even times, for example.) \\nA Hastings algorithm (Hastings, 1970) allows an asymmetric kernel q, \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 350}, page_content=\"A.4 Axioms for conditional independence 339 \\nand accepts the move with probability \\n. { 1 p(x~ )q(x~, xv)} m1n , , . \\np(xv )q(xv, xv) \\nThis process converges if it is irreducible and aperiodic. Suppose in a \\nMetropolis-Hastings algorithm we consider moves which change only \\none random variable. If this is Xv we have \\np(x'v) p(x~ I X V\\\\{v} = XV\\\\{v}) ----= ------~~--~~ \\np(xv) p(xviXV\\\\{v}=Xv\\\\{v}) \\nwhich needs only the conditional distributions as for the Gibbs sampler. \\nUnlike the Gibbs sampler it needs only ratios of distributions, so these \\nneed not be normalized. There are a few continuous analogues of the \\nMetropolis-Hastings method. \\n'Blocked' Gibbs sampler methods use the Gibbs sampler on blocks \\nof random variables rather than on single variables. Sometimes this \\nis a device to ensure irreducibility or to encourage faster convergence. \\nOn the other hand, it can be quite natural. Consider a general mixture \\ndistribution as at the end of Section A.2. We can treat the set of \\nindicators (Ii) of the unknown components for each member of the \\ntraining set as a block, and sample these simultaneously given the real \\nparameters. \\nMuch of the theoretical work when using an MCMC method is \\nproving irreducibility, and in practical examples this can be lengthy \\n(Grenander et al., 1991). In practice the difficulty is knowing when the \\nprocess has reached equilibrium, and how long to wait between sampling \\nXA if (approximately) independent samples are required. There is much \\ndiscussion of empirical methods of assessing convergence in the surveys \\ncited at the beginning of this section, but none would recognize the \\npseudo-equilibrium behaviour reported by Ripley & Kirkland (1990), \\nwho give an example in which equilibrium has not been approached \\nafter each random variable in the system has been sampled 10,000 \\ntimes, although the process has appeared stable for 9,500 passes. So \\ngreat care is needed! \\nA.4 Axioms for conditional independence \\nWe will use the notation X .JL Y I Z of Dawid (1979, 1980) to say that \\nrandom variables X and Y are independent given Z. For discrete \\nrandom variables it is clear what this means: \\nPr{X=x,Y =yiZ =z} = Pr{X=xiZ =z}Pr{Y =yiZ =z} \\nwhenever Pr{Z = z} > 0. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 351}, page_content='340 A Statistical Sidelines \\nIf just Z is discrete we can ask that the joint conditional density \\nfactorizes. In general we may ensure that the conditional densities exist \\nand require \\np( X, y I z) = p( X I z) p(y I z) \\nexcept for z belonging to a set visited by Z with probability zero. Alter\\xad\\nnatively, we will have p(x I y,z) = p(x I z), and conditional independence \\nwill hold whenever there is a version of p(x I y, z) which is a function \\nof x and z alone (since then p(x I z) = E[p(x I Y, z) I Z = z] = p(x I y, z) \\nfor any y ). Alternatively (and only for those thoroughly familiar \\nwith probability theory) we can use regular conditional probabilities \\n(Freedman, 1971, §10.10) which define a conditional probability for \\neach value of Z, and use the usual definition of independence, the \\nfactorization of Pr{X E A, Y E BIZ = z }. \\nWe will regard a group of random variables as still a random \\nvariable, so in the following X, Y , Z and W may be collections of \\nrandom variables. \\nThe following properties are easily derived from the definitions. \\nXJ.LY IZ <==> YJ.LXIZ (A.1) \\nXJ.LY,WIZ = XJ.LY IZ (A.2) \\nXJ.LY,WIZ = XJ.LY IZ,W (A.3) \\nX J.L W I Z, Y and X J.L Y I Z = X J.L Y, w I z. (A.4) \\nNote that on interchanging the roles of W and Y in (A.3) we may \\nreplace (A.2-A.4) by \\nX J.L W I Z, Y and X J.L Y I Z <==> X J.L Y, W I Z. (A.5) \\nFor strictly positive densities (only) we have also \\nX J.L WI Z, Y and X J.L Y I Z, W =X J.L Y, WI Z. (A.6) \\nProperties (A.4) and (A.6) are different, since in general neither of \\nX J.L Y I Z and X J.L Y I Z, W implies the other. \\nThe graphical interpretations of conditional independence discussed \\nin Chapter 8 are all deducible from these axioms. This has led Pearl \\nand his co-workers to term concepts which respect (A.1-A.4) graphoids These exist on Borel \\nspaces, which covers \\nany practicable \\nexample. \\nand graphoids which also obey (A.6) are called positive graphoids. This The precise definitions \\nis not pure axiomatization; there are other concepts which obey these vary across his papers. \\naxioms (such as embedded multi-valued dependencies of attributes in \\ndatabases; Fagin, 1977). If we allow non-disjoint collections of variables \\nwe also need to know \\nX..lLY IY,W. (A.7) '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 352}, page_content='A.4 Axioms for conditional independence 341 \\nJ. Q. Smith (1989) uses (A.7), (A.1) and (A.5) as his axiom system for \\na discussion of conditional independence on DAGs (less powerful than \\nthat described in Chapter 8). From these axioms we deduce \\nX,Z .JL y ,Z I z <==> X .JL y I z \\nso we can confine attention to disjoint collections of random variables. \\nThe graphical representations of conditional independence discussed \\nin Chapter 8 have stronger properties. Write A .l B I C if set C \\nseparates A from B. Separation on an undirected graph satisfies \\nX.lYIZ \\nX.lYUWIZ \\nX.lYIZ \\nX .l W I Z u Y and X .l Y I Z u W Y .l X I Z (A.8) \\nX .l Y I Z (A.9) \\nX .l Y I Z u W (A.lO) \\nX .l Y u W I Z. (A.ll) \\nNote that (A.10) is stronger than (A.3) (and we have already said is not \\ntrue for conditional independence), and that we do have the intersection \\ncondition (A.ll). The first three conditions are immediate. Condition \\n(A.ll) is easily proved by contradiction. (Suppose there is a path from \\nX to Y U W avoiding Z. Then this path can be truncated if necessary \\nto have no interior vertices in Y U W. If it ends in Y, it avoids \\nZ U W, and if it ends in W it avoids Z U Y .) Graph separation is also \\ntransitive: \\nX .l Y I Z ~ X .l { v} I Z or Y .l { v} I Z if v tt X u Y u Z. ( A.12) \\nOn the other hand, d-separation on a DAG satisfies (Pearl, 1988, \\np. 128) \\nX.lYIZ <==> Y.lXIZ (A.13) \\nX.lYUWIZ ~ X.lYIZ (A.14) \\nX.lYUWIZ ~ X.lYIZUW (A.15) \\nX .l W I Z u Y and X .l Y I Z ~ X.lYUWIZ (A.16) \\nX .l W I Z u Y and X .l Y I Z u W ~ X.lYUWIZ (A.17) \\nX .l Y I Z and X .l W I Z <==> X .l Y u W I Z. (A.18) \\nThe first four conditions map to properties of conditional independence, \\nand the fifth is valid for conditional independence under strict positivity. \\nWe also have \\nX .l Y I Z and X .l Y I Z U { v} \\n~ X .l { v} I z or Y .l { v} I z \\n{a} .l { b} I { c, d} and { c} .l { d} I {a, b} \\n~{a} .l {b} I {d} or {a} .l {b} I {d} (A.19) \\n(A.20) '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 353}, page_content='342 A Statistical Sidelines \\nnone of which are necessary for probabilistic conditional independence. \\nShenoy & Shafer (1990) also axiomatize marginalization, which en\\xad\\nables them to give an abstract version of the computations of marginal \\nprobabilities on join trees; Dempster & Kong (1988) had already shown \\nthe version of these computations for Dempster-Shafer belief functions. \\nThe local computations are about taking a potential representation of a \\ndistribution (the density is taken to be a product of potential functions \\non cliques) and rearranging the terms of the product while keeping the \\nproduct constant so that the terms are related to marginals. To pass a \\nmessage from Ci to c1 we need to be able to \\n1 form the marginal on S = Ci n c1 and \\n2 combine this new marginal with the existing marginal on c1. \\nFor this to be valid, marginals have to be combined in a way that is com\\xad\\nmutative, associative, marginalization has to be consistent (marginal\\xad\\nizing from C to S c C must be the same as first marginalizing to \\nT then S if C :::J T :::J S ), and marginalization must be distributive \\nover combination. (This means that combining marginals on Ci and \\nc1 and then marginalizing to c1 is the same as marginalizing from Ci \\nto S = C U CJ and then combining marginals on S and Cj.) These \\naxioms are true for other systems of combination and marginalization \\nthat arise in belief function theory and database theory. \\nA.5 Optimization \\nOptimization, while not strictly statistical, is used in many statistical \\nprocedures. In statistical applications it is only necessary to find a \\nparameter estimate to within a small fraction of its standard error, \\nso for our applications it is more important that the optimization \\nalgorithm is quick to reach an approximately right answer than that its \\nconvergence (to machine precision) is fast. \\nWe will concentrate on good methods for estimating many param\\xad\\neters. More detailed expositions are given by Gill et al. (1981), Dennis \\n& Schnabel (1983) and Fletcher (1987). These methods are iterative. A \\ngeneric minimization algorithm is of the form: \\n1 Choose an initial point x0. \\n2 Select a search direction p. \\n3 Select a step length a, set s = ap and x +---x + s. We normally \\nensure that f(x) is decreased. \\n4 Return to step 2 unless the convergence criteria are met. \\nThe convergence criteria will be problem-specific. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 354}, page_content=\"The gradient is a \\ncolumn vector of \\npartial derivatives. A.5 Optimization 343 \\nMethods based on Taylor expansions \\nSuppose we have a function f: 1R m ~ 1R which we wish to minimize. \\nWe assume that f is differentiable, and let g = V f denote the gradient. \\nA first-order Taylor expansion about any point xo g1ves \\nf(x) ~ f(xo) + g(xo)T (x-xo) \\nand so the value of the function will be reduced by moving to xo-1]g(xo) \\nfor small enough 17, provided the derivative is not zero at xo. This is \\nknown as the method of steepest descent, since amongst all unit-length \\nvectors a, aT g(xo) is smallest when a oc -g(xo). \\nThe Taylor expansion provides no idea how to choose 1], and there \\nare two main strategies. One is to perform a line search for the minimum \\nof f(xo + 17a). This entails that the next step will be orthogonal to this \\none, and the method tends to move in a series of zig-zag steps. The \\nother strategy is to increase 11 as far as possible while f(xo)-'711g(xo)f \\nremains an adequate approximation to f(x-1Jg(xo)). \\nThe curvature of the surface f(x) can give us information about the \\nstep length, so we now assume that f is continuously twice differentiable \\nand has Hessian matrix H(x). A second-order Taylor expansion gives \\nf(x) ~ f(xo) + g(xo)T (x-xo) + !(x-xof H(xo)(x- xo).' (A.21) \\nand the minimum of the right-hand side occurs at xo-H(xo)-1g(xo) \\nprovided that H(x) is positive-definite; otherwise the right-hand side \\ndoes not have a unique minimum. Algorithms based on this expansion \\nare called Newton methods. It is not always obvious that H(x) is \\npositive-definite, but careful algorithms will check this, and adjust it to \\nbe so if it seems likely that it fails to be positive-definite only through \\nrounding errors. \\nNote that Newton methods require no line search, but do not ensure \\nthat f(x) is reduced at each step. They can diverge dramatically. Thus \\npractical algorithms will reduce the step length to ensure that the step \\nreduces f(x) and perhaps that (A.21) is a reasonable approximation. \\nEventually the full step length will always be used, and the convergence \\nthen is second-order, that is llxt+l-Xtll = O(llxt-Xt-1112). \\nThe convergence properties of Newton methods are unsurpassed, \\nbut they are not necessarily so well behaved away from the minimum. \\nThe Hessian H(x) measures the curvature at x, but this may not be \\nuseful except very close to x. The methods described next build up a \\nquadratic approximation similar to (A.21) which is valid at about the \\nlength scale of a current step of the algorithm. The other drawback of \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 355}, page_content='344 A Statistical Sidelines \\nNewton methods is that H(x) may not be known or be very expensive \\nto compute, much more expensive than function values and derivatives. \\nMethods based on quadratic approximations \\nSuppose our function f is actually quadratic with Hessian G (which \\ndoes not depend on x), so (A.21) is an equality everywhere. We can \\ngarner information about G from the gradients at x and x + s at the \\nbeginning and end of a step, since \\ng(x + s) = g(x) + Gs. \\nA quasi-Newton method uses this to build up an approximation B to \\nG. Initially B is set to the best possible guess (the identity matrix if \\nno further information is available). Each step is along -B-1g(x), and \\nafter each step B is updated by B ~ B + U, where the correction U \\nis chosen so that after correction Bs = g(x + s)-g(x). If the function \\nis quadratic, we will learn G exactly after m linearly independent \\nsteps, and for a general function we will build up a local quadratic \\napproximation to f. \\nThere are many ways to find a correction term U. It is widely be\\xad\\nlieved that the most effective update is the Broyden-Fletcher-Goldfarb\\xad\\nShanno (BFGS) formula \\nwhere y = g(x + s)-g(x). As the search is along ocp = -aB-1g(x), the \\nBFGS update becomes \\nB ~ B-g(x)g(x)T + yyT. \\ng(x)Tp ayTp (A.22) \\nFor this method to be viable, we do need B to remain positive-definite. \\nThis needs yT s > 0, which can be ensured by choosing a sufficiently \\naccurate line search over a. \\nQuasi-Newton methods generally converge super-linearly (which \\nmeans llxt+l-Xtll/llxr- Xt-III ---+ 0), but are often more effective \\nthan Newton methods away from a local minimum. As with Newton \\nmethods, the step length a = 1 is preferred, so a line search along p is \\nonly needed in the early stages. \\nThe inverse C = B-1 is all that is needed to find the search \\ndirection, and (A.22) can be converted to an update for C using the An older name for \\nquasi-Newton methods \\nis variable metric \\nmethods. \\nThis was proposed \\nseparately by all four \\nauthors in 1970. \\nSherman-Morrison-Woodbury formula to give See the glossary. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 356}, page_content=\"Code is given by \\nShanno & Phua (1980). \\nMe~ller's comparisons \\nwith BFGS are invalid, \\nsince he fails to check if \\nrx = 1 is a sufficiently \\ngood value for the step \\nlength. A.5 Optimization 345 \\n(A.23) \\nSome but not all accounts suggest that this is less desirable than \\nupdating a factorization of B, for example the Cholesky factorization \\nB = LDLT for L lower-triangular and D diagonal. \\nWhen used on an exactly quadratic function, quasi-Newton methods \\nhave conjugate search directions p;, that is pf Gpj = 0 for j =!= i. There \\nare many other methods based on conjugate gradients with this property, \\nand all will find the minimum of a quadratic in at most m steps. We \\nare interested in methods which do not form an m x m matrix. We can \\nfind conjugate search directions by Pl = -g(xi) and for 2 ~ i ~ m, \\nfJ. _ Y[lg(xi) _ llg(xi)f \\n1\\n-ffg(xi_l)ff2 -ffg(xi_l)ff2. \\nThe first formula for f3i is the Polak-Ribiere formula, the second the \\nFletcher-Reeves formula. The two are equal with exact line searches \\non a quadratic function, but not otherwise. The Polak-Ribiere formula \\nis generally preferred, since if the algorithm is making little progress, \\nf3 ~ 0 and steepest descent is used. The theory of conjugate gradient \\nalgorithms assumes that they are re-started (by f3 = 0) every m steps. \\nHowever, as they should only be used when m is large, there will \\nbe at most a few times m iterations and the theory is not relevant. \\nFairly accurate line searches are needed for conjugate gradient methods, \\nunlike quasi-Newton ones. M0ller (1993) has developed one particular \\nversion of conjugate gradients which seems well known in the neural \\nnetwork field; it uses the out-dated Hestenes-Stiefel formula for f3i \\nwith a particular line-search algorithm. \\nAn alternative way to find conjugate search directions is to use \\n(A.22) taking C = I and exact line searches. More generally, we \\ncan retain only a small number of updates without explicitly forming \\nC. Such methods are known as limited-memory quasi-Newton meth\\xad\\nods. Shanno (1990) refers to other promising methods intermediate \\nbetween the Polak-Ribiere and quasi-Newton methods; see: Liu & \\nNocedal (1989); Gilbert & Lemarechal (1989); Buckley (1994); and \\nByrd et al. ( 1994 ). \\nNon-linear least-squares problems \\nMany fitting problems amount to minimizing a sum of squares of the \\nform n \\nE(w) = ~I: IIYj-f(xj; w)ll2 \\nj=l \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 357}, page_content='346 A Statistical Sidelines \\nfor an m-dimensional vector w for parameters. Here y and f are \\nunivariate, but multivariable problems can be put in this form by \\nmaking multiple entries for each example. \\nThe gradient and Hessian of E are of a special form, and mini\\xad\\nmization methods have been developed to exploit this. Let J denote \\nthe n x m matrix whose rows are the vectors of(xj;w)jow, and let \\nri = Yi-f(xi;w) be the residuals. Then \\noE(w) = J(w)T (r ·) \\now 1 \\nand \\n()2 E(w) = J(w)T J(w)-\"\\'\\\\:\"\" r. ()2 f(xj; w) = J(wf J(w) + Q(w), \\nOWOWT ~ 1 OWOWT j \\nsay. The specialized methods assume that Q(w) is negligible, so either \\nthe residuals or the curvatures of f are small. This is often not the \\ncase in statistical problems. \\nThe Gauss-Newton procedure is a Newton algorithm with the Hes\\xad\\nsian replaced by J(wf J(w), that is taking Q(w) = 0. This is equivalent \\nto the local linear approximation \\n(A.24) \\nIt is not uncommon to find that J(wf J(w) is ill-conditioned, and \\nridge-regression methods may be used to fit (A.24). In this context they \\nare known as Levenberg-Marquardt methods, which replace J(wf J(w) \\nby J(wf J(w)+AI. Ifthe residuals are not small, it should be better to \\nuse general quasi-Newton methods or hybrids between Gauss-Newton \\nand quasi-Newton methods (Gillet al., 1981). \\nThe local linearization (A.24) is also used to find standard errors \\nfor the parameters w. The variance of the least-squares solution w will \\nbe approximately \\n(A.25) \\nwhere a2 is the variance of the observational errors. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 358}, page_content=\"Glossary \\nAIC ('An Information Criterion') A method developed by Akaike (1973, 1974) \\nto avoid over-fitting, by penalizing the deviance by twice the number of \\nfree parameters. \\nback-fitting An iterative method of fitting additive models, by fitting each term \\nto the residuals given the rest. It is a version of the Gauss-Seidel methods \\nof numerical linear algebra. \\nback-propagation is the method used to calculate the gradient vector of a \\nfitting criterion for a feed-forward neural network with respect to the \\nparameters (weights). Also used for a steepest-descent algorithm with the \\ngradient vector computed in this way. \\nBayes formula An elementary formula of probability. If B; are disjoint events, \\nand AcU;B; then \\nPr{B; I A}= Pr{A I B;}Pr{B;} . L:j Pr{A I Bj}Pr{Bj} \\nBayes rule is a rule which attains the Bayes risk, and so is the 'gold-standard', \\nthe best possible for that problem. \\nbias has two meanings. (a) The bias of an estimator is the difference between \\nits mean and the true value. (b) For a neural network, parameters which \\nare constants (rather than multiplying signals) are often called biases. \\nBIC has two similar meanings. Akaike (1977, 1978) introduced 'informa\\xad\\ntion criterion B'. Schwarz (1978) introduced something which has become \\nknown as a 'Bayesian information criterion'. Although most references \\nmean Schwarz's BIC, to avoid confusion this is also known as SBC \\n('Schwarz Bayes Criterion'). Both penalize the deviance by log n times \\nthe number p of free parameters for n examples, but Akaike's has O(p) \\nterms not depending on n. \\nBieoayme-chebychev inequality For a random variable X with mean J1 and \\nvariance a2 < oo we have \\n(J2 \\nPr{IX- Ji.l > e} ~ 2 f' \\nfor all e > 0. This follows from Jensen's inequality applied to (X-J1)2• \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 359}, page_content=\"348 Glossary \\nbootstrap (Efron, 1979) An idea for statistical inference, using trammg sets \\ncreated by re-sampling with replacement from the original training set, so \\nexamples may occur more than once. \\nbranch-and-bound A technique in combinatorial optimization to rule out solu\\xad\\ntions without evaluating them. \\nclassification trees Classifiers which partition the examples on one feature at \\na time. See Chapter 7. \\nclassifier A rule to assign a class (or 'doubt' or 'outlier') to new examples. \\ncodebook vectors Representative examples of a probability distribution . The \\nterm comes from vector quantization. \\ncompact set A subset A c IR_m is compact if it is closed and bounded, that is \\nA c [-K,K]m for some K > 0. Compact sets are also called compacta. \\nconjugate gradients A class of methods used in optimization (and solving \\nlinear systems). See Section A.5. \\nconsistent An estimator is consistent if in large samples it converges to the \\ntrue parameter value (when there is one). \\nconcave A function f is concave if -f is convex. \\nconvex A set A c IR_m is convex if a.a + (1-a.)b E A whenever a,b EA. A \\nfunction f:A-IR is convex is f(a.a + (1-a.)b) ~ af(a) + (1-a.)f(b). \\ncross-validation A method of evaluating parameters or classifiers by dividing \\nthe training set into several parts, and in turn using one part to test the \\nprocedure fitted to the remaining parts. Sometimes used to refer to leave\\xad\\none-out (or ordinary) cross-validation, where every example is dropped in \\nturn. This term is much abused; it does not mean the use of a test set or \\nvalidation set. Generalized cross-validation is a measure of the performance \\nof a regularized classifier; see page 141. \\ndeviance A measure of fit of a statistical model. The deviance is twice (log\\xad\\nlikelihood of the best model minus log-likelihood of the current model). \\nThe best model can be the true model or an exact fit (often called a \\nsaturated model). \\ndiagnostic paradigm In the terminology of Dawid (1976), modelling the con\\xad\\nditional distribution of the class C given the features X. \\nDirichlet distribution A distribution over probability distributions (nb ... , nK) \\non K classes. Its density (Berger, 1985, p. 561) is, for a.; > 0 and \\no ~ n;,l:n; = 1, \\nK \\np(n) oc IT n~;-1 \\ni=l \\nwhich has mean at (a.;/ Lt a.t), and is increasingly concentrated as (a.;) \\nincreases. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 360}, page_content=\"Glossary 349 \\nDirichlet tessellation Given a set of points in RP, associate with each those \\npoints of RP to which it is nearest. This defines a tile, and the tiles \\npartition the space. Also known as Voronoi or Thiessen polygons in R2• \\nPreparata & Shamos (1985) give algorithmic details. \\ndissimilarity A measure of the dissimilarity of two examples based on their \\nfeatures. Must be non-negative and symmetric. \\nearly stopping A method of optimization in which the objective used is not the \\nreal goal, and optimization is stopped when another measure of fit starts \\nto rise. This may be critically dependent on the starting value chosen. \\nediting Methods of reducing the training set for use by nearest-neighbour \\nmethods. \\nefficiency A statistical term, measuring the performance of estimators. Unless \\nstated otherwise, efficiency is a measure of 1/(n x variance in samples of \\nsize n) (for large n). \\neigendecomposition of a real symmetric matrix A. This is an orthonormal \\nmatrix C and a non-negative diagonal matrix D such that A= CDCT. \\nEM algorithm A device to construct algorithms for maximum likelihood and \\nMAP estimators . See Section A.2. \\nequivariance An invariant procedure is unchanged under a transformation; \\nan equivariant procedure transforms its answer. For example, if () is a \\nparameter and ¢ = g(O), then '¢ = g(O). \\nestimator A rule to assign a parameter value to a set of observations. The \\nvalue assigned is called an estimate, and the distinction between estimators \\nand estimates is not always observed. \\nfeature A measurement on an example, so the training set of examples has \\nmeasured features and a class for each. \\nfeature extraction Creating useful new features by combinations (usually lin\\xad\\near) of existing features. \\nfeed-forward network A network in which vertices can be numbered so that \\nall connections go from a vertex to one with a higher number. In practice \\nthe vertices are arranged in layers, with connections only to higher layers. \\ngeneralization A measure of t~e ability of a classifier to perform well on future \\nexamples, or such a measure applied to a method to design classifiers. The \\nterm comes from psychology and refers to the ability to infer the correct \\nstructure from examples. \\nGibbs sampler A simulation method used in Bayesian inference. See Sec\\xad\\ntion A.3. \\nHermite polynomials Families of orthogonal polynomials. See Thisted (1988, \\n§5.3.2). \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 361}, page_content=\"350 Glossary \\nHessian The second derivative matrix of a function f(x). \\nhints The idea of 'hints' is to incorporate qualitative information into the \\nclassifier. \\nHME Hierarchical mixtures of experts. A tree-structured way to select a \\ncombination of classifiers. See Section 8.5. \\ninformation (matrix) The Hessian of the log-likelihood with respect to the \\nparameters (the observed information) or its expected value (the Fisher \\ninformation). \\nJensen's inequality Suppose f is a convex function on a convex domain, and \\nX a random variable on that domain. Then Ef(X);::: f(EX). \\nKullback-Leibler divergence between distributions on the same space with den\\xad\\nsities p and q is \\nJ p(x) \\nd(p, q) = p(x) log q(x) dx. \\nlearning Choosing the parameters of a classifier (and perhaps also the family \\nof classifiers) from the training set. \\nleast false parameter value. If the parametric family does not contain the true \\ndistribution, this is the best possible parameter value e in the sense of \\nminimum Kullback-Leibler divergence d(p,pe). See page 32. \\nlikelihood The probability density of the observations, viewed as a function of \\nthe parameters, not of the observations. \\nlogistic The logistic distribution has cumulative distribution function t(x) = \\nexp(x)/[1 + exp(x)], and this function is called the logistic function. Lo\\xad\\ngistic regression and discrimination are based on converting predictions to \\nprobabilities through the logistic function. \\nLp An Lp space is the space of random variables X such that E IXIP < oo \\nor a space offunctions f for which J f(x)Pdx < oo. Convergence Xn --+X \\nin Lp means E (Xn -X)P --+ 0. \\nLVQ Learning Vector Quantization. A method of designing examples for use \\nin nearest-neighbour procedures. See page 201. \\nMahalanobis distance Given a positive-definite symmetric matrix l: (a covari\\xad\\nance matrix), the distance between examples x and y in feature space is \\n(x-y)Tl:-l(x- y). \\nMAP estimator is the global maximum of the posterior density p(8 I:?/). See \\nSection A.l for further discussion. MAP stands for maximum a posteriori. \\nmaximum likelihood estimate is a value which maximizes the likelihood func\\xad\\ntion, or, more loosely, is a stationary point of the likelihood function. \\nmissing values are unreported values of features. These could be lost, or \\nthey could be deliberate non-measurement and so convey information. (A \\nmedical practitioner will not order tests if their value appears low.) \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 362}, page_content=\"Glossary 351 \\nmultilayer perceptron is another name for a feed-forward neural network. De\\xad\\nspite the name, the 'neurons' are not usually perceptrons. \\nmultivariate analysis is the branch of statistics concerned with multiple ob\\xad\\nservations on each example which are not only of interest to predict or \\nexplain just one of the observations. \\nnon-informative prior A prior distribution for a parameter vector e which is \\nintended to express ignorance about e. This can be tricky, and often leads \\nto the use of densities which have an infinite integral (known as improper \\npriors), such as a uniform density on R. \\nnormal distribution In our usage, this includes both univariate and multivariate \\ndistributions. The density is \\nNP-complete It is desirable that algorithms terminate in a time which is \\npolynomial in the size of the problem and the accuracy required. Let P \\ndenote all problems for which there is such an algorithm. If we allow \\nnondeterministic algorithms (which are allowed to choose the best option \\nwhenever there is a choice) the class of problems is called NP. Equivalently, \\nNP is the class of problems for which a solution could be verified in \\npolynomial time. It is widely believed that NP is strictly larger than P, \\nbut this remains an open research problem. A problem in NP is called \\nNP-complete if proving it was in P would establish P = NP, which should \\nbe regarded as strong evidence that no polynomial algorithm will ever be \\nfound. (Cormen et al., 1990; Sedgewick, 1990; Garey & Johnson, 1979.) \\nNP-bard A NP-hard problem is one that implies a solution to every problem \\nin NP (see NP-complete) but is not known to be in NP. Thus NP-hardness \\nis strong evidence that no polynomial algorithm for the problem will ever \\nbe found. \\non-line methods of parameter estimation adjust the estimate after each new \\nexample is seen. \\nordinal feature An ordinal measurement is one of a series of ordered categories, \\nfor example income ('poor', 'sufficient', 'well-off', 'rich', ... ). \\noutliers Outliers are examples which did not (or are thought not to have) \\ncome from the assumed population of examples. For example, in digit \\nrecognition, the segmentation will fail occasionally, so the data will not be \\nfrom a digit at all. \\nParzen windows A name for kernel density estimation once common in the \\npattern recognition community. \\nperceptron A simple classifier into two classes which thresholds a linear com\\xad\\nbination of the features. Much publicized by F. Rosenblatt around 1960. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 363}, page_content=\"352 Glossary \\nplug-in classifier A classifier constructed by assuming that estimated parameter \\nvalues are in fact the true ones. \\nposterior probability The probability of an event conditional on the observa\\xad\\ntions. \\npredictive classifier A classifier constructed by averaging over the uncertainty \\nin the estimated parameter values. \\nprincipal components are linear combinations of features with high variance. \\nSee Section 9.1. \\nprior probability Probabilities specified before seeing the data, and so based \\non prior experience or belief. Commonly these are the prior probabilities \\nnk of the classes. \\nprofile likelihood Suppose we divide the parameters () = ( ¢, tp ). The profile \\nlikelihood for ¢ is the likelihood for () maximized over tp. \\nprojection pursuit methods are based on extracting features (linear combina\\xad\\ntions of the original features). Exploratory projection pursuit (Section 9.1) \\nlooks for 'interesting' (non-normal) features, and projection pursuit regres\\xad\\nsion (Section 4.1) uses the extracted features in an additive model. \\npruning is the term used for removing parts of trees and networks with the \\naim of increasing generalization. See Section 7.2. \\nquasi-Newton methods are methods of optimization which approximate the \\nHessian using only gradient information. See Section A.5. \\nradial basis functions are a large class of approximating functions , computed \\nas a linear combination of non-linear functions of the distances to a set of \\ncentres: \\nrank (of a matrix). The number of linearly independent rows or columns. \\nregularization A class of methods of avoiding over-fitting to the training set \\nby penalizing the fit by a measure of 'smoothness ' of the fitted function. \\nresistant methods are designed to be little affected by outliers. For example, \\nthe median is much more resistant than the mean. \\nridge regression See shrinkage methods. \\nrisk of a classifier is the expected loss from using it. The Bayes risk is the \\nlowest attainable risk (using these features). \\nrobust methods are designed to be resistant, and also to have high efficiency \\nnear some target distribution. For example, although the median is resis\\xad\\ntant, it is inefficient compared to a trimmed mean. \\nsampling paradigm In the terminology of Dawid (1976), modelling the class\\xad\\nconditional densities Pk(x) and, perhaps, the prior probabilities of the \\nclasses nk. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 364}, page_content=\"Glossary 353 \\nSherman-Morrison-Woodbury formula Given a non-singular n x n matrix A \\nand column vectors b and d we have \\nprovided dT A-1b =F -1. If B and D are n x m matrices for m::::; n then \\nprovided the m x m matrix I+ DT A-1B is invertible (Golub & Van \\nLoan, 1989, p. 51). \\nshrinkage methods of estimation 'shrink' an estimator by moving it towards \\nsome fixed value (or an overall mean). Ridge regression shrinks regression \\ncoefficients towards zero, apart from the constant. The idea is that the \\nshrunken estimator has more bias but lower variance and hence better \\ngeneralization. The James-Stein example (Cox & Hinkley, 1974, §11.8) \\nshows that this idea works even for the mean of a normal distribution in \\np ;?; 3 dimensions . \\nsimulated annealing is a method of combinatorial optimization based on tak\\xad\\ning a series of random steps in the search space. See Ripley (1987) or \\nAarts & Korst (1989). \\nsingular value decomposition of a real matrix X = U AVT, where A is a \\ndiagonal matrix with decreasing non-negative entries, U is an n x p \\nmatrix with orthonormal columns, and V is a p x p orthogonal matrix \\n(Golub & Van Loan, 1989). \\nSOFM, SOM Self-organizing (feature) map of Kohonen. See Section 9.4. \\nsoftmax Given outputs Y1, ... , YK for each of K classes, assign posterior \\nprobabilities as \\nK \\np(k I x) = expyk / L::expyj. \\nj=l \\nThe term comes from Bridle (1990a, b), but the idea is that of multiple \\nlogistic regression. \\nsplines are used in function approximation and smoothing. They are con\\xad\\nstructed by joining functions defined over a partition of the space: the \\nsimplest case is polynomials on adjoining intervals. See Section 4.1. \\nstacked generalization A method of using cross-validation to choose a combi\\xad\\nnation of classifiers. The term is from Wolpert (1992); the idea goes back \\nat least to M. Stone (1974). \\nsteepest descent A method of minimization which takes steps along the direc\\xad\\ntion to steepest descent, the gradient vector. For maximization the method \\nis known as steepest ascent or hill-climbing . \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 365}, page_content='354 Glossary \\nstochastic approximation aims to find the value of Oo solving f(O) = 0, but \\nalthough we can measure f(O), the result will measured with error. After \\ntaking many measurements for fJ with j(fJ) near zero we will be able to \\nfind accurate estimators of 00. There are also versions which aim to find \\nthe maximizer of f(O). \\nsupervised learning Choosing a classifier from a training set of correctly clas\\xad\\nsified examples. \\nt distribution The t distribution in p dimensions with location vector J1 and \\nscale matrix ~ is the distribution of J1 +X /S where X \"\\' Np{O, ~} and \\nvS2 \"\\'x; (Johnson & Kotz, 1972, §37.3; Mardia et al., 1979, p. 57). For \\nv > 2 the mean is J1 and the covariance matrix v~/(v-2). The density is \\ntest set A set of examples used only to assess the performance of a fully\\xad\\nspecified classifier. \\ntraining set A set of examples used for learning, that is to fit the parameters \\nof the classifier. \\nuniform convergence A sequence of functions fn converges uniformly to f if \\nmaxx lfn(x)-f(x)l ---+ 0 as n ---+ oo. We have uniform convergence on \\ncompacta if this holds whenever the maximum is taken over any compact \\nset K. \\nunsupervised learning Discovering groupings in the training set when none are \\npre-specified. \\nupdating Changing the classifier when new examples become available, possi\\xad\\nbly lacking their true classifications. \\nvalidation set A set of examples used to tune the parameters of a classifier, for \\nexample to choose the number of hidden units in a neural network. \\nvector quantization A method of encoding data for signal transmission, in \\nwhich a vector is replaced by one of a finite number of representatives. \\nSee page 201. \\nweights The parameters in a neural network model. Also weights given to \\nindividual examples, for example to indicate multiple copies. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 366}, page_content='References \\nThe following volumes are abbreviated: \\nNIPSl (1989) Advances in Neural Information Pro\\xad\\ncessing Systems. Proceedings of the 1988 Confer\\xad\\nence, ed. D. S. Touretzky. San Mateo, CA: Morgan \\nKaufmann. \\nNIPS2 (1990) Advances in Neural Information Pro\\xad\\ncessing Systems 2, ed. D. S. Touretzky. San Mateo, \\nCA: Morgan Kaufmann . \\nNIPS3 (1991) Advances in Neural Information Pro\\xad\\ncessing Systems 3, eds R. P. Lippmann , J. E. Moody \\n& D. S. Touretzky. San Mateo, CA: Morgan Kauf\\xad\\nmann. \\nNIPS4 (1992) Advances in Neural Information Pro\\xad\\ncessing Systems 4, eds J. E. Moody, S. J. Hanson & \\nR. P. Lippmann. San Mateo, CA: Morgan Kauf\\xad\\nmann. \\nNIPS5 (1993) Advances in Neural Information Pro\\xad\\ncessing Systems 5, eds S. J. Hanson, J. D. Cowan & \\nC. L. Giles. San Mateo, CA: Morgan Kaufmann. \\nNIPS6 (1994) Advances in Neural Information Pro\\xad\\ncessing Systems 6, eds J. D. Cowan, G. Tesauro & \\nJ. Alspector. San Francisco , CA: Morgan Kauf\\xad\\nmann. \\nMany of the papers are reprinted in one or more of \\nthe following volumes of reprints: \\nAnderson , J. A. & Rosenfeld, E. (eds) (1988) Neuro\\xad\\ncomputing : Foundations of Research. Cambridge, \\nMA: The MIT Press. \\nAnderson, J. A., Pellionisz , A. & Rosenfeld, E. (eds) \\n(1990) Neurocomputing 2: Directions for Research. \\nCambridge, MA: The MIT Press. \\nDasarathy, B. V. (ed.) (1991) Nearest Neighbor (NN) \\nNorms: NN Pattern Classification Techniques. Los \\nAlamitos , CA: IEEE Computer Society Press. \\nLau, C. (ed.) (1992) Neural Networks: Theoretical \\nFoundations and Analysis. New York: IEEE Press. Shafer, G. & Pearl, J. (eds) (1990) Readings in \\nUncertainty Reasoning. San Mateo, CA: Morgan \\nKaufmann. \\nShavlik, J. W. & Dietterich, T. G. (eds) (1990) \\nReadings in Machine Learning. San Mateo, CA: \\nMorgan Kaufmann. \\nAarts, E. & Korst, J. (1989) Simulated Annealing and \\nBoltzmann Machines. New York: Wiley. \\nAbramowitz , M. & Stegun, I. A. (1965) Handbook of \\nMathematical Functions with Formulas , Graphs and \\nMathematical Tables. New York: Dover. \\nAbu-Mo stafa, Y. S. (1989) The Vapnik-Chervonenkis \\ndimension : information versus complexity in learn\\xad\\ning. Neural Computation 1, 312-317. \\nAbu-Mostaf a, Y. S. (1990) Learning from hints in \\nneural networks. Journal of Complexit y 6, 192-198. \\nAbu-Mostafa, Y. S. (1993) A method for learning \\nfrom hints. In NIPS5, pp. 73-80. \\nAbu-Mostafa , Y. S. (1995a) Financial market applica\\xad\\ntions of learning from hints. In Neural Networks in \\nthe Capital Markets, ed. A.-P. Refenes, pp. 221-232. \\nChichester : Wiley. \\nAbu-Mostaf a, Y. S. (1995b) Machines that learn from \\nhints. Scientific American 272(4), 64--69. \\nAbu-Mostaf a, Y. S. (1995c) Hints. Neural Computa\\xad\\ntion 7, 639-671. \\nAckley, D. H., Hinton, G. E. & Sejnowski, T. J. \\n(1985) A learning algorithm for Boltzmann ma\\xad\\nchines. Cognitive Science 9, 147-169. Reprinted in \\nAnderson & Rosenfeld (1988). \\nAgosta, J. M. (1990) The structure of Bayes networks \\nfor visual recognition. In Uncertainty in Artificial \\nIntelligence 4, eds R. D. Shachter, T. S. Levitt, \\nL. N. Kanal & J. F. Lemmer, pp. 397-405. \\nAmsterdam: North Holland. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 367}, page_content='356 References \\nAgrawala, A. K. (ed.) (1977) Machine Recognition of \\nPatterns. New York: IEEE Press. \\nAitchison , J. & Aitken, C. G. G. (1976) Multi\\xad\\nvariate binary discrimination by the kernel method. \\nBiometrika 63, 413-420. \\nAitchison, J. & Dunsmore, I. R. (1975) Statistical Pre\\xad\\ndiction Analysis. Cambridge: Cambridge University \\nPress. \\nAitchison, J., Habbema, J. D. F. & Kay, J. W. (1977) \\nA critical comparison of two methods of statistical \\ndiscrimination. Applied Statistics 26, 15-25. \\nAitken, C. G. G. (1978) Methods of discrimination in \\nmultivariate binary data. In Proceedings of COM P\\xad\\nSTAT 1978, eds L. C. A. Corsten & J. Hermans. \\npp. 155-161. Vienna: Physica-Verlag . \\nAitken, C. G. G. (1983) Kernel methods for the \\nestimation of discrete distributions. Journal of \\nStatistical Computation and Simulation 16, 189-200. \\nAizerman, M. A., Braverman, E. M. & Rozonoer, L. I. \\n(1964a) Theoretical foundations of the potential \\nfunction method in pattern recognition learning. \\nAutomation and Remote Control 25, 821-837. \\nAizerman , M. A., Braverman , E. M. & Rozonoer, \\nL. I. (1964b) The probability problem of pattern \\nrecognition learning and the method of poten\\xad\\ntial functions. Automation and Remote Control 25, \\n1175-1190. \\nAizerman, M. A., Braverman, E. M. & Rozonoer, \\nL. I. (1965) The Robbins-Munro process and \\nthe method of potential functions. Automation and \\nRemote Control 26, 1882-1885. \\nAkaike, H. (1973) Information theory and an ex\\xad\\ntension of the maximum likelihood principle. \\nIn Second International Symposium on Information \\nTheory, eds B. N. Petrov & F. Caski, pp. 267-281. \\nBudapest : Akademiai Kaid6. Reprinted in Break\\xad\\nthroughs in Statistics, eds Kotz, S. & Johnson, N. L. \\n(1992), volume I, pp. 599-624. New York: Springer. \\nAkaike, H. (1974) A new look at statistical model \\nidentification. IEEE Transactions on Automatic \\nControl 19, 716-723. \\nAkaike, H . (1977) On entropy maximization principle. \\nIn Applications of Statistics, ed. P. R. Krishnaiah, \\npp. 27-42. Amsterdam: North-Holland . \\nAkaike, H. (1978) A Bayesian analysis of the min\\xad\\nimum AIC procedure. Annals of the Institute of \\nStatistical Mathematics 30A, 9-14. \\nAkaike, H. (1985) Prediction and entropy. In A \\nCelebration of Statistics. The lSI Centenary Volume, \\neds A. C. Atkinson & S. E. Fienberg, pp. 1-24. New \\nYork: Springer. Albert, A. & Anderson, J. A. (1984) On the exis\\xad\\ntence of maximum likelihood estimates in logistic \\nregression models. Biometrika 71, 1-10. \\nAlbert, A. & Lesaffre, E. (1986) Multiple group \\nlogistic discrimination. Computers and Mathematics \\nwith Applications 12A, 209-224. \\nAlbertini, F., Sontag, E. D. & Maillot, V. (1993) \\nUniqueness of weights for neural networks. In \\nMammone (1993), pp. 115-125. \\nAleksander, I. & Morton, H. (1990) An Introduction \\nto Neural Computing. London: Chapman & Hall. \\nAlexander, K. S. (1984) Probability inequalities for \\nempirical processes and a law of the iterated \\nlogarithm. Annals of Probability 12, 1041-1067 . \\nAlmond, R. G. (1995) Graphical Belief Modeling. \\nLondon: Chapman & Hall. \\nAmari, S.-1. (1967) A theory of adaptive pattern clas\\xad\\nsifiers. IEEE Transactions on Electronic Computers \\n16, 299-307. \\nAmari, S.-I. (1993) Mathematical methods of neuro\\xad\\ncomputing . In Networks and Chaos-Statistical and \\nProbabilistic Aspects, eds 0. E. Barndorff-Nielsen, \\nJ. L. Jensen & W. S. Kendall, pp. 1-39. London: \\nChapman & Hall. \\nAmit, D. J. (1989) Modeling Brain Function. The \\nWorld of Attractor Neural Networks. Cambridge: \\nCambridge University Press. \\nAnderberg, M. R. (1973) Cluster Analysis for Appli\\xad\\ncations. New York: Academic Press. \\nAndersen, S. K., Olesen, K. G., Jensen, F. V. & Jensen, \\nF. (1989) HUGIN -a shell for building Bayesian \\nbelief universes for expert systems. In Proceedings \\nof the 11th International Joint Conference on Arti\\xad\\nficial Intelligence, pp. 1080-1085. San Mateo, CA: \\nMorgan Kaufmann. Reprinted in Shafer & Pearl \\n(1990). \\nAnderson, J. A. (1972) Separate sample logistic \\ndiscrimination. Biometrika 59, 19-35. \\nAnderson, J. A. (1982) Logistic discrimination. In \\nHandbook of Statistics 2: Classification, Pattern \\nRecognition and Reduction of Dimensionality, eds \\nP. R. Krishnaiah & L. N. Kana!, Amsterdam : \\nNorth Holland, pp. 169-191. \\nAnderson , J. A. & Phillips, P. R. (1981) Regression, \\ndiscrimination and measurement models for or\\xad\\ndered categorical variables. Applied Statistics 30, \\n22-31. \\nAnderson, J. A. & Rosenfeld, E. (eds) (1988) Neuro\\xad\\ncomputing : Foundations of Research. Cambridge, \\nMA: The MIT Press. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 368}, page_content=\"References 357 \\nAnderson , J. A., Pellionisz, A. & Rosenfeld, E. (eds) \\n(1990) Neurocomputing 2: Directions for Research . \\nCambridge, MA: The MIT Press. \\nAnderson, T. W. (1984) An Introduction to Multi\\xad\\nvariate Statistical Analysis. Second edition. New \\nYork: Wiley. \\nAnderson , T. W. & Bahadur, R. R. (1962) Classi\\xad\\nfication into two multivariate normal distributions \\nwith different covariance matrices. Annals of Math\\xad\\nematical Statistics 33, 420--431. \\nAndreassen, S., Jensen, F. V. & Olesen, K. G. \\n(1991) Medical expert systems based on causal \\nprobabilistic networks. International Journal of Bio\\xad\\nmedical Computing 28, 1-30. \\nAngluin, D. (1987) Learning regular sets from queries \\nand counterexamples. Information and Computation \\n75, 87-106. \\nAngluin, D. (1988) Queries and concept learning. \\nMachine Learning 2, 319-342. \\nAngluin, D. (1993) Learning with queries. In Com\\xad\\nputational Learning and Cognition, ed. E. B. Baum, \\npp. 1-28. Philadelphia: SIAM. \\nAngluin, D. & Valiant, L. G. (1979) Fast probabilistic \\nalgorithms for Hamiltonian circuits and matchings. \\nJournal of Computer and System Sciences 18, 155-\\n193. \\nAnthony, M. & Biggs, N. (1992) Computational \\nLearning Theory: An Introduction . Cambridge: \\nCambridge University Press. \\nAnthony, M. & Shawe-Taylor, J. (1993) A result of \\nVapnik with applications. Discrete Applied Math\\xad\\nematics 47, 207-217. Erratum (1994) 52, 211 (the \\nproof of theorem 2.1 is corrected) . \\nApolloni, B. & de Falco, D. (1991) Learning by \\nasymmetric parallel Boltzmann Machines . Neural \\nComputation 3, 402-408. \\nArgentiero, P., Chin, R. & Beaudet, P. (1982) An \\nautomated approach to the design of decision tree \\nclassifiers . IEEE Transactions on Pattern Analysis \\nand Machine Intelligence 4, 51-57. \\nArbib, M.A. (ed.) (1995) The Handbook of Brain The\\xad\\nory and Neural Networks. Cambridge , MA: MIT \\nPress. \\nArkedev, A. G. & Braverman , E. M. (1966) Com\\xad\\nputers and Pattern Recognition . Washington, DC: \\nThompson. \\nAsh, T. (1989) Dynamic mode creation in back\\xad\\npropagation neural networks. Connection Science: \\nJournal of Neural Computing, Artificial Intelligence \\nand Cognitive Research 1, 365-375. Asimov, D. (1985) The grand tour: a tool for viewing \\nmultidimensional data. SIAM Journal on Scientific \\nand Statistical Computing 6, 128-143. \\nAssouad , P. (1983) Densite et dimension. Annates de \\nl'Institut Fourier Grenoble 33, 233-282. \\nAverintsev , M. V. (1975) Gibbs description of random \\nfields whose conditional probabilities may vanish. \\nProblemy Peredaci Informatsii 11, 86-96. \\nBaba, N., Mogami, Y., Kohzaki, M., Shiraishi , Y. & \\nYoshida, Y. (1994) A hybrid algorithm for finding \\nthe global minimum of error function of neural \\nnetworks and its applications. Neural Networks 7, \\n1253-1265. \\nBahadur , R. R. (1961a) A representation of the \\njoint distribution of responses to n dichotomous \\nitems. In Studies in Item Analysis and Prediction, ed. \\nH. Solomon , pp. 158-167. Palo Alto, CA: Stanford \\nUniversity Press. \\nBahadur, R. R. (1961b) On classification based on \\nresponses to n dichotomous items. In Studies \\nin Item Analysis and Prediction , ed. H. Solomon , \\npp. 169-176. Palo Alto, CA: Stanford University \\nPress. \\nBah!, L. R., Brown, P. F., de Souza, P. V. & Mercer, \\nR. L. (1989) A tree-based statistical language \\nmodel for natural language speech recognition . \\nIEEE Transactions on Acoustics, Speech and Signal \\nProcessing 37, 1001-1008. \\nBailey, T. & Jain, A. K. (1978) A note on distance\\xad\\nweighted k-nearest neighbor rules. IEEE Transac\\xad\\ntions on Systems, Man and Cybernetics 8, 311-313. \\nBaird, H. S. (1993) Recognition technology frontiers. \\nPattern Recognition Letters 14, 327-334. \\nBaldi, P. & Hornik, K. (1989) Neural networks \\nand principal components analysis: learning from \\nexamples without local minima. Neural Networks 2, \\n53-58. Reprinted in Anderson et al. (1990). \\nBall, G. B. (1965) Data analysis in the social sci\\xad\\nences: what about the details? In Proceedings of \\nthe Fall Joint Computing Conference , pp. 533-559. \\nWashington , DC: Spartan Books. \\nBanfield, J. D. & Raftery, A. E. (1993) Model-based \\nGaussian and non-Gaussian clustering . Biometric s \\n49, 803-821. \\nBarlow, R. E., Bartholomew, D., Bremner, J. E. & \\nBrunk, H. M. (1972) Statistical Inference under \\nOrder Restrictions. The Theory and Application of \\nIsotonic Regression. London: Wiley. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 369}, page_content=\"358 References \\nBarron, A. R. (1990) Complexity regularization \\nwith application to artificial neural networks . In \\nNonparametric Functional Estimation and Related \\nTopics, ed. G. Roussas, pp. 561-576. Dordrecht: \\nKluwer Academic Publishers. \\nBarron, A. R. (1993) Universal approximation bounds \\nfor superpositions of a sigmoid function. IEEE \\nTransactions on Information Theory 39, 93(}-945. \\nBarron, A. R. (1994) Approximation and estimation \\nbounds for artificial neural networks. Machine \\nLearning 14, 115-133. \\nBarron, A. R. & Cover, T. M. (1991) Minimum \\ncomplexity density estimation. IEEE Transactions \\non Information Theory 37, 1034-1054 . \\nBarry, D. (1986) Nonparametric Bayesian regression. \\nAnnals of Statistics 14, 934-953. \\nBartlett, P. L. (1993) Vapnik-Chervonenkis dimension \\nbounds for two-and three-layer networks. Neural \\nComputation 5, 371-373. \\nBartlett, P. L. & Williamson, R. C. (1996) The VC di\\xad\\nmension and pseudodimension of two-layer neural \\nnetworks with discrete inputs. Neural Computation \\n8, 625-628. \\nBasford, K. E. & McLachlan , G. J. (1985) Estimation \\nof a!Jocation rates in a cluster analysis context. \\nJournal of the American Statistical Association 80, \\n286-293. \\nBashkirov , 0. A., Braverman, E. M. & Muchnik, \\nI. B. (1964) Potential function algorithms for \\npattern recognition learning machines. Automation \\nand Remote Control 25, 629-631. \\nBates, D. M. & Watts, D. G. (1988) Nonlinear Re\\xad\\ngression Analysis and its Applications. New York: \\nWiley. \\nBather, J. (1996) A conversation with Herman Cher\\xad\\nnoff. Statistical Science 11, 335-350. \\nBattiti, R. (1989) Accelerated backpropagation learn\\xad\\ning: two optimization methods. Complex Systems \\n3, 331-342. \\nBattiti, R. (1992) First-and second-order methods for \\nlearning: between steepest descent and Newton's \\nmethod. Neural Computation 4, 141-166. \\nBattiti, R. & Massuli, F. (1990) BFGS optimization \\nfor faster and automated supervised learning. In \\nProceedings of the International Neural Network \\nConference (Paris, 1990) 2, 757-760. \\nBaum, E. B. (1988) On the capabilities of multilayer \\nperceptrons. Journal of Complexity 4, 193-215. \\nBaum, E. B. & Haussler, D. (1989) What size net \\ngives valid generalization? Neural Computation I, \\n151-160. Reprinted in Shavlik & Dietterich (1990). Baum, L. E., Petrie, T., Soules, G. & Weiss, N. \\n(1970) A maximization technique occurring in the \\nstatistical analysis of probabilistic functions of \\nMarkov chains. Annals of Mathematical Statistics \\n41, 164-171. \\nBaxt, W. G. (1992) Improving the accuracy of an \\nartificial neural network using multiple differently \\ntrained networks. Neural Computation 4, 772-780. \\nBeaulieu , J.-M. & Goldberg, M. (1989) Hierarchy \\nin picture segmentation: a stepwise optimization \\napproach. IEEE Transactions on Pattern Analysis \\nand Machine Intelligence 11, 15(}-163. \\nBeeri, C., Fagin, R., Maier, D. & Yannakakis , M. \\n(1983) On the desirability of acyclic database \\nschemes. Journal of the Association for Computing \\nMachinery 30, 479-513. \\nBegg, C. B. & Gray, R. (1984) Calculation of poly\\xad\\nchotomous logistic regression parameters using in\\xad\\ndividuali zed regressions. Biometrika 71, 11-18. \\nBeigi, H. S.M. & Li, C. J. (1990) Learning algorithms \\nfor neural networks based on quasi-Newton with \\nself-scaling. Intelligent Control Systems 23, 23-28. \\nBeigi, H. S.M. & Li, C. J. (1993) Learning algorithms \\nfor neural networks based on quasi-Newton with \\nself-scaling. Journal of Dynamical Systems. Mea\\xad\\nsurement , and Control- Transactions of the ASME \\n115, 38-43. \\nBenediktsson, J. A. & Swain, P. H. (1992) Consensus \\ntheoretic classification methods. IEEE Transactions \\non Systems, Man and Cybernetics 22, 688-704. \\nBerge, C. (1973) Graphs and Hypergraph s. Amster\\xad\\ndam: North-Holland. \\nBerger, J. 0. (1985) Statistical Decision Theory and \\nBayesian Analysis. New York: Springer. \\nBerger, J. 0. & Delampady, M. (1987) Testing precise \\nhypotheses (with discussion). Statistical Science 2, \\n317-352. \\nBernardo , J. M. & Smith, A. F. M. (1994) Bayesian \\nTheory. Chichester: Wiley. \\nBesag, J., & Green, P. J. (1993) Spatial statistics and \\nBayesian computation (with discussion) . Journal of \\nthe Royal Statistical Society series B 55, 25-37. \\nBesag, J., Green, P., Higdon, D. & Mengersen, K. \\n(1995) Bayesian computation and stochastic sys\\xad\\ntems (with discussion) . Statistical Science 10, 3-66. \\nBest, D. J. & Rayner, J. C. W. (1988) A test \\nfor bivariate normality. Statistics and Probability \\nLetters 6, 407-412. \\nBezdek, J. C. ( 1974) Cluster validity with fuzzy sets. \\nJournal of Cybernetics 3, 58-72. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 370}, page_content=\"References 359 \\nBhattacharyya, A. ( 1943) On a measure of divergence \\nbetween two statistical population s defined by their \\nprobability distribution s. Bulletin of the Calcutta \\nMathematics Society 35, 99-110. \\nBichsel, M. & Seitz, P. (1989) Minimum class entropy: \\na maximum information approach to layered net\\xad\\nworks. Neural Networks 2, 133-141. \\nBienenstock , E., Cooper, L. N. & Munro, W. (1982) \\nTheory for the development of neuron selectivity: \\norientation specificity and binocular interaction in \\nthe visual cortex. Journal of Neuroscience 2, 32-48. \\nReprinted in Anderson & Rosenfeld (1988). \\nBinford, T. 0., Levitt, T. S. & Mann, W. B. (1989) \\nBayesian inference in model-bas ed machine vi\\xad\\nsion. In Uncertainty in Artificial Intelligence 3, eds \\nL. N. Kanal, T. S. Levitt & J. F. Lemmer. Amster\\xad\\ndam: Elsevier. \\nBishop, C. (1991) Improving the generalization prop\\xad\\nerties of radial basis function neural networks . \\nNeural Computation 3, 579-588. \\nBishop, C. (1992) Exact calculation of the Hessian \\nmatrix for the multilayer perceptron. Neural Com\\xad\\nputation 4, 494-501. \\nBishop, C. M. (1993) Curvature-driven smoothing: a \\nlearning algorithm for feed forward networks. IEEE \\nTransactions on Neural Networks 4, 882-884. \\nBishop, C. M. (1995a) Neural Networks .for Pattern \\nRecognition. Oxford: Clarendon Press. \\nBishop, C. M. (1995b) Training with noise is equiva\\xad\\nlent to Tikohonov regularization. Neural Computa\\xad\\ntion 7, 108-116. \\nBlair, J. R. S. & Peyton, B. (1993) An introduction to \\nchordal graphs and clique trees. In Graph Theory \\nand Sparse Matrix Computations, eds A. George, \\nJ. R. Gilbert & J. H. U. Liu, pp. 1-29. New York: \\nSpringer. \\nBlock, H. D. ( 1962) The percept ron: a model for brain \\nfunctioning I. Reviews of Modern Physics 34, 123-\\n135. Reprinted in Anderson & Rosenfeld (1988). \\nBlock, H. D. & Levin, S. A. (1970) On the bounded\\xad\\nness of an iterative procedure for solving a system \\nof linear inequalities. Proceedings of the American \\nMathematical Society 26, 229-235. \\nBlock, H. D., Knight, B. W. Jr & Rosenblatt , F. (1962) \\nAnalysis of a four-layer series-coupled perceptron \\nII. Reviews of Modern Physics 34, 135-142. \\nBlue, J. L., Candela, G. T., Grother, P. J. and Wilson, \\nC. L. (1994) Evaluation of pattern classifiers for \\nfingerprint and OCR applications. Pattern Recog\\xad\\nnition 27, 485-501. Blumer, A., Ehrenfeucht, A., Haussler, D. & War\\xad\\nmuth, M. K. (1987) Occam Razor. Information Pro\\xad\\ncessing Letters 24, 377-280. Reprinted in Shavlik \\n& Dietterich (1990). \\nBlumer, A., Ehrenfeucht , A., Haussler, D. & War\\xad\\nmuth, M. K. (1989) Learnability and the Vapnik\\xad\\nChervonenkis dimension . Journal of the Association \\n.for Computing Machinery 36, 926-965. \\nde Boor, C. (1978) A Practical Guide to Splines. New \\nYork: Springer. \\nBourlard, H. & Kamp, Y. (1988) Auto-association by \\nmultilayer perceptrons and singular value decom\\xad\\nposition. Biological Cybernetics 59, 291-294. \\nBouton, C. & Pages, G. (1993) Self-organization of \\nthe one-dimensional Kohonen algorithm with non\\xad\\nuniformly distributed stimuli. Stochastic Processes \\nand their Applications 47, 249-274. \\nBouton, C. & Pages, G. (1994) Convergence in distri\\xad\\nbution of the one-dimensional Kohonen algorithms \\nwhen the stimuli are not uniform. Advances in \\nApplied Probabilit y 26, 80-103. \\nBox, G. E. P. & Tiao, G. C. (1962) A further look \\nat robustness via Bayes's theorem. Biometrika 49, \\n419-432. \\nBox, G. E. P. & Tiao, G. C. (1973) Bayesian Inference \\nin Statistical Analysis. New York: Wiley. (Formerly \\nReading, MA: Addison-Wesley.) \\nBox, G. E. P., Hunter, W. G. & Hunter, J. S. \\n(1978) Statistics .for Experimenters: An Introduction \\nto Design, Data Analysis and Model Building. New \\nYork: Wiley. \\nBoyles, R. A. (1983) On the convergence of the EM \\nalgorithm. Journal of the Royal Statistical Society \\nseries B 45, 47-50. \\nBratko, I. & Kononenko , I. (1987) Learning diag\\xad\\nnostic rules from incomplete and noisy data. In \\nInteractions in Artificial Intelligence and Statistical \\nMethods, ed. B. Phelps, pp. 142-153. Aldershot: \\nGower Technical Press. \\nBratko, I. & Muggleton , S. (1995) Applications of \\ninductive logic programming. Communications of \\nthe Association .for Computing Machinery 38, 65-70. \\nBraverman , E. M. (1965) On the method of poten\\xad\\ntial functions. Automation and Remote Control 26, \\n2130-2138. \\nBreiman, L. (1991) The TI-method for estimating \\nmultivariate functions from noisy data (with dis\\xad\\ncussion). Technometri cs 33, 125-160. \\nBreiman, L. (1992) Stacked regressions. Technical \\nReport 367, Dept of Statistics , University of Cali\\xad\\nfornia, Berkeley. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 371}, page_content='360 References \\nBreiman, L. (1993) Hinging hyperplanes for regres\\xad\\nsion, classification and function approximation . \\nIEEE Transactions on Information Theory 3, 999-\\n1013. \\nBreiman, L. & Ihaka, R. (1984) Nonlinear discrimi\\xad\\nnant analysis via ACE and scaling. Technical Re\\xad\\nport 40, Dept of Statistics, University of California, \\nBerkeley. \\nBreiman, L., Friedman, J. H., Olshen, R. A. & Stone, \\nC. J. (1984) Classification and Regression Trees. \\nMonterey, CA: Wadsworth and Brooks/Cole. \\nBrent, R. P. (1991) Fast training algorithms for multi\\xad\\nlayer neural nets. IEEE Transactions on Neural \\nNetworks 2, 346--354. \\nBridle, J. S. (1990a) Probabilistic interpretation of \\nfeedforward classification network outputs, with \\nrelationships to statistical pattern recognition. In \\nNeuro-computing: Algorithms, Architectures and Ap\\xad\\nplications, eds F. Fogelman Soulie & J. Herault, \\npp. 227-236. Berlin: Springer. (Although the vol\\xad\\nume was published in 1990, the article gives a 1989 \\ncopyright date.) \\nBridle, J. S. (1990b) Training stochastic model recog\\xad\\nnition algorithms as networks can lead to maxi\\xad\\nmum mutual information estimation of parameters. \\nIn NIPS2, pp. 211-217. \\nBridle, J. S. & Cox, S. J. (1991) RecNorm: simulta\\xad\\nneous normalisation and classification applied to \\nspeech recognition. In NIPS3, pp. 234-240. \\nBrier, G. W. (1950) Verification of forecasts expressed \\nin terms of probabilities . Monthly Weather Review \\n78, 1-3. \\nBrockett, R. W. (1991) Dynamical systems that \\nsort lists, diagonalize matrices and solve linear \\nprogramming problems. Linear Algebra and its \\nApplications 146, 79-91. \\nBroffit, B., Clarke, W. R. & Lachenbruch, P. A. (1980) \\nThe effect of Huberizing and trimming on the \\nquadratic discriminant function. Communications in \\nStatistics-Theory and Methods A9, 13-25. \\nBronowski, J. & Long, W. M. (1951) Statistical \\nmethods in anthropology . Nature 1168, 794. \\nBroomhead, D. S. & Lowe, D. (1988) Multivariable \\nfunctional interpolation and adaptive networks. \\nComplex Systems 2, 321-355. \\nBrown, P. J. & Rundell, P. W. K. (1985) Kernel \\nestimates for categorical data. Technometrics 28, \\n293-299. \\nBrown, T. A. & Kaplowitz , J. (1979) The weighted \\nnearest neighbor rule for class dependent sample \\nsizes. IEEE Transactions on Information Theory 25, \\n617-619. Bryan, J. G. (1951) The generalized discriminant func\\xad\\ntion: mathematical foundations and computational \\nroutine. Harvard Educational Review 21, 90--95. \\nBryant, J. (1989) A fast classifier for image data. \\nPattern Recognition 22, 45-48. \\nBryson, A. E. & Ho, Y.-C. (1969) Applied Optimal \\nControl. New York: Blaisdell. (Revised printing \\nNew York: Hemisphere, 1975.) \\nBuckland, S. T. (1992a) Fitting density functions with \\npolynomials. Applied Statistics 41, 63-76. \\nBuckland, S. T. (1992b) Algorithm AS270. Maximum \\nlikelihood fitting of Hermite and simple polynomial \\ndensities. Applied Statistics 41, 241-266. \\nBuckley, A. G. (1994) A Fortran-90 code for un\\xad\\nconstrained nonlinear minimization. ACM Trans\\xad\\nactions on Mathematical Software 20, 354-372. \\nBuntine, W. L. (1992) Learning classification trees. \\nStatistics and Computing 2, 63-73. \\nBuntine, W. L. & Weigend, A. S. (1991) Bayesian \\nback-propagation . Complex Systems 5, 603-643. \\nBuntine, W. L. & Weigend, A. S. (1994) Calculating \\nsecond derivatives on feed-forward networks : a \\nreview. IEEE Transactions on Neural Networks 5, \\n480-488. \\nBurrascano , P. (1991) Learning vector quantization \\nfor the probabilistic neural network. IEEE Trans\\xad\\nactions on Neural Networks 2, 458-461. \\nByrd, R. H., Nocedal, J. & Schnabel, R. B. (1994) \\nRepresentations of quasi-Newton matrices and \\ntheir use in limited memory methods. Mathematical \\nProgramming 63, 129-156. \\nByth, K. & McLachlan , G. J. (1978) The biases \\nassociated with maximum likelihood methods of \\nestimation of the multivariate logistic risk function. \\nCommunications in Statistics-Theory and Methods \\nA7, 877-890. \\nCacoullos, T. (1966) Estimation of a multivariate \\ndensity. Annals of the Institute of Statistical Math\\xad\\nematics 18, 179-189. \\nCampbell, N. A. (1980a) Shrunken estimators in dis\\xad\\ncriminant and canonical variate analysis. Applied \\nStatistics 29, 5-14. \\nCampbell, N. A. (1980b) Robust procedures in multi\\xad\\nvariate analysis I. Robust covariance estimation . \\nApplied Statistics 29, 231-237. \\nCampbell, N. A. (1982) Robust procedures in multi\\xad\\nvariate analysis II. Robust canonical variate anal\\xad\\nysis. Applied Statistics 31, 1-8. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 372}, page_content='References 361 \\nCampbell, N. A. & Mahon, R. J. (1974) A multi\\xad\\nvariate study of variation in two species of rock \\ncrab of genus Leptograpsus. Australian Journal of \\nZoology 22, 417-425. \\nCandela, G. T. & Chellappa , R. (1993) Comparative \\nperformance of classification methods for finger\\xad\\nprints. US National Institute of Standards and \\nTechnology report NISTIR 5163. \\nCannings, C. & Thompson , E. A. (1981) Genealogi\\xad\\ncal and Genetic Structure . Cambridge: Cambridge \\nUniversity Press. \\nCannings , C., Thompson , E. A. & Skolnick, M. H. \\n(1978) Probability functions on complex pedigrees . \\nAdvances in Applied Probability 10, 26-61. \\nCarbonell , J. G. (ed.) ( 1990) Machine Learning. \\nParadigms and Methods. Cambridge, MA: The \\nMIT Press. \\nCarpenter, G. A. & Grossberg, S. (1987a) A massively \\nparallel architecture for a self-organizing neu\\xad\\nral pattern recognition machine. Computer Vision, \\nGraphics, and Image Processing 37, 54-115. \\nCarpenter , G. A. & Grossberg , S. (1987b) ART 2: \\nstable self-organization of stable category recogni\\xad\\ntion codes for analog input patterns. Applied Optics \\n26, 4919-4930. Reprinted in Anderson eta/. (1990). \\nCarpenter, G. A. & Grossberg, S. (1990) ART 3: \\nhierarchical search using chemical transmitters in \\nself-organizing pattern recognition architectures . \\nNeural Networks 3, 129-152. \\nCarpenter , G. A. & Grossberg , S. (1994) Self\\xad\\norganizing neural networks for supervised and un\\xad\\nsupervised learning and prediction . In Cherkassky \\neta/. (1994), pp. 319-348. \\nCarpenter, G. A., Grossberg, S. & Reynolds, J. H. \\n(1991a) ARTMAP: supervised real-time learning \\nand classification of nonstationary data by a self\\xad\\norganizing neural network. Neural Networks 4, 565-\\n588. \\nCarpenter , G. A., Grossberg, S. & Rosen, D. B. \\n(1991b) Fuzzy ART: fast stable learning and \\ncategoriz ation of analog patterns by an adaptive \\nresonance system. Neural Networks 4, 759-771. \\nCarpenter, G. A., Grossberg, S., Markuzon, N., \\nReynolds , J. H. & Rosen, D. B. (1992) Fuzzy \\nAR TMAP: a neural network architecture for in\\xad\\ncremental supervised learning of analog multi\\xad\\ndimensional maps. IEEE Transactions on Neural \\nNetworks 3, 698-713. Carroll, S.M. & Dickinson, B. W. (1989) Construction \\nof neural nets using the Radon transform. In \\nProceedings of the International Joint Conference \\non Neural Networks I, 607-611. New York: IEEE \\nPress. \\nCarter, C. & Catlett, J. (1987) Assessing credit card \\napplications using machine learning. IEEE Expert \\n2(3), 71-79. \\nCasey, R. G & Nagy, G. (1984) Decision tree design \\nusing a probabilistic model. IEEE Transaction s on \\nInformation Theory 30, 93-99. \\nCeleux, G. & Diebolt, J. (1992) A stochastic ap\\xad\\nproximation type EM algorithm for the mixture \\nproblem. Stochastics and Stochastics Reports 41, \\n119-134. \\nCestnik, B., Kononenko, I. & Bratko, I. (1987) \\nASSISTANT 86: a knowledge-elicitation tool for \\nsophisticated users. In Progress in Machine Learn\\xad\\ning, eds I. Bratko & N. Lavrac, pp. 31-45. Wilm\\xad\\nslow: Sigma Press. \\nChambers, J. M. & Hastie, T. J. (eds) (1992) Statistical \\nModels in S. Pacific Grove, CA: Wadsworth and \\nBrooks/Cole. \\nChan, C. & Bao, J. (1991) On the design of a tree \\nclassifier and its application to speech recognition. \\nInternational Journal of Pattern Recognition and \\nArtificial Intelligence 5, 677-692. \\nChan, K. S. (1993) Asymptotic behaviour of the \\nGibbs sampler. Journal of the American Statistical \\nAssociation 88, 320-326. \\nChandran, P. S. (1994) Comments on \"Compara\\xad\\ntive analysis of backpropagation and the extended \\nKalman filter for training multilayer perceptrons\" . \\nIEEE Transactions on Pattern Analysis and Ma\\xad\\nchine Intelligence 16, 862-863. \\nChang, C. L. (1974) Finding prototypes for nearest \\nneighbor classifiers. IEEE Transactions on Comput\\xad\\ners 23, 1179-1184. Reprinted in Dasarathy (1991). \\nCharniak, E. (1991) Bayesian networks without tears. \\nAI Magazine 12(4), 50-63. \\nChauvin, Y. (1989) A back-propagation algorithm \\nwith optimal use of hidden units. In NIPSJ, \\npp. 519-526. \\nChavez, R. M. & Cooper, G. F. (1990) A randomized \\napproximation algorithm for probabilistic inference \\non Bayesian belief networks. Networks 20, 661-685. \\nCheeseman, P. (1995) On Bayesian model selection. \\nIn Wolpert (1995), pp. 315-330. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 373}, page_content='362 References \\nCheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, \\nW. & Freeman, D. (1988a) AutoClass: a Bayesian \\nclassification system. In Proceedings of the Fifth \\nInternational Workshop on Machine Learning , Ann \\nArbor, pp. 54-64. San Mateo, CA: Morgan Kauf\\xad\\nmann. \\nCheeseman, P., Self, M., Kelly, J., Taylor, W., Freeman, \\nD. & Stutz, J. (1988b) Bayesian classification. In \\nProceedings of the Seventh AAAI National Confer\\xad\\nence on Artificial Intelligence , St Paul, MN, pp. 607-\\n611. San Mateo, CA: Morgan Kaufmann. \\nChen, D. S. & Jain, R. C. (1994) A robust back prop\\xad\\nagation learning algorithm for function approxi\\xad\\nmation. IEEE Transactions on Neural Networks 5, \\n467-479. \\nChen, S., Cowan, C. F. N. & Grant, P. M. (1991) \\nOrthogonal least squares learning algorithm for \\nradial basis function networks. IEEE Transactions \\non Neural Networks 2, 302-309. \\nCheng, Y.-Q., Zhuang, Y.-M. & Yang, J.-Y. (1992) \\nOptimal Fisher discriminant analysis using the \\nrank decomposition. Pattern Recognition 25, 101-\\n111. \\nCherkassky, V. & Mulier, F. (1994) Self-organizing \\nnetworks for nonparametric regression . In \\nCherkassky et a!. (1994), pp. 188-212. \\nCherkassky, V., Friedman, J. H. & Wechsler, H. (eds) \\n(1994) From Statistics to Neural Networks. The\\xad\\nory and Pattern Recognition Applications. Berlin: \\nSpringer. \\nChernick, M. R., Murthy, V. K. & Nealy, C. D. (1985) \\nApplication of bootstrap and other resampling \\ntechniques: evaluation of classifier performance. \\nPattern Recognition Letters 3, 167-178. \\nChernoff , H. (1952) A measure of asymptotic effi\\xad\\nciency for tests of a hypothesis based on the sum \\nof observations. Annals of Mathematical Statistics \\n23, 493-507. \\nChernoff, H. (1973) Some measures for discriminating \\nbetween normal multivariate distributions with un\\xad\\nequal covariance matrices. In Multivariate Analysis \\nIII, ed. P. R. Krishnaiah, pp. 337-344. New York: \\nAcademic Press. \\nChidananda Gowda, K. & Krishna, G. (1979) The \\ncondensed nearest neighbor rule using the concept \\nof mutual nearest neighborhood. IEEE Transac\\xad\\ntions on Information Theory 25, 488-490. Reprinted \\nin Dasarathy (1991). Chou, P. A. (1989) Recognition of equations using a \\ntwo-dimensional stochastic context-free grammar . \\nIn Visual Communications and Image Processing IV, \\ned. W. A. Pearlman . SPIE Proceedings Series 1199, \\n852-863. \\nChou, P. A. (1991) Optimal partitioning for classi\\xad\\nfication and regression trees. IEEE Transactions \\non Pattern Analysis and Machine Intelligence 13, \\n340-354. \\nChou, W.-S. & Chen, Y.-C. (1992) A new fast algo\\xad\\nrithm for the effective training of neural classifiers . \\nPattern Recognition 25, 423-429. \\nChow, C. K. (1970) On optimum recognition error \\nand reject tradeoff. IEEE Transactions on Informa\\xad\\ntion Theory 16, 41-46. \\nChow, C. K. & Liu, C. N. (1968) Approximating \\ndiscrete probability distributions with dependence \\ntrees. IEEE Transactions on Information Theory 14, \\n462-467. \\nCiampi, A., Chang, C.-H., Hogg, S. & McKinney , \\nS. (1987) Recursive partition : a versatile method \\nfor exploratory data analysis in biostatistics . In \\nBiostatistics, eds I. B. MacNeil & G. J. Umphrey, \\npp. 23-50. Dordrecht: Reidel. \\nClark, L. A. & Pregibon, D. (1992) Tree-based \\nmodels. Chapter 9 of Chambers & Hastie (1992). \\nClark, P. & Niblett, T. (1989) The CN2 induction \\nalgorithm. Machine Learning 3, 261-283. \\nCleveland, W. S., Grosse, E. & Shyu, W. M. (1992) \\nLocal regression models. Chapter 8 of Chambers \\n& Hastie (1992). \\nClifford, P. (1990) Markov random fields in statistics. \\nIn Disorder in Physical Systems. A Volume in Hon\\xad\\nour of John M. Hammersley, eds G. R. Grimmett \\n& D. J. A. Welsh, pp. 19-32. Oxford: Clarendon \\nPress. \\nClunies-Ross, C. W. & Rilfenburgh , R. H. (1960) \\nGeometry and linear discrimination. Biometrika 47, \\n185-189. \\nCohen, E., Hull, J. J. & Srihari, S. N. (1991) \\nUnderstanding handwritten text in a structured en\\xad\\nvironment : determining ZIP codes from addresses. \\nInternational Journal of Pattern Recognition and \\nArtificial Intelligence 5, 221-264. \\nCohn, D. & Tesauro, G. (1992) How tight are the \\nVapnik-Chervonenkis bounds? Neural Computa\\xad\\ntion 4, 249-269. \\nCook, D., Buja, A. & Cabrera, J. (1993) Projection \\npursuit indices based on orthonormal function \\nexpansions. Journal of Computational and Graphical \\nStatistics 2, 225-250. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 374}, page_content=\"References 363 \\nCoomans, D. & Broeckaert, I. (1986) Potential Pat\\xad\\ntern Recogniti on in Chemical and Medical Decision \\nMaking. Letchworth: Research Studies Press. \\nCooper, G. F. (1984) NESTOR: a computer-based \\nmedical diagnostic aid that integrates causal and \\nprobabilistic knowledge, Ph.D. thesis, Dept of Com\\xad\\nputer Science, Stanford University. \\nCooper, G. F. (1989) Current research directions \\nin the development of expert systems based on \\nbelief networks . Applied Stochastic Models and Data \\nAnalysis 5, 39-52. \\nCooper, G. F. (1990) The computational complexity \\nof probabilistic inference using Bayesian belief \\nnetworks. Artificial Intelligen ce 42, 393-405. \\nCooper, G. F. & Herskovits, E. (1992) A Bayesian \\nmethod for the induction of probabilistic networks \\nfrom data. Machine Learning 9, 309-347. \\nCormen, T. H., Leiserson , C. E. & Rivest, R. L. (1990) \\nIntroduction to Algorithms. Cambridge MA: The \\nMIT Press and New York: McGraw-Hill. \\nCortes, C. & Vapnik, V. (1995) Support-vector net\\xad\\nworks. Machine Learning 20, 273-297. \\nCosslett, S. R. (1981) Maximum likelihood estimators \\nfor choice-based samples. Econometrica 49, 1289-\\n1316. \\nCottrell, G. W. & Metcalfe, J. (1991) EMPATH: face, \\nemotion and gender recognition using holons. In \\nNIPS3, pp. 564-571. \\nCottrell, M. & Fort, J. C. ( 1987) Etude d'un al\\xad\\ngorithme d'auto-organisation. Annales de l'Institut \\nHenri Poincare 23, 1-20. \\nCover, T. M. ( 1965) Geometrical and statistical prop\\xad\\nerties of systems of linear inequalities with appli\\xad\\ncations in pattern recognition . IEEE Transactions \\non Electroni c Computers 14, 326-334. \\nCover, T. M. (1968) Rates of convergence of nearest \\nneighbor procedures. In Proceedings of the First \\nAnnual Hawaii Conference on Systems Theory, Hon\\xad\\nolulu, pp. 413-418. \\nCover, T. M. (1969) Learning in pattern recognition. \\nIn Methodol ogies of Pattern Recognition, ed. S. \\nWatanabe, pp. 111-132. New York: Academic \\nPress. \\nCover, T. M. & Hart, P. E. (1967) Nearest neighbor \\npattern classification. IEEE Transactions on Infor\\xad\\nmation Theory 13, 21-27. Reprinted in Anderson \\neta/. (1990), Dasarathy (1991) and Lau (1992). Cowell, R. G. (1992) BAlES-a probabilistic ex\\xad\\npert reasoning shell with qualitative and quanti\\xad\\ntative learning. In Bayesian Statistics 4, eds J. M. \\nBernardo , J. 0. Berger, A. P. Dawid & A. F. M. \\nSmith, pp. 595-600. Oxford: Clarendon Press. \\nCowell, R. G. (1995) A C++ class library for building \\nBayesian belief networks. In Gammerman (1995), \\npp. 159-165. \\nCowell, R. G. & Dawid, A. P. (1992) Fast retraction of \\nevidence in a probabilistic expert system. Statistics \\nand Computing 2, 37-40. \\nCox, D. R. ( 1958) Two further applications of a model \\nfor binary regression . Biometrika 45, 562-565. \\nCox, D. R. & Hinkley, D. V. (1974) Theoretical \\nStatistics. London: Chapman & Hall. \\nCox, D. R. & Snell, E. J. (1989) Analysis of Binary \\nData. Second edition. London: Chapman & Hall. \\nCox, T. F. & Cox, M. A. A. (1994) Multidimensional \\nScaling. London: Chapman & Hall. \\nCraven, P. & Wahba, G. (1979) Smoothing noisy data \\nwith spline functions : estimating the correct degree \\nof smoothing by the method of generalized cross\\xad\\nvalidation. Numerische Matematik 31, 377-403. \\nCrawford, S. L. (1989) Extensions to the CART \\nalgorithm . International Journal of Man-Machine \\nStudies 31, 197-217. \\nCrevier, D. (1993) AI. The Tumultuous History of the \\nSearch for Artificial Intelligence. New York: Basic \\nBooks. \\nCybenko , G. (1988) Continuous valued neural net\\xad\\nworks with two hidden layers are sufficient. Tech\\xad\\nnical Report, Dept of Computer Science, Tufts \\nUniversity. \\nCybenko, G. (1989) Approximation by superpositions \\nof a sigmoidal function. Mathematics of Control \\nSignals, and Systems 2, 303-314. \\nDarken, C. & Moody, J. (1991) Note on learning rate \\nschedules for stochastic optimization. In NIPS3, \\npp. 832-838. \\nDasarathy , B. V. (ed.) (1991) Nearest Neighbor (NN) \\nNorms: NN Pattern Classification Techniques. Los \\nAlamitos, CA: IEEE Computer Society Press. \\nDattatreya, G. R. & Sarma, V. V. S. (1981) Bayesian \\nand decision tree approaches for pattern recog\\xad\\nnition including feature measurement costs. IEEE \\nTransactions on Pattern Analysis and Machine In\\xad\\ntelligence 3, 293-298. \\nDattatreya, G. R. & Sarma, V. V. S. (1985) Decision \\ntrees in pattern recognition. In Progress in Pattern \\nRecognition 2, eds L. N. Kanal & A. Rosenfeld. \\nAmsterdam: Elsevier. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 375}, page_content='364 References \\nDawid, A. P. (1976) Properties of diagnostic data \\ndistributions. Biometrics 32, 647-658. \\nDawid, A. P. (1979) Conditional independence in \\nstatistical theory (with discussion). Journal of the \\nRoyal Statistical Society series B 41, 1-31. \\nDawid, A. P. (1980) Conditional independence for \\nstatistical operations. Annals of Statistics 8, 598-\\n617. \\nDawid, A. P. (1982) The well-calibrated Bayesian \\n(with discussion). Journal of the American Statis\\xad\\ntical Association 77, 605-613. \\nDawid, A. P. (1986) Probability forecasting. In \\nEncyclopedia of Statistical Sciences, eds S. Kotz, \\nN. L. Johnson & C. B. Read, pp. 210-218. New \\nYork: Wiley. \\nDawid, A. P. (1992) Applications of a general prop\\xad\\nagation algorithm for probabilistic expert systems. \\nStatistics and Computing 2, 25-36. \\nDawid, A. P. & Lauritzen, S. L. (1993) Hyper Markov \\nlaws in the statistical analysis of decomposable \\ngraphical models. Annals of Statistics 21, 1272-\\n1317. \\nDeely, J. J. & Lindley, D. V. (1981) Bayes empirical \\nBayes. Journal of the American Statistical Associa\\xad\\ntion 76, 833-841. \\nDeMers, D. & Cottrell, G. (1993) Non-linear dimen\\xad\\nsionality reduction. In NIPS5, pp. 580-587. \\nDempster, A. P. & Kong, A. (1988) Uncertain evi\\xad\\ndence and artificial analysis. Journal of Statistical \\nPlanning and Inference 20, 355-368. Reprinted in \\nShafer & Pearl (1990). \\nDempster, A. P., Laird, N. M. & Rubin, D. B. (1977) \\nMaximum likelihood from incomplete data via the \\nEM algorithm (with discussion). Journal of the \\nRoyal Statistical Society series B 39, 1-38. \\nDennis, J. E. & Schnabel, R. B. (1983) Numerical \\nMethods for Unconstrained Optimization and Non\\xad\\nlinear Equations. Englewood Cliffs, NJ: Prentice\\xad\\nHall. \\nDevijver, P. A. & Kittler, J. V. (1982) Pattern Recogni\\xad\\ntion. A Statistical Approach. Englewood Cliffs, NJ: \\nPrentice- Hall. \\nDevijver, P. A. & Kittler, J. V. (eds) (1987) Pat\\xad\\ntern Recognition Theory and Applications. Berlin: \\nSpringer. \\nDevlin, S. J., Gnanadesikan, R. & Kettenring, J. R. \\n(1981) Robust estimation of dispersion matrices \\nand principal components. Journal of the American \\nStatistical Association 76, 354-362. DeVore, R. A., Howard, R. & Micchelli, C. A. (1989) \\nOptimal nonlinear approximation. Manuscripta \\nMathematica 63, 469-478. \\nDevroye, L. (1981a) On the inequality of Cover and \\nHart in nearest neighbor discrimination. IEEE \\nTransactions on Pattern Analysis and Machine In\\xad\\ntelligence 3, 7 5-78. \\nDevroye, L. ( 1981 b) On the almost everywhere con\\xad\\nvergence of nonparametric regression function es\\xad\\ntimates. Annals of Statistics 9, 1310-1319. \\nDevroye, L. (1982) Bounds for the uniform devia\\xad\\ntion of empirical measures. Journal of Multivariate \\nAnalysis 12, 72-79. \\nDevroye, L. (1988) Automatic pattern recognition: a \\nstudy of the probability of error. IEEE Transactions \\non Pattern Analysis and Machine Intelligence 10, \\n530-543. \\nDiaconis, P. & Freedman, D. (1984) Asymptotics of \\ngraphical projection pursuit. Annals of Statistics 12, \\n793-815. \\nDiaconis, P. & Shahshahani, M. (1984) On non-linear \\nfunctions of linear combinations. SIAM Journal on \\nScientific and Statistical Computing 5, 17 5-191. \\nDie bolt, J. & Robert, C. P. (1994) Estimation of finite \\nmixture distributions through Bayesian sampling. \\nJournal of the Royal Statistical Society series B 56, \\n363-375. \\nDietterich, T. G. (1990) Machine learning. Annual \\nReview of Computer Science 4, 255-306. \\nDietterich, T. G. & Bakiri, G. (1991) Error-correcting \\noutput codes: a general method for improving mul\\xad\\nticlass inductive learning programs. In Proceedings, \\nNinth AAAI National Conference on Artificial Intel\\xad\\nligence, Menlo Park, CA: AAAI Press, pp. 572-577. \\n(An identical paper appears as pp. 395-407 of \\nWolpert (1995).) \\nDietterich, T. G. & Bakiri, G. (1995) Solving multi\\xad\\nclass learning problems via error-correcting output \\ncodes. Journal of Artificial Intelligence Research 2, \\n263-286. \\nDiggle, P. J. & Hall, P. (1986) The selection of terms \\nin a orthogonal series density estimator. Journal of \\nthe American Statistical Association 81, 230-233. \\nDonoho, D. L. & Johnstone, I. M. (1989) Projection\\xad\\nbased approximation and a duality with kernel \\nmethods. Annals of Statistics 17, 58-106. \\nDoyle, P. (1973) The use of automatic interaction \\ndetector and similar search procedures. Operational \\nResearch Quarterly 24, 465-467. \\nDoyle, P. & Fenwick, I. (1975) The pitfalls of AID \\nanalysis. Journal of Marketing Research 12, 408-\\n413. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 376}, page_content='References 365 \\nDraper, D. (1995) Assessment and propagation of \\nmodel uncertainty (with discussion). Journal of the \\nRoyal Statistical Society series B 51, 45-97. \\nDuchon, J. (1977) Spline minimizing rotation\\xad\\ninvariant semi-norms in Sobolev spaces. In Con\\xad\\nstructive Theory of Functions of Several Variables, \\neds W. Schempp & K. Zeller. Lecture Notes in \\nMathematics 571, 85-100. \\nDuda, R. 0. & Hart, P. E. (1973) Pattern Classification \\nand Scene Analysis. New York: Wiley. \\nDudani, S. A. (1976) The distance-weighted k\\xad\\nnearest-neighbor rule. IEEE Transactions on Sys\\xad\\ntems, Man and Cybernetics 6, 325-327. Reprinted \\nin Dasarathy (1991). \\nDunn, J. C. (1974) A fuzzy relative of the ISO DATA \\nprocess and its use in detecting compact well\\xad\\nseparated clusters. Journal of Cybernetics 3, 32-57. \\nDyn, N. (1987) Interpolation of scattered data by \\nradial functions. In Topics in Multivariate Approx\\xad\\nimation, eds C. K. Chui, L. L. Schumaker & \\nF. I. Utreras. New York: Academic Press. \\nEaton, H. A. C. & Oliver, T. L. (1992) Learning \\ncoefficient dependence on training set size. Neural \\nNetworks 5, 283-288. \\nEdwards, D. (1995) Introduction to Graphical Mod\\xad\\nelling. New York: Springer. \\nEdwards, D. & Havarnek, T. (1985) A fast procedure \\nfor model-search in multidimensional contingency \\ntables. Biometrika 12, 339-351. \\nEfron, B. (1975) The efficiency of logistic regression \\ncompared to normal discriminant analysis. Journal \\nof the American Statistical Association 70, 892-898. \\nEfron, B. (1979) Bootstrap methods: another look at \\nthe jackknife. Annals of Statistics 1, 1-26. \\nEfron, B. (1982) The Jackknife , the Bootstrap and \\nOther Resampling Plans. Philadelphia: SIAM. \\nEfron, B. (1983) Estimating the error rate of a \\nprediction rule. Improvements on cross-validation. \\nJournal of the American Statistical Association 78, \\n316-331. \\nEfron, B. (1986) How biased is the apparent error \\nrate of a prediction rule? Journal of the American \\nStatistical Association 81, 461-470. \\nEfron, B. & Gong, G. (1983) A leisurely look at \\nthe bootstrap, the jackknife, and cross-validation. \\nAmerican Statistician 37, 36-48. \\nEfron, B. & Tibshirani, R. J. (1993) An Introduction \\nto the Bootstrap. New York: Chapman & Hall. Ehrenfeucht, A., Haussler, D., Kearns, M. & Valiant, \\nL. (1989) A general lower bound on the number \\nof examples needed for learning. Information and \\nComputation 82, 247-261. \\nEisenberger , I. (1964) Genesis of bimodal distribu\\xad\\ntions. Technometrics 6, 357-363. \\nEriksen, P. S. (1987) Proportionality of covariances. \\nAnnals of Statistics 15, 732-748. \\nErwin, E., Obermayer, K. & Schulten, K. (1992) Self\\xad\\norganizing maps: ordering, convergence properties \\nand energy functions. Biological Cybernetics 67, \\n47-55. \\nEslava-G6mez, G. (1989) Projection Pursuit and \\nOther Graphical Methods for Multivariate Data. Un\\xad\\npublished D. Phil thesis, University of Oxford. \\nEslava, G. & Marriott, F. H. C. (1994) Some criteria \\nfor projection pursuit. Statistics and Computing 4, \\n13-20. \\nEvans, M. & Swartz, T. (1995) Methods for approxi\\xad\\nmating integrals in statistics with special emphasis \\non Bayesian integration problems. Statistical Sci\\xad\\nence 10, 254-272. \\nFagin, R. (1977) Multivalued dependencies and a \\nnew normal form for relational databases . ACM \\nTransactions on Database Systems 2, 262-278. \\nFahlman, S. E. (1989) Faster-learning variations on \\nback-propagation: an empirical study. In Pro\\xad\\nceedings of the 1988 Connectionist Models Summer \\nSchool, Pittsburg, eds D. Touretzky, G. Hinton & \\nT. Sejnowski , pp. 38-51. San Mateo, CA: Morgan \\nKaufmann. \\nFahlman , S. E. & Lebiere, C. (1990) The cascade\\xad\\ncorrelation learning architecture. In NIPS2, \\npp. 524-532. \\nFauquet, C., Desbois, D., Fargette, D. & Vidal, G. \\n(1988) Classification of furoviruses based on the \\namino acid composition of their coat proteins. In \\nViruses with Fungal Vectors, eds J. I. Cooper & \\nM. J. C. Asher, pp. 19-38. Edinburgh: Association \\nof Applied Biologists. \\nFayyad, U. M. & Irani, K. B. (1992) On the handling \\nof continuous-valued attributes in decision tree \\ngeneration. Machine Learning 8, 87-102. \\nFefferman, C. & Markel, S. (1994) Recovering a \\nfeed-forward network from its output. In NIPS6, \\npp. 335-342. \\nFeldman, J. A. (1985) Connectionist models and their \\napplications: introduction. Cognitive Science 9, 1-2. \\nFienberg, S. E. & Holland, P. W. (1973) Simultaneous \\nestimation of multinomial cell probabilities. Journal \\nof the American Statistical Association 68, 683-691. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 377}, page_content=\"366 References \\nFinnoff, W., Hergert, F. & Zimmerman, H. G. (1993) \\nImproving model selection by nonconvergent meth\\xad\\nods. Neural Networks 6, 771-783. \\nFisher, R. A. ( 1936) The use of multiple measure\\xad\\nments in taxonomic problems. Annals of Eugenics \\n7, 179-188. \\nFix, E. & Hodges, J. L. (1951) Discriminatory \\nanalysis-non parametric discrimination: consis\\xad\\ntency properties. Report no. 4, US Air Force \\nSchool of Aviation Medicine , Random Field, \\nTexas. [Published in Agrawala (1977), Silverman \\nand Jones (1989) and Dasarathy (1991).] \\nFlanagan, J. K., Morrell, D. R., Frost, R. L., Read, \\nC. J. & Nelson, B. E. (1989) Vector quantization \\ncodebook generation using simulated annealing . \\nIn Proceedings of the International Conference on \\nAcoustics , Speech and Signal Processing (Glasgow, \\nMay 1989), pp. 1759-1762 . \\nFleiss, J. L. (1981) Statistical Methods for Rates and \\nProportions . Second edition. New York: Wiley. \\nFletcher, R. (1987) Practical Methods of Optimization. \\nChichester: Wiley. \\nFlocchini, P., Gardin, F., Mauri, G., Pensini, M. P., \\n& Stofella, P. (1992) Combining image processing \\noperators and neural networks in a face recognition \\nsystem. International Journal of Pattern Recognition \\nand Artificial Intelligence 6, 447-467. \\nFlury, B. (1986) Proportionality of k covariance \\nmatrices. Statistics and Probability Letters 4, 29-33. \\nFlury, B., Schmid, M. J. & Natayanan, A. (1994) Error \\nrates in quadratic discrimination with constraints \\non the covariance matrices. Journal of Classification \\n11, 101-120. \\nForgy, E. W. (1965) Cluster analysis of multivariate \\ndata: efficiency vs interpretability of classifications. \\nBiometrics 21, 768-769. \\nFort, M. & Pages, G. (1993) Sur Ia convergence p.s. \\nde I'algorithme de Kohonen generalise. Note aux \\nCompte Rendus de l'Academie des Sciences de Paris \\n317, Serie I, 389-394. \\nFrank, I. E. & Friedman, J. H. (1993) A statistical \\nview of some chemometrics regression tools (with \\ndiscussion) . Technometrics 35, 109-148. \\nFraser, D. A. S. (1968) The Structure of Inference. \\nNew York: Wiley. \\nFrean, M. (1990) The upstart algorithm: A method \\nfor constructing and training feedforward neural \\nnetworks. Neural Computation 2, 198-209. \\nFreedman, D. (1971) Markov Chains. San Francisco: \\nHolden-Day. Friedman, H. P. & Rubin, J. (1967) Some invariant \\ncriteria for grouping data. Journal of the American \\nStatistical Association 62, 1159-1178 . \\nFriedman , J. H. (1977) A recursive partitioning \\ndecision rule for nonparametric classification . IEEE \\nTransactions on Computers 26, 404-408. \\nFriedman, J. H. (1984) SMART users' guide. Labora\\xad\\ntory for Computational Statistics Technical Report \\nNo. 1, Dept of Statistics, Stanford University. \\nFriedman , J. H. (1987) Exploratory projection pur\\xad\\nsuit. Journal of the American Statistical Associati on \\n82, 249-266. \\nFriedman, J. H. (1989) Regularized discriminant anal\\xad\\nysis. Journal of the American Statistical Association \\n84, 165-175. \\nFriedman, J. H. (1991) Multivariate adaptive regres\\xad\\nsion splines (with discussion). Annals of Statistics \\n19, 1-141. \\nFriedman, J. H. & Silverman, B. W. (1989) Flexible \\nparsimonious smoothing and additive modeling \\n(with discussion) . Technometri cs 31, 3-39. \\nFriedman, J. H. & Stuetzle, W. (1981) Projection pur\\xad\\nsuit regression. Journal of the American Statistical \\nAssociation 76, 817-823. \\nFriedman , J. H. & Tukey, J. W. (1974) A projection \\npursuit algorithm for exploratory data analysis. \\nIEEE Transactions on Computers 23, 881-890. \\nFriedman, J. H., Baskett, F. & Shustek, L. J. (1975) \\nAn algorithm for finding nearest neighbors. IEEE \\nTransactions on Computers 24, 1000-1006. \\nFriedman , J. H., Bentley, J. L. & Finkel, R. A. \\n(1977) An algorithm for finding best matches in \\nlogarithmic expected time. ACM Transactions on \\nMathematical Software 3, 209-226. \\nFriedman, J. H., Stuetzle, W. & Schroeder, A. (1984) \\nProjection /pursuit density estimation. Journal of the \\nAmerican Statistical Association 79, 599-608. \\nFu, K.-S. (1982) Syntactic Pattern Recognition and \\nApplications. Engel wood Cliffs, NJ: Prentice Hall. \\nFukunaga, K. (1990) Introduction to Statistical Pat\\xad\\ntern Recognition . Second edition. Boston: Aca\\xad\\ndemic Press. (First edition, 1972). \\nFukunaga, K. & Flick, T. E. (1984) An optimal \\nglobal nearest neighbor metric. IEEE Transactions \\non Pattern Analysis and Machine Intelligence 6, \\n314--318. Reprinted in Dasarathy (1991). \\nFukunaga, K. & Flick, T. E. (1985) The 2-NN rule for \\nmore accurate NN risk estimation. IEEE Transac\\xad\\ntions on Pattern Analysis and Machine Intelligenc e \\n7, 107-112. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 378}, page_content='References 367 \\nFukunaga , K. & Hummels , D. M. (1987a) Bias of \\nnearest neighbor error estimates . IEEE Transac\\xad\\ntions on Pattern Analysis and Machine Intelligence \\n9, 103-112. Reprinted in Dasarathy (1991). \\nFukunaga, K. & Hummels, D. M. (1987b) Bayes \\nerror estimation using Parzen and k-NN pro\\xad\\ncedures. IEEE Transactions on Pattern Analysis \\nand Machine Intelligence 9, 634-643. Reprinted in \\nDasarathy (1991). \\nFukunaga , K. & Kessell, D. L. (1971) Estimation of \\nclassification error. IEEE Transactions on Comput\\xad\\ners 20, 1521-1527. \\nFukunaga, K. & Mantock , J. M. (1984) Non\\xad\\nparametric data reduction. IEEE Transactions on \\nPattern Analysis and Machine Intelligence 6, 115-\\n118. \\nFukunaga , K. & Narendra, P. M. (1975) A branch \\nand bound algorithm for computing k-nearest \\nneighbors . IEEE Transactions on Computers 24, \\n750-753. \\nFunahashi, K. (1989) On the approximat e realization \\nof continuous mappings by neural networks. Neural \\nNetworks 2, 183-192. \\nFung, R. M. & Crawford , S. L. (1990) Constructor: \\na system for induction of probabilistic models. In \\nProceedings, Eighth AAAI National Conference on \\nArtificial Intelligence, Boston, pp. 762-769. Menlo \\nPark, CA: AAAI Press. \\nFurman, W. & Lindsay, B. (1994) Testing for the \\nnumber of components in a mixture of normal dis\\xad\\ntributions using moment estimators. Computational \\nStatistics and Data Analysis 17, 473-492. \\nFurnival, G. M. & Wilson, R. W. Jr (1974) Re\\xad\\ngressions by leaps and bounds. Technometrics 16, \\n499-511. \\nGader, P., Forester, B., Ganzberger , M., Gillies, A., \\nMitchell, B., Whalen, M. & Yocum, T. (1991) \\nRecognition of handwritten digits using template \\nand model matching. Pattern Recognition 24, 421-\\n431. \\nGallant, S. I. (1990) Perceptron-based learning algo\\xad\\nrithms. IEEE Transactions on Neural Networks 1, \\n179-191. \\nGallant, S. I. (1993) Neural Network Learning and \\nExpert Systems. Cambridge , MA: The MIT Press. \\nGammerman, A. (ed.) (1995) Probabilistic Reasoning \\nand Bayesian Belief Networks . Henley-on- Thames: \\nAlfred Waller. \\nGammerman, A., Luo, Z., Aitken, C. G. G. & Brewer, \\nM. J. (1995) Exact and approximate algorithms and \\ntheir implementations in mixed graphical models. \\nIn Gammerman (1995), pp. 33-53. Garey, M. R. & Johnson, D. S. (1979) Computers \\nand Intractability: A Guide to the Theory of NP\\xad\\ncompletene ss. New York: Freeman. \\nGates, G. W. (1972) The reduced nearest neighbor \\nrule. IEEE Transactions on Information Theory 18, \\n431-433. \\nGeiger, D. & Pearl, J. (1990) On the logic of causal \\nmodels. In Uncertainty in Artificial Intelligence 4, \\neds R. D. Shachter, T. S. Levitt, L. N. Kanal \\n& J. F. Lemmer, pp. 3-14. Amsterdam: North\\xad\\nHolland. \\nGeiger, D. & Pearl, J. (1993) Logical and algorithmic \\nproperties of conditional independence and graph\\xad\\nical models. Annals of Statistics 21, 2001-2021. \\nGeiger, D., Verma, T. & Pearl, J. (1990) Recognizing \\nindependence in Bayesian networks . Networks 20, \\n507-534. \\nGeisser, S. (1964) Posterior odds for multivariate nor\\xad\\nmal classifications. Journal of the Royal Statistical \\nSociety series B 26, 69-76. \\nGeisser, S. (1966) Predictive discrimination. In Multi\\xad\\nvariate Analysis, ed. P. R. Krishnaiah , pp. 149-163. \\nNew York: Academic Press. \\nGeisser, S. (1975) The predictive sample reuse method \\nwith applications. Journal of the American Statisti\\xad\\ncal Association 70, 320-328. \\nGeisser, S. (1984) On prior distributions for binary \\ntrials (with discussion). American Statistician 38, \\n244-251. \\nGeisser, S. (1987) Comment on Hodges (1987). \\nStatistical Science 2, 277-279. \\nGeisser, S. (1993) Predictive Inference: An Introduc\\xad\\ntion. New York: Chapman & Hall. \\nGeisser, S. & Cornfield, J. (1963) Posterior distribu\\xad\\ntions for multivariate normal parameters. Journal \\nof the Royal Statistical Society series B 25, 368-376. \\nGelfand, A. E. & Dey, D. K. (1994) Bayes model \\nchoice: asymptotics and exact calculations. Journal \\nof the Royal Statistical Society series B 56, 501-514. \\nGelfand, A. E. & Smith, A. F. M. (1990) Sampling\\xad\\nbased approaches to calculating marginal densities. \\nJournal of the American Statistical Association 85, \\n398-409. \\nGelfand, A. E., Hills, S. E., Racine-Poon , A. & Smith, \\nA. F. M. (1990) Illustration of Bayesian inference in \\nnormal data models using Gibbs sampling. Journal \\nof the American Statistical Association 85, 972-985. \\nGelfand, S. B. & Delp, E. J. (1991) On tree structured \\nclassifiers. In Sethi & Jain (1991), pp. 51-70. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 379}, page_content=\"368 References \\nGelfand, S. B. & Mitter, S. K. (1991) Recursive \\nstochastic algorithms for global optimization in \\nIR.d. SIAM Journal on Control and Optimization 29, \\n999-1018. \\nGelfand, S. B., Ravishankar, C. S. & Delp, E. J. \\n(1991) An iterative growing and pruning algorithm \\nfor classification tree design. IEEE Transactions \\non Pattern Analysis and Machine Intelligence 13, \\n163-174. \\nGelman, A., Carlin, J. B., Stern, H. S. & Rubin, \\nD. B. (1995) Bayesian Data Analysis. New York: \\nChapman & Hall. \\nGeman, D. (1990) Random fields and inverse prob\\xad\\nlems in imaging. In Ecole d'Ete de Probabilites de \\nSaint-Flour XVIII -1988, ed. P. L. Hennequin. \\nLecture Notes in Mathematics 1427, 113-193. \\nGeman, S. & Geman, D. (1984) Stochastic relaxation, \\nGibbs distributions and the Bayesian restoration \\nof images. IEEE Transactions on Pattern Analysis \\nand Machine Intelligence 6, 721-741. Reprinted in \\nShafer & Pearl (1990). \\nGeman, S. & Hwang, C.-R. (1982) Nonparametric \\nmaximum likelihood estimation by the method of \\nsieves. Annals of Statistics 10, 401-414. \\nGeman, S., Bienenstock, E. & Doursat, R. (1992) \\nNeural networks and the bias/variance dilemma. \\nNeural Computation 4, 1-58. \\nGeorge, E. I. & McCulloch, R. E. (1993) Variable \\nselection via Gibbs sampling. Journal of the Amer\\xad\\nican Statistical Association 88, 881-889. \\nGeorgiopoulos, M., Heileman, G. L. & Huang, J. \\n(1990) Convergence properties of learning in \\nARTl. Neural Computation 2, 502-509. \\nGeorgiopoulos, M., Heileman, G. L. & Huang, J. \\n(1991) Properties of learning related to pattern \\ndiversity in ARTl. Neural Networks 4, 751-757. \\nGersho, A. & Gray, R. M. (1992) Vector Quantization \\nand Signal Compression. Boston: Kluwer Academic \\nPublishers. \\nGeyer, C. (1992) Practical Markov chain Monte Carlo \\n(with discussion). Statistical Science 7, 473-511. \\nGhurye, S. G. & Olkin, I. (1969) Unbiased estimation \\nof some multivariate probability densities and \\nrelated functions. Annals of Mathematical Statistics \\n40, 1261-1271. \\nGilbert, J. C. & Lemarechal, C. (1989) Some nu\\xad\\nmerical experiments with variable storage quasi\\xad\\nNewton methods. Mathematical Programming 45, \\n407-436. \\nGill, P. E., Murray, W. & Wright, M. H. (1981) \\nPractical 0 ptimization. London: Academic Press. Girosi, F. & Anzellotti, G. (1993) Rates of conver\\xad\\ngence for radial basis functions and neural net\\xad\\nworks. In Mammone (1993), pp. 97-114. \\nGirosi, F. & Poggio, T. (1990) Networks and the best \\napproximation property. Biological Cybernetics 63, \\n169-176. \\nGirosi, F., Jones, M. & Poggio, T. (1995) Regular\\xad\\nization theory and neural networks architectures. \\nNeural Computation 7, 219-269. \\nGlick, N. (1972) Sample-based classification proce\\xad\\ndures derived from density estimators. Journal of \\nthe American Statistical Association 67, 116-122. \\nGlick, N. (1976) Sample-based classification proce\\xad\\ndures related to empiric distributions. IEEE Trans\\xad\\nactions on Information Theory 22, 454-461. \\nGlymour, C., Scheines, R., Spirtes, P. & Kelly, \\nK. (1987) Discovering Causal Structure: Artificial \\nIntelligence, Philosophy of Science, and Statistical \\nModeling. San Diego: Academic Press. \\nGoldstein, M. & Dillon, W. R. (1978) Discrete \\nDiscriminant Analysis. New York: Wiley. \\nGolomb, B. A., Lawrence, D. T. & Sejnowski, T. J. \\n(1991) SEXNET: A neural network identifies sex \\nfrom human faces. In NIPS3, pp. 572-577. \\nGolombic, M. C. (1980) Algorithmic Graph Theory \\nand Perfect Graphs. New York: Academic Press. \\nGolub, G. H. & Van Loan, C. F. (1989) Matrix \\nComputations. Second edition. Baltimore: Johns \\nHopkins University Press. \\nGonzalez, R. C. & Thomason, M.G. (1978) Syntac\\xad\\ntic Pattern Recognition: An Introduction. Reading, \\nMA: Addison-Wesley. \\nGood, I. J. (1965) The Estimation of Probabilities. \\nCambridge, MA: The MIT Press. \\nGood, I. J. ( 1983) Good Thinking: The Foundations \\nof Probability and its Applications. Minneapolis: \\nUniversity of Minnesota Press. \\nGoodman, R. M. & Smyth, P. (1988) Decision tree \\ndesign from a communication theory standpoint. \\nIEEE Transactions on Information Theory 34, 979-\\n994. \\nGordon, A. D. (1981) Classification. Methods for \\nExploratory Analysis of Multivariate Data. London: \\nChapman & Hall. \\nGori, M. & Tesi, A. (1992) On the problem of local \\nminima in backpropagation. IEEE Transactions on \\nPattern Analysis and Machine Intelligence 14, 76-\\n86. \\nGower, J. C. (1966) Some distance properties of \\nlatent root and vector methods used in multivariate \\nanalysis. Biometrika 53, 325-328. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 380}, page_content=\"References 369 \\nGower, J. C. (1971) A general coefficient of similarity \\nand some of its properties . Biometrics 27, 857-871. \\nGower, J. C. & Legendre , P. (1986) Metric and \\nEuclidean properties of dissimilarity coefficients. \\nJournal of Classification 3, 5-48. \\nGray, R. M. (1984) Vector quantization . IEEE ASSP \\nMagazine 1(2), 4-29. \\nGreen, P. J. & Silverman, B. W. (1994) Non\\xad\\nparametri c Regression and Generalized Linear Mod\\xad\\nels. A Roughness Penalty Approach . London: Chap\\xad\\nman & Hall. \\nGrenander , U. (1981) Abstract Inference. New York: \\nWiley. \\nGrenander, U., Chow, Y. & Keenan, D. M. (1991) \\nHands. A Pattern Theoretic Study of Biological \\nShapes. New York: Springer. \\nGrinold, R. C. (1969) Comment on 'Pattern clas\\xad\\nsification design by linear programming'. IEEE \\nTransactions on Computers 18, 378-379. \\nGrother, P. J. & Candela, G. T. (1993) Comparison of \\nhandprinted digit classifiers. US National Institute \\nof Standards and Technology report N ISTIR 5209. \\nGu, C. (1990) Adaptive spline smoothing in non\\xad\\nGaussian regression models. Journal of the Ameri\\xad\\ncan Statistical Association 85, 801-807. \\nGu, C. & Wahba, G. (1991) Minimizing GCV /GML \\nscores with multiple smoothing parameters via the \\nNewton method. SIAM Journal on Scientific and \\nStatistical Computing 12, 383-398. \\nGu, C., Bates, D. M., Chen, Z. & Wahba, G. (1989) \\nThe computation of generalized cross-validation \\nfunctions through Householder tridiagonalization \\nwith applications to the fitting of interaction \\nspline models. SIAM Journal on Matrix Analysis \\nand Applications 10, 459-480. \\nGuo, H. & Gelfand, S. B. (1992) Classification \\ntrees with neural network feature extraction. IEEE \\nTransactions on Neural Networks 3, 923-933. \\nGuyon, I., Vapnik, V., Boser, B., Bottou, L. & \\nSolla, S. A. (1992) Structural risk minimization for \\ncharacter recognition. In NIPS4, pp. 471-479. \\nHall, D. J. & Ball, G. B. (1965) ISODATA: a novel \\nmethod of data analysis and pattern classifica\\xad\\ntion. Technical report, Stanford Research Institute, \\nMenlo Park CA. \\nHall, D. J. & Khanna, D. (1977) The ISODATA \\nmethod of computation for relative perception of \\nsimilarities and differences in complex and real \\ncomputers. In Statistical Methods for Digital Com\\xad\\nputers 3, eds K. Enslein, A. Ralston & H. S. Wilf, \\npp. 340-373. New York: Wiley. Hall, P. (1981) On nonparametric multivariate binary \\ndiscrimination . Biometrika 68, 287-294. \\nHall, P. (1989) On polynomial-based projection in\\xad\\ndices for exploratory projection pursuit. Annals of \\nStatistics 17, 589-605. \\nHall, P. & Wand, M. P. (1988) On nonparametric \\ndiscrimin ation using density differences. Biometrika \\n75, 541-547. \\nHampel, F. R., Ronchetti, E. M., Rousseeuw , P. J. \\n& Stahel, W. A. (1986) Robust Statistics: The \\nApproach Based on Influence Function s. New York: \\nWiley. \\nHampson, S. E. & Volper, D. J. (1986) Linear \\nfunction neurons: structure and training. Biological \\nCybernetics 53, 203-217. \\nHand, D. J. (1981) Discrimination and Classification. \\nChichester: Wiley. \\nHand, D. J. (1982) Kernel Discriminant Analysis. \\nChichester: Research Studies Press. \\nHand, D. J. & Batchelor, B. G. (1978) An edited con\\xad\\ndensed nearest neighbor rule. Information Sciences \\n14, 171-180. \\nHannan, E. J. & Quinn, B. G. (1979) The determina\\xad\\ntion of the order of an autoregression . Journal of \\nthe Royal Statistical Society series B 41, 190--195. \\nHansen, L. K. & Salamon, P. (1990) Neural network \\nensembles. IEEE Transactions on Pattern Analysis \\nand Machine Intelligence 12, 993-1001. \\nHanson, S. J. & Pratt, L. Y. (1989) Comparing \\nbiases for minimal network construction with back\\xad\\npropagation . In NIPSJ, pp. 177-185. \\nHardie, W. (1990) Applied Nonparametri c Regression. \\nCambridge: Cambridge University Press. \\nHardie, W. (1991) Smoothing Techniques with Imple\\xad\\nmentation in S. New York: Springer. \\nHardy, R. L. (1971) Multiquadric equations of \\ntopography and other irregular surfaces. Journal \\nof Geophysi cal Research 76, 1906--1915 . \\nHardy, R. L. (1990) Theory and applications of \\nthe multiquadric-biharmonic method: 20 years of \\ndiscovery 1968-1988. Computers and Mathematics \\nwith Applications 19, 163-208. \\nHart, P. E. (1968) The condensed nearest neighbor \\nrule. IEEE Transactions on Information Theory 14, \\n515-516. \\nHartigan, J. A. (1975) Clustering Algorithm s. New \\nYork: Wiley. \\nHartigan, J. A. & Wong, M. A. (1979) Algorithm \\nAS136. A K-means clustering algorithm. Applied \\nStatistics 28, 100--108. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 381}, page_content=\"370 References \\nHartman, E. J., Keeler, J. D. & Kowalski, J. M. \\n(1990) Layered neural networks with Gaussian \\nhidden units as universal approximations. Neural \\nComputation 2, 210-215. \\nHassibi, B. & Stork, D. G. (1993) Second derivatives \\nfor network pruning: Optimal Brain Surgeon. In \\nNIPS5, pp. 164-171. \\nHassibi, B., Stork, D. G., Wolff, G. & Watanabe, \\nT. (1994) Optimal Brain Surgeon: extensions and \\nperformance comparisons. In NIPS6, pp. 263-270. \\nHastie, T. & Mallows, C. (1993) Discussion of Frank \\n& Friedman (1993). Technometrics 35, 140-143. \\nHastie, T. & Stuetzle, W. (1989) Principal curves. \\nJournal of the American Statistical Association 84, \\n502-516. \\nHastie, T. J. & Tibshirani, R. J. (1990) Generalized \\nAdditive Models. London: Chapman & Hall. \\nHastie, T. & Tibshirani, R. (1996) Discriminant \\nanalysis by Gaussian mixtures. Journal of the Royal \\nStatistical Society series B 58, 155-176. \\nHastie, T., Buja, A. & Tibshirani, R. (1995) Penalized \\ndiscriminant analysis. Annals of Statistics 23, 73-\\n102. \\nHastie, T., Tibshirani, R. & Buja, A. (1994) Flexible \\ndiscriminant analysis by optimal scoring. Journal of \\nthe American Statistical Association 89, 1255-1270. \\nHastings, W. K. (1970) Monte Carlo sampling meth\\xad\\nods using Markov chains and their applications. \\nBiometrika 57, 97-109. \\nHathaway, R. J. (1985) A constrained formulation of \\nmaximum-likelihood estimation for normal mix\\xad\\nture distributions. Annals of Statistics 13, 795-800. \\nHauck, W. W. Jr & Donner, A. (1977) Wald's test as \\napplied to hypotheses in logit analysis. Journal of \\nthe American Statistical Association 72, 851-853. \\nHaussler, D. (1992) Decision theoretic generalizations \\nof the PAC model for neural net and other learning \\napplications. Information and Computation 100, 78-\\n150. Reprinted as pp. 37-116 of Wolpert (1995). \\nHaykin, S. (1994) Neural Networks. A Comprehensive \\nFoundation. New York: Macmillan College Pub\\xad\\nlishing. \\nHebb, D. 0. (1949) The Organization of Behavior. \\nNew York: Wiley. \\nHellman, M. E. (1970) The nearest neighbor classifi\\xad\\ncation rule with a reject option. IEEE Transactions \\non Systems Science and Cybernetics 6, 179-185. \\nReprinted in Dasarathy (1991). Henrichon, E. G. Jr & Fu, K.-S. (1968) On mode \\nestimation in pattern recognition. In Proceedings \\nof the Seventh Symposium on Adaptive Processes, \\nUCLA, p. 3-a-1. \\nHenrichon, E. G. Jr & Fu, K.-S. (1969) A non\\xad\\nparametric partitioning procedure for pattern clas\\xad\\nsification. IEEE Transactions on Computers 18, \\n614-624. \\nHenrion, M. (1988) Propagating uncertainty in \\nBayesian networks by probabilistic logic sam\\xad\\npling. In Uncertainty in Artificial Intelligence 2, eds \\nJ. Lemmer & L. N. Kana!, pp. 149-163. Amster\\xad\\ndam: North-Holland. \\nHenrion, M., Breese, J. S. & Horvitz, E. J. (1991) \\nDecision analysis and expert systems. AI Magazine \\n12(4), 64-91. \\nHermans, J., Habbema, J. D. F. & Schaefer, J. R. \\n(1982) The ALLOC80 package for discriminant \\nanalysis. Statistical Software Newsletter 8, 15-20. \\nHertz, J., Krogh, A. & Palmer, R. G. (1991) Introduc\\xad\\ntion to the Theory of Neural Computation. Redwood \\nCity, CA: Addison-Wesley. \\nHeskes, T. M. & Kappen, B. (1991) Learning pro\\xad\\ncesses in neural networks. Physical Reviews A 44, \\n2718-2726. \\nHighleyman, W. H. (1962a) The design and analysis \\nof pattern recognition experiments. Bell Systems \\nTechnical Journal 41, 723-744. \\nHighleyman, W. H. (1962b) Linear decision functions, \\nwith application to pattern recognition. Proceedings \\nof the IRE 50, 1501-1514. \\nHills, M. (1966) Allocation rules and their error rates \\n(with discussion). Journal of the Royal Statistical \\nSociety series B 28, 1-31. \\nHinton, G. E. (1986) Learning distributed represen\\xad\\ntations of concepts. In Proceedings of the Eighth \\nAnnual Conference of the Cognitive Science Society \\n(Amherst, 1986), pp. 1-12. Hillsdale: Erlbaum. \\nHinton, G. E. (1989a) Connectionist learning proce\\xad\\ndures. Artificial Intelligence 40, 185-234. (Reprinted \\nin Carbonell, 1990.) \\nHinton, G. E. (1989b) Deterministic Boltzmann ma\\xad\\nchine learning performs steepest descent in weight \\nspace. Neural Computation 1, 143-150. \\nHinton, G. E. & Sejnowski, T. J. (1983) Optimal \\nperceptual inference. In Proceedings of the IEEE \\nConference on Computer Vision and Pattern Recog\\xad\\nnition (Washington. 1983 ), pp. 448--453. New York: \\nIEEE Press. \\nHjort, N. L. (1986) Notes on the Theory of Statistical \\nSymbol Recognition. Norwegian Computing Center \\nReport 778. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 382}, page_content=\"References 371 \\nHjort, N. L. & Glad, I. K. (1995) Nonparametric \\ndensity estimation with a parametric start. Annals \\nof Statistics 23, 882-904. \\nHjort, N. L. & Jones, M. C. (1996) Locally para\\xad\\nmetric nonparametric d ensity estimation . Annals of \\nStatistics 24, 1619-1647 . \\nHo, Y.-C. & Kashyap, R. L. (1965) An algorithm \\nfor linear inequalities and its applications. IEEE \\nTransacti ons on Electroni c Computers 14, 683-688. \\nHodges, J. S. (1987) Uncertainty, policy analysis and \\nstatistics (with discussion). Statistical Science 2, \\n259-291. \\nHoeffding , W. ( 1963) Probability inequalitie s for \\nsums of bounded random variables. Journal of the \\nAmerican Statistical Association 58, 13-30. \\nHoerl, A. E. & Kennard , R. W. (1970a) Ridge \\nregression : biased estimation for nonorthogonal \\nproblems. Technometrics 12, 55-67. \\nHoerl, A. E. & Kennard , R. W. (1970b) Ridge \\nregression : applications to nonorthogonal prob\\xad\\nlems. Technometrics 12, 69-82. \\nHoffgen, K.-U., Simon, H.-U. & Van Horn, K. S. \\n(1995) Robust trainability of single neurons. Jour\\xad\\nnal of Computer and System Sciences 50, 114-125. \\nHolt, M. J. J. & Semnani, S. (1990) Convergence \\nof back-propagation in neural networks using a \\nlog-likelihood cost function. Electronics Letters 26, \\n1964-1965. \\nHopfield, J. J. (1982) Neural networks and physical \\nsystems with emergent collective computational \\nfacilities. Proceedings of the National Academy of \\nSciences of the USA 79, 2554-2558. Reprinted in \\nAnderson & Rosenfeld (1988) and Lau (1992). \\nHopfield, J. J. (1987) Learning algorithms and prob\\xad\\nability distributions in feed-forward and feed-back \\nnetworks. Proceedings of the National Academy of \\nSciences of the USA 84, 8429-8433. \\nHornik, K., Stinchcombe, M. & White, H. (1989) \\nMultilayer feedforward networks are univer\\xad\\nsal approximators . Neural Networks 2, 359-366. \\nReprinted in White ( 1992). \\nHornik, K., Stinchcombe, M. & White, H. (1990) \\nUniversal approximation of an unknown mapping \\nand its derivatives using feedforward networks. \\nNeural Networks 3, 551-560. Reprinted in White \\n( 1992). \\nHrycej, T. (1990) Gibbs sampling in Bayesian net\\xad\\nworks. Artificial Intelligen ce 46, 351-363. \\nHrycej, T. (1992) Modular Learning in Neural Net\\xad\\nworks. A Modulari zed Appproach to Neural Network \\nClassification . New York: Wiley. Huang, J., Georgiopoulos, M. & Heileman, G. L. \\n(1995) Fuzzy ART properties . Neural Networks 8, \\n203-213. \\nHuber, P. J. (1967) The behavior of maximum \\nlikelihood estimates under nonstandard conditions. \\nIn Proceedings of the Fifth Berkeley Symposium on \\nMathematical Statistics and Probability, eds L. M. \\nLe Cam & J. Neyman, 1, pp. 221-233. Berkeley : \\nUniversity of California Press. \\nHuber, P. J. (1981) Robust Statistics. New York: \\nWiley. \\nHuber, P. J. (1985) Projection pursuit (with discus\\xad\\nsion). Annals of Statistics 13, 435-525. \\nHunt, E. B., Marin, J. & Stone, P. J. (1966) Experi\\xad\\nments in Induction. New York: Academic Press. \\nHwang, J.-N., Lay, S.-R., Maechler , M., Martin, D. \\n& Schimert, J. (1994a) Regression modeling in \\nback-propagation and projection pursuit learning. \\nIEEE Transactions on Neural Networks 5, 342-353. \\nHwang, J.-N., Li, H., Maechler, M., Martin, D. & \\nSchimert, J. (1992a) A comparison of projection \\npursuit and neural network regression modeling . \\nIn NIPS4, pp. 1159-1166. \\nHwang, J.-N., Li, H., Maechler , M., Martin, D. & \\nSchimert, J. (1992b) Projection pursuit learning \\nnetworks for regression. Engineering Applications \\nof Artificial Intelligence 5, 193-204. \\nHwang, J.-N., Li, H., Martin, D. & Schimert , J. (1991) \\nThe learning parsimony of projection pursuit and \\nback-propagation networks. In 25th Asilomar Con\\xad\\nference on Signals, Systems and Computers, Pacific \\nGrove, CA, pp. 491-495. Los Alamitos, CA: IEEE \\nComputer Society Press. \\nHwang, J.-N., You, S.-S., Lay, S.-R. & Jou, I.-C. \\n(1994b) What's wrong with a cascaded correlation \\nlearning network: a projection pursuit learning \\nperspective. Technical Report, Dept of Electrical \\nEngineering, University of Washington. \\nHwang, J.-N., You, S.-S., Lay, S.-R. & Jou, I.-C. (1996) \\nThe cascaded correlation learning: A projection \\npursuit perspective . IEEE Transactions on Neural \\nNetworks 7, 278-289. \\nHyafil, R. & Rivest, R. L. (1976) Constructing \\noptimal binary trees is NP-complete . Information \\nProcessing Letters 5, 15-17. \\nImpedovo, S., Ottaviano, L. & Occhinegro, S. (1991) \\nOptical character recognition -a survey. Interna\\xad\\ntional Journal of Pattern Recogniti on and Artificial \\nIntelligence 5, 1-24. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 383}, page_content=\"372 References \\nIngrassia, S. (1992) A comparison between the simu\\xad\\nlated annealing and the EM algorithms in normal \\nmixture decompositions. Statistics and Computing \\n2, 203-211. \\nIntrator, N. (1990) A neural network for feature \\nextraction. In NIPS2, pp. 719-726. \\nIntrator, N. (1991) Exploratory feature extraction in \\nspeech signals. In NIPS3, pp. 241-247. \\nIntrator, N. (1992) Feature extraction using an un\\xad\\nsupervised neural network. Neural Computation 4, \\n98-107. \\nIntrator, N. & Cooper, L. N. (1992) Objective \\nfunction formulation of the BCM theory of visual \\ncortical plasticity: statistical connections , stability \\nconditions. Neural Networks 5, 3-17. \\nIntrator, N. & Gold, J. I. (1993) Three-dimensional \\nobject recognition using an unsupervised BCM \\nnetwork: the usefulness of distinguishing features. \\nNeural Computation 5, 61-74. \\nIsham, V. (1981) An introduction to spatial point \\nprocesses and Markov random fields. International \\nStatistical Review 49, 21-43. \\nJackson, J. E. (1991) A User's Guide to Principal \\nComponents. New York: Wiley. \\nJacobs, R. A (1988) Increased rates of convergence \\nthrough learning rate adaptation. Neural Networks \\n1, 295-307. \\nJacobs, R. A, Jordan, M. I., Nowlan, S. J. & Hinton, \\nG. E. (1991) Adaptive mixtures of local experts. \\nNeural Computation 3, 79-87. \\nJain, A K. & Dubes, R. C. (1988) Algorithms for \\nClustering Data. Englewood Cliffs, NJ: Prentice\\xad\\nHall. \\nJain, A K., Dubes, R. C. & Chen, C.-C. (1987) \\nBootstrap techniques for error estimation . IEEE \\nTransactions on Pattern Analysis and Machine In\\xad\\ntelligence 9, 628-633. \\nJames, M. (1988) Pattern Recognition. New York: \\nWiley. \\nJancey, R. C. (1966) Multidimensional group analysis. \\nAustralian Journal of Botany 14, 127-130. \\nJeffreys, H. (1961) Theory of Probability . Third edi\\xad\\ntion. Oxford: Clarendon Press. \\nJensen, F. V. (1991) Calculation in HUGIN of \\nprobabilities for specific configurations -a trick \\nwith many applications . In Proceedings of the \\nScandinavian Conference on Artificial Intelligence , \\npp. 176-186. Amsterdam : lOS Press. \\nJensen, F. V. ( 1996) An introduction to Belief Networks. \\nLondon: UCL Press (Taylor & Francis Ltd) and \\nNew York: Springer. Jensen, F. V. & Liang, J. (1995) drHugin. A system \\nfor hypothesis driven data request. In Gammer\\xad\\nman (1995), pp. 109-124. \\nJensen, F. V., Lauritzen , S. L. & Olesen, K. G. \\n( 1990) Bayesian updating in causal probabilistic \\nnetworks by local computations. Computational \\nStatistics Quarterly 5, 269-282. \\nJiang, Q. & Zhang, W. (1993) An improved method \\nfor finding nearest neighbors . Pattern Recognition \\nLetters 14, 531-535. \\nJohansson, E. M., Dowla, F. U. & Goodman, D. M. \\n(1991) Back-propagation learning for multi-layer \\nfeed-forward neural networks using the conjugate \\ngradient method. International Journal of Neural \\nSystems 2, 291-302. \\nJohnson, N. L. & Kotz, S. (1972) Distributions \\nin Statistics: Continuous Multivariate Distributions . \\nNew York: Wiley. \\nJolliffe, I. T. (1986) Principal Component Analysis. \\nNew York: Springer. \\nJones, L. K. (1987) On a conjecture of Huber \\nconcerning the convergence of projection pursuit \\nregression. Annals of Statistics 15, 880-882. \\nJones, L. K. (1992) A simple lemma on greedy \\napproximation in Hilbert space and convergence \\nrates for projection pursuit regression and neural \\nnetwork training. Annals of Statistics 20, 608-613. \\nJones, M. C. & Sibson, R. (1987) What is projection \\npursuit (with discussion)? Journal of the Royal \\nStatistical Society series A 150, 1-36. \\nJordan, M. I. & Jacobs, R. A (1992) Hierarchies of \\nadaptive experts. In NIPS4, pp. 985-992. \\nJordan, M. I. & Jacobs, R. A (1994) Hierarchical \\nmixtures of experts and the EM algorithm. Neural \\nComputation 6, 181-214. \\nKalantari, I. & McDonald, G. (1983) A data structure \\nand an algorithm for the nearest point problem. \\nIEEE Transactions on Software Engineering 9, 631-\\n634. \\nKambhatla, N. & Leen, T. K. (1994) Fast non-linear \\ndimension reduction. In NIPS6, pp. 152-159. \\nKamgar-Parsi, B. & Kana!, L. N. (1985) An improved \\nbranch and bound algorithm for computing k\\xad\\nnearest neighbours. Pattern Recognition Letters 3, \\n7-12. \\nKansa, E. J. (1990) Multiquadrics-a scattered data \\napproximation scheme with applications to com\\xad\\nputational fluid dynamics. 1. Computers and Math\\xad\\nematics with Applications 19, 127-145. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 384}, page_content='References 373 \\nKappen, H. J. (1995) Deterministic learning rules for \\nBoltzmann machines. Neural Networks 8, 537-548. \\nKarpinski , M. & Macintyre , A. (1995a) Bound\\xad\\ning VC-dimension of neural networks : Progress \\nand prospects . In Proceedings of the Second Euro\\xad\\npean Conference on Computational Learning Theory \\n(Barcelona , Spain), ed. P. Vitanyi, number 904 in \\nLecture Notes in Artificial Intelligence , pp. 337-\\n341. Berlin: Springer. \\nKarpinski, M. & Macintyre , A. (1995b) Polynomial \\nbounds for VC dimension of sigmoidal neural \\nnetworks. In Proceedings of the Twenty-Seventh \\nAnnual ACM Symposium on Theory of Computing \\n(Las Vegas), pp. 200-208. ACM Press. \\nKashyap, R. L. & Blaydon, C. C. (1968) Estimation \\nof probability density and distribution functions. \\nIEEE Transactions on Information Theory 14, 549-\\n556. \\nKass, G. V. (1980) An exploratory technique for \\ninvestigating large quantitie s of categorical data. \\nApplied Statistics 29, 119-127. \\nKass, R. E. & Raftery, A. E. (1995) Bayes factors. \\nJournal of the American Statistical Association 90, \\n733-795. \\nKass, R. E. & Vaidyanathan, S. K. (1992) Approx\\xad\\nimate Bayes factors and orthogonal parameters , \\nwith application to testing equality of two binomial \\nproportions. Journal of the Royal Statistical Society \\nseries B 54, 129-144. \\nKaufman, L. & Rousseeuw, P. J. (1990) Finding \\nGroups in Data. An Introduction to Cluster Analysis. \\nNew York: Wiley. \\nKendall, M. G. & Stuart, A. (1966) The Advanced \\nTheory of Statistics, volume III. London: Griffin. \\nKent, J. T., Tyler, D. E. & Vardi, Y. (1994) \\nA curious likelihood identity for the multi\\xad\\nvariate t-distribution. Communications in Statistics \\n-Simulation and Computation 23, 441-453. \\nKibler, D. & Aha, D. W. (1987) Learning representa\\xad\\ntive exemplars of concepts: an initial case study. In \\nProceedings of the Fourth International Workshop \\non Machine Learning (Irvine, 1987 ), ed. P. Langley, \\npp. 24-30. Palo Alto, CA: Morgan Kaufmann. \\nReprinted in Shavlik & Dietterich (1990). \\nKiiveri, H., Speed, T. P. & Carlin, J. B. (1984) \\nRecursive causal models. Journal of the Australian \\nMathemati cal Society (series A) 36, 30-52. \\nKim, B. S. & Park, S. B. (1986) A fast k nearest \\nneighbor finding algorithm based on the ordered \\npartition. IEEE Transactions on Pattern Analysis \\nand Machine Intelligence 8, 761-766. Reprinted in \\nDasarathy (1991). Kim, J. H. & Pearl, J. (1983) A computational model \\nfor combined causal and diagnostic reasoning in \\ninference systems. In Proceedings of the Eighth \\nInternational Joint Conference on Artificial Intelli\\xad\\ngence (Karlsruhe , 1983), pp. 190-193. Menlo Park, \\nCA:AAAI. \\nKing, R. D., Muggleton, S., Lewis, R. A. & Sternberg, \\nM. J. E. (1992) Drug design by machine learning: \\nThe use of inductive logic programming to model \\nthe structure-activity relationships of trimethoprim \\nanalogues binding to dihydrofolate reductase. Pro\\xad\\nceedings of the National Academy of Sciences of the \\nUSA 89, 11322-11326. \\nKjrerulff, U. (1992) Optimal decomposition of proba\\xad\\nbilistic networks by simulated annealing. Statistics \\nand Computing 2, 7-17. \\nKleijnen, J.P. C. (1987) Statistical Tools for Simulation \\nPractitioners. New York: Marcel Dekker. \\nKleijnen, J. P. C. & van Groenendaal , W. (1992) \\nSimulation : A Statistical Perspective . Chichester: \\nWiley. \\nKnerr, S., Personnaz , L. & Dreyfus, G. (1992) Hand\\xad\\nwritten digit recognition by neural networks with \\nsingle-layer training. IEEE Transactions on Neural \\nNetworks 3, 962-968. \\nKnuth, D. E. (1968) The Art of Computer Program\\xad\\nming, Volume 1: Fundamental Algorithms. Reading, \\nMA: Addison-Wesley. (Second edition, 1973.) \\nKohonen, T. (1982a) Self-organized formation of \\ntopologically correct feature maps. Biological Cy\\xad\\nbernetics 43, 59-69. Reprinted in Anderson & \\nRosenfeld (1988). \\nKohonen, T. (1982b) Analysis of a simple self\\xad\\norganizing process. Biological Cybernetics 43, 135-\\n140. \\nKohonen, T. (1988a) An introduction to neural \\ncomputing. Neural Networks l, 3-16. \\nKohonen, T. (1988b) Learning vector quantization. \\nNeural Networks 1 (suppl. 1), 303. \\nKohonen, T. (1989) Self-Organization and Associative \\nMemory. Third edition. Berlin: Springer. [First \\nedition, 1984] \\nKohonen, T. (1990a) The self-organizing map. Pro\\xad\\nceedings of the IEEE 78, 1464-1480. Reprinted in \\nLau (1992). \\nKohonen, T. (1990b) Improved versions of learning \\nvector quantization. In Proceedings of the IEEE \\nInternational Conference on Neural Networks , San \\nDiego I, 545-550. New York: IEEE Press. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 385}, page_content=\"374 References \\nKohonen, T. ( 1993) Physiological interpretation of the \\nself-organizing map algorithm. Neural Networks 6, \\n895-905. \\nKohonen, T. (1995) Self-Organizing Maps. Berlin: \\nSpringer. \\nKohonen, T., Barna, G. & Chrisley, R. (1988) Sta\\xad\\ntistical pattern recognition with neural networks: \\nbenchmarking studies. In Proceedings of the IEEE \\nInternational Conference on Neural Networks , San \\nDiego, I, 61-68. Long Beach, CA: IEEE Press. \\nReprinted in Anderson et al. (1990). \\nKohonen, T., Kangas, T., Laaksonen, J. & Torkkola, \\nK. (1992) LVQ_PAK. The learning vector quan\\xad\\ntization program package version 2.1. Laboratory \\nof Computer and Information Science, Helsinki \\nUniversity of Technology . [Version 3.1 became \\navailable in 1995.] \\nKoiran, P. & Sontag, E. D. (1996) Neural networks \\nwith quadratic VC dimension . In Advances in \\nNeural Information Processing Systems 8 eds D. S. \\nTouretzky , M. C. Moser & M. E. Hasselmo, \\npp. 197-203. Cambridge, MA: MIT Press. \\nKong, A (1991) Efficient methods for computing \\nlinkage likelihoods of recessive diseases in inbred \\npedigrees. Genetic Epidemiology 8, 81-103. \\nKononenko, 1., Bratko, I. & Roskar, E. (1984) \\nExperiments in the automatic learning of medical \\ndiagnosis rules. Technical Report, Josef Stefan \\nInstitute, Ljubljana. \\nKoontz, W. L. G., Narendra , P. M. & Fukunaga , K. \\n(1975) A branch and bound clustering algorithm. \\nIEEE Transactions on Computers 24, 908-915. \\nKramer, A H. & Sangiovanni-Vincentelli, A (1989) \\nEfficient parallel learning algorithms for neural \\nnetworks. In NIPSI, pp. 40-48. \\nKramer, M. A .. (1991) Nonlinear principal com\\xad\\nponent analysis using autoassociative neural net\\xad\\nworks. AICHE Journal 37, 233-243. \\nKrishnaiah, P. R. & Kana!, L. N. (eds) (1982) Hand\\xad\\nbook of Statistics 2: Classification, Pattern Recogni\\xad\\ntion and Reduction of Dimensionality. Amsterdam: \\nNorth Holland. \\nKruskal, J. B. (1964a) Multidimensional scaling by \\noptimizing goodness-of-fit to a nonmetric hypoth\\xad\\nesis. Psychometrika 29, 1-29. \\nKruskal, J. B. (1964b) Non-metric multidimensional \\nscaling: a numerical method. Psychometrika 29, \\n115-129. Kruskal, J. B. (1969) Toward a practical method \\nwhich helps uncover the structure of a set of multi\\xad\\nvariate observations by finding the linear transfor\\xad\\nmation which optimizes a new 'index of condensa\\xad\\ntion'. In Statistical Computation, eds R. C. Milton \\n& J. A Neider, pp. 427-440. New York: Academic \\nPress. \\nKruskal, J. B. (1971) Monotone regression: continuity \\nand differentiability properties . Psychometrika 36, \\n57-62. \\nKruskal, J. B. ( 1972) Linear transformation of multi\\xad\\nvariate data to reveal clustering. In Multidimen\\xad\\nsional Scaling: Theory and Application in the Be\\xad\\nhavioural Sciences, eds R. N. Shephard , A K. Rom\\xad\\nney & S. K. Nerlove, pp. 179-191. New York: \\nSeminar Press. \\nKrzanowski, W. J. (1975) Discrimination and classifi\\xad\\ncation using both binary and continuous variables. \\nJournal of the American Statistical Association 70, \\n782-790. \\nKung, S. Y. & Diamantaras, K. I. (1990) A neural \\nnetwork learning algorithm for Adaptive Principal \\ncomponent EXtraction (APEX). In Proceedings \\nof the IEEE International Conference on Acoustics, \\nSpeech and Signal Processing (Albuquerque, NM, \\n1990) 2, pp. 861-864. Long Beach, CA: IEEE Press. \\nKurkova, V. (1991) Kolmogorov's theorem is relevant. \\nNeural Computation 3, 617-622. \\nKurkova, V. (1992) Kolmogorov's theorem and multi\\xad\\nlayer neural networks. Neural Networks 5, 501-506. \\nKurzynski , M. W. (1983a) Decision rules for a \\nhierarchical classifier. Pattern Recognition Letters \\n1, 305--310. \\nKurzynski, M. W. (1983b) The optimal strategy of \\na tree classifier. Pattern Recognition 16, 81-87. \\n(Correction page 361). \\nKushner, H. (1987) Asymptotic global behavior \\nfor stochastic approximation and diffusions with \\nslowly decreasing noise effects: global minimiza\\xad\\ntion via Monte Carlo. SIAM Journal on Applied \\nMathematics 47, 169-185. \\nKwok, S. W. & Carter, C. (1990) Multiple decision \\ntrees. In Uncertainty in Artificial Intelligence 4, eds \\nR. D. Shachter, T. S. Levitt, L. N. Kana! & \\nJ. F. Lemmer, pp. 327-335. Amsterdam: North \\nHolland. \\nLachenbruch, P. A (1975) Discriminant Analysis. New \\nYork: Hafner Press. \\nLachenbruch, P. A & Mickey, M. R. (1968) Es\\xad\\ntimation of error rates in discriminant analysis. \\nTechnometrics 10, 1-11. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 386}, page_content=\"References 375 \\nLange, K. L., Little, R. J. A. & Taylor, J. M. G. (1989) \\nRobust statistical modeling using the t distribution. \\nJournal of the American Statistical Association 84, \\n881-896. \\nLangley, P. (1996) Elements ofMachine Learning. San \\nFrancisco: Morgan Kaufmann. \\nLangley, P. & Simon, H. A. (1995) Applications of \\nmachine learning and rule induction. Communica\\xad\\ntions of the Association for Computing Machinery \\n38, 54--64. \\nLau, C. (ed.) (1992) Neural Networks: Theoretical \\nFoundations and Analysis. New York: IEEE Press. \\nLauritzen, S. (1989) Mixed graphical association \\nmodels (with discussion). Scandinavian Journal of \\nStatistics 16, 273-306. \\nLauritzen, S. (1992) Propagation of probabilities, \\nmeans and variances in mixed graphical associ\\xad\\nation models. Journal of the American Statistical \\nAssociation 87, 1089-1108. \\nLauritzen, S. L. (1996) Graphical Models. Oxford: \\nClarendon Press. \\nLauritzen, S. & Spiegelhalter, D. J. (1988) Local \\ncomputations with probabilities on graphical struc\\xad\\ntures and their application to expert systems (with \\ndiscussion). Journal of the Royal Statistical Society \\nseries B 50, 157-224. Reprinted in Shafer & Pearl \\n(1990). \\nLauritzen, S. L., Dawid, A. P., Larsen, B. N. & Leimer, \\nH.-G. (1990) Independence properties of directed \\nMarkov fields. Networks 20, 491-505. \\nLauritzen, S. L., Thiesson, B. & Spiegelhalter, D. J. \\n(1994) Diagnostic systems created by model selec\\xad\\ntion methods-a case study. In Selecting Models \\nfrom Data: AI and Statistics IV, eds P. Cheeseman \\n& R. W. Oldford, pp. 143-152. Lecture Notes in \\nStatistics 89. New York: Springer. \\nLazarsfeld, P. F. (1961) The algebra of dichotomous \\nsystems. In Studies in Item Analysis and Prediction, \\ned. H. Solomon, pp. 111-157. Palo Alto, CA: \\nStanford University Press. \\nLeBlanc, M. & Tibshirani, R. J. (1993) Combining \\nestimates in regression and classification. Preprint, \\nDepts of Preventive Medicine and Biostatistics and \\nof Statistics, University of Toronto. \\nLe Cun, Y., Boser, B., Denker, J. S., Henderson, D., \\nHoward, R. E., Hubbard, W. & Jackel, L. D. (1989) \\nBackpropagation applied to handwritten Zip code \\nrecognition. Neural Computation 1, 541-551. Le Cun, Y., Boser, B., Denker, J. S., Henderson, \\nD., Howard, R. E., Hubbard, W. & Jackel, L. D. \\n(1990a) Handwritten digit recognition with a back\\xad\\npropagation network. In NIPS2, pp. 396-404. \\nLe Cun, Y., Denker, J. S. & Solla, S. A. (1990b) \\nOptimal brain damage. In NIPS2, pp. 598-605. \\nLee, S. & Kil, R. M. (1988) Multi-layer feedforward \\npotential function network. In Proceedings of the \\nIEEE International Conference on Neural Networks, \\nSan Diego, I, pp. 161-171. Long Beach, CA: IEEE \\nPress. \\nLee, T.-C., Peterson, A. M. & Tsai, J. C. (1990) \\nA multi-layer feed-forward neural network with \\ndynamically adjustable structures. In Proceedings \\nof the IEEE International Conference on Systems, \\nMan and Cybernetics, Los Angeles, pp. 367-369. \\nLong Beach, CA: IEEE Press. \\nLee, Y. (1991) Handwritten digit recognition using K \\nnearest-neighbor, radial-basis function, and back\\xad\\npropagation neural networks. Neural Computation \\n3, 440-449. \\nde Leeuw, J. (1984) Differentiability of Kruskal's \\nstress at a local minimum. Psychometrika 49, 111-\\n113. \\nLehmann, E. L. (1983) Theory of Point Estimation. \\nNew York: Wiley. \\nLehmann, E. L. (1986) Testing Statistical Hypotheses. \\nSecond edition. Pacific Grove, CA: Wadsworth & \\nBrooks/Cole. (Formerly New York: Wiley.) \\nLeonard, J. A., Kramer, M. A. & Ungar, J. H. \\n(1992) Using radial basis functions to approximate \\na function and its error bounds. IEEE Transactions \\non Neural Networks 3, 624--627. \\nLesaffre, E. & Albert, A. (1989) Partial separation \\nin logistic discrimination. Journal of the Royal \\nStatistical Society series B 51, 109-116. \\nLevin, A. U., Leen, T. K. & Moody, J. E. (1994) Fast \\npruning using principal components. In NIPS6, \\npp. 35-42. \\nLevitt, T. S., Binford, T. 0. & Ettinger, G. L. (1990) \\nUtility-based control for computer vision. In Uncer\\xad\\ntainty in Artificial Intelligence 4, eds R. D. Shachter, \\nT. S. Levitt, L. N. Kana! & J. F. Lemmer, pp. 407-\\n422. North Holland, Amsterdam. \\nLi, X. B. & Dubes, R. C. (1986) Tree classifier design \\nwith a permutation statistic. Pattern Recognition \\n19, 229-235. \\nLincoln, W. P. & Skrzypek, J. (1990) Synergy of \\nclustering multiple backpropagation networks. In \\nNIPS2, pp. 650-657. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 387}, page_content='376 References \\nLindley, D. V. (1980) Approximate Bayesian meth\\xad\\nods. In Bayesian Statistics , eds J. M. Bernardo , \\nM. H. DeGroot, D. V. Lindley & A. F. M. Smith, \\npp. 223-237. Valencia: Valencia University Press. \\nLittle, R. J. A. & Rubin, D. B. (1987) Statistical \\nAnalysis with Missing Data. New York: Wiley. \\nLiu, D. C. & Nocedal, J. (1989) On the limited mem\\xad\\nory BFGS method for large-scale optimization. \\nMathematical Programming 45, 503-528. \\nLiu, L., Wilkins, D. C., Ying, X. & Bain, Z. (1991) \\nMinimum error tree decomposition . In Proceedings \\nof the Conference on Uncertainty in AI (Cambridge , \\nMA), pp. 180-185. \\nLiu, Y. (1993) Neural network model selection \\nusing asymptotic jackknife estimator and cross\\xad\\nvalidation method. In NIPS5, pp. 599-606. \\nLiu, Y. (1994) Robust parameter estimation and \\nmodel selection for neural network regression. In \\nN IPS6, pp. 192-199. \\nLiu, Y. (1995) Unbiased estimate of generalization \\nerror and model selection in neural network. Neural \\nNetworks 8, 215-219. \\nLloyd, S. P. (1957, 1982) Least squares quantization \\nin PCM. Technical Note, Bell Laboratories. Pub\\xad\\nlished in 1982 in IEEE Transactions on Iriformation \\nTheory 28, 128-137. \\nLoizou, G. & Maybank, S. J. (1987) The nearest \\nneighbor and the Bayes error rates. IEEE Transac\\xad\\ntions on Pattern Analysis and Machine Intelligence \\n9, 254-262. \\nLouis, T. A. (1982) Finding the observed information \\nmatrix when using the EM algorithm . Journal of \\nthe Royal Statistical Society series B 44, 226-233. \\nLunts, A. L. & Brailovsky , V. L. ( 1967) Evaluation \\nof attributes obtained in statistical decision rules. \\nEngineering Cybernetics 3, 98-109. \\nLuttrell, S. P. (1989) Hierarchical vector quantization. \\nlEE Proceedings I 136, 405-413. \\nMaass, W. G. (1994a) Neural networks with superlin\\xad\\near VC dimension. Neural Computation 6, 877-884. \\nMaass, W. G. (1994b) Perspectives of current re\\xad\\nsearch about the complexity of learning on neural \\nnets. Chapter 5 of Theoretical Advances in Neural \\nComputation and Learning, eds V. Roychowdhury, \\nK.-Y. Siu & A. Orlitsky, pp. 153-172. Boston: \\nKluwer Academic Publishers. \\nMaass, W. & Turan, G. (1994) How fast can a \\nthreshold gate learn? In Computational Learning \\nTheory and Natural Learning Systems: Constraints \\nand Prospects, eds S. J. Hanson, G. A. Drastal & \\nR. L. Rivest, volume I, pp. 381-414. MIT Press. Macintyre , A. & Sontag, E. D. (1993) Finiteness \\nresults for sigmoidal \"neural\" networks . Proceed\\xad\\nings of the 25th Annual ACM Symposium Theory \\nof Computing, San Diego, 1993, pp. 325-334. New \\nYork: ACM Press. \\nMacKay, D. J. C. (1992a) Bayesian interpolation. \\nNeural Computation 4, 415-447. \\nMacKay, D. J. C. (1992b) A practical Bayesian frame\\xad\\nwork for backprop networks. Neural Computation \\n4, 448-472. \\nMacKay, D. J. C. (1992c) Information-based objective \\nfunctions for active data selection. Neural Compu\\xad\\ntation 4, 590-604. \\nMacKay , D. J. C. (1992d) The evidence framework \\napplied to classification networks. Neural Compu\\xad\\ntation 4, 720-736. \\nMacKay, D. J. C. (1992e) Bayesian model comparison \\nand backprop nets. In NIPS4, pp. 839-846. \\nMacKay, D. M. & McCulloch, W. S. (1952) The \\nlimiting information capacity of a neuronal link. \\nBulletin of Mathematical Biophysics 14, 127-135. \\nMacLeod, J. E. S., Luk, A. & Titterington, D. M. \\n(1987) A re-examination of the distance-weighted \\nk-nearest neighbor classification rule. IEEE Trans\\xad\\nactions on Systems, Man and Cybernetic s 17, 689-\\n696. Reprinted in Dasarathy (1991). \\nMacnaughton-Smith, P., Williams, W. T., Dale, M. B. \\n& Mockett, L. G. (1964) Dissimilarity analysis: a \\nnew technique of hierarchical sub-division. Nature \\n202, 1034-1035. \\nMacQueen , J. ( 1967) Some methods for classification \\nand analysis of multivariate observations . In Pro\\xad\\nceedings of the Fifth Berkeley Symposium on Math\\xad\\nematical Statistics and Probability , eds L. M. Le \\nCam & J. Neyman, 1, pp. 281-297. Berkeley, CA: \\nUniversity of California Press. \\nMadigan, D. & Raftery, A. E. (1994) Model selection \\nand accounting for model uncertainty in graphical \\nmodels using Occam\\'s window. Journal of the \\nAmerican Statistical Association 89, 1535-1546. \\nMadigan, D. & York, J. (1995) Bayesian graphical \\nmodels for discrete data. International Statistical \\nReview 63, 215-232. \\nMadych, W. R. & Nelson, S. A. (1990) Multivariate \\ninterpolation and conditionally positive definite \\nfunctions II. Mathematics of Computation 54, 211-\\n230. \\nMahalanobis , P. C. (1936) On generalized distance \\nin statistics. Proceedings of the National lnst. Sci. \\n(India) 12, 49-55. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 388}, page_content='References 377 \\nMaier, D. (1983) The Theory of Relational Databases. \\nRockville, Md: Computer Science Press. \\nMakram-Ebeid, S., Sirat, J.-A. & Viala, J.-R. (1989) A \\nrationalized back-propagation learning algorithm. \\nIn International Joint Conference on Neural Net\\xad\\nworks (Washington, 1989) II, 373-380. New York: \\nIEEE Press. \\nMammone, R. J. (ed.) (1993) Artificial Neural Net\\xad\\nworks for Speech and Vision. London: Chapman & \\nHall. \\nMangarasian, 0. L. (1968) Multisurface methods of \\npattern separation. IEEE Transactions on Informa\\xad\\ntion Theory 14, 801-807. \\nMangarasian, 0. L., Setiono, R. & Wolberg, W. H. \\n(1990) Pattern-recognition via linear-programming: \\ntheory and application to medical diagnosis. \\nIn Large-Scale Numerical Optimization, 1990, eds \\nT. F. Coleman & Y. Li, pp. 22-31. Philadelphia: \\nSIAM. \\nManly, B. F. J. & Rayner, J. C. W. (1987) The \\ncomparison of sample covariance matrices using \\nlikelihood ratio tests. Biometrika 74, 841-847. \\nMansfield, A. J. (1991) Comparison of perceptron \\ntraining by linear-programming and by the per\\xad\\nceptron convergence procedure. Proceedings of the \\nInternational Joint Conference on Neural Networks \\n(Seattle 1991) II, 25-30. Long Beach, CA: IEEE \\nPress. \\nMardia, K. V., Kent, J. T. & Bibby, J. M. (1979) \\nMultivariate Analysis. London: Academic Press. \\nMaritz, J. S. & Lwin, T. (1989) Empirical Bayes \\nMethods. Second edition. London: Chapman & \\nHall. \\nMarks, S. & Dunn, 0. J. (1974) Discriminant func\\xad\\ntions when covariance matrices are unequal. Jour\\xad\\nnal of the American Statistical Association 69, 555-\\n559. \\nMaronna, R. A. (1976) Robust M-estimators of multi\\xad\\nvariate location and scatter. Annals of Statistics 4, \\n51-67. \\nMarriott, F. H. C. (1975) Separating mixtures of \\nnormal distributions. Biometrics 31, 767-769. \\nMartin, G. L. & Pitman, J. A. (1990) Recognizing \\nhand-printed letters and digits. In NIPS2, pp. 405-\\n414. \\nMartin, G. L. & Pitman, J. A. (1991) Recognizing \\nhand-printed letters and digits using backpropaga\\xad\\ntion learning. Neural Computation 3, 258-267. \\nMassart, D. L., Plastria, F. & Kaufman, L. (1983) \\nNon-hierarchical clustering with MASLOC. Pat\\xad\\ntern Recognition 16, 507-516. Mathieson, M. J. (1996) Ordinal models for neu\\xad\\nral networks. In Neural Networks in Financial En\\xad\\ngineering. eds A.-P. Refenes, Y. Abu-Mostafa & \\nJ. Moody. Singapore: World Scientific, 523-536. \\nMatus, F. (1992) On equivalence of Markov prop\\xad\\nerties over undirected graphs. Journal of Applied \\nProbability 29, 745-749. \\nMax, J. (1960) Quantizing for minimum distortion. \\nIRE Transactions on Information Theory 6, 7-12. \\nMcCullagh, P. & Neider, J. A. (1989) Generalized \\nLinear Models. Second edition. London: Chapman \\n& Hall. \\nMcCulloch, W. S. & Pitts, W. (1943) A logical calculus \\nof ideas immanent in nervous activity. Bulletin of \\nMathematical Biophysics 5, 115-133. Reprinted in \\nAnderson & Rosenfeld (1988). \\nMcKay, R. J. & Campbell, N. A. (1982a) Variable \\nselection techniques in discriminant analysis. I: \\nDescription. British Journal of Mathematical and \\nStatistical Psychology 35, 1-29. \\nMcKay, R. J. & Campbell, N. A. (1982b) Variable \\nselection techniques in discriminant analysis. II: \\nAllocation. British Journal of Mathematical and \\nStatistical Psychology 35, 30-41. \\nMcLachlan, G. J. (1992) Discriminant Analysis and \\nStatistical Pattern Recognition. New York: Wiley. \\nMcLachlan, G. J. & Basford, K. E. (1988) Mixture \\nModels: Inference and Applications to Clustering. \\nNew York: Marcel Dekker. \\nMeinguet, J. (1979) Multivariate interpolation at \\narbitrary points made simple. Journal of Applied \\nMathematics and Physics (ZAMP) 30, 292-304. \\nMeisel, W. S. (1972) Computer-Oriented Approaches to \\nPattern Recognition. New York: Academic Press. \\nMetropolis, N., Rosenbluth, A., Rosenbluth, M., \\nTeller, A. & Teller, E. (1953) Equations of state \\ncalculations by fast computing machines. Journal \\nof Chemical Physics 21, 1087-1091. \\nMhaskar, H. N. & Micchelli, C. A. (1992) Approxi\\xad\\nmation by superposition of sigmoidal function and \\nradial basis functions. Advances in Applied Mathe\\xad\\nmatics 13, 350-373. \\nMichalski, R. S. (1980) Pattern recognition as rule\\xad\\nguided inductive inference. IEEE Transactions on \\nPattern Analysis and Machine Intelligence 2, 349-\\n361. \\nMichie, D. (1989) Problems of computer-aided con\\xad\\ncept formation. In Applications of Expert Systems \\nvolume 2, ed. J. R. Quinlan, pp. 310-333. Glasgow: \\nTuring Institute Press/ Addison- Wesley. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 389}, page_content='378 References \\nMichie, D., Spiegelhalter, D. J. & Taylor, C. C. (eds) \\n(1994) Machine Learning, Neural and Statistical \\nClassification. New York: Ellis Horwood. \\nMingers, J. (1987) Expert systems-rule induction \\nwith statistical data. Journal of the Operational \\nResearch Society 38, 39-47. \\nMinnick, R. C. (1961) Linear-input logic. IRE Trans\\xad\\nactions on Electronic Computers 10, 6-16. \\nMinsky, M. (1961) Steps towards artificial intelli\\xad\\ngence. Proceedings of the IRE 49, 8-30. \\nMinsky, M. L. & Papert, S. A. (1988) Perceptrons. An \\nIntroduction to Computational Geometry . Expanded \\nedition. Cambridge, MA: The MIT Press. \\nMoller, M (1993) A scaled conjugate gradient algo\\xad\\nrithm for fast supervised learning. Neural Networks \\n6, 525-533. \\nMoody, J. E. (1989) Fast learning in multi-resolution \\nhierarchies. In NIPSJ, pp. 29-39. \\nMoody, J. E. (1991) Note on generalization, regu\\xad\\nlarization and architecture selection in nonlinear \\nlearning systems. In First IEEE-SP Workshop on \\nNeural Networks in Signal Processing, pp. 1-10. Los \\nAlamitos , CA: IEEE Computer Society Press. \\nMoody, J. E. (1992) The effective number of param\\xad\\neters: an analysis of generalization and regular\\xad\\nization in nonlinear learning systems. In N I PS4, \\npp. 847-854. \\nMoody, J. & Darken, C. J. (1989) Fast learning in \\nnetworks of locally-tuned processing units. Neural \\nComputation 1, 281-294. \\nMoody, J. & Utans, J. (1992) Principled architecture \\nselection for neural networks: application to corpo\\xad\\nrate bond rating prediction. In NIPS4, pp. 683-690. \\nMoody, J. & Utans, J. (1995) Architecture selection \\nstrategies for neural networks: application to cor\\xad\\nporate bond rating prediction. In Neural Networks \\nin the Capital Markets, ed. A.-P. Refenes, pp. 277-\\n300. Chichester: Wiley. \\nMoore, B. (1989) ART 1 and pattern clustering. \\nIn Proceedings of the 1988 Connectionist Models \\nSummer School eds D. Touretzky , G. Hinton & \\nT. Sejnowski, pp. 174-185. San Mateo, CA: Morgan \\nKaufmann. \\nMoran, M. A. & Murphy, B. J. (1979) A closer \\nlook at two alternative methods of statistical \\ndiscrimination. Applied Statistics 28, 223-232. \\nMorgan, J. N. & Messenger, R. C. (1973) THAID: a \\nSequential Search Program for the Analysis of Nom\\xad\\ninal Scale Dependent Variables. Survey Research \\nCenter, Institute for Social Research, University \\nof Michigan. Morgan, J. N. & Sonquist, J. A. (1963) Problems in \\nthe analysis of survey data, and a proposal. Journal \\nof the American Statistical Association 58,415-434. \\nMorin, R. L. & Raeside, D. E. (1981) A reappraisal of \\ndistance-weighted k-nearest neighbor classification \\nfor pattern recognition with missing data. IEEE \\nTransactions on Systems, Man and Cybernetics 11, \\n241-243. \\nMosteller, F. & Wallace, D. L. (1963) Inference in \\nan authorship problem. Journal of the American \\nStatistical Association 58, 275-309. \\nMoulton, B. R. (1991) A Bayesian-approach to regres\\xad\\nsion selection and estimation with application to a \\nprice-index for radio services. Journal of Economet\\xad\\nrics 49, 169-193. \\nMoussouris , J. (1974) Gibbs and Markov random sys\\xad\\ntems with constraints. Journal of Statistical Physics \\n10, 11-33. \\nMurata, N., Yoshizawa , S. & Amari, S. (1991) A \\ncriterion for determining the number of parameters \\nin an artificial neural network model. In Artificial \\nNeural Networks. Proceedings of ICANN-91, eds \\nT. Kohonen , K. Miikisara, 0. Simula & J. Kangas, \\nvolume I, pp. 9-14. Amsterdam: North Holland. \\nMurata, N., Yoshizawa , S. & Amari, S. (1993) \\nLearning curves, model selection and complexity \\nof neural networks . In NIPS5, pp. 607-614. \\nMurata, N., Yoshizawa , S. & Amari, S. (1994) \\nNetwork information criterion-determining the \\nnumber of hidden units for artificial neural network \\nmodels. IEEE Transactions on Neural Networks 5, \\n865-872. \\nMuroga, S. (1965) Lower bounds of the number of \\nthreshold functions and a maximum weight. IEEE \\nTransactions on Electronic Computers 14, 136-148. \\nMuroga, S. (1971) Threshold Logic and its Applica\\xad\\ntions. New York: Wiley. \\nMuroga, S., Toda, I. & Takasu, S. (1961) Theory of \\nmajority decision elements. Journal of the Franklin \\nInstitute 271, 376--418. \\nMurphy, P. M. & Aha, D. W. (1995) UCJ Repository \\nof Machine Learning Databases [Machine-readable \\ndata repository] . Irvine, CA: University of Califor\\xad\\nnia, Dept of Information and Computer Science. \\nAvailable by anonymous ftp from ics. uci. edu in \\ndirectory pub/machine-learning-databases. \\nMurtagh, F. (1985) A survey of algorithms for \\ncontiguity-constrained clustering and related prob\\xad\\nlems. Computer Journal 28, 82-88. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 390}, page_content=\"References 379 \\nMurtagh, F. (1995a) Contiguity-constrained hierar\\xad\\nchical clustering. In Partitioning Data Sets, eds \\nI. J. Cox, P. Hansen & B. Julesz. DIMACS. pp. 143-\\n152. Providence , RI: American Mathematical So\\xad\\nciety. \\nMurtagh, F. (1995b) Interpreting the Kohonen \\nself-organizing feature map using contiguity\\xad\\nconstrained clustering. Pattern Recognition Letters \\n16, 399-408. \\nMurthy, V. K. (1966) Nonparametric estimation of \\nmultivariate densities with applications. In Multi\\xad\\nvariate Analysis, ed. P. R. Krishnaiah, pp. 43-56. \\nNew York: Academic Press. \\nMusavi, M. T., Ahmed, W., Chan, K. H., Faris, K. B. \\n& Hummels , D. M. (1992) On the training of \\nradial basis function classifiers. Neural Networks \\n5, 595-603. \\nMyles, J. P. & Hand, D. J. (1990) The multiclass \\nmetric problem in nearest neighbour discrimination \\nrules. Pattern Recognition 23, 1291-1297. \\nNarendra, P. M. & Fukunaga , K. (1977) A branch \\nand bound algorithm for feature subset selection. \\nIEEE Transactions on Computers 26, 917-922. \\nNash, J. C. (1990) Compact Numerical Methods for \\nComputer s. Linear Algebra and Function Minimiza\\xad\\ntion. Second edition. Bristol: Adam Hilger. \\nNeal, R. (1992a) Connectionist learning of belief \\nnetworks. Artificial Intelligence 56, 7l-ll3. \\nNeal, R. M. (1992b) Asymmetric parallel Boltzmann \\nmachines are belief networks . Neural Computation \\n4, 832-834. \\nNeal, R. (1993) Bayesian learning via stochastic \\ndynamics . In NIPS5, pp. 475-482. \\nNeal, R. M. (1996) Bayesian Learning for Neural \\nNetworks . Lecture Notes in Statistics 118. New \\nYork: Springer. \\nNeapolitan, E. (1990) Probabili stic Reasoning in Ex\\xad\\npert Systems. Theory and Algorithms. New York: \\nWiley. \\nNiblett, T. (1987) Constructing decision trees in noisy \\ndomains. In Progress in Machine Learning , eds I. \\nBratko & N. Lavrac, pp. 67-78. Wilmslow: Sigma \\nPress. \\nNiblett, T. & Bratko, I. (1987) Learning decision rules \\nin noisy domains. In Research and Development in \\nExpert Systems Ill. Proceedings of Expert Systems \\n'86, Brighton 1986, ed. M. A. Bramer, pp. 25-34. \\nCambridge: Cambridge University Press. Niemann, H. & Goppert, G. (1988) An efficient \\nbranch-and-bound nearest neighbour classifier. \\nPattern Recognitio n Letters 7, 67-72. Reprinted in \\nDasarathy (1991). \\nNowlan, S. J. & Hinton, G. E. (1992a) Adaptive soft \\nweight tying using Gaussian mixtures. In NIPS4, \\npp. 993-1000. \\nNowlan, S. J. & Hinton, G. E. (1992b) Simplifying \\nneural networks by soft weight-sharing. Neural \\nComputation 4, 473-493. Reprinted with an intro\\xad\\nduction as pp. 369-394 of Wolpert (1995). \\nOja, E. (1982) A simplified neuron model as a prin\\xad\\ncipal component analyzer. Journal of Mathematical \\nBiology 16, 267-273. \\nOja, E. (1989) Neural networks, principal components \\nand subspaces. International Journal of Neural Sys\\xad\\ntems 1, 61-68. \\nOja, E. (1992) Principal components, minor compo\\xad\\nnents and linear neural networks . Neural Networks \\n5, 927-935. \\nOja, E. & Karhunen, J. (1985) On stochastic\\xad\\napproximation of the eigenvectors and eigenvalues \\nof the expectation of a random matrix. Journal \\nof Mathematical Analysis and its Applications 106, \\n69-84. \\nOlesen, K. G. (1993) Causal probabilistic networks \\nwith both discrete and continuous variables. IEEE \\nTransactions on Pattern Analysis and Machine In\\xad\\ntelligence 15, 275-279. \\nOliver, L. H., Poulsen, R. S., Toussaint, G. T. & \\nLouis, C. (1979) Classification of atypical cells in \\nthe automated cytoscreening for cervical cancer. \\nPattern Recognition 11, 205-212. \\nOliver, R. M. & Smith, J. Q. (eds) (1990) Influence \\nDiagrams. Belief Nets and Decision Analysis. Chich\\xad\\nester: Wiley. \\nOlkin, I. & Tate, R. F. (1961) Multivariate correlation \\nmodels with mixed discrete and continuous vari\\xad\\nates. Annals of Mathematical Statistics 32, 445-465. \\nvan Ooyen, A. & Nienhuis, B. (1992) Improving the \\nconvergence of the back-propagation algorithm . \\nNeural Networks 5, 465-471. (See also letter to the \\neditor and response, 6, 6ll-612.) \\nOtt, J. (1989) Computer-simulation methods in hu\\xad\\nman linkage analysis. Proceedings of the National \\nAcademy of Sciences of the USA 86,4175-4178. \\nOwen, A. (1984) A neighbourhood-based LANDSAT \\nclassifier. Canadian Journal of Statistics 12, 191-\\n200. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 391}, page_content='380 References \\nOwens, A. J. & Filkin, D. L. (1989) Efficient train\\xad\\ning of the back propagation network by solving \\na system of stiff ordinary differential equations. \\nIn Proceedings of the International Conference on \\nNeural Networks (Washington, 1989), II, 381-386. \\nNew York: IEEE Press. \\nPagallo, G. (1989) Learning DNF by decision trees. In \\nProceedings of the Eleventh International Joint Con\\xad\\nference on Artificial Intelligen ce (Detroit, 1989 ), \\npp. 639-644. \\nPagallo, G. & Haussler, D. (1989) Two algorithms \\nthat learn DNF by discovering relevant features. In \\nProceedings of the Sixth International Workshop on \\nMachine Learning (Ithaca, 1989 ), ed. A. M. Segre, \\npp. 119-123. San Mateo, CA: Morgan Kaufmann. \\nPagallo, G. & Haussler, D. (1990) Boolean feature \\ndiscovery in empirical learning. Machine Learning \\n5, 71-99. \\nParberry, I. (1994) Circuit Complexity and Neural \\nNetworks. Cambridge, MA: MIT Press. \\nPark, J. & Sandberg , I. W. (1991) Universal ap\\xad\\nproximation using radial-basis-function networks. \\nNeural Computation 3, 246-257. \\nParrondo, J. M. R. & Van der Broeck, C. (1993) \\nVapnik-Chervonenkis bounds for generalization. J. \\nPhysics A 26, 2211-2223. \\nParthasarthy, G. & Chatterji, B. N. (1990) A class of \\nnew KNN methods for low sample problems. IEEE \\nTransactions on Systems, Man and Cybernetics 20, \\n715-718. \\nParzen, E. (1962) On the estimation of a probability \\ndensity function and mode. Annals of Mathematical \\nStatistics 33, 1065-1076. \\nPatrick, E. A. & Fisher, F. P. II (1969) Nonparametric \\nfeature selection. IEEE Transactions on Information \\nTheory 15, 577-584. \\nPatterson, A. & Niblett, T. (1983) ACLS User Manual. \\nGlasgow: Intelligent Terminals Ltd. \\nPavlidis, T. (1993) Recognition of printed text under \\nrealistic conditions. Pattern Recognition Letters 14, \\n317-326. \\nPayne, H. J. & Meisel, W. S. (1977) An algorithm for \\nconstructing optimal binary decision trees. IEEE \\nTransactions on Computers 26, 905-916. \\nPearl, J. (1979) Capacity and error estimates for \\nBoolean classifiers with limited capacity. IEEE \\nTransactions on Pattern Analysis and Machine In\\xad\\ntelligence 1, 350--356. Pearl, J. (1982) Reverend Bayes on inference engines: \\na distributed hierarchical approach . In Proceedings \\nof the AAAI National Conference on Artificial In\\xad\\ntelligence (Pittsburgh), pp. 133-136. Menlo Park, \\nCA:AAAI. \\nPearl, J. (1986) Fusion, propagation, and structuring \\nin belief networks. Artificial Intelligence 29, 241-\\n288. Reprinted in Shafer & Pearl (1990). \\nPearl, J. (1987) Evidential reasoning using stochastic \\nsimulation of causal models. Artificial Intelligence \\n32, 245-257. \\nPearl, J. (1988) Probabilistic Inference in Intelligent \\nSystems. Networks of Plausible Inference. San Ma\\xad\\nteo, CA: Morgan Kaufmann . \\nPearl, J. ( 1993a) Belief networks revisited. Artificial \\nIntelligence 59, 49-56. \\nPearl, J. (1993b) Graphical models, causality and \\nintervention. Contribution to the discussion of \\nSpiegelhalter et al. (1993), pp. 266-269. \\nPearl, J. (1995) From Bayesian networks to causal \\nnetworks . In Gammerman (1995), pp. 1-31. \\nPearlmutter , B. A. (1994) Fast exact multiplication by \\nthe Hessian. Neural Computation 6, 147-160. \\nPearlmutter , B. A. & Rosenfeld , R. (1991) Chaitin\\xad\\nKolmogorov complexity and generalization in neu\\xad\\nral networks. In NIPS3, pp. 925-931. \\nPeck, R., Fisher, L. & Van Ness J. (1989) Approximate \\nconfidence intervals for the number of clusters. \\nJournal of the American Statistical Association 84, \\n184-191. \\nPeng, F., Jacobs, R. A. & Tanner, M. A. (1994) \\nBayesian inference in mixtures-of-experts and hier\\xad\\narchical mixtures-of-experts architectures . Techni\\xad\\ncal report, Dept of Biostatistics , University of \\nRochester, NY. \\nPenrod, C. S. & Wagner, T. J. (1977) Another look at \\nthe edited nearest neighbor rule. IEEE Transactions \\non Systems, Man and Cybernetics 7, 92-94. \\nPeretto, P. (1992) An Introduction to the Modeling of \\nNeural Networks. Cambridge: Cambridge Univer\\xad\\nsity Press. \\nPerrone, M.P. & Cooper, L. N. (1993) When networks \\ndisagree: Ensemble methods for hybrid neural \\nnetworks. In Mammone (1993), pp. 126-142. \\nPeskun, P. H. ( 1973) Optimal Monte-Carlo sampling \\nusing Markov chains. Biometrika 60, 607-612. \\nPeterson, C. & Anderson, J. R. (1987) A mean field \\nlearning algorithm for neural networks. Complex \\nSystems 1, 995-1019. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 392}, page_content='References 381 \\nPitas, I. (ed.) (1993) Parallel Algorithms for Digital \\nImage Processing, Computer Vision and Neural Net\\xad\\nworks. Chichester: Wiley. \\nPloughman, L. M. & Boehnke, M. (1989) Estimating \\nthe power of a proposed linkage study for a \\ncomplex genetic trait. American Journal of Human \\nGenetics 44, 543-551. \\nPoggio, T. & Girosi, F. (1990a) Regularization algo\\xad\\nrithms for learning that are equivalent to multilayer \\nnetworks. Science 247, 978-982. \\nPoggio, T. & Girosi, F. (1990b) Networks for approx\\xad\\nimation and learning. Proceedings of the IEEE 78, \\n1481-1497. Reprinted in Lau (1992). \\nPollard, D. (1984) Convergence of Stochastic Pro\\xad\\ncesses. New York: Springer. \\nPollard, D. (1986) Rates of uniform almost-sure con\\xad\\nvergence for empirical processes indexed by un\\xad\\nbounded classes of functions. Unpublished paper, \\nDept of Statistics, Yale University. \\nPollard, D. (1990) Empirical Processes: Theory and \\nApplications. Hayward, CA: Institute of Mathemat\\xad\\nical Statistics and American Statistical Association. \\nPosse, C. (1990) An effective two-dimensional \\nprojection pursuit algorithm. Communications in \\nStatistics-Simulation and Computation 19, 1143-\\n1164. \\nPosse, C. (1995a) Tools for two-dimensional ex\\xad\\nploratory projection pursuit. Journal of Computa\\xad\\ntional and Graphical Statistics 4, 83-100. \\nPosse, C. (1995b) Projection pursuit exploratory data \\nanalysis. Computational Statistics and Data Analy\\xad\\nsis. \\nPowell, M. J. D. (1987) Radial basis functions for \\nmultivariable interpolation: a review. In Algorithms \\nfor Approximation, eds J. C. Mason & M. G. Cox, \\npp. 143-167. Oxford: Clarendon Press. \\nPowell, M. J. D. (1992) The theory of radial function \\napproximation in 1990. In Advances in Numerical \\nAnalysis volume II, ed. W. Light, pp. 105-210. \\nOxford: Clarendon Press. \\nPrechelt, L. ( 1994) A study of experimental evaluation \\nof current neural network learning algorithms: \\ncurrent research practice. Technical Report 19/94, \\nFakultat fiir Informatik, Universitat Kahlsruhe. \\nPrentice, R. & Pyke, R. (1979) Logistic disease inci\\xad\\ndence models and case-control studies. Biometrika \\n66, 403-411. \\nPreparata, F. P. & Shamos, M. I. (1985) Computational \\nGeometry. An Introduction. New York: Springer. Press, W. H., Flannery, B. P., Teukolsky, S. A. & \\nVetterling, W. T. (1992) Numerical Recipes in C. \\nSecond edition. Cambridge: Cambridge University \\nPress. \\nPreston, C. J. (1974) Gibbs States on Countable Sets. \\nLondon: Cambridge University Press. \\nPreston, C. J. (1976) Random Fields. Lecture Notes in \\nMathematics 534. Berlin: Springer. \\nPrzytula, K. W. & Prasanna, V. K. (1993) Parallel Dig\\xad\\nital Implementation of Neural Networks. Englewood \\nCliffs, NJ: Prentice Hall. \\nQuenouille, M. H. (1949) Approximate tests of corre\\xad\\nlation in time series. Journal of the Royal Statistical \\nSociety series B 11, 68-84. \\nQuinlan, J. R. (1979) Discovering rules by induction \\nfrom large collections of examples. In Expert \\nSystems in the Microelectronic Age, ed. D. Michie, \\npp. 168-201. Edinburgh: Edinburgh University \\nPress. \\nQuinlan, J. R. (1983) Learning efficient classification \\nprocedures and their application to chess end\\xad\\ngames. In Machine Learning, eds R. S. Michalski, \\nJ. G. Carbonell & T. M. Mitchell, pp. 463--482. Palo \\nAlto, CA: Tioga. \\nQuinlan, J. R. (1986) Induction of decision trees. \\nMachine Learning 1, 81-106. Reprinted in Shavlik \\n& Dietterich (1990). \\nQuinlan, J. R. (1987a) Simplifying decision trees. \\nInternational Journal of Man-Machine Studies 27, \\n221-234. \\nQuinlan, J. R. (1987b) Generating production rules \\nfrom decision trees. In Proceedings of the Tenth \\nInternational Joint Conference on Artificial Intelli\\xad\\ngence, Milan, pp. 304-307. \\nQuinlan, J. R. (1988) Decision trees and multi\\xad\\nvalued attributes. In Machine Intelligence 11, eds \\nJ. E. Hayes, D. Michie & J. Richards, pp. 305-318. \\nOxford: Clarendon Press. \\nQuinlan, J. R. (1990) Decision trees and decision \\nmaking. IEEE Transactions on Systems, Man and \\nCybernetics 20, 339-346. \\nQuinlan, J. R. (1993) C4.5: Programs for Machine \\nLearning. San Mateo, CA: Morgan Kaufmann. \\nRaftery, A. E. (1993) Approximate Bayes factors and \\naccounting for model uncertainty in generalized \\nlinear models. Technical report 255, Dept of Statis\\xad\\ntics, University of Washington. \\nRao, C. R. (1948) The utilization of multiple mea\\xad\\nsurements in problems of biological classification \\n(with discussion). Journal of the Royal Statistical \\nSociety series B 10, 159-203. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 393}, page_content='382 References \\nRao, C. R. ( 1960) Multivariate analysis: an indispens\\xad\\nable statistical aid in applied research. Sankhyii 22, \\n317~338. \\nRayens, W. & Greene, T. (1991) Covariance pooling \\nand stabilization for classification. Computational \\nStatistics and Data Analysis 11, 17-42. \\nRedner, R. A & Walker, H. F. (1984) Mixture den\\xad\\nsities, maximum likelihood and the EM algorithm. \\nSIAM Review 26, 195~239. \\nReed, R. (1993) Pruning algorithms-a survey. IEEE \\nTransactions on Neural Networks 4, 740--747. \\nReilly, D. L., Cooper, L. N. & Elbaum, C. (1982) \\nA neural model for category learning. Biological \\nCybernetics 45, 35~41. Reprinted in Anderson et \\nal. (1990). \\nRichards, L. E. (1972) Refinement and extension \\nof distribution-free discriminate analysis. Applied \\nStatistics 21, 174-176. \\nRiffenburgh, R. H. & Clunies-Ross, C. W. (1960) \\nLinear discriminant analysis. Pacific Science 14, \\n251~256. \\nRimey, R. & Brown, C. (1992) Task-oriented vision \\nwith multiple Bayes nets. In Active Vision, eds \\nA Blake & A Yuille, pp. 217~236. Cambridge, \\nMA: The MIT Press. \\nRipley, B. D. (1977) Modelling spatial patterns (with \\ndiscussion). Journal of the Royal Statistical Society \\nseries B 39, 172~212. \\nRipley, B. D. (1979) Algorithm AS137. Simulating \\nspatial patterns: dependent samples from a multi\\xad\\nvariate density. Applied Statistics 28, 109~112. \\nRipley, B. D. (1987) Stochastic Simulation. New York: \\nWiley. \\nRipley, B. D. (1988) Statistical Inference for Spatial \\nProcesses. Cambridge: Cambridge University Press. \\nRipley, B. D. (1993) Statistical aspects of neural \\nnetworks. In Networks and Chaos-Statistical and \\nProbabilistic Aspects, eds 0. E. Barndorff-Nielsen, \\nJ. L. Jensen & W. S. Kendall, pp. 40--123. London: \\nChapman & Hall. \\nRipley, B. D. (1994a) Neural networks and related \\nmethods for classification (with discussion). Journal \\nof the Royal Statistical Society series B 56, 409~456. \\nRipley, B. D. (1994b) Neural networks and flexible \\nregression and discrimination. In Statistics and \\nImages 2, ed. K. V. Mardia. Advances in Applied \\nStatistics 2, pp. 39~57. Abingdon: Carfax. \\nRipley, B. D. (1994c) Flexible non-linear approaches \\nto classification. In Cherkassky et a/. (1994), \\npp. 105~126. Ripley, B. D. (1995) Statistical ideas for selecting \\nnetwork architectures. In Neural Networks: Arti\\xad\\nficial Intelligence and Industrial Applications, eds \\nB. Kappen & S. Gielen. London: Springer. \\nRipley, B. D. & Kelly, F. P. (1977) Markov point pro\\xad\\ncesses. Journal of the London Mathematical Society \\n(2) 15, 188~192. \\nRipley, B. D. & Kirkland, M. D. (1990) Iterative \\nsimulation methods. Journal of Computational and \\nApplied Mathematics 31, 165~172. \\nRissanen, J. (1983) A universal prior for integers and \\nestimation by minimum description length. Annals \\nof Statistics 11, 416-431. \\nRissanen, J. (1987) Stochastic complexity (with dis\\xad\\ncussion). Journal of the Royal Statistical Society \\nseries B 49, 223~239. \\nRissanen, J. (1989) Stochastic Complexity in Statistical \\nInquiry. Singapore: World Scientific Publishing Co. \\nRitter, G. L., Woodruff, H. B., Lowry, S. R. & \\nIsenhour, T. L. (1975) An algorithm for a selective \\nnearest neighbor decision rule. IEEE Transactions \\non Information Theory 21, 665~669. Reprinted in \\nDasarathy (1991). \\nRitter, H., Martinetz, T. & Schulten, K. (1992) \\nNeural Computation and Selj:Organizing Maps. An \\nIntroduction. Reading, MA: Addison-Wesley. \\nRoberts, S. & Tarassenko, L. (1995) Automated sleep \\nEEG analysis using an RBF network. In Neural \\nNetwork Applications, ed. A F. Murray, pp. 305~ \\n322. Dordrecht: Kluwer Academic Publishers. \\nRobinson, R. W. (1977) Counting unlabeled acyclic \\ndigraphs. In Combinatorial Mathematics V, ed. \\nC. H. C. Little. Lecture Notes in Mathematics 622, \\npp. 28-43. Berlin: Springer. \\nRoeder, K. (1990) Density estimation with confi\\xad\\ndence sets exemplified by superclusters and voids \\nin galaxies. Journal of the American Statistical As\\xad\\nsociation 85, 617~624. \\nRoosen, C. B. & Hastie, T. J. (1994) Automatic \\nsmoothing spline projection pursuit. Journal of \\nComputational and Graphical Statistics 3, 235~248. \\nRose, D. J., Tarjan, R. E. & Lueker, G. S. (1976) Al\\xad\\ngorithmic aspects of vertex elimination on graphs. \\nSIAM Journal on Computing 5, 266~283. \\nRosenblatt, F. (1957) The perceptron-a perceiving \\nand recognizing automaton. Report 85-460-1, Cor\\xad\\nnell Aeronautical Laboratory. \\nRosenblatt, F. (1958) The perceptron: a probabilistic \\nmodel for information storage and organization \\nin the brain. Psychological Review 65, 386-408. \\nReprinted in Shavlik & Dietterich (1990). '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 394}, page_content='References 383 \\nRosenblatt , F. (1962) Principles of Neurodynamics. \\nWashington, DC: Spartan Books. \\nRosenblatt , M. (1956) Remarks on some non\\xad\\nparametric estimates of a density function. Annals \\nof Mathematical Statistics 27, 832-837. \\nRounds, E. M. (1980) A combined nonparametric \\napproach to feature selection and binary decision \\ntree design. Pattern Recognition 12, 313-317. \\nRousseeuw, P. J. & Leroy, A. M. (1987) Robust \\nRegression and Outlier Detection . New York: Wiley. \\nRousseeuw, P. J. & van Zomeren , B. C. (1990) Un\\xad\\nmasking multivariate outliers and leverage points \\n(with discussion). Journal of the American Statisti\\xad\\ncal Association 85, 633-651. \\nRuck, D. W., Rogers, S. K., Kabrisky , M., Maybeck, \\nP. S. & Oxley, M. E. (1992) Comparative analysis \\nof back propagation and the extended Kalman filter \\nfor training multilayer perceptrons . IEEE Transac\\xad\\ntions on Pattern Analysis and Machine Intelligence \\n14, 686--691. \\nRuiz, E. V. (1986) An algorithm for finding nearest \\nneighbours in (approximately) constant average \\ntime. Pattern Recognition Letters 4, 145-158. \\nRumelhart, D. E. & McClelland , J. L. (eds) (1986) \\nParallel Distributed Processing: Exploratio ns in the \\nMicrostructure of Cognition. Volume I. Foundations. \\nCambridge, MA: The MIT Press. \\nRumelhart, D. E., Hinton, G. E. & Williams, \\nR. J. (1986) Learning representations by back\\xad\\npropagating errors. Nature 323, 533-536. Reprinted \\nin Anderson & Rosenfeld (1988). \\nRussell, S. J. & Norvig, P. (1995) Artificial Intelli\\xad\\ngence. A Modem Approach. Englewood Cliffs, NJ: \\nPrentice-Hall. \\nRuzicka, P. (1993) On the convergence of learning \\nalgorithm for topological maps. Neural Network \\nWorld 4, 413-424. \\nde Sa, V. R. & Ballard, D. H. (1993) A note on \\nlearning vector quantization. In N JPS5, pp. 220-\\n227. \\nSaarinen, S., Bramley, R. & Cybenko , G. (1993) Ill\\xad\\nconditioning in neural network training problems. \\nSIAM Journal on Scientific Computing 14, 693-714. \\nSafavian, S. Rand Landgrebe, D. (1991) A survey of \\ndecision tree classifier methodology. IEEE Transac\\xad\\ntions on Systems, Man and Cybernetics 21, 660-674. \\nSakurai, A. (1993) Tighter bounds of the VC\\xad\\ndimension of three-layer networks. In Proceedings \\nof the 1993 World Congress on Neural Networks, \\nvolume 3, pp. 540-543. Hillsdale, NJ: Erlbaum. Salomon , R. (1991) Improved convergence rate of \\nback-propagation with dynamic adaption of the \\nlearning rate. ln Parallel Problem Solving From Na\\xad\\nture (Dortmund, 1990 ). Lecture Notes in Computer \\nScience 496, 269-273. \\nSarna!, A. & Iyengar, P. A. (1992) Automatic recog\\xad\\nnition and analysis of human faces and facial ex\\xad\\npressions : a survey. Pattern Recognition 25, 65-77. \\nSammon , J. W. Jr (1969) A non-linear mapping \\nfor data structure analysis. IEEE Transactions on \\nComputers 18, 401-409. \\nSanger, T. D. (1989) Optimal unsupervised learning \\nin a single-layer linear feedforward network. Neural \\nNetworks 2, 459-473. \\nSankar, A. & Mammone, R. J. (1993) Growing and \\npruning neural tree networks. IEEE Transactions \\non Computers 42, 291-299. \\nSanter, T. J. & Duffy, D. E. (1986) A note on A. Albert \\nand J. A. Anderson \\'s conditions for the existence of \\nmaximum likelihood estimates in logistic regression \\nmodels. Biometrika 73, 755-758. \\nSchalkoff , R. J. (1992) Pattern Recognition : Statisti\\xad\\ncal, Structural and Neural Approaches . New York: \\nWiley. \\nSchlimmer, J. C. & Fisher, D. H. (1986) A case study \\nof incremental concept induction. In Proceedings \\nof the Fifth National Conference on Artificial Intel\\xad\\nligence, Philadelphia, pp. 496-501. San Mateo, CA: \\nMorgan Kaufmann. \\nSchlimmer , J. C. & Granger, R. H. Jr (1986) Incre\\xad\\nmental learning from noisy data. Machine Learning \\n1, 317-354. \\nSchmidhuber , J. (1989) Accelerated learning in back\\xad\\npropagation nets. In Connectionism in Perspective, \\npp. 439-445. Amsterdam: Elsevier. \\nSchoenberg, I. J. (1935) Remarks to Maurice \\nFrechet\\'s article \"Sur Ia definition axiomatique \\nd\\'une classe d\\'espaces distancies vectoriellement \\napplicable sur l\\'espace de Hilbert\". Annals of Math\\xad\\nematics 36, 724--732. \\nSchuermann, J. & Doster, D. (1984) A decision\\xad\\ntheoretic approach in hierarchical classifier design. \\nPattern Recognition 17, 359-369. \\nSchwarz, G. (1978) Estimating the dimension of a \\nmodel. Annals of Statistics 6, 461-464. \\nSchwemer, G. T. & Dunn, 0. J. (1980) Posterior \\nprobability estimators in classification simulations. \\nCommuni cations in Statistics-Simulation and Com\\xad\\nputation B9, 133-140. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 395}, page_content=\"384 References \\nScott, A. J. & Symons, M. J. (1971) Clustering meth\\xad\\nods based on likelihood ratio criteria. Biometrics \\n27, 387-397. \\nScott, A. J. & Wild, C. J. (1986) Fitting logistic mod\\xad\\nels under case-control or choice based sampling. \\nJournal of the Royal Statistical Society series B 48, \\n170-182. \\nScott, D. W. (1992) Multivariate Density Estimation. \\nTheory, Practice and Visualization. New York: Wi\\xad\\nley. \\nSeber, G. A. F. & Wild, C. J. (1989) Nonlinear \\nRegression. New York: Wiley. \\nSebestyen, G. S. (1962) Pattern recognition by an \\nadaptive process of sample set construction. IEEE \\nTransactions on Information Theory 8, S 82-S 91. \\nSedgewick, R. (1990) Algorithms in C. Reading, MA: \\nAddison-Wesley. \\nSen, A. & Srivastava, M. (1990) Regression Analy\\xad\\nsis. Theory, Methods and Applications. New York: \\nSpringer. \\nSethi, I. K. (1990) Entropy nets: from decision trees \\nto neural networks. Proceedings of the IEEE 78, \\n1605-1613. Reprinted in Lau (1992). \\nSethi, I. K. (1991) Decision tree performance enhance\\xad\\nment using an artificial neural network implemen\\xad\\ntation. In Sethi & Jain (1991), pp. 71-88. \\nSethi, I. K. & Jain, A. K. (eds) (1991) Artificial Neural \\nNetworks and Statistical Pattern Recognition. Old \\nand New Connections. Amsterdam: North Holland. \\nSethi, I. K. & Sarvarayudu, G. P. R. (1982) Hier\\xad\\narchical classifier design using mutual informa\\xad\\ntion. IEEE Transactions on Pattern Analysis and \\nMachine Intelligence 4, 441-445. \\nShachter, R. D. & Peot, M.A. (1990) Simulation ap\\xad\\nproaches to general probabilistic inference on belief \\nnetworks. In Uncertainty in Artificial Intelligence 5, \\neds M. Henrion, R. D. Shachter, L. N. Kana! & \\nJ. F. Lemmer, pp. 221-231. Amsterdam: North\\xad\\nHolland. \\nShafer, G. (1996) Probabilistic Expert Systems. Num\\xad\\nber 67 in CBMS-NSF Regional Conference Series \\nin Applied Mathematics. Philadelphia, PA: SIAM. \\nShafer, G. & Pearl, J. (eds) (1990) Readings in \\nUncertainty Reasoning. San Mateo, CA: Morgan \\nKaufmann. \\nShafer, G. & Shenoy, P. P. (1986) Propagating belief \\nfunctions with local computations. IEEE Expert \\n1(3), 43-52. Shanno, D. F. (1990) Recent advances in numerical \\ntechniques for large-scale optimization. In Neural \\nNetworks for Control, eds W. T. Miller III, R. S. \\nSutton & P. J. Werbos, pp. 171-178. Cambridge, \\nMA: The MIT Press. \\nShanno, D. F. & Phua, K. H. (1980) Remark on algo\\xad\\nrithm 500: Minimization of unconstrained multi\\xad\\nvariable functions. ACM Transactions on Mathe\\xad\\nmatical Software 6, 618-622. \\nShavlik, J. W. & Dietterich, T. G. (eds) (1990) \\nReadings in Machine Learning. San Mateo, CA: \\nMorgan Kaufmann. \\nShawe-Taylor, J. & Anthony, M. (1991) Sample sizes \\nfor multiple-output threshold networks. Network 2, \\n107-117. \\nSheehan, N. & Thomas, A. (1993) On the irre\\xad\\nducibility of a Markov chain defined on a space \\nof genotype configurations by a sampling scheme. \\nBiometrics 49, 163-175. \\nShenoy, P. P. (1989) A valuation-based language for \\nexpert systems. International Journal of Approxi\\xad\\nmate Reasoning 3, 383-411. \\nShenoy, P. P. & Shafer, G. (1990) Axioms of prob\\xad\\nability and belief-function propagation. In Uncer\\xad\\ntainty in Artificial Intelligence 4, eds R. D. Shachter, \\nT. S. Levitt, L. N. Kana! & J. F. Lemmer, pp. 169-\\n198. Amsterdam: North-Holland. Reprinted in \\nShafer & Pearl (1990). \\nShenoy, P. P., Shafer, G. & Mellouli, K. (1988) \\nPropagation of belief functions: a distributed ap\\xad\\nproach. In Uncertainty in Artificial Intelligence 2, \\neds J. F. Lemmer & L. N. Kana!, pp. 325-335. \\nAmsterdam: North-Holland. \\nShepanski, J. F. (1987) Fast learning in artificial neu\\xad\\nral systems: multilayer perceptron training using \\noptimal estimation. In Proceedings of IEEE First \\nInternational Conference on Neural Networks, San \\nDiego, 1987, eds M. Caudill & C. Butler I, 465-472. \\nLong Beach, CA: IEEE Press. \\nShepard, R. N. (1962a) The analysis of proximi\\xad\\nties: multidimensional scaling with an unknown \\ndistance function I. Psychometrika 27, 125-139. \\nShepard, R. N. (1962b) The analysis of proximi\\xad\\nties: multidimensional scaling with an unknown \\ndistance function II. Psychometrika 27, 219-246. \\nShibata, R. (1976) Selection of the order of an auto\\xad\\nregressive model by Akaike's Information Crite\\xad\\nrion. Biometrika 63, 117-126. \\nShibata, R. (1980) Asymptotically efficient selection \\nof the order of the model for estimating parameters \\nof a linear process. Annals of Statistics 8, 147-164. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 396}, page_content=\"References 385 \\nShibata, R. (1981) An optimal selection of regression \\nvariables. Biometrika 68, 45~54. \\nShort, R. D. & Fukunaga , K. (1980) A new near\\xad\\nest neighbor distance measure. In Proceedings of \\nthe Fifth IEEE International Conference on Pattern \\nRecognition (Miami Beach, 1980), pp. 81~86. Los \\nAlamitos , CA: IEEE Computer Society Press. \\nShort, R. D. & Fukunaga , K. (1981) The optimal \\ndistance measure for nearest neighbor classifica\\xad\\ntion. IEEE Transactions on Information Theory 27, \\n622~627 . Reprinted in Dasarathy (1991). \\nSietsma, J. & Dow, R. J. F. (1991) Creating artificial \\nneural networks that generalize . Neural Networks \\n4, 67~79. \\nSilva, F. M. & Almeida, L. B. (1990) Speeding up \\nback-propagation . In Advanced Neural Computers , \\ned. R. Eckmiller , pp. 151-158. Amsterdam: Else\\xad\\nvier. \\nSilvapulle, M. J. & Burridge, J. (1986) Existence of \\nmaximum likelihood estimates in regression models \\nfor grouped and ungrouped data. Journal of the \\nRoyal Statistical Society series B 48, 100--106. \\nSilverman, B. W. (1985) Some aspects of the spline \\nsmoothing approach to non-parametric regression \\ncurve fitting (with discussion). Journal of the Royal \\nStatistical Society series B 47, 1-52. \\nSilverman , B. W. ( 1986) Density Estimation for Statis\\xad\\ntics and Data Analysis. London: Chapman & Hall. \\nSilverman , B. W. & Jones, M. C. (1989) E. Fix and \\nJ. L. Hodges (1951): An important contribution \\nto nonparametric discriminant analysis and density \\nestimation. International Statistical Review 57, 233-\\n247. \\nSimard, P., Le Cun, Y. & Denker, J. (1993) Efficient \\npattern recognition using a new transformation \\ndistance. In NIPS5, pp. 50--58. \\nSimmons, G. F. (1963) Introduction to Topology and \\nModern Analysis. New York: McGraw-Hill. \\nSinger, Y. & Tishby, N. (1994) Decoding cursive \\nscripts. In N IPS6, pp. 833-840. \\nSinghal, S. & Wu, L. (1989) Training multilayer \\nperceptrons with the extended Kalman filter. In \\nNIPS1, pp. 133-140. \\nSmith, A. F. M. (1991) Discussion of'Posterior Bayes \\nfactors'. Journal of the Royal Statistical Society \\nseries B 53, 132-133. \\nSmith, A. F. M. & Roberts, G. 0. (1993) Bayesian \\ncomputation via the Gibbs sampler and related \\nMarkov chain Monte Carlo methods (with discus\\xad\\nsion). Journal of the Royal Statistical Society series \\nB 55, 3-23. Smith, A. F. M. & Spiegelhalter, D. J. (1980) \\nBayes factors and choice criteria for linear models. \\nJournal of the Royal Statistical Society series B 42, \\n213~220. \\nSmith, C. A. B. (1947) Some examples of discrimina\\xad\\ntion. Annals of Eugenics 13, 272-282. \\nSmith, E. E. & Medin, D. L. (1981) Categories and \\nConcepts. Cambridge, MA: Harvard University \\nPress. \\nSmith, F. W. (1968) Pattern classifier design by linear \\nprogramming. IEEE Transactions on Computers 17, \\n367-372. \\nSmith, F. W. (1969) Design of multicategory pat\\xad\\ntern classifiers with two-category classifier design \\nprocedures. IEEE Transactions on Computers 18, \\n548~551. \\nSmith, J. Q. (1989) Influence diagrams for statistical \\nmodelling. Annals of Statistics 17, 654--672. \\nSmith, J. W., Everhart, J. E., Dickson, W. C., \\nKnowler, W. C. & Johannes, R. S. (1988) Using the \\nADAP learning algorithm to forecast the onset of \\ndiabetes mellitus. In Proceedings of the Symposium \\non Computer Applications in Medical Care (Wash\\xad\\nington, 1988), ed. R. A. Greenes, pp. 261-265. Los \\nAlamitos, CA: IEEE Computer Society Press. \\nSolla, S. A., Levin, E. & Fleisher, M. (1988) Acceler\\xad\\nated learning in layered neural networks. Complex \\nSystems 2, 625-639. \\nSontag, E. D. ( 1992) Feedback stabilization using \\ntwo-hidden-layer nets. IEEE Transactions on Neu\\xad\\nral Networks 3, 981-990. \\nSpackman, K. A. (1992) Maximum likelihood train\\xad\\ning of connectionist models: comparison with least\\xad\\nsquares back propagation and logistic regression. \\nIn Proceedings of the Fifteenth Annual Symposium \\non Computer Applications in Medical Care, Wash\\xad\\nington 1991, ed. P. D. Clayton, pp. 285-289. New \\nYork: McGraw-Hill. \\nSpath, H. ( 1985) Cluster Dissection and Analysis. \\nTheory, FORTRAN programs, examples. Chich\\xad\\nester: Ellis Horwood. \\nSpecht, D. F. (1967a) Vectorcardiographic diagnosis \\nusing the polynomial discriminant method of pat\\xad\\ntern recognition . IEEE Transactions on Bio-medical \\nEngineering 14, 90--95. \\nSpecht, D. F. (1967b) Generation of polynomial dis\\xad\\ncriminant functions for pattern recognition. IEEE \\nTransactions on Electronic Computers 16, 308-319. \\nSpecht, D. F. (1990a) Probabilistic neural networks. \\nNeural Networks 3, 109-118. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 397}, page_content=\"386 References \\nSpecht, D. F. (1990b) Probabilistic neural networks \\nand the polynomial Adaline as complementary \\ntechniques for classification. IEEE Transactions on \\nNeural Networks 1, 111-121. \\nSpecht, D. F. (1991) A general regression neural \\nnetwork. IEEE Transactions on Neural Networks 2, \\n568-576. \\nSpeed, T. (1990) Complexity, calibration and causality \\nin influence diagrams. In Oliver & Smith (1990), \\npp. 49-63. \\nSpiegelhalter, D. J. (1990) Fast algorithms for prob\\xad\\nabilistic reasoning in influence diagrams , with ap\\xad\\nplications in genetics and expert systems. In Oliver \\n& Smith (1990), pp. 361-384. \\nSpiegelhalter, D. J. & Lauritzen , S. L. (1990) Se\\xad\\nquential updating of conditional probabilities on \\ndirected graphical structures. Networks 20, 579-\\n605. \\nSpiegelhalter, D. J. & Smith, A. F. M. (1982) Bayes \\nfactors for linear and log-linear models with vague \\nprior information. Journal of the Royal Statistical \\nSociety series B 44, 377-387. \\nSpiegelhalter, D. J., Dawid, A. P., Lauritzen , S. L. \\n& Cowell, R. G. (1993) Bayesian analysis in expert \\nsystems (with discussion). Statistical Science 8, 219-\\n283. \\nSpirtes, P., Glymour, C. & Scheines, R. (1993) \\nCausality, Prediction, and Search. Lecture Notes in \\nStatistics 81. New York: Springer. \\nSrihari, S. N. (1992) High-performance reading ma\\xad\\nchines. Proceedings of the IEEE 80, 1120--1132. \\nSrinvas, S. & Breese, J. (1990) IDEAL: a soft\\xad\\nware package for the analysis of influence dia\\xad\\ngrams. In Uncertainty in Artificial Intelligence 6, eds \\nL. N. Kana!, J. Lemmer & T. S. Levitt, pp. 212-219. \\nAmsterdam: North-Holland. \\nStace, C. (1991) New Flora of the British Isles. Cam\\xad\\nbridge: Cambridge University Press. \\nStanfill, C. & Waltz, D. (1986) Toward memory-based \\nreasoning. Communications of the Association for \\nComputing Machinery 29, 1213-1228. \\nStewart, L. (1987) Hierarchical Bayesian analysis \\nusing Monte Carlo integration: computing pos\\xad\\nterior distributions when there are many possible \\nmodels. The Statistician 36, 211-219. \\nStinchcombe, M. & White, H. (1989) Universal \\napproximation using feedforward networks with \\nnon-sigmoid hidden layer activation functions. In \\nProceedings of the International Joint Conference \\non Neural Networks I, 613-617. Long Beach, CA: \\nIEEE Press. Stinchcombe , M. & White, H. ( 1990) Approximating \\nand learning unknown mappings using multilayer \\nfeedforward networks with bounded weights. In \\nProceedings of the International Joint Conference on \\nNeural Networks, San Diego, III, 7-16. Long Beach, \\nCA: IEEE Press. \\nStone, C. J. (1977) Consistent nonparametric re\\xad\\ngression (with discussion). Annals of Statistics 5, \\n595-645. \\nStone, C. J. (1985) Additive regression and other \\nnon parametric models. Annals of Statistics 13, 689-\\n705. \\nStone, C. J. (1986) The dimensionality reduction \\nprinciple for generalized additive models. Annals \\nof Statistics 14, 590--606. \\nStone, M. (1974) Cross-validatory choice and assess\\xad\\nment of statistical predictions (with discussion). \\nJournal of the Royal Statistical Society series 8 36, \\n111-147. \\nStone, M. (1977a) Asymptotics for and against cross\\xad\\nvalidation. Biometrika 64, 29-35. \\nStone, M. (1977b) An asymptotic equivalence of \\nchoice of model by cross-validation and Akaike's \\ncriterion. Journal of the Royal Statistical Society \\nseries 8 39, 44--47. \\nStone, M. (1979) Comments on model selection \\ncriteria of Akaike and Schwarz. Journal of the \\nRoyal Statistical Society series B 41, 276-278. \\nStreit, R. L. & Luginbuhl, T. E. (1994) Maximum \\nlikelihood training of probabilistic neural networks. \\nIEEE Transactions on Neural Networks 5, 764-783. \\nStromberg, J. E., Zrida, J. & lsaksson, A. (1991) \\nNeural trees-using neural nets in a tree classi\\xad\\nfier structure. In IEEE International Conferen ce on \\nAcoustics, Speech and Signal Processing (Toronto , \\n1991), pp. 137-140. Long Beach, CA: IEEE Press. \\nStyblinski , M.A. & Tang, T.-S. (1990) Experiments in \\nnonconvex optimization: stochastic approximation \\nand simulated annealing. Neural Networks 3, 467-\\n483. \\nSuen, C. Y., Legault, R., Nadal, C., Cheriet, M. & \\nLam, L. (1993) Building a new generation of hand\\xad\\nwriting recognition systems. Pattern Recognition \\nLetters 14, 303-315. \\nSuen, C. Y., Nadal, C., Legault, R., Mai, T. A. & Lam, \\nL. (1992) Computer recognition of unconstrained \\nhandwritten numerals. Proceedings of the IEEE 80, \\n1162-1180. \\nSussmann , H. J. (1992) Uniqueness of the weights \\nfor minimal feedforward nets with a given input\\xad\\noutput map. Neural Networks 5, 589-593. \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 398}, page_content='References 387 \\nSwain, P. H. & Hauska, H. (1977) The decision tree \\nclassifier: design and potential. IEEE Transactions \\non Geoscience Electronics 15, 142-147. \\nSwayne, D. F., Cook, D. & Buja, A. (1991) XGobi: \\ninteractive dynamic graphics in the X window \\nsystem with a link to S. In Proceedings of the ASA \\nSection on Statistical Graphics, pp. 1-8. Alexandria, \\nVA: American Statistical Association. \\nSwonger, C. W. (1972) Sample set condensation for \\na condensed nearest neighbor decision rule for \\npattern recognition . In Frontiers of Pattern Recog\\xad\\nnition, ed. S. Watanabe, pp. 511-519. Orlando: \\nAcademic Press. Reprinted in Dasarathy (1991). \\nTarassenko, L., Hayton, P., Cerneaz, N. & Brady, \\nM. (1995) Novelty detection for the identification \\nof masses in mammograms. In Proceedings of the \\nFourth International lEE Con{erence on Artificial \\nNeural Networks (Cambridge, 1995). lEE Confer\\xad\\nence Publication 409, 442-447. lEE Press. \\nTarjan, R. E. & Yannakakis, M. (1984) Simple linear\\xad\\ntime algorithms to test chordality of graphs, test \\nacyclicity of hypergraphs , and selectively reduce \\nacyclic hypergraphs. SIAM Journal of Computing \\n13, 566-579. \\nTarter, M. E. & Lock, M.D. (1993) Model-Free Curve \\nEstimation. New York: Chapman & Hall. \\nTherrien, C. W. (1989) Decision, Estimation, and Clas\\xad\\nsification: An Introduction to Pattern Recognition \\nand Related Topics. New York: Wiley. \\nThisted, R. A. (1988) Elements of Statistical Comput\\xad\\ning. Numerical Computation. New York: Chapman \\n&Hall. \\nThompson, E. A. (1985) Pedigree Analysis in Human \\nGenetics. Baltimore , MD: Johns Hopkins Univer\\xad\\nsity Press. \\nThornton, C. J. (1992) Techniques in Computational \\nLearning. An Introduction. London: Chapman & \\nHall. \\nTibshirani, R. (1992) Principal curves revisited. Statis\\xad\\ntics and Computing 2, 183-190. \\nTierney, L. (1994) Markov chains for exploring \\nposterior distributions (with discussion). Annals of \\nStatistics 22, 1701-1762. \\nTierney, L. & Kadane, J. B. (1986) Accurate ap\\xad\\nproximations for posterior moments and marginal \\ndensities. Journal of the American Statistical Asso\\xad\\nciation 81, 82-86. \\nTitterington, D. M. (1976) Updating a diagnostic \\nsystem using unconfirmed cases. Applied Statistics \\n25, 238-247. Titterington , D. M. (1980) A comparative study of \\nkernel-based density estimates for categorical data. \\nTechnometrics 22, 259-268. \\nTitterington, D. M. (1984) Recursive parameter esti\\xad\\nmation using incomplete data. Journal of the Royal \\nStatistical Society series B 46, 257-267. \\nTitterington , D. M., Murray, G. D., Murray, L. S., \\nSpiegelhalter , D. J., Skene, A. M., Habbema , \\nJ. D. F. & Gelpka, G. J. (1981) Comparison of \\ndiscrimination techniques applied to a complex \\ndata set of head injured patients (with discussion). \\nJournal of the Royal Statistical Society series A 144, \\n145-174. \\nTitterington, D. M., Smith, A. F. M. & Makov, \\nU. E. ( 1985) Statistical Analysis of Finite Mixture \\nDistributions. Chichester: Wiley. \\nTodd, B. S. (1995) Weighted inference rules and \\nBayesian belief networks. In Gammerman (1995), \\npp. 205-225. \\nTollenaere, T. (1990) SuperSAB: fast adaptive back \\npropagation with good scaling propertie s. Neural \\nNetworks 3, 561-573. \\nTomek, I. (1976a) A generalization of the k-NN rule. \\nIEEE Transactions on Systems, Man and Cybernet\\xad\\nics 6, 121-126. Reprinted in Dasarathy (1991). \\nTomek, I. (1976b) An experiment with the edited \\nnearest-neighbor rule. IEEE Transactions on Sys\\xad\\ntems, Man and Cybernetics 6, 448-452. Reprinted \\nin Dasarathy (1991). \\nTomek, I. (1976c) Two modifications of CNN. IEEE \\nTransactions on Systems, Man and Cybernetics 6, \\n769-772. \\nTorgerson , W. S. (1952) Multidimensional scaling I. \\nTheory and method. Psychometrika 17, 401-419. \\nTorgerson , W. S. (1958) Theory and Methods of \\nScaling. New York: Wiley. \\nTraven, H. G. C. (1991) A neural network approach \\nto statistical pattern classification by \"semipara\\xad\\nmetric\" estimation of probability density functions. \\nIEEE Transactions on Neural Networks 2, 366-377. \\nTsypkin, Ya. Z. (1966) Use of the stochastic approx\\xad\\nimation method in estimating unknown distribu\\xad\\ntion densities from observations. Automation and \\nRemote Control 27, 432-434. \\nTutz, G. (1986) An alternative choice of smoothing \\nfor kernel-based density estimates in discrete dis\\xad\\ncriminant analysis. Biometrika 73, 405-411. \\nTutz, G. (1988) Smoothing for discrete kernels m \\ndiscrimin ation. Biometrical Journal 6, 729-739. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 399}, page_content='388 References \\nTutz, G. (1989) On cross-validation for discrete \\nkernel estimates in discrimination. Communications \\nin Statistics-Theory and Methods 18, 4145-4162. \\nUllmann, J. R. (1974) Automatic selection of refer\\xad\\nence data for use in a nearest-neighbor method of \\npattern classification. IEEE Transactions on Infor\\xad\\nmation Theory 20, 541-543. \\nUltsch, A. (1993a) Knowledge extraction from self\\xad\\norganizing neural networks. In Information and \\nClassification, eds 0. Opitz, B. Lausen & R. Klar, \\npp. 301-306. Berlin: Springer. \\nUltsch, A. (1993b) Self-organizing neural networks \\nfor visualization and classification. In Information \\nand Classification, eds 0. Opitz, B. Lausen & \\nR. Klar, pp. 307-313. Berlin: Springer. \\nUpton, G. J. G. (1991) The exploratory analysis of \\nsurvey data using log-linear models. The Statisti\\xad\\ncian 40, 169-182. \\nUsui, S., Nakauchi, S. & Nakano, M. (1991) Internal \\ncolor representation acquired by a five-layer neural \\nnetwork. In Artificial Neural Networks. Proceedings \\nof ICANN-91, eds T. Kohonen, K. Makisara, \\n0. Simula & J. Kangas, volume I, pp. 867-872. \\nAmsterdam: North Holland. \\nUtgoff, P. E. (1988a) ID5: an incremental ID3. In \\nProceedings of the Fifth International Conference on \\nMachine Learning, ed. J. Laird pp. 107-120. San \\nMateo, CA: Morgan Kaufmann. \\nUtgoff, P. E. (1988b) Perceptron trees: a case study in \\nhybrid concept representations. In Proceedings of \\nthe Seventh AAAI National Conference on Artificial \\nIntelligence, St Paul, pp. 601-606. San Mateo, CA: \\nMorgan Kaufmann. \\nUtgoff, P. E. (1989) Improved training via incremental \\nlearning. In Proceedings of the Sixth International \\nWorkshop on Machine Learning (Ithaca, 1989 ), ed. \\nA.M. Segre, pp. 362-365. San Mateo, CA: Morgan \\nKaufmann. \\nUtgoff, P. E. (1990) Incremental induction of decision \\ntrees. Machine Learning 4, 161-186. \\nUtgoff, P. E. & Brodley, C. E. (1990) An incremental \\nmethod for multivariate splits in decision trees. \\nIn Proceedings of the Seventh International Work\\xad\\nshop on Machine Learning, eds B. W. Porter & \\nR. J. Mooney, pp. 58-65. San Mateo, CA: Morgan \\nKaufmann. \\nValiant, L. G. (1984) A theory of the learnable. \\nCommunications of the Association for Computing \\nMachinery 27, 1134--1142. Reprinted in Shavlik & \\nDietterich (1990). Van de Weide, W. (1989) IDL, or taming the \\nmultiplexer. In Proceedings of the Fourth Euro\\xad\\npean Working Session on Learning, ed. K. Morik, \\npp. 211-226. London: Pitman. \\nVan de Weide, W. (1990) Incremental induction \\nof topologically minimal trees. In Proceedings \\nof the Seventh International Workshop on Machine \\nLearning, eds B. W. Porter & R. J. Mooney, pp. 66-\\n74. San Mateo, CA: Morgan Kaufmann. \\nVan Ryzin, J. (1966) Bayes risk consistency of \\nclassification procedures using density estimation. \\nSankhyii A28, 261-270. \\nVapnik, V. N. (1982) Estimation of Dependencies based \\non Empirical Data. New York: Springer. \\nVapnik, V. (1992) Principles of risk minimization for \\nlearning theory. In NIPS4, pp. 831-838. \\nVapnik, V. N. (1995) The Nature of Statistical Learn\\xad\\ning Theory. New York: Springer. \\nVapnik, V. N. (1998) Statistical Learning Theory. New \\nYork: Wiley. \\nVapnik, V. N. & Chervonenkis, A. Ya. (1971) On \\nthe uniform convergence of relative frequencies of \\nevents to their probabilities. Theory of Probability \\nand its Applications 16, 264--280. \\nVenables, W. N. & Ripley, B. D. (1994) Modern \\nApplied Statistics with S-Plus. New York: Springer. \\nVerma, T. & Pearl, J. (1990) Causal networks: seman\\xad\\ntics and expressiveness. In Uncertainty in Artificial \\nIntelligence 4, eds R. D. Shachter, T. S. Levitt, \\nL. N. Kanal & J. F. Lemmer, pp. 69-76. Ams\\xad\\nterdam: North-Holland. \\nVerma, T. S. & Pearl, J. (1991) Equivalence and syn\\xad\\nthesis of causal models. In Uncertainty in Artificial \\nIntelligence 6, eds P. P. Bonissone, M. Henrion, \\nL. N. Kanal & J. F. Lemmer, pp. 255-268. Ams\\xad\\nterdam: North Holland. \\nVillegas, C. (1969) On the a priori distribution \\nof the covariance matrix. Annals of Mathematical \\nStatistics 40, 1098-1099. \\nVinod, H. (1969) Integer programming and the theory \\nof grouping. Journal of the American Statistical \\nAssociation 64, 506-517. \\nVlachonikolos, I. (1990) Predictive discrimination and \\nclassification with mixed binary and continuous \\nvariables. Biometrika 77, 657-662. \\nWagner, T. J. (1973) Convergence of the edited near\\xad\\nest neighbor. IEEE Transactions on Information \\nTheory 19, 696-697. \\nWahba, G. (1990) Spline Models for Observational \\nData. Philadelphia: SIAM. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 400}, page_content='References 389 \\nWahba, G. ( 1995) Generalization and regularization \\nin nonlinear learning systems. In The Handbook of \\nBrain Theory and Neural Networks, ed. M. Arbib, \\npp. 426--430. Cambridge , MA: The MIT Press. \\nWahba, G. & Wold, S. (1975) A completely automatic \\nFrench curve. Communications in Statistics 4, 1-17. \\nWahba, G., Gu, C., Wang, Y. & Chappell , R. \\n(1995) Soft classification a.k.a. risk estimation via \\npenalized log likelihood and smoothing spline \\nanalysis of variance. In Wolpert (1995), pp. 331-\\n359. \\nWakahara, T. (1993) Towards robust handwritten \\ncharacter recognition. Pattern Recognition Letters \\n14, 345-354. \\nWallace, C. S. & Freeman, P. R. (1987) Estimation \\nand inference by compact encoding (with discus\\xad\\nsion). Journal of the Royal Statistical Society series \\nB 49, 240--265. \\nWand, M.P. & Jones, M. C. (1995) Kernel Smoothing. \\nLondon: Chapman & Hall. \\nWang, C., Venkatesh, S. S. & Judd, J. S. (1994) \\nOptimal stopping and effective machine complexity \\nin learning. In NIPS6, pp. 303-310. \\nWang, Q. R. & Suen, C. Y. (1984) Analysis and \\ndesign of a decision tree based on entropy re\\xad\\nduction and its application to large character set \\nrecognition. IEEE Transactions on Pattern Analysis \\nand Machine Intelligence 6, 406--417. \\nWang, Q. R. & Suen, C. Y. (1987) Large tree classifier \\nwith heuristic search and global training. IEEE \\nTransactions on Pattern Analysis and Machine In\\xad\\ntelligence 9, 91-102. \\nWard, J. H. Jr (1963) Hierarchical grouping to \\noptimize an objective function. Journal of the \\nAmerican Statistical Association 58, 236-244. \\nWarner, H. R., Toronto, A. F., Veasey, L. R. & \\nStephenson, R. (1961) A mathematical model \\nfor medical diagnosis-application to congenital \\nheart disease. Journal of the American Medical \\nAssociation 177, 177-184. \\nWasserman, P. D. (1993) Advanced Methods in Neural \\nComputing. New York: Van Nostrand Reinhold. \\nWatanabe, S. (1969) Knowing and Guessing. New \\nYork: Wiley. \\nWaterhouse, S. R. & Robinson, A. J. (1994) Classi\\xad\\nfication using hierarchical mixtures of experts. In \\nProceedings of the 1994 IEEE Workshop on Neural \\nNetworks for Signal Processing IV, pp. 177-186. \\nLong Beach, CA: IEEE Press. Watrous, R. L. (1987) Learning algorithms for con\\xad\\nnectionist networks: applied gradient methods of \\nnonlinear optimization. In Proceedings of the IEEE \\nFirst International Conference on Neural Networks \\n(San Diego, 1987 ), eds M. Caudill & C. Butler II, \\n619-627. New York: IEEE Press. \\nWebb, A. R. (1994) Functional approximation by \\nfeed-forward networks: a least-squares approach. \\nIEEE Transactions on Neural Networks 5, 363-371. \\nWeigend, A. S. & Gershenfeld, N. A. (eds) (1993) \\nTime Series Prediction: Forecasting the Future and \\nUnderstanding the Past. Reading, MA: Addison\\xad\\nWesley. \\nWeigend, A. S., Huberman, B. A. & Rumelhart, \\nD. E. (1990) Predicting the future: a connectionist \\napproach. International Journal of Neural Systems \\n1, 193-209. \\nWeigend, A. S., Huberman, B. A. & Rumelhart , D. E. \\n(1992) Predicting sunspots and exchange rates with \\nconnectionist networks. In Nonlinear Modeling and \\nForecasting, eds M. Casdagli & S. Eubank, pp. 395-\\n432. Redwood City, CA: Addison- Wesley. \\nWeigend, A. S., Rumelhart, D. E. & Huberman, B. A. \\n(1991) Generalization by weight-elimination with \\napplication to forecasting. InN IPS3, pp. 875-882. \\nWeiss, S.M. (1991) Small sample error rate estimation \\nfor k-NN classifiers. IEEE Transactions on Pattern \\nAnalysis and Machine Intelligence 3, 285-289. \\nWeiss, S. M. & Kulikowski , C. A. (1991) Com\\xad\\nputer Systems that Learn: Classification and Predic\\xad\\ntion Methods from Statistics, Neural Nets, Machine \\nLearning and Expert Systems. San Mateo, CA: \\nMorgan Kaufmann. \\nWen, W. X. (1990) Optimal decomposition of belief \\nfunctions . In Proceedings of the Sixth Workshop \\non Uncertainty in Artificial Intelligence (Cambridge , \\nMA), pp. 245-256. \\nWerbos, P. J. (1974) Beyond Regression: New Tools for \\nPrediction and Analysis in the Behavioural Sciences. \\nPh.D. thesis, Harvard University. Reprinted in \\nWerbos (1994). \\nWerbos, P. J. (1988) Backpropagation: past and \\nfuture. In Proceedings of the IEEE International \\nConference on Neural Networks, San Diego, 1988 I, \\n343-353. Long Beach, CA: IEEE Press. \\nWerbos, P. J. (1994) The Roots of Backpropagation. \\nFrom Ordered Derivatives to Neural Networks and \\nPolitical Forecasting. New York: Wiley. \\nWest, M. & Harrison, P. J. (1989) Bayesian Forecast\\xad\\ning and Dynamic Models. New York: Springer. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 401}, page_content='390 References \\nWetterschereck, D. & Dietterich, T. (1992) Improving \\nthe performance of radial basis function networks \\nby learning center locations. In N IPS4, pp. 1133-\\n1140. \\nWhite, H. (1982) Maximum-likelihood estimation of \\nmis-specified models. Econometrica 50, 1-25. \\nWhite, H. (1989a) Learning in artificial neural net\\xad\\nworks: a statistical perspective. Neural Computation \\n1, 425-464. Reprinted in White (1992). \\nWhite, H. (1989b) Some asymptotic results for learn\\xad\\ning in single hidden-layer feedforward networks. \\nJournal of the American Statistical Association 84, \\n1003-1013. Reprinted in White (1992). Correction: \\n87, 1252. \\nWhite, H. (1990) Connectionist nonparametric regres\\xad\\nsion: multilayer feedforward networks can learn \\narbitrary mappings. Neural Networks 3, 535-549. \\nReprinted in White (1992). \\nWhite, H. (1992) Artificial Neural Networks: Approx\\xad\\nimation and Learning Theory. Oxford: Blackwell. \\nWhite, H. & Woolridge, J. (1991) Some results \\non sieve estimation with dependent observations. \\nIn Nonparametric and Semi-Parametric Methods in \\nEconometrics and Statistics, eds W. Barnett, J. Pow\\xad\\nell & G. Tauchen. New York: Cambridge Univer\\xad\\nsity Press. Reprinted in White (1992). \\nWidrow, B. & Hoff, M. E. Jr. (1960) Adaptive switch\\xad\\ning circuits. IRE WESCON Convention Record \\n4, 96-104. Reprinted in Anderson & Rosenfeld \\n(1988). \\nWilliams, W. T. & Lambert, J. M. (1959) Multivariate \\nmethods in plant ecology. I. Association-analysis in \\nplant communities. Journal of Ecology 47, 83-101. \\nWilson, D. L. (1972) Asymptotic properties of nearest \\nneighbor rules using edited data. IEEE Transac\\xad\\ntions on Systems, Man and Cybernetics 2, 408-421. \\nReprinted in Dasarathy (1991). \\nWinston, P. H. (1992) Artificial Intelligence. Third \\nedition. Reading, MA: Addison- Wesley. \\nWolberg, W. H. & Mangarasian, 0. L. (1990) Multi\\xad\\nsurface method of pattern separation for medical \\ndiagnosis applied to breast cytology. Proceedings \\nof the National Academy of Sciences of the USA 87, \\n9193-9196. \\nWolfe, J. H. (1970) Pattern clustering via multivariate \\nmixture analysis. Multivariate Behavioural Research \\n5, 329-350. \\nWolpert, D. H. (1992) Stacked generalization . Neural \\nNetworks 5, 241-259. \\nWolpert, D. H. (1993) On the use of evidence m \\nneural networks. In NIPS5, pp. 539-546. Wolpert, D. H. (1994a) Bayesian backpropagation \\nover 1-0 functions rather than weights. In NIPS6, \\npp. 200--207. \\nWolpert, D. H. (1994b) Discussion of Ripley (1994a). \\nJournal of the Royal Statistical Society series B 56, \\n450-451. \\nWolpert, D. H. (ed.) (1995) The Mathematics of \\nGeneralization . Reading, MA: Addison-Wesley. \\nWu, C. F. J. (1983) On the convergence properties of \\nthe EM algorithm. Annals of Statistics 11, 95-103. \\nXu, L., Kryzak, A. & Suen, C. Y. (1992) Methods \\nof combining multiple classifiers and their appli\\xad\\ncations to handwriting recognition. IEEE Transac\\xad\\ntions on Systems, Man and Cyberneti cs 22, 418-435. \\nXu, L., Kryzak, A. & Yuille, A. (1994) On radial \\nbasis function nets and kernel regression: statistical \\nconsistency, convergence rates, and receptive field \\nsizes. Neural Networks 7, 609-628. \\nYair, E. & Gersho, A. (1990a) The Boltzmann per\\xad\\nceptron network: a soft classifier. Neural Networks \\n3, 203-221. \\nYair, E. & Gersho, A. (1990b) Maximum a posteriori \\ndecision and evaluation of class probabilities by \\nBoltzmann perceptron classifiers. Proceeding s of \\nthe IEEE 78, 1620--1678. Reprinted in Lau (1992). \\nYannakakis, M. (1981) Computing the minimal fill-in \\nis NP-complete . SIAM Journal of Algebraic and \\nDiscrete Methods 2, 77-79. \\nYork, J. (1992) Use of the Gibbs sampler in expert \\nsystems. Artificial Intelligence 56, 115-130, 397-\\n398. \\nYoung, G. & Householder, A. S. (1938) Discussion of \\na set of points in terms of their mutual distances . \\nPsychometrika 3, 19-22. \\nYoung, T. Y. & Calvert, T. W. (1974) Classification, \\nEstimation and Pattern Recognition. New York: \\nAmerican Elsevier. \\nZador, P. L. (1982) Asymptotic quantization error of \\ncontinuous signals and the quantization dimension . \\nIEEE Transaction s on Itiformation Theory 28, 139-\\n149. \\nZeger, K., Vaisey, J. & Gersho, A. (1992) Globally \\noptimal vector quantization design by stochastic \\nrelaxation. IEEE Transactions on Signal Processing \\n40, 310-322. \\nZhao, Y. & Atkeson, C. G. (1992) Some approx\\xad\\nimation properties of projection pursuit learning \\nnetworks. In NJPS4, pp. 936-943. '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 402}, page_content='Author Index \\nAarts, E., 275, 353 \\nAbramowitz, M., 56, 185 \\nAbu-Mostafa, Y. S., 7, 83 \\nAckley, D. H., 279 \\nAgosta, J. M., 245 \\nAgrawala, A.K., 191 \\nAha, D. W., 13, 14, 201 \\nAhmed, W., 131, 134 \\nAitchison, J., 11, 46, 50-52, 56, 190 \\nAitken, C. G. G., 113, 190, 245, 273, \\n275 \\nAizerman, M. A, 135, 147 \\nAkaike, H., 34, 61, 347 \\nAlbert, A, 113 \\nAlbertini , F., 159 \\nAleksander, 1., 4 \\nAlexander , K. S., 83 \\nAlmeida, L. B., 156 \\nAlmond, R. G., 245 \\nAmari, S.-1., 16, 33, 34, 61, 140, 155, \\n171 \\nAmit, D. J., 16 \\nAnderberg, M. R., 312 \\nAndersen, S. K., 245 \\nAnderson , J. A., 7, 109, 111-113 \\nAnderson, J. R., 281 \\nAnderson , T. W., 16, 37, 101 \\nAndreassen , S., 245 \\nAngluin, D., 7, 80 \\nAnthony, M., 81, 83, 87, 88, 180 \\nAnzellotti , G., 133, 178 \\nApolloni , B., 282 \\nArgentiero , P., 237 \\nArkedev , A. G., 135 \\nArbib, M. A, 16 \\nAsh, T., 172 \\nAsimov, D., 301 \\nAssouad, P., 81 \\nAtkeson, C. G., 126 Averintsev , M. V., 252 \\nBaba, N., 160 \\nBahadur, R. R., 37, 190 \\nBah!, L. R., 225 \\nBailey, T., 198 \\nBain, Z., 279 \\nBaird, H. S., 16 \\nBakiri, G., 91, 121 \\nBaldi, P., 292 \\nBall, G. B., 312 \\nBallard, D. H., 205 \\nBanfield, J. D., 108, 314 \\nBao, J., 237 \\nBarlow, R. E., 310 \\nBarna, G., 281 \\nBarron, A. R., 61, 176, 178 \\nBarry, D., 139 \\nBartholomew, D., 310 \\nBartlett, P. L., 180 \\nBasford, K. E., 75, 208, 210 \\nBashkirov , 0. A, 135 \\nBaskett, F., 198 \\nBatchelor, B. G., 199 \\nBates, D. M., 139, 148 \\nBather, J., 80 \\nBattiti, R., 160 \\nBaum, E. B., 179 \\nBaum, L. E., 335 \\nBaxt, W. G., 66 \\nBeaudet, P., 237 \\nBeaulieu , J.-M., 326 \\nBeeri, C., 259 \\nBegg, C. B., 110 \\nBeigi, H. S. M., 160 \\nBenediktsson, J. H., 66 \\nBentley, J. L., 198 \\nBerge, C., 246 \\nBerger, J. 0., 20, 46, 50, 54, 62, 166, \\n348 Bernardo, J. M., 63 \\nBesag, J., 275, 337 \\nBest, D. J., 298 \\nBezdek, J. C., 317 \\nBhattacharyya, A., 328 \\nBibby, J. M., 16, 21, 36, 39, 49, 56, \\n354 \\nBichsel, M., 149 \\nBienenstock, E., 140, 300 \\nBiggs, N. L., 81 \\nBinford, T. 0., 245, 258 \\nBishop, C. M., 16, 136, 138, 151, 167 \\nBlair, J. R. S., 259 \\nBlaydon, C. C., 136 \\nBlock, H. D., 116, 118 \\nBlue, J. L., 76, 332. \\nBlumer, A., 79, 81-83 \\nBoehnke. M., 273 \\nde Boor, C., 123 \\nBoser, B., 16 \\nBottou, L., 16 \\nBourlard , H., 292 \\nBouton, C., 325 \\nBox, G. E. P., 8, 50, 63, 76 \\nBoyles, R. A., 336 \\nBrady, M., xi, 26 \\nBrailovsky, V. L., 70 \\nBramley, R., 160 \\nBratko, 1., 16, 235, 236 \\nBraverman, E. M., 135, 147 \\nBreese, J. S., 245, 273 \\nBreiman , L., 65, 101-103, 121, 127, \\n128, 173,213,217,218,221, \\n222, 224, 232, 237, 238, 240 \\nBremner, J. E., 310 \\nBrent, R. P., 241 \\nBrewer, M. J., 245, 273, 275 \\nBridle, J. S., 66, 109, 149, 353 \\nBrier, G. W., 69 '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 403}, page_content='392 \\nBrockett, R. W., 292 \\nBrodley, C. E., 239 \\nBroeckaert, I., 16, 183 \\nBroffit, B., 106 \\nBronowski, J., 26 \\nBroomhead, D. S., 131 \\nBrown, C., 245 \\nBrown, P. F., 225 \\nBrown, P. J., 190 \\nBrown, T. A., 191 \\nBrunk, H. M., 310 \\nBryan, J. G., 93 \\nBryant, J., 198 \\nBryson, A. E., 151 \\nBuckland, S. T., 187 \\nBuckley, A. G., 345 \\nBuja, A., 105, 108, 299 \\nBuntine, W. L., 153, 163-167, 169, \\n225, 226, 241 \\nBurrascano P., 210 \\nBurridge, J., 113 \\nByrd, R. H., 345 \\nByth, K., 114 \\nCabrera, J ., 299 \\nCacoullos, T., 184 \\nCalvert, T., 136 \\nCampbell, N. A., 13, 100, 106, 107 \\nCandela, G. T., 76, 332 \\nCannings, C., 246 \\nCarlin, J. B., 209, 266, 337 \\nCarpenter, G. A., 288, 315 \\nCarroll, S. M., 147 \\nCarter, C., 240, 242 \\nCasey, R. G., 236, 237 \\nCatlett, J., 240 \\nCeleux, G., 336 \\nCerneaz, N., xi, 26 \\nCestnik, B., 235 \\nChan, C., 237 \\nChan, K. H., 131, 134 \\nChan, K. S., 338 \\nChandran, P. S., 157 \\nChang, C.-H., 219, 222, 237 \\nChang, C. L., 202 \\nChappell, R., 15, 125 \\nCharniak, E., 245 \\nChatterji, B. N., 198 \\nChauvin, Y., 170 \\nChavez, R. M., 273 \\nCheeseman, P., 62, 317 \\nChellappa, R., 76, 334 \\nChen, C.-C., 74 Author Index \\nChen, D. S., 150 \\nChen, S., 134 \\nChen, Y.-C., 208 \\nChen, Z., 139 \\nCheng, Y.-Q., 96 \\nCheriet, M., 16 \\nCherkassky, V., 16, 323 \\nChernick, M. R., 74 \\nChernoff, H., 80, 328 \\nChervonenkis, A. Ya., 82 \\nChidananda Gowda, K., 200 \\nChin, R., 237 \\nChou, P. A., 6, 220, 225, 226, 238 \\nChou, W.-S., 208 \\nChow, C. K., 20, 277 \\nChow, Y., 339 \\nChrisley, R., 281 \\nCiampi. A., 219, 222, 237 \\nClark, L. A., 219, 225, 226 \\nClark, P., 236 \\nClarke, W. R., 106 \\nCleveland, W. S., 123 \\nClifford, P., 250 \\nClunies-Ross, C. W., 37 \\nCohen, E., 16 \\nCohn, D., 82 \\nCook, D., 299 \\nCoomans, D., 16, 183 \\nCooper, G. F., 243, 245, 258, 278 \\nCooper, L. N., 66, 131, 300 \\nCormen, T. H., 246, 277, 351 \\nCornfield, J ., 50 \\nCortes, C., 120 \\nCosslett, S., 111 \\nCottrell, G. W., 304 \\nCottrell, M., 325 \\nCover, T. M., 62, 71, 81, 119, 192, 195 \\nCowan, C. F. N., 135 \\nCowell, R. G., 245, 262, 276, 279 \\nCox, D. R., 7, 25, 33, 69, 109, 110, \\n169, 353 \\nCox, M. A. A., 305 \\nCox, S. J., 66 \\nCox, T. F., 305 \\nCraven, P., 141 \\nCrawford, S. L., 74, 238, 239, 276 \\nCrevier, D., 243 \\nCybenko, G., 147, 160 \\nDale, M. B., 320 \\nDarken, C. J., 131, 132, 134, 156 \\nDasarathy, B. V., 191, 198 \\nDattatreya, G. R., 237 Dawid, A. P., 6, 27, 69, 245, 260, 262, \\n265, 266, 276, 279, 339, 348, 352 \\nDeely, J. J., 55 \\nDelampady, M., 62 \\nDelp, E. J., 223, 226, 227, 230 \\nDeMers, D., 304 \\nDempster, A. P., 245, 335, 336, 342 \\nDenker, J. S., 6, 16, 169 \\nDennis, J. E, 342 \\nDesbois, D., 12, 13 \\nDevijver, P. A., 15, 16, 20, 101, 191, \\n195, 196, 198, 289, 318, 329, \\n330, 332 \\nDevlin, S. J., 293 \\nDeVore, R., 178 \\nDevroye, L., 83, 84, 192, 195 \\nDey, D. K., 64 \\nDiaconis, P., 126, 174, 296 \\nDiamantaras, K. I., 292 \\nDickinson, B. W., 147 \\nDickson, W. C., 14 \\nDiebolt, J., 209, 336 \\nDietterich, T. G., 91, 121, 134, 216, \\n239 \\nDiggle, P. J., 187 \\nDillon, W. R., 16 \\nDonner, A., 170 \\nDonoho, D. L., 126 \\nDoster, D., 237 \\nDoursat, R., 140 \\nDow, R. J. F., 138 \\nDowla, F. U., 160 \\nDoyle, P., 213 \\nDraper, D., 63, 64 \\nDreyfus, G., 16 \\nDubes, R. C., 74, 237, 312 \\nDuchon, J., 139 \\nDuda, R. 0., 15, 101, 118, 121, 182, \\n184 \\nDudani, S. A., 198 \\nDuffy, D. E., 113 \\nDunn, J. C., 317 \\nDunn, 0. J., 75, 106 \\nDunsmore, I. R., 11, 46, 50-52 \\nDyn, N., 137 \\nEaton, H. A. C., 156 \\nEdwards, D., 41, 276 \\nEfron, B., 45, 73-75, 101, 238, 348 \\nEhrenfeucht, A., 79, 81-83 \\nEisenberger, 1., 42 \\nElbaum, C., 131 \\nEriksen, P. S., 108 '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 404}, page_content='Erwin, E., 324 \\nEslava-G6mez, G., 12, 300, 301 \\nEttinger, G. L., 245 \\nEvans, M., 63 \\nEverhart, 1. E., 14 \\nFagin. R., 245, 259, 340 \\nFahlman, S. E., 156, 158, 172 \\nde Falco, D., 282 \\nFargette, D., 12, 13 \\nFaris, K. B., 131, 134 \\nFauquet, C., 12, 13 \\nFayyad, U. M., 237 \\nFefferman, C., 159 \\nFeldman, 1. A., 146 \\nFenwick, 1., 213 \\nFienberg, S. E., 190 \\nFilkin, D. L., 157 \\nFinkel, R. A., 198 \\nFinnoff, W., 155 \\nFisher, D. H., 239 \\nFisher, F. P. II, 329 \\nFisher, L., 210 \\nFisher, R. A., 23, 36, 93, 101 \\nFix, E., 184, 191 \\nFlanagan, 1. K., 312 \\nFlannery, B. P., 158, 159 \\nFleisher, M., 149 \\nFleiss, 1. L., 77 \\nFletcher, R., 158, 342 \\nFlick, T. E., 196, 197 \\nFlocchini, P., 16 \\nFlury, B., 108 \\nForester, B., 16 \\nForgy, E. W., 312 \\nFort, 1. C., 325 \\nFort, M., 325 \\nFrank, I. E., 108 \\nFraser, D. A. S., 50 \\nFrean, M., 171 \\nFreedman, D., 296, 340 \\nFreeman, D., 317 \\nFreeman, P. R., 62 \\nFriedman, H. P., 314 \\nFriedman, 1. H., 16, 107, 108, 122, \\n125, 126, 128, 187, 188, 198, \\n213, 217, 218, 221, 222, 224, \\n232, 237, 238, 240, 296~298, 303 \\nFrost, R. L., 312 \\nFu, K.-S., 6, 237, 240, 317 \\nFukunaga, K., 2, 15, 20, 100, 101, \\n195~198, 200, 312, 331 \\nFunahashi, K., 14 7 Author Index \\nFung, R., 276 \\nFurman, W., 210 \\nFurnival, G. M., 331 \\nGader, P., 16 \\nGallant, S. 1., 171 \\nGammerman, A., 245, 273, 275 \\nGanzberger, M., 16 \\nGardin, F., 16 \\nGarey, M. R., 351 \\nGates, G. W., 200 \\nGeiger, D., 252, 267, 268 \\nGeisser, S., 46-50, 52, 54, 62, 63, 71 \\nGelfand, A. E., 64, 337 \\nGelfand, S. B., 156, 223, 226, 227, \\n230, 240 \\nGelman, A., 209, 337 \\nGelpka, G. 1., 18 \\nGeman, D., 136, 248, 337, 338 \\nGeman, S., 88, 136, 140, 248, 337, 338 \\nGeorge, E. 1., 63 \\nGeorgiopoulos, M., 315 \\nGershenfeld, N. A., 16 \\nGersho, A., 202, 282, 312, 320 \\nGeyer, C., 337 \\nGhurye, S. G., 56 \\nGilbert, 1. C., 345 \\nGill, P. E., 118, 158, 342, 346 \\nGillies, A., 16 \\nGirosi, F., 131~135, 137, 178 \\nGlad, I. K., 185 \\nGlick, N., 29 \\nGlymour, C., 265, 266, 276 \\nGnanadesikan, R., 293 \\nGold, 1. I., 300 \\nGoldberg, M., 326 \\nGoldstein, M., 16 \\nGolomb, B. A., 16 \\nGolombic, M. C., 246 \\nGolub, G. H., 94, 289, 353 \\nGong, G., 74 \\nGonzalez, R. C., 6 \\nGood, I. 1., 55, 69 \\nGoodman, D. M., 160 \\nGoodman, R. M., 237 \\nGoppert, G., 198 \\nGordon, A. D., 288, 326 \\nGori, M., 159 \\nGower, 1. C., 305~307 \\nGranger, R. H. 1r, 239 \\nGrant, P. M., 135 \\nGray, R., 110 \\nGray, R. M., 202, 320 Green, P. 1., 123, 139, 275, 337 \\nGreene, T., 107 \\nGrenander, U., 88, 339 \\nGrinold R. C., 117 \\nvan Groenendaal, W., 76 \\nGrossberg, S., 288, 315 \\nGrosse, E., 123 \\nGrother, P. 1., 332 \\nGu, C., 15, 125, 139 \\nGuo, H., 223, 240 \\nGuyon, 1., 16 393 \\nHabbema, 1. D. F., 18, 52, 56, 100, \\n183, 244 \\nHall, D. 1., 312 \\nHall, P., 182, 187, 190, 299 \\nHampel, F. R., 135, 150 \\nHampson, S. E., 118 \\nHand, D. 1., 16, 101, 183, 197, 199 \\nHannan, E. 1., 61 \\nHansen, L. K., 66 \\nHanson, S. 1., 170 \\nHardie, W., 182, 198 \\nHardy, R. L., 131 \\nHarrison, P. 1 ., 62 \\nHart, P. E., 15, 101, 118, 121, 182, \\n184, 192, 195, 200 \\nHartigan, 1. A., 312 \\nHartman, E. 1., 133 \\nHassibi, B., 169 \\nHastie, T. 1., 105, 108, 122, 124, 125, \\n127, 141, 142, 210, 304 \\nHastings, W. K., 338 \\nHathaway, R. 1., 42 \\nHauck, W. W. 1 r, 170 \\nHauska, H., 237 \\nHaussler, D., 79~83, 88, 179, 180, 239 \\nHavarnek, T., 276 \\nHaykin, S., 4, 16, 281, 292, 324 \\nHayton, P., xi, 26 \\nHebb, D. 0., 146 \\nHeileman, G. L., 315 \\nHellman, M. E., 191 \\nHenderson, D., 16 \\nHenrichon, E. G. 1r, 237, 240, 317 \\nHenrion, M., 273 \\nHergert, F., 155 \\nHermans, 1., 100, 183 \\nHerskovits, E., 278 \\nHertz, 1., 16 \\nHeskes, T. M., 155 \\nHigdon, D., 337 \\nHighleyman, W. H., 67, 101 '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 405}, page_content='394 \\nHills, M., 70 \\nHills, S. E., 337 \\nHinkley, D. V., 25, 33, 169, 353 \\nHinton, G. E., 2, 66, 149, 151, 157, \\n167, 168, 279, 281, 283 \\nHjort, N. L., x, 32, 43, 50, 52, 75, 100, \\n185, 330 \\nHo, Y.-C., 118, 151 \\nHodges, 1. L., 184, 191 \\nHoeffding, W., 79, 87 \\nHoed, A. E., 108 \\nHoff, M. E., 117, 143, 147 \\nHoffgen, K.-U., 116 \\nHogg, S., 219, 222, 237 \\nHolland, P. W., 190 \\nHolt, M. 1. 1., 149 \\nHopfield, J. 1., 2, 283 \\nHornik, K., 147, 176,292 \\nHorvitz, E. 1., 273 \\nHouseholder, A. S., 307 \\nHoward, R., 178 \\nHoward, R. E., 16 \\nHrycej, T., 157, 273, 275 \\nHuang, J., 315 \\nHubbard, W., 16 \\nHuber, P. 1., 32, 35, 39, 40, 57, 135, \\n150, 296, 301 \\nHuberman, B. A., 170 \\nHull, J. 1., 16 \\nHummels, D. M., 131, 134, 195 \\nHunt, E. B., 235 \\nHunter, J. S., 8, 76 \\nHunter, W. G., 8, 76 \\nHwang, C.-R., 88 \\nHwang, J.-N., 127, 173 \\nHyafil, R., 216 \\nIhaka, R., 101-103, 121 \\nImpedovo, S., 16 \\nIngrassia, S., 209 \\nIntrator, N., 300 \\nIrani, K. B., 237 \\nIsaksson, A., 240 \\nIsenhour, T. L., 200 \\nIsham, V., 248 \\nIyengar, P. A., 16 \\nJackel, L. D., 16 \\nJackson, J. E., 289 \\nJacobs, R. A., 66, 156, 283, 285, 336 \\nJain, A. K., 74, 198, 312 \\nJain, R. C., 150 \\nJames, M., 15 Author Index \\nJancey, R. C., 312 \\nJeffreys, H., 50, 64 \\nJensen, F., 245 \\nJensen, F. V., 245, 262, 269, 272 \\nJiang, Q., 198 \\nJohannes, R. S., 14 \\nJohansson, E. M., 160 \\nJohnson, D. S., 351 \\nJohnson, N. L., 39, 354 \\nJohnstone, I. M., 126 \\nJolliffe, I. T., 289 \\nJones, L. K., 126, 176, 177 \\nJones, M., 132, 137 \\nJones, M. C., 182, 185, 191, 296-298 \\nJordan, M. I., 66, 283, 285, 336 \\nlou, I.-C., 173 \\nJudd, 1. S., 155 \\nKabrisky, M., 157 \\nKadane, J. B., 63 \\nKalantari, I., 198 \\nKambhatla, N., 304, 305 \\nKamgar-Parsi, B., 198 \\nKamp, Y., 292 \\nKana!, L. N., 16, 198 \\nKangas, T., 203, 204, 206 \\nKansa, E. 1., 131 \\nKappen, B., 155 \\nKappen, H. 1., 283 \\nKarhunen, J., 292 \\nKarpinski, M., 180 \\nKashyap, R. L., 118, 136 \\nKass, G. V., 233, 236 \\nKass, R. E., 63, 64 \\nKaufman, L., 306, 311, 313, 314, 317, \\n320, 321, 322 \\nKay, 1. W., 52, 56 \\nKearns, M., 82 \\nKeeler, 1. D., 133 \\nKeenan, D. M., 339 \\nKelly, F. P., 252 \\nKelly, 1., 317 \\nKelly, K., 276 \\nKendall, M.G., 236 \\nKennard, R. W., 108 \\nKent, J. T., 16, 21, 36, 39, 49, 56, 354 \\nKessell, D. L., 100 \\nKettenring, J. R., 293 \\nKhanna, D., 312 \\nKibler, D., 201 \\nKiiveri, H., 266 \\nKil, R. M., 131 \\nKim, B. S., 198 Kim, 1. H., 258 \\nKing, R. D., 16 \\nKirkland, M. D., 274, 339 \\nKittler, 1. V., 15, 16, 20, 101, 191, 195, \\n196, 198, 289, 318, 329, 330, 332 \\nKj.erulff, U., 260 \\nKleijnen, J. P. C., 76 \\nKnerr, S., 16 \\nKnight, B. W. Jr, 115 \\nKnowler, W. C., 14 \\nKnuth, D. E., 246 \\nKohonen, T., 202-204, 206, 281, 323, \\n325 \\nKohzaki, M., 160 \\nKoiran, P., 180 \\nKong, A., 245, 273, 342 \\nKononenko, I., 235, 236 \\nKoontz, W. L. G., 312 \\nKaplowitz, 1., 191 \\nKorst, 1., 275, 353 \\nKotz, S., 39, 354 \\nKowalski, J. M., 133 \\nKramer, A. H., 160 \\nKramer, M. A., 134, 304 \\nKrishna, G., 200 \\nKrishnaiah, P. R., 16 \\nKrogh, A., 16 \\nKruskal, J. B., 296, 310 \\nKryzak, A., 66, 132 \\nKrzanowski, W. 1., 41 \\nKulikowski, C. A., 157 \\nKung, S. Y., 292 \\nKurkova, v., 176 \\nKurzynski, M. W., 237 \\nKushner, H., 156 \\nKwok, S. W., 242 \\nLaaksonen, J., 203, 204, 206 \\nLachenbruch, P. A., 16, 70, 106 \\nLaird, N. M., 335, 336 \\nLam, L., 16 \\nLambert, 1. M., 321 \\nLandgrebe, D., 216, 237 \\nLange, K., 39, 40 \\nLangley, P., 16 \\nLarsen, B. N., 265, 266 \\nLauritzen, S. L., 41, 243-245, 259, \\n260, 261-266, 272, 276, 279, 285 \\nLawrence, D. T., 16 \\nLay, S.-R., 127, 173 \\nLazarsfeld, P. F., 190 \\nLe Cun, Y., 6, 16, 169 \\nLebiere, C., 172 '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 406}, page_content='LeBlanc, M., 66 \\nLee, S., 131 \\nLee, T.-C., 173 \\nLee, Y., 16, 133 \\nLeen, T. K., 170, 304, 305 \\nde Leeuw, J., 310 \\nLegault, R., 16 \\nLegendre, P., 305 \\nLehmann, E. L., 20, 25, 33, 60, 169 \\nLeimer, H.-G., 265, 266 \\nLeiserson, C. E., 246, 277, 351 \\nLemarechal, C., 345 \\nLeonard, J. A., 134 \\nLeroy, A. M., 135, 293 \\nLesaffre, E., 113 \\nLevin, A. U., 170 \\nLevin, E., 149 \\nLevin, S. A., 118 \\nLevitt, T. S., 245, 258 \\nLewis, R. A., 16 \\nLi, C. J., 160 \\nLi, H., 127 \\nLi, X., 237 \\nLiang, J., 272 \\nLincoln, W. P., 66 \\nLindley, D. V., 55, 63 \\nLindsay, B., 210 \\nLittle, R. J. A., 24, 39, 40 \\nLiu, C. N., 277 \\nLiu, D. C., 345 \\nLiu, L., 279 \\nLiu, Y., 71, 140, 150 \\nLloyd, S. P., 312 \\nLock, M. D., 185, 186 \\nLoizou, G., 191, 195 \\nLong, W. M., 26 \\nLouis, C., 41 \\nLouis, T. A., 336 \\nLowe, D., 131 \\nLowry, S. R., 200 \\nLueker, G. S., 260 \\nLuginbuhl, T. E., 208 \\nLuk, A., 198 \\nLunts, A. L., 70 \\nLuo, Z., 245, 273, 275 \\nLuttrell, S. P., 324 \\nLwin, T., 54 \\nMaass, W. G., 118, 180 \\nMacintyre, A., 180 \\nMacKay, D. J. C., 164, 166, 168 \\nMacKay, D. M., 146 \\nMacLeod, J. E. S., 198 Author Index \\nMacnaughton-Smith, P., 320 \\nMacQueen, J. B., 312 \\nMadigan, D., 62, 63, 279 \\nMadych, W. R., 137 \\nMaechler, M., 127 \\nMahalanobis, P. C., 21 \\nMahon, R. J., 13 \\nMai, T. A., 16 \\nMaier, D., 246, 259 \\nMaillot, V., 159 \\nMakov, U. E., 43, 208 \\nMakram-Ebeid, S., 160 \\nMallows, C., 108 \\nMammone, R. J., 240 \\nMangarasian, 0. L., 119 \\nManly, B. F. J., 108 \\nMann, W. B., 245, 258 \\nMansfield, A. J., 118 \\nMantock, J. M., 200 \\nMardia, K. V., 16, 21, 36, 39, 49, 56, \\n354 \\nMarin, J., 234 \\nMaritz, J. S., 54 \\nMarkel, S., 159 \\nMarks, S., 106 \\nMarkuzon, N., 288, 315 \\nMaronna, R. A., 39 \\nMarriott, F. H. C., 300, 314 \\nMartin, D., 127 \\nMartin, G. L., 16 \\nMartinetz, T., 323 \\nMassar!, D. L., 314 \\nMassuli, F., 160 \\nMathieson, M. J., 112 \\nMatus, F., 252 \\nMauri, G., 16 \\nMax, J., 202 \\nMaybank, S. J., 191, 195 \\nMaybeck, P. S., 157 \\nMcClelland, J. L., 2, 143, 151, 153, \\n279 \\nMcCullagh, P., 110, 112, 125, 189 \\nMcCulloch, R. E., 63 \\nMcCulloch, W. S., 145, 146 \\nMcDonald, G., 198 \\nMcKay, R. J., 100 \\nMcKinney, S., 219, 222, 237 \\nMcLachlan, G. J., 16, 46, 57, 75, 100, \\n114, 208, 210 \\nMedin, D. L., 201 \\nMeinguet, J., 139 \\nMeisel, W. S., 136, 216 \\nMellouli, K., 245 Mengersen, K., 337 \\nMercer, R. L., 225 \\nMessenger, R. C., 213, 236 \\nMetcalfe, J., 304 \\nMetropolis, N., 337 \\nMhaskar, H. N., 178 \\nMicchelli, C. A., 178 \\nMichalski, R. S., 214 \\nMichie, D., 3, 5, 9, 16, 215 \\nMickey, M. R., 70 \\nMingers, J., 237 \\nMinnick, R. C., 117 \\nMinsky, M. L., 109, 118 \\nMitchell, B., 16 \\nMitter, S. K., 156 \\nMockett, L. G., 320 \\nMogami, Y., 160 \\nMoller, M., 345 395 \\nMoody, J. E., 34, 61, 131, 132, 134, \\n140, 141, 156, 170--172 \\nMoore, B., 315, 316 \\nMoran, M. A., 52, 56 \\nMorgan, J. N., 213, 236 \\nMorin, R. L., 198 \\nMorrell, D. R., 312 \\nMorton, H., 4 \\nMosteller, F., 70 \\nMoulton, B. R., 63 \\nMoussouris, J., 252 \\nMuchnik, I. B., 135 \\nMuggleton, S., 16 \\nMulier, F. P., 323 \\nMunro, W., 300 \\nMurata, N., 33, 34, 61, 140, 171 \\nMuroga, S., 81, 117, 118 \\nMurphy, B. 1., 52, 56 \\nMurphy, P.M., 13, 15 \\nMurray, G. D., 18, 244 \\nMurray, L. S., 18, 244 \\nMurray, W., 118, 158, 342, 346 \\nMurtagh, F., 326 \\nMurthy, V. K., 74, 184 \\nMusavi, M. T., 131, 134 \\nMyles, J. P., 197 \\nNadal, C., 16 \\nNagy, G., 236, 237 \\nNakano, M., 304 \\nNakauchi, S., 304 \\nNarendra, P.M., 198, 312, 331 \\nNash, J. C., 158, 159 \\nNatayanan, A., 108 \\nNeal, R. M., 165, 166, 282 '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 407}, page_content='396 \\nNealy, C. D., 74 \\nNeapolitan, E., 245 \\nNeider, J. A., 110, 112, 125, 189 \\nNelson, B. E., 312 \\nNelson, S. A., 137 \\nNiblett, T., 235, 236 \\nNiemann, H., 198 \\nNienhuis, B., 149, 156 \\nNocedal, J., 345 \\nNorvig, P., 16 \\nNowlan, S. J., 66, 167, 168, 283 \\nObermayer, K., 324 \\nOcchinegro, S., 16 \\nOja, E., 292 \\nOlesen, K. G., 245, 262, 272 \\nOliver, L. H., 41 \\nOliver, R. M., 245 \\nOliver, T. L., 156 \\nOlkin, I., 41, 56 \\nOlshen, R. A., 213,217,218,221,222, \\n224, 232, 237, 238, 240 \\nvan Ooyen, A., 149, 156 \\nOtt, J., 273 \\nOttaviano, L., 16 \\nOwen, A., 108 \\nOwens, A. J., 157 \\nOxley, M. E., 157 \\nPagallo, G., 239 \\nPages, G., 325 \\nPalmer, R. G., 16 \\nPapert, S. A., 118 \\nParberry, I., 118 \\nPark, J., 132 \\nPark, S. B., 198 \\nParrondo, J. M. R., 82 \\nParthasarthy , G., 198 \\nParzen, E., 184 \\nPatrick, E. A., 329 \\nPatterson, A., 235 \\nPavlidis, T., 16 \\nPayne, H. J., 216 \\nPearl, J., 83, 244-246, 248, 252, 253, \\n256, 258, 263, 266--269, 273, \\n275, 279, 341 \\nPearlmutter , B. A., 66, 153 \\nPeck, R., 210 \\nPeng, F., 285, 286 \\nPenrod, C. S., 199 \\nPensini, M. P., 16 \\nPeot, M. A., 273 \\nPeretto, P., 16 Author Index \\nPerrone, M. P., 66 \\nPersonnaz , L., 16 \\nPeskun, P. H., 274 \\nPeterson, A. M., 173 \\nPeterson, C., 281 \\nPetrie, T., 335 \\nPeyton, B., 259 \\nPhillips, P. R., 112 \\nPhua, K. H., 345 \\nPitas, I., 4 \\nPitman, J. A., 16 \\nPitts, W., 145 \\nPlastria, F., 314 \\nPloughman , L., 273 \\nPoggio, T., 131-135, 137 \\nPollard, D., 32, 80, 81, 85, 88 \\nPosse, C., 297, 300, 301 \\nPoulsen, R. S., 41 \\nPowell, M. J. D., 131 \\nPratt, L., 170 \\nPrechelt, L., 9 \\nPregibon , D., 219, 225, 226 \\nPrentice, R., 111 \\nPreparata, F. P., 198, 349 \\nPress, W. H., 158, 159 \\nPreston, C. J., 248 \\nPyke, R., 111 \\nQuenouille, M., 72 \\nQuinlan, J. R., 214, 216, 217, 223, \\n227, 232, 233, 235, 236, 239, 240 \\nQuinn, B. G., 61 \\nRacine-Poon, A., 337 \\nRaeside, D. E., 198 \\nRaftery, A. E., 62-64, 108, 279, 314 \\nRao, C.-R., 26, 36, 93 \\nRavishankar, C. S., 223, 226, 227, 230 \\nRayens, W., 107 \\nRayner, J. C. W., 108, 298 \\nRead, C. J., 312 \\nRedner, R. A., 208 \\nReed, R., 169 \\nReilly, D. L., 131 \\nReynolds , J. H., 288, 315 \\nRichards, L. E., 236 \\nRiffenburgh , R. H., 37 \\nRimey, R., 245 \\nRipley, B. D., x, xi, 11, 16, 42, 48, 77, \\n113, 129, 151, 163, 165, 171, \\n173, 182, 252, 274, 275, \\n337-339, 353 \\nRissanen , J ., 61 Ritter, G. L., 200 \\nRitter, H., 323 \\nRivest, R. L., 216, 246, 277, 351 \\nRobert, C. P., 209 \\nRoberts, G. 0., 135, 337, 338 \\nRoberts, S., 135 \\nRobinson, A. J., 283, 285 \\nRobinson , R. W., 278 \\nRoeder, K., 207 \\nRogers, S. K., 1 57 \\nRonchetti , E. M., 135, 150 \\nRoosen, C. B., 127 \\nRose, D. J., 260 \\nRosen, D. B., 288, 315 \\nRosenblatt , F., 116, 143, 147 \\nRosenblatt, M., 184 \\nRosenbluth, A., 337 \\nRosenbluth, M., 337 \\nRosenfeld, R., 66 \\nRoskar, E., 235 \\nRounds, E. M., 237 \\nRousseeuw, P. J., 58, 135, 150, 293, \\n306, 311, 313, 317,320,321,322 \\nRozonoer, L. I., 13 5, 14 7 \\nRubin, D. B., 24, 335, 336 \\nRubin, J., 314 \\nRuck, D. W., 157 \\nRuiz, E. V., 198 \\nRumelhart, D. E., 2, 143, 151, 153, \\n170, 279 \\nRundell, P. W. K., 190 \\nRussell, S. J., 16 \\nRuzicka, P., 324 \\nde Sa, V. R., 205 \\nSaarinen , S., 160 \\nSafavian , S. R, 216, 237 \\nSakurai, A., 180 \\nSalamon, P., 66 \\nSalomon, R., 156 \\nSarna!, A., 16 \\nSammon, J. W., 308 \\nSandberg, I. W., 132 \\nSanger, T. D., 292 \\nSangiovanni- Vincentelli, A., 160 \\nSankar, A., 240 \\nSanter, T. J., 113 \\nSarma, V. V. S., 237 \\nSarvarayudu, G. P. R., 213, 217, 237 \\nSchaefer , J. R., 100, 183 \\nSchalkoff, R. J., 15 \\nScheines, R., 265, 266, 276 \\nSchimert, J., 127 '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 408}, page_content='Schlimmer , J. C., 239 \\nSchmid, M. J., 108 \\nSchmidhuber, J., 156 \\nSchnabel, R. B., 342, 345 \\nSchoen berg, I. J ., 307 \\nSchroeder, A., 187, 188 \\nSchuerm ann, J., 237 \\nSchulten, K., 323, 324 \\nSchwarz, G., 61, 64, 347 \\nSchwemer , G. T., 75 \\nScott, A. J., 111, 314 \\nScott, D. W., 182, 198 \\nSeber, G. A. F., 148 \\nSebestyen, G., 208 \\nSedgewick, R., 246, 277, 351 \\nSeitz, P., 149 \\nSejnowski , T. J., 16, 279 \\nSelf, M., 317 \\nSemnani, S., 149 \\nSen, A., 108 \\nSethi, I. K., 213, 217, 237, 241 \\nSetiono, R., 119 \\nShachter , R. D., 273 \\nShafer, G., 245, 262, 342 \\nShahshahani , M, 126, 174 \\nShamos, M. I., 198, 349 \\nShanno, D. F., 345 \\nShawe-Taylor, J., 83, 87, 88, 180 \\nSheehan, N., 338 \\nShenoy, P. P., 245, 262, 342 \\nShepanski , J. F., 157 \\nShepard, R. N., 310 \\nShibata, R., 61 \\nShiraishi, Y., 160 \\nShort, R. D., 197 \\nShustek, L. J., 198 \\nShyu, W. M., 123 \\nSibson, R., 296-298 \\nSietsma, J., 138 \\nSilva, F. M., 156 \\nSilvapulle, M. J., 113 \\nSilverman , B. W., 122, 123, 139, 141, \\n182, 191 \\nSimard, P., 6, 16 \\nSimmons , G. F., 175 \\nSimon, H. A., 16 \\nSimon, H.-U., 116 \\nSinger, Y., 16 \\nSinghal, S., 157 \\nSirat, J.-A., 160 \\nSkene, A., 18, 244 \\nSkolnick , M. H., 246 \\nSkrzypek, J., 66 Author Index \\nSmith, A. F. M., 43, 62-64, 208, 337, \\n338 \\nSmith, C. A. B., 37 \\nSmith, E. E., 201 \\nSmith, F. W., 117, 118 \\nSmith, J. Q., 245, 341 \\nSmith, J. W., 14 \\nSmyth, P., 237 \\nSnell, E. J., 109, 110 \\nSolla, S. A., 16, 149, 169 \\nSonquist , J. A., 213 \\nSontag, E. D., 159, 179, 180 \\nSoules, G., 335 \\nde Souza, P. V., 225 \\nSpackman, K. A., 149 \\nSpath, H., 312 \\nSpecht D. F., 184, 210 \\nSpeed, T. P., 246, 266 \\nSpiegelhalter, D. J., 3, 5, 9, 16, 18, 62, \\n64, 243-246, 262-264, 272, 276, \\n279, 285 \\nSpirtes, P., 265, 266, 276 \\nSrihari, S. N., 16, 66 \\nSrinvas, S., 245 \\nSrivastava, M., 108 \\nStace, C., 214 \\nStahel, W. A., 135, 150 \\nStanfill, C., 200 \\nStegun, I. A., 56, 185 \\nStephenson, R., 244 \\nStern, H. S., 209, 337 \\nSternberg, M. J. E., 16 \\nStewart, L., 62 \\nStinchcombe, M., 147, 176 \\nStofella, P., 16 \\nStone, C. J., 125, 192, 195, 213, 217, \\n218, 221, 222, 224, 232, 237, \\n238, 240 \\nStone, M., 33, 34, 61, 65, 71, 353 \\nStone, P. J., 235 \\nStork, D. G., 169 \\nStreit, R. L., 208 \\nStromberg , J. E., 240 \\nStuart, A., 236 \\nStuetzle, W., 125, 187, 188, 304 \\nStutz, J., 317 \\nStyblinski, M. A., 156 \\nSuen, C. Y., 16, 66, 237 \\nSussmann, H. J., 159 \\nSwain, P. H., 66, 237 \\nSwartz, T., 63 \\nSwayne, D. F., 299 \\nSwonger , C. W., 200 Symons, M. J., 314 \\nTakasu, S., 117, 118 \\nTang, T.-S., 156 \\nTanner, M. A., 285, 286 \\nTarassenko, L., xi, 26, 135 \\nTarjan, R. E., 259, 260 \\nTarter, M. E., 185, 186 \\nTate, R. F., 41 \\nTaylor, C. C., 3, 5, 9, 16 \\nTaylor, J. M. G., 39, 40 \\nTaylor, W., 317 \\nTeller, A., 337 \\nTeller, E., 337 \\nTesauro, G., 82 \\nTesi, A., 159 \\nTeukolsky, S. A., 158, 159 \\nTherrien, C. W., 15 \\nThiesson, B., 276 \\nThisted, R. A., 185, 299, 349 \\nThomas, A., 338 \\nThomason , M. G., 6 \\nThompson , E. A., 246 \\nThornton, C. J., 155 \\nTiao, G. C., 50, 63 397 \\nTibshirani, R. J., 66, 74, 105, 108, \\n122, 124, 125, 141, 142, 210, 305 \\nTierney, L., 63, 337, 338 \\nTishby, N., 16 \\nTitterington, D. M., 11, 18, 43, 190, \\n198, 208, 244, 336 \\nToda, I., 117, 118 \\nTodd, B. S., 263 \\nTollenaere , T., 156 \\nTomek, I., 195, 199, 200 \\nTorgerson, W. S., 307 \\nTorkkola, K., 203, 204, 206 \\nToronto, A. F., 244 \\nToussaint, G. T., 41 \\nTriiven, H. G. C., 208, 209 \\nTsai, J. C., 173 \\nTsypkin, Ya. Z., 136 \\nTukey, J. W., 72, 296 \\nTuran, G., 118 \\nTutz, G. E., 190 \\nTyler, D. E., 39 \\nUllmann, J. R., 200 \\nUltsch, A., 326 \\nUngar, J. H., 134 \\nUpton, G. J. G., 276 \\nUsui, S., 304 \\nUtans, J., 171, 172 '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 409}, page_content='398 \\nUtgoff, P. E., 239 \\nVaidyanathan, S. K., 63 \\nVaisey, J., 312 \\nValiant, L. G., 77, 80, 82 \\nVan der Broeck, C., 82 \\nVan Horn, K. S., 116 \\nVan Loan, C. F., 94, 289, 353 \\nVan Ness, J., 210 \\nVan Ryzin, J., 29 \\nVan de Weide, W., 237 \\nVapnik, V. N., 16, 62, 82-84, 120 \\nVardi, Y., 39 \\nVeasey, L. R., 244 \\nVenables, W. N., xi, 42, 182 \\nVenkatesh, S. S., 155 \\nVerma, T. S., 268, 279 \\nVetterling, W. T., 158, 159 \\nViala, J.-R., 160 \\nVidal, G., 12, 13 \\nVillegas, C., 50 \\nVinod, H., 313 \\nVlachonikolos, I., 52 \\nVolper, D. J., 118 \\nWagner, T. J., 199 \\nWahba, G., 15, 123, 125, 137, 139, \\n141, 142 \\nWakahara, T., 16 \\nWalker, H. F., 208 \\nWallace, C. S., 62 \\nWallace, D. L., 70 \\nWaltz, D., 200 \\nWand, M.P., 182 \\nWang, C., 155 Author Index \\nWang, Q. R., 237 \\nWang, Y., 15, 125 \\nWard, J. H. Jr, 319 \\nWarmuth, M. K., 79, 81-83 \\nWarner, H. R., 244 \\nWasserman, P. D., 184 \\nWatanabe, S., 289 \\nWatanabe, T., 169 \\nWaterhouse, S. R., 283, 285 \\nWatrous, R.L., 160 \\nWatts, D. G., 148 \\nWebb, A. R., 138 \\nWechsler, H., 16 \\nWeigend, A. S., 16, 153, 163-167, 169, \\n170 \\nWeiss, N., 335 \\nWeiss, S. M., 74, 157 \\nWen, W. X., 260 \\nWerbos, P. J., 150, 151, 153 \\nWest, M., 62 \\nWetterschereck, D., 134 \\nWhalen, M., 16 \\nWhite, H., 32, 140, 147, 155, 156, 176 \\nWidrow, B., 117, 143, 147 \\nWild, C. J., 111, 148 \\nWilkins, D. C., 279 \\nWilliams, R. J., 151 \\nWilliams, W. T., 320, 321 \\nWilliamson, R. C., 180 \\nWilson, C. L., 76, 332 \\nWilson, D. L., 199 \\nWilson, R. W. Jr, 331 \\nWinston, P. H., 16, 331 \\nWolberg, W. H., 119 Wold, S., 123 \\nWolfe, J. H., 316 \\nWolff, G., 169 \\nWolpert, D. H., 65, 79, 164, 168, 353 \\nWong, M. A., 312 \\nWoodruff, H. B., 200 \\nWoolridge, J., 140 \\nWright, M. H., 118, 158, 342, 346 \\nWu, C. F. J., 336 \\nWu, L., 157 \\nXu, L., 66, 132 \\nYair, E., 282 \\nYang, J.-Y., 96 \\nYannakakis, M., 259, 260 \\nYing, X., 279 \\nYocum, T., 16 \\nYork, J., 63, 273, 279 \\nYoshida, Y., 160 \\nYoshizawa, S., 33, 34, 61, 140, 171 \\nYou, S.-S., 173 \\nYoung, G., 307 \\nYoung, T. Y., 136 \\nYuille, A., 132 \\nZador, P. L., 202 \\nZeger, K., 312 \\nZhang, W., 198 \\nZhao, Y., 126 \\nZhuang, Y.-M., 96 \\nZimmerman, H. G., 155 \\nvan Zomeren, B. C., 58 \\nZrida, J., 240 '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 410}, page_content='Subject Index \\nPage numbers in bold refer to entries in the glossary. \\nLp. 91, 132, 133, 176, 178, 350 \\nn-method, 128, 129 \\nabductive inference, 256 \\nadaptive resonance theory, 315, 316 \\nadditive model, 122-125 \\nAIC, 34, 61, 65, 169, 211, 222, 347 \\nancestral subgraph, 247 \\nART, 315, 316 \\nARTMAP, 288 \\nASSISTANT, 236 \\nASSISTANT86, 236 \\nassociation analysis, 321 \\nattributes, 217 \\nAutoCiass, 317 \\nB-splines, 123 \\nback-fitting, 124, 127, 128, 172, 347 \\nback-propagation, 150-155, 347 \\nBahadur-Lazarsfeld expansion, 190 \\nbatch methods, 4 \\nBaum-Welch algorithm, 335 \\nBayes \\nerror, 20 \\nfactor, 62, 141 \\nformula, 347 \\nnetworks, 244 \\nrisk, 20 \\nestimating, 196 \\nrule, 20, 36, 347 \\nBayesian expert systems, 244 \\nBCM model, 300 \\nbest linear rule, 36, 43 \\nbest quadratic rule, 36, 43, 100 \\nBFGS formula, 344 \\nBhattacharyya bound, 328 \\nbias, 35, 45, 67, 68, 347, see also \\nde biasing \\ncorrection, 36, 66, 72-76, 330 BIC, 61, 64, 347 \\nBienayme-Chebychev inequality, 79, \\n85, 176, 347 \\nBoltzmann machines, 279-283 \\nradial basis, 28 3 \\nBoltzmann perceptron network, 282 \\nbootstrap, 66, 73-75, 238, 348 \\nboundary, 249 \\nbounds \\nBhattacharyya, 328 \\nChernoff, 80, 328 \\nbranch-and-bound, 314, 331, 348 \\nBrier scores, 69 \\nBRUTO, 124, 125, 129, 130, 142 \\nC4.5, 228, 232, 233, 235 \\ncanonical correlation analysis, 96 \\ncanonical var:i,ate, 95, 96, 210 \\nCantor construction, 303 \\ncascade correlation, 172 \\ncausal Markov, 265 \\ncausal networks, 244 \\ncausality, 246 \\nCG distribution, 41 \\nCHAID, 236 \\ncharacter recognition, 16 \\nChernoff bound, 80, 328 \\nchordal graph, 252 \\nclass separation measures, 330 \\nclassification trees, 213-242, 348 \\nclassifier, 5, 18, 348 \\ndesign issues, 6 \\nplug-in, 28 \\npredictive, 45-55, 113-114, 122, \\n164-167, 173 \\ntree-structured, 213-242 \\nclique, 247 clustering, 287, 311-322 \\ncomplete-link, 319 \\ncontiguity-constrained, 325, 326 \\nfuzzy, 316 \\ngroup-average, 319 \\nhierarchical, 318-322 \\nmonothetic, 321 \\npartitioning, 312-318 \\nsingle-link, 319 \\nCN2, 236 \\ncodebook vectors, 202, 348 \\ncoding class targets, 91 \\ncompact set, 348 \\ncompacta, 126, 133, 147, 173, 348 \\ncomplete subgraph, 247 \\ncomplete-link clustering, 319 \\nconcave, 110, 218, 221, 348 \\nconditional Gaussian distribution, 41 \\nconditional independence, 243, 244, \\n248, 339-342 \\ndefinitions of, 339, 340 \\nconfusion matrix, 75 \\nconjugate gradients, 158, 160, 345, \\n348 \\nconnected subgraph, 247 \\nconnectionist school, 2 \\nconsistent, 29, 30, 32, 42, 45, 71, 314, \\n348 \\nconvex, 348 \\ncombination, 37, 190, 225 \\ncost-complexity pruning, 221-226 \\ncrabs example, 13, 77, 97, 105, 116, \\n121, 129, 287, 288, 291, 292, \\n301, 303, 321, 322, 325 \\ncross-validation, 62, 65, 66, 69-72, \\n141,200,225,226,230,348 \\ngeneralized, 141, 142 \\nin LDA, 100 '),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 411}, page_content=\"400 \\nCushing's syndrome , 11, 25, 36, 37, \\n40, 44, 53, 56, 160, 161, 165, \\n166, 182, 287, 288 \\nD-separation, 267 \\nd-separation , 266, 267, 341 \\nDAG, 247 \\ndata editing, 198-200 \\ndebiasing , 52, 55, 56, 330 \\ndecision trees, see classification trees \\ndecomposable distributions, 258 \\ndelta rule, 117 \\ngeneralized , 149 \\ndendrogram, 287, 288, 318, 320 \\ndensity estimation \\nkernel, 181-185 \\northogonal series, 185-187 \\ndesign of experiments, 76 \\ndeviance , 60, 62, 65, 66, 71, 110, 138, \\n142, 163, 219, 221, 222, 225, \\n238, 241, 348 \\nexpected , 33, 61 \\ndiabetes, 14, 23, 99, 105, 114, 115, \\n125, 131, 161, 201, 207, 211, \\n228, 229, 233, 234 \\ndiagnostic paradigm , 7, 27, 28, 181, \\n348 \\ndigamma function, 56 \\ndirected Markov, 265 \\nDirichlet \\ndistribution, 53, 209, 242, 279, 348 \\nkernel, 186 \\ntessellation, 191, 210, 313, 349 \\ndiscrete distributions \\nestimating , 188-190 \\ndiscriminants \\nflexible, 121-142 \\nlinear, 92-100 \\ndissimilarity, 306, 307, 349 \\ndivergence, 329 \\ndouble-blind trials, 9 \\ndoubt probabilities, 18 \\ndoubt reports, xii, 5, 17, 23, 191 \\nearly stopping , 155, 349 \\nediting, 198-200, 349 \\nefficiency, 45, 72, 101, 110, 349 \\neigendecomposition , 349 \\nEM algorithm , 43, 208, 209, 284, \\n334-337, 349 \\ngeneralized , 335 \\nempirical Bayes, 54, 55, 166 \\nempirical risk minimization, 84 \\nentropy index, 217 Subject Index \\nentropy nets, 241 \\nequivariance , 106, 137, 319, 334, 349 \\nerror rate, 66-69 \\napparent , 67 \\ncomparisons of, 76, 77 \\nerror-based pruning, 227 \\nestimator, 349 \\nexact test, 77 \\nexamples \\ncrabs, 13, 77, 97, 105, 116, 121, \\n129, 287, 288, 291, 292, 301, \\n303, 321, 322, 325 \\nCushing's syndrome, 11, 25, 36, 37, \\n40, 44, 53, 56, 160, 161, 165, \\n166, 182, 287, 288 \\ndiabetes, 14, 23, 99, 105, 114, 115, \\n125, 131, 161, 201, 207, 211, \\n228, 229, 233, 234 \\nforensic glass, 13, 38, 97, 98, 105, \\n106, 113, 115, 129-131, 161, \\n162, 201, 207, 211, 230, 231 \\nsynthetic data, 11, 12, 199, 200, \\n204, 206, 208, 210, 211 \\nviruses, 12, 13, 290, 291, 301, 302, \\n309-311, 313, 319, 320 \\nexperiments \\ncomparative, 9 \\ndesign of, 76 \\nexpert systems, 243 \\nface recognition, 1, 16 \\nfeature, 5, 349 \\nextraction , 331, 332, 349 \\nnon-linear, 303-305 \\nordinal, 351 \\nselection, 327-331 \\nfeed-forward network, 143-180, 349 \\nFejer kernel, 186 \\nfiducial distribution, 50 \\nFisher's linear discriminant, 93-96, \\n102 \\nflexible discriminants, 121-142 \\nforensic glass, 13, 38, 97, 98, 105, 106, \\n113, 115, 129-131, 161, 162, \\n201, 207, 211, 230, 231 \\nFourier series, 186 \\nfRINGE, 239 \\nfuzzy \\nART, 315 \\nARTMAP, 288 \\nk-means, 317 \\ngating network, 283 \\nGauss-Newton procedure, 126, 346 GCV, see cross-validation, \\ngeneralized \\nGEM algorithm, 335 \\ngeneral regression neural network, \\n184 \\ngeneralization, 17, 77-89, 349 \\nstacked, 65 \\ngeneralized delta rule, 149 \\ngeneralized linear discrimination , 121 \\nGibbs sampler, 23, 273, 274, 280, 285, \\n338, 339, 349 \\nGini index, 217, 219, 221, 222, 225, \\n226, 228 \\nglass fragments , 13, 38, 97, 98, 105, \\n106, 113, 115, 129-131, 161, \\n162, 201, 207, 211, 230, 231 \\ngooseberries, 3 \\ngrammars \\nstochastic, 6 \\ngrand tour, 301 \\ngraph, 247 \\nchordal, 252, 258 \\ndecomposable , 258 \\ndirected, 24 7 \\nmoral, 264 \\ntriangulated, 252, 258 \\nundirected, 24 7 \\ngraphoids, 340 \\nGreen's function, 137 \\ngroup-average clustering, 319 \\nHammersley-Clifford theorem, 250 \\nHastings algorithm, 338, 339 \\nHermite polynomials , 127, 186, 299, \\n349 \\nHessian, 33, 35, 63, 64, 110, 113, 151, \\n153, 157-160, 165, 169, 350 \\nvia EM, 336 \\nhierarchical Bayes, 54 \\nhierarchical mixtures of experts, \\n283-286 \\nhinging hyperplanes, 127 \\nhints, 7, 10, 350 \\nHME, 283-286, 350 \\nHoeffding's inequality, 79, 87 \\nhold-out method, 67 \\nhyperparameters, 54, 55, 164 \\n1-map, 248 \\nperfect, 248 \\nID3, 235, 236, 239 \\nidiot's Bayes rule, 244, 317 \\nimportance sampling, 76, 113, 165, \\n173 \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 412}, page_content=\"impurity measure, 217 \\nincremental learning, 238 \\ninformation, 350 \\nFisher, 33 \\ngain, 235 \\nobserved, 35 \\nIris, 214 \\nirreducibility, 338 \\nISODATA algorithm, 312 \\nisotonic regression, 310 \\nJaccard coefficient, 306 \\njackknife, 72, 73 \\nJeffreys' information principle, 50 \\nJensen's inequality, 218, 350 \\njoin tree, 259 \\njunction tree, 259 \\nk-means, 134, 202, 210, 312 \\nfuzzy, 317 \\nk-medoid clustering, 313 \\nk-nearest neighbour rule, 191 \\nKalman filter, 157 \\nkernel density estimation, 99, \\n181-185, 207 \\ndiscrete distributions, 190 \\nkernel regression, 184 \\nknots, 123 \\nKohonen mapping, 322 \\nKolmogorov-Smirnov distance, 237 \\nKullback-Leibler divergence, 32, 188, \\n277, 350 \\nLaplace's method, 63 \\nLaplacian error estimate, 236 \\nleaf, 213 \\nlearning, 3, 350 \\nby queries, 7 \\nfrom hints, 7 \\nlearning vector quantization, 201-207 \\ninitialization, 205-207 \\nleast false parameter, 32, 350 \\nLegendre series, 298 \\nLeptograpsus crabs, 13, 77, 97, 105, \\n116, 121, 129, 287, 288, 291, \\n292, 301, 303, 321, 322, 325 \\nLevenberg-Marquardt methods, 346 \\nlikelihood, 30, 350 \\nweighted, 59 \\nlinear discriminant, 95 \\nlinear discriminant analysis, 21, 36, \\n92-100 \\nlinear rule \\nbest, 36 \\nlinear separation, 116 Subject Index \\nlocal minima, 154, 156, 159, 161, 165, \\n169-172, 285, 297, 301 \\nlocation model, 41, 52 \\nlocation vector, 39 \\nloess, 123 \\nlog-linear model, 43, 189, 275 \\nlogarithmic scoring, 69 \\nlogic sampling, 273, 282 \\nlogistic, 350 \\ndiscrimination, 7, 43-45, 109-115 \\npredictive approach, 113, 114 \\nregression, 69 \\nordinal, 112 \\nloss function, 18 \\nLVQ, 134, 201-207, 211, 350 \\nLVQ1, 203 \\nLVQ2.1, 204 \\nLVQ3, 205 \\nmachine learning, 3, 16, 213 \\nMahalanobis distance, 21, 92, 350 \\nMAP estimator, 167, 168, 334, 350 \\nmarginal representation, 256, \\n259-262, 269, 278 \\nMarkov chain Monte Carlo, 63, 273, \\n337-339 \\nMarkov network, 248 \\nMarkov properties, 248 \\non a DAG, 265 \\nMarkov trees, 252-258 \\nlearning the structure of, 277, 278 \\nMARS, 128-131, 139 \\nMaurey's lemma, 177 \\nmaximal cardinality search, 259 \\nmaximum a posteriori, 334 \\nmaximum likelihood, 333, 334 \\nmaximum likelihood estimate, 350 \\nMCMC, see Markov chain Monte \\nCarlo \\nMcNemar's test, 77 \\nMDL, 61 \\nmean field approximation, 281 \\nmessage passing, 254, 256, 262 \\nmetric, 305 \\nMetropolis algorithm, 338 \\nMetropolis-Hastings method, 338, \\n339 \\nminimum description length, 61 \\nminimum message length, 61 \\nmisclassification probabilities, 18 \\nmissing values, 15, 23, 24, 28, 279, \\n281, 285, 335, 336, 350 \\nin classification trees, 222, 231-233, \\n236 401 \\nmixture distributions, 41, 42, 165, \\n207-211 \\nEM for, 336 \\nin clustering, 316 \\ntwo components, 41, 42 \\nmixtures of experts, 283 \\nML-11, 55, 166 \\nMML, 61 \\nmodel combination, 65, 66 \\nmodel selection, 8, 59-65 \\nin LDA, 99, 100 \\nin neural networks, 168-173 \\npredictive, 62-65 \\nmomentum, 154 \\nmoral graph, 264 \\nmultidimensional scaling, 288, \\n305-311 \\nclassical, 307, 308 \\nordinal, 309-311 \\nSammon, 308, 309 \\nmultilayer perceptron, 351 \\nmultiple logistic model, 44, 91, 109, \\n122, 149 \\nmultiquadric, 131 \\nmultivariate analysis, 351 \\nnaive Bayes rule, 244 \\nnearest neighbour methods, 191-201 \\nchoice of metric, 197 \\ndata editing, 198-200 \\nneural networks, 2, see also \\nBoltzmann machines, ART, \\nLVQ, SOM \\nartificial, 2 \\ndefinitions, 2, 4 \\nfeed-forward, 143-180 \\ngeneral regression, 184 \\nprobabilistic, 184 \\nneural trees, 240 \\nNewton methods, 343 \\nNIC, 34, 61, 65, 71, 140, 171 \\nnon-informative prior, 351 \\nnormal distribution, 21, 35, 48, 351 \\nNP-complete, 216, 260, 351 \\nNP-hard, 245, 351 \\nOLVQ1, 203, 205 \\non-line methods, 4, 351 \\none SE rule, 225 \\nOptimal Brain Damage, 169 \\nOptimal Brain Surgeon, 169 \\noption trees, 242 \\nordered derivatives, 150 \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 413}, page_content=\"402 \\nordinal feature, 351 \\nordinal logistic models, 112 \\nordination, 305 \\northant, 88 \\northogonal expansion estimators, \\n185-187 \\noutliers, 5, 17, 24-26, 293, 296-298, \\n300, 301, 310, 313, 315, 322, 351 \\nPAC-learning, 77 \\nparadigm \\ndiagnostic, 7, 27, 28, 181, 348 \\nsampling, 6, 27, 28, 55, 181, 352 \\nparallel distributed processing, 2 \\nparallelization, 4 \\nParzen windows, see kernel density \\nestimation, 351 \\npath, 247 \\nPatrick-Fisher coefficient, 329 \\npattern recognition, 1, 2 \\nstatistical, 6 \\nsupervised, 3 \\nsyntactic, 6 \\nunsupervised, 3, 287-326 \\npenalization, 60, 136 \\nperceptron, 81, 116,136, 351 \\ncapacity of, 119 \\nmultilayer, see feed-forward neural \\nnetwork • · · \\nperceptron trees, 240 \\nperfect clique sequence, 259 \\nperfect map, 248 \\nperformance assessment, 8, 9, 66-77 \\npessimistic pruning, 227 \\nPima Indians, 14, 23, 99, 105, 114, \\n115, 125, 131, 161, 201, 207, \\n211, 228, 229, 233, 234 \\nPIMPLE, 128, 129 \\nplug-in classifier, 28, 352 \\nPoisson distribution, 22, 48 \\npolytree, 247 \\nposterior probability, 352 \\npotential functions, 135 \\npotential representation, 250, 252, \\n253, 259-264, 266, 268, 271 \\nPPR, see projection pursuit \\nregression \\nprecise hypotheses, 62 \\npredictive classifier, 45-55, 113, 114, \\n122, 164-167, 173, 352 \\nmodel selection, 62-65 \\nprincipal components, 170, 289-296, \\n352 Subject Index \\nprincipal coordinate analysis, 307 \\nprincipal curves, 304, 305 \\nprior \\nimproper, 49, 54, 64, 141, 166, 170, \\n334, 351 \\nnon-informative, 351 \\nprior probability, 352 \\nprobabilistic expert systems, 244 \\nprobabilistic neural network, 184 \\nprofile likelihood, 31, 111, 112, 314, \\n352 \\nprojection pursuit, 296-303, 332, 352 \\ndensity estimation, 187, 188 \\nindices, 298-303 \\nregression, 125-128 \\npruning, 221-231, 352 \\ncost-complexity, 221-226 \\nerror-based, 227 \\nneural networks, 169, 170 \\npessimistic, 227 \\npseudo-dimension, 88, 180 \\nQR decomposition, 94 \\nquadratic rule, 36, 106 \\nquasi-Newton, 158-160, 344, 352 \\nlimited-memory, 345 \\nqueries, 7 \\nQuickprop, 156, 158 \\nradial basis functions, 131-136, 352 \\nrank, 352 \\nRBF, see radial basis functions \\nrecursive model, 263 \\nregression spline, 123 \\nregularization, 136-142, 352 \\nregularized discriminant analysis, 107 \\nreject option, see doubt reports \\nrejects, 5 \\nresistant methods, 39, 352, see also \\nrobust methods \\nridge regression, 108, 157, 346, 352 \\nrisk, 19, 352 \\nconsistency, 29, 30, 140 \\noverall, 46 \\ntotal, 19 \\nrisk averaging, 68, 69, 75, 196, 197 \\nrobust methods, 57, 58, 123, 150, 352 \\nfor principal components, 293 \\nfor RBFs, 135 \\nin LOA, 105, 106 \\nrunning intersection property, 259 \\nsaddle-point approximation, 63 \\nSammon mapping, 308, 309 sampling paradigm, 6, 27, 28, 55, 181, \\n352 \\nSauer's lemma, 81 \\nscale matrix, 39 \\nscores \\ncanonical, 97 \\nselection \\nstepwise, 60, 169, 170, 330 \\nself-organizing map, 134, 322-325 \\nseparation \\nlinear, 116 \\non a DAG, 266-268, 341 \\non a graph, 248, 341 \\nset-chain representation, 260 \\nSherman-Morrison-Woodbury \\nformula, 169, 344, 353 \\nshrinkage methods, 106-109, 225, \\n226, 353 \\nsimilarity, see dissimilarity \\nsimple matching coefficient, 306 \\nsimulated annealing, 156, 209, 260, \\n275, 312, 353 \\nsingle-link clustering, 319 \\nsingular value decomposition, 94, \\n289, 353 \\nSMART, 126, 172 \\nsmoothing splines, 123, 137 \\nSOFM, see self-organizing map \\nsoft splits, 239, 240 \\nsoftmax, 149, 353 \\nSOM, see self-organizing map \\nsphering, 296, 297, 306, 314 \\nsplines, 123, 126, 353 \\nadditive, 139 \\ninteraction, 139 \\nmultidimensional, 138, 139 \\nregression, 123 \\nsmoothing, 123 \\nthin-plate, 131, 138 \\nstabilizer, 136 \\nstacked generalization, 65, 353 \\nSTAGGER, 239 \\nstatistical pattern recognition, 6 \\nsteepest descent, 343, 353 \\nstepwise selection, 60, 169, 170, 330 \\nstochastic approximation, 156, 354 \\nStone-Weierstrass theorem, 133, 175 \\nstratified sampling, 76 \\nstructural risk minimization, 62 \\nsubgraph, 247 \\nsubtree, 213 \\nrooted, 213 \\nsuper-smoother, 126 \\nsupervised learning, 3, 354 \"),\n",
       " Document(metadata={'source': '..\\\\..\\\\data\\\\pdf_books\\\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf', 'page': 414}, page_content='t distribution, 39, 50, 166, 354 \\ntest set, 7, 354 \\nbiased, 67 \\nTHAID, 236 \\nthin-plate splines, 131, 138 \\ntime series, 16 \\ntopological ordering, 323 \\ntopological sort, 262 \\ntotal risk, 19 \\ntraining set, 3, 28, 44, 354 \\ntree, 247 \\njoin, 259 \\njunction, 259 \\npruning, 221-231 \\nshrinking, 225 \\ntriangulated graph, 252 Subject Index \\nultrametric, 318 \\nunclassified observations, 31 \\nuniform approximation, 176 \\nuniform convergence, 126, 133, 147, \\n173, 354 \\nuniversal approximation, 126, 127, \\n132, 173, 174, 178 \\nunsupervised learning, 354 \\nupdating, 42, 43, 354 \\nvalidation set, 7, 154, 225, 354 \\nVapnik-Chervonenkis dimension, see \\nVC dimension \\nvariable metric, 344 \\nvariance \\nbetween-group, 93 \\nwithin-group, 93 403 \\nVC dimension, 81, 179 \\nvector quantization, 202, 207, 354 \\ntree-structured, 320 \\nviruses example, 12, 13, 290, 291, 301, \\n302, 309-311, 313, 319, 320 \\nvisualization, 287, 296, 299, see also \\nmultidimensional scaling \\ngrand tour, 301 \\nof SOMs, 326 \\nweight decay, 157, 163, 164 \\nweights, 220, 280, 354 \\nWidrow-Holf learning, 117 \\nXGobi, 299 \\nZip codes, 1, 3, 5, 6, 25 ')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"{len(pages)} Pages in the PDF\")\n",
    "pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting text into chunks...\n",
      "[]\n",
      "[]\n",
      "['Pattern Recognition and Neural Networks B. D. RIPLEY University of Oxford']\n",
      "['PUBLISHED BY THE PRESS SYNDICATE OF THE UNIVERSITY OF CAMBRIDGE The Pitt Building, Trumpington Street, Cambridge, United Kingdom CAMBRIDGE UNIVERSITY PRESS The Edinburgh Building, Cambridge CB2 2RU, UK 40 West 20th Street, New York NY 10011-4211, USA 477 Williamstown Road, Port Melbourne, VIC 3207, Australia Ruiz de Alarcon 13, 28014 Madrid, Spain Dock House, The Waterfront, Cape Town 8001, South Africa http://www.cambridge.org © B. D. Ripley 1996 This book is in copyright. Subject to statutory exception and to the provisions of relevant collective licensing agreements, no reproduction of any part may take place without the written permission of Cambridge University Press. First published 1996 Eighth printing 2005 Printed in the United Kingdom at the University Press, Cambridge A catalogue record for this book is available from the British Library Library of Congress Cataloguing in Publication data Ripley, Brian D., 1952-Pattern recognition and neural networks I B.D. Ripley. p. em.', 'data Ripley, Brian D., 1952-Pattern recognition and neural networks I B.D. Ripley. p. em. Includes bibliographical references and index. ISBN 0 521 46086 7 1. Neural networks Computer science). 2. Pattern Recognition systems I. Title QA 76.87.R56 1996 006.4--dc20 95-25223 CIP ISBN 0 521 46086 7 hardback']\n",
      "['Contents Preface ix Notation xii 1 Introduction and Examples 1 1.1 How do neural methods differ? 4 1.2 The pattern recognition task . . 5 1.3 Overview of the remaining chapters . 9 1.4 Examples 10 1.5 Literature 15 2 Statistical Decision Theory 17 2.1 Bayes rules for known distributions 18 2~2 Parametric models ... 26 2.3 Logistic discrimination 43 2.4 Predictive classification 45 2.5 Alternative estimation procedures 55 2.6 How complex a model do we need? . 59 2.7 Performance assessment . . . . . . . . 66 2.8 Computational learning approaches . 77 3 Linear Discriminant Analysis 91 3.1 Classical linear discrimination 92 3.2 Linear discriminants via regression 101 3.3 Robustness ..... 105 3.4 Shrinkage methods . 106']\n",
      "['VI Contents 3.5 Logistic discrimination 0 0 .... 109 3.6 Linear separation and perceptrons . 116 4 Flexible Discriminants 121 4.1 Fitting smooth parametric functions 122 4.2 Radial basis functions 131 4.3 Regularization • 0 • • 136 5 Feed-forward Neural Networks 143 5.1 Biological motivation 145 5.2 Theory ........ 147 5.3 Learning algorithms 148 5.4 Examples ...... 160 5.5 Bayesian perspectives 163 5.6 Network complexity . 168 5.7 Approximation results . 173 6 Non-parametric Methods 181 6.1 Non-parametric estimation of class densities . 181 6.2 Nearest neighbour methods .. 191 6.3 Learning vector quantization . 201 6.4 Mixture representations 207 7 Tree-structured Classifiers 213 7.1 Splitting rules . 216 7.2 Pruning rules 221 7.3 Missing values 231 7.4 Earlier approaches 235 7.5 Refinements . . . . 237 7.6 Relationships to neural networks 240 7.7 Bayesian trees . •• 0 ••••• ••• 241 8 Belief Networks 243 8.1 Graphical models and networks 246 8.2 Causal networks 0 ••• 0 ••• • 262 8.3', 'Belief Networks 243 8.1 Graphical models and networks 246 8.2 Causal networks 0 ••• 0 ••• • 262 8.3 Learning the network structure 275']\n",
      "['Contents 8.4 Boltzmann machines . . . . . . 8.5 Hierarchical mixtures of experts 9 Unsupervised Methods 9.1 Projection methods . 9.2 Multidimensional scaling 9.3 Clustering algorithms 9.4 Self-organizing maps ... 10 Finding Good Pattern Features 10.1 Bounds for the Bayes error . 10.2 Normal class distributions . 10.3 Branch-and-bound techniques 10.4 Feature extraction A Statistical Sidelines A.1 Maximum likelihood and MAP estimation . A.2 The EM algorithm . . . . . . . . . . . . A.3 Markov chain Monte Carlo . . . . . . . A.4 Axioms for conditional independence . A.5 Optimization . . . . . . . . . . . . . . . Glossary References Author Index Subject Index Vll 279 283 287 288 305 311 322 327 328 329 330 331 333 333 334 337 339 342 347 355 391 399']\n",
      "[]\n",
      "[\"Preface Pattern recognition has a long and respectable history within engineering, especially for military applications, but the cost of the hardware both to acquire the data (signals and images) and to compute the answers made it for many years a rather specialist subject. Hardware advances have made the concerns of pattern recognition of much wider applicability. In essence it covers the following problem: 'Given some examples of complex signals and the correct decisions for them, make decisions automatically for a stream of future examples.' There are many examples from everyday life: N arne the species of a flowering plant. Grade bacon rashers from a visual image. Classify an X-ray image of a tumour as cancerous or benign. Decide to buy or sell a stock option. Give or refuse credit to a shopper. Many of these are currently performed by human experts, but it is increasingly becoming feasible to design automated systems to replace the expert and either perform better (as in credit\", \"feasible to design automated systems to replace the expert and either perform better (as in credit scoring) or 'clone' the expert (as in aids to medical diagnosis). Neural networks have arisen from analogies with models of the way that humans might approach pattern recognition tasks, although they have developed a long way from the biological roots. Great claims have been made for these procedures, and although few of these claims have withstood careful scrutiny, neural network methods have had great impact on pattern recognition practice. A theoretical understanding of how they work is still under construction, and is attempted here by viewing neural networks within a statistical framework, together with methods developed in the field of machine learning. One of the aims of this book is to be a reference resource, so almost all the results used are proved (and the remainder are given references to complete proofs). The proofs are often original, short and I believe\"]\n",
      "['X Preface show insight into why the methods work. Another unusual feature of this book is that the methods are illustrated on examples, and those examples are either real ones or realistic abstractions. Unlike the proofs, the examples are not optional! The formal pre-requisites to follow this book are rather few, especially if no attempt is made to follow the proofs. A background in linear algebra is needed, including eigendecompositions. (The singular value decomposition is used, but explained.) A knowledge of calculus and its use in finding extrema (such as local minima) is needed, as well as the simplest notions of asymptotics (Taylor series expansions and O(n) notation). Graph theory is used in Chapter 8, but developed from scratch. Only a first course in probability and statistics is assumed, but considerable experience in manipulations will be needed to follow the derivations without writing out the intermediate steps. The glossary should help readers with non-technical', 'without writing out the intermediate steps. The glossary should help readers with non-technical backgrounds. A graduate-course knowledge of statistical concepts will be needed to appreciate fully the theoretical developments and proofs. The sections on examples need a much less mathematical background; indeed a good overview of the state of the subject can be obtained by skimming the theoretical sections and concentrating on the examples. The theory and the insights it gives are important in understanding the relative merits of the methods, and it is often very much harder to show that an idea is unsound than to explain the idea. Several chapters have been used in graduate courses to statisticians and to engineers, computer scientists and physicists. A core of material would be Sections 2.1-2.3, 2.6, 2.7, 3.1, 3.5, 3.6, 4.1, 4.2, 5.1-5.4, 6.1-6.4, 7.1-7.3 and 9.1-9.4, supplemented by material of particular interest to the audience. For example, statisticians should cover 2.4, 2.5,', \"material of particular interest to the audience. For example, statisticians should cover 2.4, 2.5, 3.3, 3.4, 5.5, 5.6 and are likely to be interested in Chapter 8, and a fuller view of neural networks in pattern recognition will be gained by adding 3.2, 4.3, 5.5-5.7, 7.6 and 8.4 to the core. Acknowledgements This book was originally planned as a joint work with Nils Lid Hjort (University of Oslo), and his influence will be immediately apparent to those who have seen Hjort (1986), a limited circulation report. My own interest in neural networks was kindled by the invitation from Ole Barndorff-Nielsen and David Cox to give a short course at SemStat in 1992, which resulted in Ripley (1993). The book was planned and parts were written during a six-month period of leave at the programme on 'Computer Vision' at the Isaac Newton Institute Those hardy perennials, the 'exclusive or' and 'two spirals' problems, do not appear in this book.\"]\n",
      "['Some of the software used is supplied with Venables & Ripley (1994). Preface Xl for the Mathematical Sciences in Cambridge (England) ; discussions with the participants helped shape my impressions of leading-edge pattern recognition problems. Discussions with Lionel Tarassenko, Wray Buntine, John Moody and Chris Bishop have also helped to shape my treatment. I was introduced to the machine-learning literature and its distinctive goals by Donald Michie. Several people have read and commented on chapters, notably Phil Dawid, Francis Marriott and Ruth Ripley. I am grateful to Lionel Tarassenko and his coauthors for the cover picture of outlier detection in a mammogram (from Tarassenko et al., 1995). Parts of this book have been used as source material for graduate lectures and seminar courses at Oxford, and I am grateful to my students and colleagues for feedback ; present readers will appreciate the results of their insistence on more details in the mathematics. The examples were', 'appreciate the results of their insistence on more details in the mathematics. The examples were computed within the statistical system S-Pius of MathSoft Inc., using software developed by the author and other contributors to the library of software for that system (notably Trevor Hastie and Rob Tibshirani). It has been a pleasure to work with CUP staff on the design and production of this volume; especial thanks go to David Tranah, the editor for this project who also contributed many aspects of the design. B. D. Ripley Oxford, June 1995']\n",
      "[\"Notation The notation used generally follows the standard conventions of mathematics and statistics. Random variables are usually denoted by capital letters; if X is a random variable then x denotes its value. Often bold letters denote vectors, so x = (xi) is a vector with components xi, i = 1, ... , m, with m being deduced from the context. ~ E !(A) Np{,u, :E} O(g(n)) Op(g(n)) ((j p(x) Pr{A} Pr{A I B} .IRm xT e e,e []+ l J r 1 is the 'doubt' report. denotes expectation. A suffix denotes the random variable or distribution over which the averaging takes place. is the indicator function of event A, one if A happens, otherwise zero. denotes a normal distribution in p dimensions. f(n) = O(g(n)) means lf(n)/g(n)l is bounded as n-oo. Xn = Op(g(n)) means given E > 0 there is a constant B such that Pr{IXn/g(n)l > B} < E for all n. is the outlier report. denotes a probability density function. denotes the probability of an event A. denotes the conditional probability of A given B. m\", 'denotes the probability of an event A. denotes the conditional probability of A given B. m -dimensional Euclidean space . denotes the transpose of a matrix X. a parameter or vector of parameters. a parameter estimate. the positive part, the maximum of the expression and zero. the integer part (rounding down). The floor function. the nearest integer (rounding up). The ceiling function.']\n",
      "['1 Introduction and Examples This book is primarily about pattern recognition, which covers a wide range of activities from many walks of life. It is something which we humans are particularly good at; we receive data from our senses and are often able, immediately and without conscious effort, to identify the source of the data. For example, many of us can recognize faces we have not seen for many years, even in disguise, recognize voices over a poor telephone line, as babies recognize our mothers by smell, distinguish the grapes used to make a wine, and sometimes even recognize the vineyard and year, identify thousands of species of flowers and spot an approaching storm. Science, technology and business has brought to us many similar tasks, including diagnosing diseases, detecting abnormal cells in cervical smears, recognizing dangerous driving conditions, identifying types of car, aeroplane, ... , identifying suspected criminals by fingerprints and DNA profiles, reading Zip codes (US', \"... , identifying suspected criminals by fingerprints and DNA profiles, reading Zip codes (US postal codes) on envelopes, reading hand-written symbols (on a penpad computer), reading maps and circuit diagrams, classifying galaxies by shape, picking an optimal move or strategy in a game such as chess, identifying incoming missiles from radar or sonar signals, detecting shoals of fish by sonar, checking packets of frozen peas for 'foreign bodies', spotting fake 'antique' furniture,\"]\n",
      "[\"2 1 Introduction and Examples deciding which customers will be good credit risks and spotting good opportunities on the financial markets. Humans can (and do) do some of the tasks quite well, but the technological pressure is to build machines which can perform such tasks more accurately or faster or more cheaply than humans, or even to release humans from drudgery. There are also purely technological tasks such as reading bar codes at which humans are poor. Pattern recognition is the discipline of building such machines: 'It is felt that the decision-making processes of a human being are somewhat related to the recognition of patterns; for example the next move in a chess game is based upon the present position on the board, and buying or selling stocks is decided by a complex pattern of information. The goal of pattern recognition research is to clarify these complicated mechanisms of decision-making processes and to automate these functions using computers. However, because of the\", \"decision-making processes and to automate these functions using computers. However, because of the complex nature of the problem, most pattern recognition research has been concentrated on more realistic problems, such as the recognition of Latin characters and the classification of waveforms.' (Fukunaga, 1990, p. 1) Since the best humans can perform many of these tasks very well, even better than the best machines, it has been of great interest to understand how we do so, and this is of independent scientific interest. So there has for many years been an interchange of ideas between engineers building pattern recognition systems and psychologists and physiologists studying human and animal brains. Twice this has led to great enthusiasm about machines influenced by ideas from psychology and biology. The first was in the late 1950s with the perceptron, the second in the mid 1980s over neural networks. Both rapidly left their biological roots, and were studied by mathematical techniques\", \"networks. Both rapidly left their biological roots, and were studied by mathematical techniques against engineering performance goals as pattern recognizers. This book is not about the impact of the study of neural networks as models of animal brains, but discusses what are more accurately (but rarely) called artificial neural networks which have been developed by a community which was originally biologically motivated (although many 'neural network' methods were not). Thus for the purposes of this book, a neural network is a method which arose or was popularized by the neural network community and has been or could be used for pattern recognition. Many of the originators of the current wave of interest were more careful in their terminology; whereas Hopfield (1982) did talk about neural networks, Rumelhart & McClelland (1986) used the term 'parallel distributed processing', and 'connectionist' has also been popular (for example, see Hinton, 1989a). Marginal notes such as this replace\", 'has also been popular (for example, see Hinton, 1989a). Marginal notes such as this replace footnotes and offer explanation, sidelines, and opinion. Many of the ideas had arisen earlier in the pattern recognition context, but without the seductive titles had made little impact.']\n",
      "['Gooseberries are the fruits of the species Ribes grossulari a. We should never underestimate the power of simply remembering some or all of the examples and comparing test examples with our memory. 1 Introduction and Examples 3 One characteristic of human pattern recognition is that it is mainly learnt. We cannot describe the rules we use to recognize a particular face, and will probably be unable to describe it well enough for anyone else to use the description for recognition. On the other hand, botanists can give the rules they use to identify flowering plants. Most learning involves a teacher. If we try enough different wines from unlabelled bottles, we may well discover that there are common groupings, and that one group has the aroma of gooseberries (if the latter have been experienced) . But we will need a teacher to tell us that the common factor is that they were made (in part) from the sauvignon blanc grape. The discovery of new groupings is called unsupervised pattern', \"part) from the sauvignon blanc grape. The discovery of new groupings is called unsupervised pattern recognition. A more common mode of learning both for us and for machines is to be given a collection of labelled examples, known as the training set, and from these to distil the essence of the grouping. This is supervised pattern recognition and is used to classify future examples into one of the same set of classes (or say it is none of these). There is a subject known as machine learning which has emerged from the artificial intelligence and computer science communities. It too is concerned with distilling structure from labelled examples , although the labels are usually 'true' and 'false'. 'Machine Learning is generally taken to encompass automatic learning procedures based on logical or binary operations, that learn a task from a series of examples.' 'Machine Learning aims to generate classifying expressions simple enough to be understood easily by humans. They must mimic human\", \"classifying expressions simple enough to be understood easily by humans. They must mimic human reasoning sufficiently well to provide insight into the decision process. Like statistical approaches, background knowledge may be exploited in development , but operation is assumed without human intervention.' (Michie et al., 1994, p. 2) This stresses the need for a comprehensible explanation, which is needed in some but not all pattern recognition tasks. We have already noted that we cannot explain our identification of faces, and to recognize Zip codes no explanation is needed, just speed and accuracy. This quotation mentions statistical approaches, and statistics is the oldest of the disciplines concerned with automatically finding structure in examples. As in the quotation, statistics is often thought of as being less automatic than the other disciplines , but this is largely an artefact of its greater age; its current research frontiers are very much concerned with replacing the human\", 'of its greater age; its current research frontiers are very much concerned with replacing the human choice of methods by computation . Furthermore, statistics encompasses what the community of statisticians do, of whom your author is one!']\n",
      "[\"4 1 Introduction and Examples 1.1 How do neural methods differ? Assertions are often made that neural networks provide a new approach to computing, involving analog (real-valued) rather than digital signals and massively parallel computation. For example, Haykin (1994, p. 2) offers a definition of a neural network adapted from Aleksander & Morton (1990): 'A neural network is a massively parallel distributed processor that has a natural propensity for storing experiential knowledge and making it available for use. It resembles the brain in two respects: 1. Knowledge is acquired by the network through a learning process. 2. Interneuron connection strengths known as synaptic weights are used to store the knowledge.' In practice the vast majority of neural network applications are run on single-processor digital computers, although specialist parallel hardware is being developed (if not yet massively parallel). However, all the other methods we consider use real signals and can be\", 'not yet massively parallel). However, all the other methods we consider use real signals and can be parallelized to a considerable extent; it is far from clear that neural network methods will have an advantage as parallel computation becomes common, although they are frequently so slow that they need a speed-up. (Parallelization on real hardware has proved to be non-trivial; see Pitas, 1993 and Przytula & Prasanna, 1993.) We will argue that a large speed-up can be achieved by designing better learning algorithms using experience borrowed from other fields. The traditional methods of statistics and pattern recognition are either parametric based on a family of models with a small number of parameters, or non-parametric in which the models used are totally flexible. One of the impacts of neural network methods on pattern recognition has been to emphasize the need in large-scale practical problems for something in between, families of models with large but not unlimited flexibility', \"problems for something in between, families of models with large but not unlimited flexibility given by a large number of parameters. The two most widely used neural network architectures, multi-layer perceptrons and radial basis functions (RBFs ), provide two such families (and several others already existed in statistics). Another difference in emphasis is on 'on-line' methods, in which the data are not stored except through the changes the learning algorithm has made. The theory of such algorithms is studied for a very long stream of examples, but the practical distinction is less clear, as this stream is made up either by repeatedly cycling through the training set or by sampling the training examples (with replacement). In contrast, methods which use all the examples together are called 'batch' methods. Many neural networks are excluded by this definition, including those of Kohonen. One could ask how a machine comes to have 'natural' properties. The name 'multi-layer\", \"of Kohonen. One could ask how a machine comes to have 'natural' properties. The name 'multi-layer perceptrons' is confusing; they are not multiple layers of perceptrons. We call them feed-forward neural nets.\"]\n",
      "[\"Someone else may have made the measurements for us. It may help to know which classes are plausible. This might be unrealistic for hand-written addresses, and is well beyond current performance levels. 1.2 The pattern recognition task 5 It is often forgotten that there are intermediate positions, such as using small batches chosen from the training set. 1.2 The pattern recognition task Except in Chapter 9 we will be exclusively concerned with supervised pattern recognition. Thus we are given a set of K pre-determined classes, and assume (in theory) the existence of an oracle that could correctly label each example which might be presented to us. When we receive an example, some measurements are made, known as features, and these data are fed into the pattern recognition machine, known as the classifier. This is allowed to report 'this example is from class t' or 'this example is from none of these classes' or 'this example is too hard for me'. The second category are called outliers\", \"of these classes' or 'this example is too hard for me'. The second category are called outliers and the third rejects or 'doubt' reports. Both can have great importance in applications . Suppose we have a medical diagnosis aid. We would want it to report any patient who apparently had an unknown disease, and we would also want it to ask the opinion of a senior doctor if there was real doubt. Often rejects are referred to a more expensive second tier of classification, perhaps a human or (as in Zip code recognition) a slower but more powerful method or even (as in analytical chemistry) for more expensive measurements to be made. Many pattern recognition systems always make a firm classification , but this seems to us more often to be bad design than a conscious decision that a firm decision was necessary. The primary assessment of a system will be by its performance; a Zip code recognition system might be required to reject less than 2% of the examples and mis-read less than 0.5% of\", \"system might be required to reject less than 2% of the examples and mis-read less than 0.5% of the remainder . In medical diagnosis we will be more interested in some errors than others, in particular in missing a disease, so the errors will need to be weighted. There may be a cost trade-off between rejection and error rate. The other aspect of performance stressed in the quote from Michie et al. (1994, p. 2) is the power of explanation. Users need to have confidence in the system before it will be adopted. No one really cares if an odd letter is mis-routed , but patients do care if they mis-diagnosed , and when a civilian airliner is mistaken for an enemy aircraft, questions are raised. So for some tasks 'black boxes' are unacceptable whatever their performance advantage (possibly even if they appear perfect on test). The methods of Chapters 7 and 8 are often found to be more acceptable for such tasks.\"]\n",
      "['6 1 Introduction and Examples Some tasks are slightly different. We (and medics) often think of medical diagnosis as deciding which disease a patient has, but this ignores the possibility of two or more concurrent diseases; what we should really be asking is whether the patient has this disease for each of a range of diseases. This can be thought of as a compound decision, the classes each being a subset of the diseases, but it is normally helpful to make use of special structure within the classes. Design issues Although most of this book is about designing the pattern recognition machine, often the most important aspect of design is to choose the right features. If the wrong things are measured (or, more often these days with digital data, if the data are condensed too much) the task may be unachievable. Much of the enhanced success of Zip-code recognition systems has come from better features (for example, Simard et al., 1993) rather than through more complicated classifiers.', 'features (for example, Simard et al., 1993) rather than through more complicated classifiers. Sometimes good features can be found by training a classifier on a large number of features and extracting good ones (for example, by the methods of Chapters 9 and 10), but most often problem-specific insights are used. In a few problem domains very specific rules are known which can be used to design a classifier; as an extreme example compilers can classify C programs as correct or invalid without needing to see any previous programs. Such information is often in the form of a formal grammar, and systems based on specifying such grammars are often called syntactic pattern recognition systems (Fu, 1982; Gonzalez & Thomason, 1978), but are of very restricted application. Allowing stochastic grammars in which the structure is given but the probabilities are learnt allows a little more flexibility. Chou (1989) gives an example of recognizing typeset mathematical expressions using a stochastic', 'Chou (1989) gives an example of recognizing typeset mathematical expressions using a stochastic grammar. In the vast majority of applications no structural assumptions are made, all the structure in the classifier being learnt from data. In the pattern recognition literature this is known as statistical pattern recognition. The training set is regarded as a sample from a population of possible examples, and the statistical similarities of each class extracted, or more precisely the significant differences between classes are found. A parametric or non-parametric model is constructed for the distribution of features for examples from each class, and statistical decision theory used to find an optimal classification. This is sometimes known (Dawid, 1976) as the sampling paradigm.']\n",
      "['Note that this is not the procedure called cross-validation, despite the misuse of that term in the neural networks literature. 1.2 The pattern recognition task 7 Another view, the diagnostic paradigm, goes back in the statistical literature at least to Cox (1958), and was developed in medical applications by Jerome Cornfield. This said that we were not interested in what the classes looked like, but only given an example in what the distribution over classes is for similar examples. The main method of this approach became known as logistic discrimination (Anderson, 1982), but was never widely known even in statistics and (as far as we could ascertain) appears in no pattern recognition text. This is the main approach of the neural network school. When humans are learning concepts, we are often able to ask questions or to seek the classifications of examples which we synthesize (this being a paradigm of experimental science). Alternatively, we may describe our understanding to an', \"being a paradigm of experimental science). Alternatively, we may describe our understanding to an expert, who will then supply a counter-example. Can we allow our machines to do the same? The idea has occurred in machine learning (Angluin, 1987, 1988, 1993), but apparently only for learning logical concepts. We will sometimes have qualitative knowledge about the task in hand; we might know that only the sign of one of the features was material, or that the probability of a positive outcome was increasing in some continuous feature. Of course we should design the classifier to agree with such information, which Abu-Mostafa (1990, 1993, 1995a, b, c) calls 'hints'. Sometimes this is easy (just use the sign of the feature) but it can be very difficult (as in monotonicity). Generally hints (if true) help to avoid over-fitting to the training set, and this seems to be the real explanation of the gains in exchange-rate performance observed by Abu-Mostafa (1995a). Method tuning and checking\", \"the gains in exchange-rate performance observed by Abu-Mostafa (1995a). Method tuning and checking All methods have some knobs which can be tweaked. Sometimes taking the class of the nearest training-set example is regarded as a fully automatic method, but we need to specify the metric used to find the nearest. (If the answer is 'use Euclidean distance' we still have to specify the units of measurement.) How should those knobs be set? The most obvious way is to choose them to maximize performance. One thing we should not do is to evaluate the performance on a test set and choose the bestperforming classifier, since we will then have no way to measure the true performance. We can keep back another test set, called a validation set, and use the performance on that to set the knobs. However, to obtain a sensitive measure of the performance, the validation set will\"]\n",
      "['8 1 Introduction and Examples need to be very large, and this is data which could otherwise be used for learning. This problem has been ignored for a long time, but now methods to use the training set for both learning and knob-setting are beginning to be used. These are discussed in Chapter 2 and illustrated on the quite small running examples that we chose. To see why this is a real issue, consider Figure 1.1. Without knowing the true curve, it is hard to tell which of plots (b) and (c) is closer to the truth. ~ ~ .. .. ... . . .-·.·:., .. ... ... ~ ... .. . ··,: ... .. . . . ... . . . ~ ~ 0.0 0.5 1.0 1.5 2.0 2.S 3.0 0.0 0.5 1.0 1.5 2.0 (a) (b) . :\\' .-:, \\' .... ·. : \\' ~ \\' : ~ 0! . •, ·. :\\' . ···. :,· i. ~1 ~1 . . .·.\\' ~~\\'-------- ------ ------~ \";\\' 0.0 0.5 1.0 3.0 (c) (d) Performance assessment We will often want to choose between different candidate classifiers, and it will be usual to check that the performance targets are likely to be met. This needs an experimental test of the', 'to check that the performance targets are likely to be met. This needs an experimental test of the classifiers on some unseen examples. Such experiments are often (usually?) very poorly designed, and slanted towards a favourite method. The reader is urged to consult a good book on experimental design (such as Box et al., 1978) before conducting such experiments. Many of the experiments reported in the literature are designed to compare methods, when there is even more scope for confusion. In Figure 1.1 : An illustration of model selection. Plot (a) shows 250 points generated by from the curve shown plus random noise, and plots (b-d) show fits by a single-hidden-layer neural network with 2, 4 and 8 hidden units.']\n",
      "[\"This test does not consider experimental biases nor if an evaluation of the significance of the results was made. 1.3 Overview of the remaining chapters 9 medicine, methods (treatments) are compared in double-blind trials so there can be no preferential treatment , and in pure science experiments must be repeatable. (The large-scale trial of the StatLog project reported in Michie et al., 1994, was designed to be run in these ways.) One source of confusion is that such trials may confuse the merits of the methods with the expertise of the experimenter in using them; this is a particular difficulty when the experimenter's own invention is in the trial. Two cases are of interest. One is where every method is used by a real expert and so assesses the best attainable performance. The other is when all methods are used by typical (or even new) users, which might provide a basis for recommendations to such users. Prechelt (1994) surveyed two le~1.ding neural network journals for 1993 and half\", \"to such users. Prechelt (1994) surveyed two le~1.ding neural network journals for 1993 and half of 1994. He deemed an evaluation of an algorithm acceptable if it used two or more realistic or real problems and compared at least one alternative algorithm . Only 18% passed-in his words 'sad, but true'. Note that this book is not about evaluating algorithms , but we have used real examples to explore the merits and limitations of the methods. Amazingly, almost all books on pattern recognition or neural networks include no real or realistic examples. 1.3 Overview of the remaining chapters Our approach to building a classifier will be based on statistical decision theory. In Chapter 2 we consider the Bayes rule, the best possible classifier if we knew everything about the population of examples , and then various approximations we can make if we have to learn from a training set. This includes several ways to use parametric models (which we assume to be false but perhaps convenient\", 'includes several ways to use parametric models (which we assume to be false but perhaps convenient approximations) ; these sections include the classic methods based on the multidimensional normal distribution but also some improvements which are much less well known. The next questions are: how complicated do our models need to be, and how well do they perform? These are discussed in Sections 2.6 and 2.7. There is a trade-off between adapting well to complexity of the real structure in the examples and fitting the structure of our particular training set (Figure 1.1). This explains why we are not interested in the usual asymptotics of mathematical statistics; as we receive more data we will want to choose more complicated models, and only limit the model complexity to avoid over-fitting the current training set. Another view of the effect of model flexibility on over-fitting is the study of generalization in Section 2.8.']\n",
      "['10 1 Introduction and Examples Chapters 3 to 5 make weaker assumptions than standard parametric models. In Chapter 3 we study how we could use linear methods. Both Chapters 4 and 5 discuss how to apply flexible families of functions from the feature space !![ to d-dimensional Euclidean space .JRd, building on the linear methods, and consider the commonest such families, neural networks and radial basis functions, as well as splines and their generalizations. The sixth chapter is on (nearly) non-parametric methods, where minimal assumptions are made about the classes. Most of these methods are based on looking at the classes of nearby examples, in some methods after designing a set of representative examples to replace the training set. That chapter also includes the use of mixtures of densities to model very general distributions. Chapter 7 is about a rather different class of methods that partition the feature space !![ into regions and assign a class to each. This is done by', \"that partition the feature space !![ into regions and assign a class to each. This is done by splitting along a feature at a time, and then subdividing each subregion recursively. Classification trees have been considered in both statistics and machine learning; they are often easy to interpret but not amongst the highest performers. Belief networks, also known as causal probability networks and Bayes networks, are not primarily designed for classification, but to explain the relationships between all of the observations. They are the subject of Chapter 8. They are very good for explanation, but may be less good for classification (as the finite amount of training data has to be used to learn more structure than just the relationship of the class to their features). Their strength is that they can incorporate qualitative knowledge about causal relationships amongst the features (an earlier and more sophisticated use of 'hints'). Also included in that chapter are the methods of\", \"earlier and more sophisticated use of 'hints'). Also included in that chapter are the methods of Boltzmann machines and hierarchical mixtures of experts which can be considered within the framework of belief nets. Chapters 9 and 10 are concerned with finding good features and choosing which features to use. The appendix discusses a number of complements; some are statistical background and some explore issues a little further than is needed for pattern recognition. 1.4 Examples The examples have been chosen to illustrate the properties of the methods we describe; not every method is used on each. . . . and many more names beside\"]\n",
      "[\"Figure 1.2: Results of two diagnostic tests on patients with Cushing's syndrome. Cushing's syndrome 0 ;:; 1.4 Examples u b b b b 10 T etrahydrocortisone 11 50 These data are taken from Aitchison & Dunsmore (1975, Tables 11.1-3) on diagnostic tests on patients with Cushing's syndrome , a hypersensitive disorder associated with over-secretion of cortisol by the adrenal gland. This dataset has three recognized types of the syndrome represented as a, b, c. (These encode 'adenoma' , 'bilateral hyperplasia ' and 'carcinoma' , and represent the underlying cause of over-secretion. This can only be determined histopathologically.) The observations are urinary excretion rates (mg/24h) of the steroid metabolites tetrahydrocortisone and pregnanetriol , and are considered on log scale. One of the patients of unknown type (marked u) was later found to be of a fourth type, and another was measured faultily. Titterington (1976) discusses a different dataset which· had 87 patients, five types, and\", \"faultily. Titterington (1976) discusses a different dataset which· had 87 patients, five types, and fifteen measurements per patient, which suggests the current dataset is an abstraction of the full problem. Synthetic two-class problem This is a 'realistic' problem from Ripley (1994a), used there (and here) to illustrate how methods work. There are two features and two classes; each class has a bimodal distribution as should be clear from Figure 1.3. The class distributions were chosen to allow a best-possible error rate of about 8%, and are in fact equal mixtures of two normal distributions. The component normal distributions have a common covariance matrix.\"]\n",
      "['12 ., \"\\' 1\\'l ~ ~ § ~ 0 ~ \"\\' ci 0 ci 120 .• ·1.0 1 Introduction and Examples 0 • 0 . . ./• .· .. .. . ,.. . .. . .. . ~ . ·. -0.5 0.0 oo o:\\'o • Ba.,on , .~s .. . .· co0o,c 0 ... .. 0.5 1.0 --·---·· 140 160 160 200 220 total residue ~ --= ---Herd Tobr Toba Furo virus group 240 Viruses This is a dataset on 61 viruses with rod-shaped particles affecting various crops (tobacco, tomato, cucumber and others) described by Fauquet et al. (1988) and analysed by Eslava-G6mez (1989). There are 18 measurements on each virus, the number of amino acid residues per molecule of coat protein; the data come from a total of 26 sources. There is an existing classification by the number of RNA molecules and mode of transmission, into 39 Tobamoviruses with monopartite genomes spread by contact, 6 Tobraviruses with bipartite genomes spread by nematodes, 3 Hordeiviruses with tripartite genomes, transmission mode unknown and 13 \\'furoviruses\\', 12 of which are known to be spread fungally. Figure 1.3: Two-class', \"unknown and 13 'furoviruses', 12 of which are known to be spread fungally. Figure 1.3: Two-class synthetic data from Ripley (1994a). The two classes are shown by solid circles and open squares: there are 125 points in each class. Figure 1.4: Histogram and boxplot by group of the viruses dataset. A boxplot is a representation of the distribution; the central grey box shows the middle 50% of the data, with median as a white bar. 'Whiskers' go out to plausible extremes, with outliers marked by bars. No experimental details are provided in the source.\"]\n",
      "['1.4 Examples 13 The question of interest to Fauquet et al. was whether the furoviruses form a distinct group, and they performed various multivariate analyses. One initial question with this dataset is whether the numbers of residues are absolute or relative. The data are counts from 0 to 32, with the totals per virus varying from 122 to 231. The average numbers for each amino acid range from 1.4 to 20.3. As a classification problem, this is very easy as Figure 1.4 shows. The histogram shows a multimodal distribution, and the boxplots show an almost complete separation by virus type. The only exceptional value is one virus in the furovirus group with a total of 170; this is the only virus in that group whose mode of transmission is unknown and Fauquet et al. (1988) suggest it has been tentatively classified as a Tobamovirus. The other outlier in that group (with a total of 198) is the only beet virus. The conclusions of Fauquet et al. may be drawn from the totals alone. It is', 'is the only beet virus. The conclusions of Fauquet et al. may be drawn from the totals alone. It is interesting to see if there are subgroups within the groups, so we will only use this dataset in Chapter 9, principally to investigate further the largest group (the Tobamoviruses ). There are two viruses with identical scores, of which only one is included in the analyses. (No analysis of these data could differentiate between the two.) Leptograpsus crabs Campbell & Mahon (1974) studied rock crabs of the genus Leptograpsus. One species, L. variegatus, had been split into two new species, previously grouped by colour form, orange and blue. Preserved specimens lose their colour, so it was hoped that morphological differences would enable museum material to be classified. Data are available on 50 specimens of each sex of each species, collected on sight at Fremantle, Western Australia. Each specimen has measurements on the width of the frontal lip FL, the rear width RW, and length along', 'specimen has measurements on the width of the frontal lip FL, the rear width RW, and length along the midline CL and the maximum width CW of the carapace, and the body depth BD in mm. Forensic glass Our next example comes from forensic testing of glass collected by B. German on 214 fragments of glass, and taken from Murphy & Aha (1995). Each case has a measured refractive index and composition (weight percent of oxides of Na, Mg, AI, Si, K, Ca, Ba and Fe). The fragments were originally classed as seven types, one of which was absent in this dataset. The categories which occur are window float glass (70), window non-float glass (76), vehicle window glass (17), containers']\n",
      "['14 1 Introduction and Examples (13), tableware (9) and vehicle headlamps (29). The composition sums to around 100%; what is not anything else is sand. Refractive Index ~ -~ ~ ---.,., - ;! .,., ~ = ; $ ~ 19 ~ a ~ ~ !\" ~ ..:.. -~ WtnF WlnNF Veh Con Tabl Head \"\\'nF Aluminium ~ ~ :e 0 ~ .,., .,., ~ ~ ;! N .,., 0 -I .,., ;? iliJ N ~ = s ~ ~ 0 $ ~ -~ -;:: ~ --..:.. ~ - ~ -W1nF WinNF Voh Con Tabl Head WlnF Calcium ~ 0 ,; ;\\': - ~ N !\" 0 i N ~ ~ \"\\' r::J -¢J $ ~ Q ~ ..:.. ~ ~ 0 0 = 0 WmF WW<F Veh Con Tabl Head \"\\'nF Sodium = .,., ~ Cl !!: Q ~ -WW<F Voh Con T ... Silica .,., $ t!!l ~ ! ~ -~ WmNF Veh Con Tabl Barium iii ;::::; --WinNF Veh Con Tobl .,., ~ ..:.. Hoad ~ ..:.. Head ~ ~ Head Magnesium 9 il i:5 \"\\' -Winf WII\\'INF Veh Con Tab! Heed Potassium WlnF WlnNF Veh Con Tabl H .. d Iron ~--------~-----. 0 Figure 1.5 shows boxplots of the features. Some discrimination between glass types is apparent even from single features; for example headlamp glass is high in barium (although some examples have', 'from single features; for example headlamp glass is high in barium (although some examples have none), high in sodium and aluminium and low in iron. The three types of window glass appear similar, with one exceptional fragment of window non-float glass having a high refractive index, high barium and calcium and low magnesium and sodium. The containers group also contains a couple of exceptions. Characterizing populations with exceptions (especially 2 out of 13) can be difficult, and it may be easier to remove the exceptions in the training phase. This example is really too small to divide, so methods have been Figure 1.5: Boxplots of the features of the forensic glass data. assessed by 10-fold cross-validation using the same random partition This is discussed in for each method. The best methods have an estimated error rate of Section 2.7. about 24%. Diabetes in Pima Indians A population of women who were at least 21 years old, of Pima Indian heritage and living near Phoenix, Arizona,', 'of women who were at least 21 years old, of Pima Indian heritage and living near Phoenix, Arizona, was tested for diabetes according to World Health Organization criteria. The data were collected']\n",
      "['1.5 Literature 15 by the US National Institute of Diabetes and Digestive and Kidney Diseases, and are available from Murphy & Aha (1995). A previous report by Smith et al. (1988) found an error rate of about 24%. The reported variables are number of pregnancies plasma glucose concentration in an oral glucose tolerance test diastolic blood pressure (mm Hg) triceps skin fold thickness (mm) serum insulin ( f1 U /ml) body mass index (weight in kg/(height in m)2) diabetes pedigree function age m years Many of these had zero values where these were impossible, so are taken to be missing values. Of the 768 records, 376 were incomplete (most prevalently in serum insulin). Most of our illustrations omit serum insulin and use the 532 complete records on the remaining variables. These were randomly split into a training set of size 200 and a test set of size 332. Methods which can deal with missing values were given 100 of the incomplete cases as part of the training set. Note that 33% of the', 'values were given 100 of the incomplete cases as part of the training set. Note that 33% of the population were reported to have diabetes, so an error rate of 33% can be achieved by declaring all test cases to be non-diabetic. Our best methods reduce this to about 20%. Some aspects of this dataset were considered by Wahba et al. (1995). Data availability All these datasets are available by anonymous ftp from the Internet site ftp.stats.ox.ac.uk IP address 163 .1. 20.1 in directory /pub/PRNN. The datasets and other material are available by pointing your World Wide Web browser at http://www.stats.ox.ac.uk/-ripley/PRbook/ 1.5 Literature The classic books on pattern recogmtwn are Duda & Hart (1973), Devijver & Kittler (1982) and Fukunaga (2nd edn 1990), all of which pre-date the impact of neural networks on the subject. There are a small number of introductory texts (James, 1988; Therrien, 1989; Schalkoff, 1992) and two specialist monographs on kernel methods']\n",
      "['16 1 Introduction and Examples (Hand, 1982; Coomans & Broeckaert, 1986). Some conference proceedings, for example Devijver & Kittler (1987), provide a good overview of applications. Classical statistical techniques are discussed in most texts on multivariate analysis such as T. W. Anderson (1984) and Mardia et al. (1979) and in slightly more specialized books by Lachenbruch (1975), Goldstein & Dillon (1978), Hand (1981) and McLachlan (1992). There are now very many books on neural networks, particularly on parts of the subject not discussed here. Approaches to modelling memory from the point of view of statistical physics are covered by Amit (1989), Peretto (1992) and Hertz et al. (1991). Haykin (1994) is modern, comprehensive but unselective (and untroubled by real applications). Amari (1993) and Ripley (1993) give two statistical views of the neural network field, and Bishop (1995a) is slanted towards pattern recognition. Arbib (1995) provides many short sketches of topics over a', 'is slanted towards pattern recognition. Arbib (1995) provides many short sketches of topics over a very wide range of neural networks. One important area of neural network methods which we do not consider is the prediction of time series, the subject of a competition analysed by Weigend & Gershenfeld (1993), including expository papers. There is now one text on general machine learning, Langley (1996), and it appears in some artificial intelligence texts (for example, Winston, 1992; Russell & Norvig, 1995). There are many more aspects than we shall consider, including incorporating domain knowledge as illustrated by King et al. (1992). Langley & Simon (1995) and Bratko & Muggleton (1995) discuss applications of machine learning with claimed real-world benefits. Books which cover more than one of these three areas are rare. Krishnaiah & Kanal (1982) was a very good overview at its time; the recent edited volumes by Cherkassky et al. (1994) and Michie et al. (1994) contain several good', 'the recent edited volumes by Cherkassky et al. (1994) and Michie et al. (1994) contain several good overviews. Face recognition is a popular application of pattern recognition surveyed by Samal & Iyengar (1992). Golomb et al. (1991) and Flocchini et al. (1992) give two example systems. There is a very large literature on character recognition, and nonEuropean alphabets with at least hundreds of classes provide a severe test of pattern recognition methods. The articles by Baird (1993), Cohen et al. (1991), Le Cun et al. (1989, 1990a), Gader et al. (1991), Guyon et al. (1992), Impedovo et al. (1991), Knerr et al. (1991), Lee (1991), Martin & Pitman (1990, 1991), Pavlidis (1993), Simard et al. (1993), Singer & Tishby (1994), Suen et al. (1992, 1993) and Wakahara (1993) provide some flavours.']\n",
      "[\"We regard probability mass functions of discrete distributions as densities. 2 Statistical Decision Theory This chapter presents basic statistical decision theory for classification problems with predefined classes. The framework in its simplest form is as follows. Certain objects are to be classified as coming from one of a fixed number of types, or classes, say 1, ... , K. Each object gives rise to certain measurements which together form the feature vector X, belonging to a suitable feature space f£. This is typically a subset of .IR..P or perhaps of the type f£ 1 x · · · x f£ P with each f£ J either a given finite set or .IR. The proportion of class k cases in the population under study is some known or unknown nk. Feature vectors from class k are distributed according to the density Pk(x). The task is to classify an object, which means reaching one of K +2 possible decisions 1, ... ,K,~,(!) on the basis of the observed value X = x; decision k corresponds to claiming 'X is from\", \",K,~,(!) on the basis of the observed value X = x; decision k corresponds to claiming 'X is from class k ', whereas ~ means 'being in doubt', possibly postponing the decision until further measurements have been extracted , and (!) signifies an outlier, an object definitely not belonging to any of the K predefined classes. Section 2.1 treats the idealized case when class densities Pk(x) as well as class prior probabilities (nk) are known. This gives valuable insight and also provides limits for the performance of real-life classifiers that in some way must estimate class densities. In Section 2.2 some of the most important parametric models for classification are studied. The parameters are typically estimated using maximum likelihood, but alternatives are discussed in Section 2.3 (assuming less of the model), Section 2.4 (taking the variability of the parameters into account) and Section 2.5 (bias correction). Sections 2.6 and 2.7 discuss how we assess the adequacy of a parametric\", \"2.5 (bias correction). Sections 2.6 and 2.7 discuss how we assess the adequacy of a parametric model and estimate the performance of a classification rule. The final section considers 'generalization' , a more abstract way to find\"]\n",
      "[\"18 2 Statistical Decision Theory bounds on the expected performance of a class of classifiers. Not all of the material here is essential for the later applications. The most important sections are Section 2.1, Section 2.2 omitting 'Bayes risk consistency' and 'Fitting parametric families when they are wrong', Sections 2.3, 2.4, 2.6 and 2.7. 2.1 Bayes rules for known distributions In this present section we assume that the class densities Pk and the prior probabilities nk are known. This makes it possible to construct classification procedures with well understood optimal properties. Such results are not directly applicable since the class densities, at least, are unknown in practice, but they will serve as guidelines for the estimated rules of Sections 2.2 and 2.3, and have intrinsic theoretical interest. Let C denote the class label of a random feature vector X, in particular C = k with probability nk. The classification task is to estimate the true C after having observed X. Let c:\", \"probability nk. The classification task is to estimate the true C after having observed X. Let c: :!C ~ {1, ... ,K, .@} be a classification procedure (also known as a classifier). (We will deal with outlier decisions in a later subsection.) To determine whether such a procedure is 'good' or not one has to agree on reasonable overall criteria, for example involving the misclassification probabilities pmc(k) = Pr{c(X) =I= k, c(X) E {1, 0 0 0 ,K} I c = k} (2.1) and the reject or doubt probabilities pd(k) = Pr{c(X) = .@I C = k}. The quantities pmc and pd denote the unconditional misclassification probability Pr{c(X) =I= C} and doubt probability Pr{c(X) = .@} respectively. Minimizing the expected error rate The usual way of formalizing a goodness criterion is by means of a loss function. Let L(k, l) be the loss incurred by making decision l if the true class is C = k. One should have L(k,k) = 0 and maybe L(k, .@) = d for all k, whereas the other L(k, l) 's could in principle be any set of\", \"= 0 and maybe L(k, .@) = d for all k, whereas the other L(k, l) 's could in principle be any set of positive numbers. If every misclassification is equally serious, then Marginal notes point out the less important material. { 0 if l = k (correct decision), L(k,l) = 1 if l =I= k and l E {1, ... ,K} (wrong decision), d if l = fi2 (being in doubt), (2.2) Loss (2.2) is used unless otherwise stated. for k = 1, ... , K and l = 1, ... , K, .@, is a reasonable choice. In what follows we will often employ the loss function (2.2) to illustrate\"]\n",
      "['A medical example with assessed costs is Table 4 of Titterington et al. (1981, p. 154). 2.1 Bayes rules for known distributions 19 important concepts; it is often used when there is no way to assign more accurate costs. However, we should warn that its use when inappropriate can cause difficulties or even be dangerous; the costs of failing to spot a disease are usually very much higher than those of a false positive in a series of screening tests. The risk function for classifier c is the expected loss when using it, as a function of the unknown class k : R(C,k) = E[L(k,C(X))IC =k] K = L L(k, l) Pr{c(X) = 11 c = k} + L(k,9&) Pr{c(X) = 9& 1 c = k} 1=1 = pmc(k) + d pd(k). The total risk is the total expected loss, viewing both the class C and the vector X as random; K K R(c) = ER(c, C)= L nk pmc(k) + d L nk pd(k). (2.3) k=l k=l This is seen to be the overall misclassification probability plus d times the overall doubt (or reject) probability. It is also the long-term average loss, the', \"plus d times the overall doubt (or reject) probability. It is also the long-term average loss, the limit of n-1 L.'J=l L(Cj,C(Xj)), where {(Cj,Xj)} is a random sample of size n. For our first main result, let nk pk(x) p(klx)=Pr{C=kiX=x}= K (2.4) L.l=l n1 PI(x) be the posterior probability of class k given X = x. Then the following holds. Proposition 2.1 The classification rule which minimizes the total risk under loss (2.2) is c(x) = l,;;K { k if p(k I x) = maxp(ll x) and this exceeds 1-d, 9& if each p(k I x) ~ 1-d, (2.5) and for a general loss function is { k if this attains min L L(j, l)p(j l.x) < d, c(x) = I,;;K . J 9& otherwise. (2.6)\"]\n",
      "[\"20 2 Statistical Decision Theory Proof: We have R(c) = E [E[L(C,c(X)) I X]] = L E[L(C,c(x)) I X= x] p(x)dx where p(x) = L::f=l TrkPk(x) is the marginal density for X. It suffices to minimize the conditional expectation, which we can write as 'L:~= l L(k, c) p(k I x), with respect to c, for each x. For c = ~ we have 'L:k=l L(k, {J2) p(k I x) =d. Under loss (2.2) the minimand becomes 1 -p( 11 X), ... ' 1 -p(K I X), d when c = 1, ... ,K, {J2 respectively. Thus we look for the maximum of p( 11 x ), ... , p(K I x ), 1 -d, and find the solution given. D Under loss (2.2) another way to write the optimal rule is to choose the class with the highest nk Pk(x) provided this exceeds (1 -d) p(x), from (2.4). This optimal classifier is also referred to as the Bayes rule. When two or more classes attain the maximal p(k I x) the tie can be broken arbitrarily. The value R(c) of the total risk (2.3) for the Bayes rule is called the Bayes risk. This value is the best one can achieve if the nk 's and Pk\", \"the Bayes rule is called the Bayes risk. This value is the best one can achieve if the nk 's and Pk 's are known, and provides a benchmark for all other procedures. For two classes and without the doubt option the Bayes risk is Emin[p(11x),p(21x)], for any number of classes it is E[1-maxp(klx)]. Let r(x) = 1-maxkp(klx) . Then with the doubt option pmc = E{r(X)J[r(X) ~ d]}, pd = Pr{r(X) > d}, and the Bayes risk is R = pmc + dpd. The error-reject curve plots pmc against pd for varying d. Note (Chow, 1970) that pmc(d) = -ld ( dpd(O so pd(d) as a function of d determines all the performance quantities. Proposition 2.1 highlights the central role of the posterior probabilities. Most of the rest of the theory presented here can be regarded as ways to estimate or approximate the posterior probabilities from the training set. This definition follows Lehmann (1986), Devijver & Kittler (1982) and many others; another school, represented by Berger (1985), calls the total risk the Bayes risk, and\", 'many others; another school, represented by Berger (1985), calls the total risk the Bayes risk, and the Bayes risk the minimum Bayes risk. Fukunaga (1990) calls it the Bayes error.']\n",
      "['2.1 Bayes rules for known distributions 21 Example: Normal classes with common covariance matrix The most important distribution in statistical theory is the normal distribution, with the familiar density (.j2no-)-1exp[-!(x-,u)2jo-2] in the one-dimensional case. We write X \"\\' N{,u, o-2} to signify that X has this distribution, with mean parameter ,u and variance parameter o-2, and will also say that \\'X is normal (,u, o-2)\\'. In p dimensions the density is We write X \"\\' Np{,u, L} or say that X is normal (,u, L) when X is a pdimensional vector with this distribution. (Thus we omit the qualifying \\'multi\\' or \\'multivariate\\' that often is included.) The expected value is ,u and the covariance matrix is L (Mardia et al., 1979, p. 37). Suppose the feature vectors from class k are Np{.Uk. L}. If we disregard the doubt option a new feature vector x is allocated to the class k with smallest value of <5(x, ,Uk)2-2log nk. where <5(x,,uk) = [(x-,uk)TL-1(x- .Uk)]112 is (the definition of) the', 'value of <5(x, ,Uk)2-2log nk. where <5(x,,uk) = [(x-,uk)TL-1(x- .Uk)]112 is (the definition of) the Mahalanobis (1936) distance from x to the centre of class k. Since the quadratic term xTL-1 x is common to each class the optimal rule can be written minimize -2,u[L-1 x + ,u[L-1 ,Uk -2log nk over k = 1, ... , K. (2.8) This is called (the population version of, or the theoretical version of) linear discriminant analysis. If the classes are equally likely a priori then x is classified as coming from the nearest class, in the sense of having the smallest Mahalanobis distance to its mean. If in addition L is proportional to the identity matrix then distance can be Euclidean distance. The error rate for the optimal rule can be computed explicitly in the two-class case. One should allocate to class 1 whenever This can be reorganized as (2.9) where Ji = !(,u1 + ,u2). If X comes from class 1 then A\"\\' N{!<52,<52}, in terms of the Mahalanobis distance \"\\' { T -1 )}1/2 u = (,u1 -.U2) L (,u1 -.U2', 'A\"\\' N{!<52,<52}, in terms of the Mahalanobis distance \"\\' { T -1 )}1/2 u = (,u1 -.U2) L (,u1 -.U2 (2.10)']\n",
      "['22 2 Statistical Decision Theory between the two classes. Similarly, if X comes from class 2, then A\"\\' N{(-~<52, <52}. Accordingly pmc = 1qPr[N{ib2,b2}:::::; log(n2/nl)] + n2Pr[N{ -~<52, <52} > log(n2/nt)] = n1<1>( -~b +;! log(n2/nd) + n2<1>( -~b-! log(n2/n!)), where <I>(·) is the cumulative distribution function for the standard normal. Note that the error rate is expressed in terms of the onedimensional normal distribution even when the class distributions are p-dimensional normal. In the symmetric case with equal prior probabilities both class-wise error rates are equal, and the minimum attainable misclassification rate is pmc =<I>( -~b). Example: three Poisson groups Suppose there are three equally likely groups of Poisson data, with mean parameters A.1 = 10, A.2 = 15, A.3 = 20. Then the optimal rule is to allocate to class 1 if X :::::; 12, to class 2 if 13 :::::; X :::::; 17, and to class 3 if X ~ 18. The class-wise success rates, or probabilities of correct classification, are', 'to class 3 if X ~ 18. The class-wise success rates, or probabilities of correct classification, are pcc(1) = Pr{X:::::; 121 C = 1} = 0.792, pcc(2) = Pr{13:::::; X:::::; 171 C = 2} = 0.481, pcc(3) = Pr{X ~ 181 C = 3} = 0.703. The overall error rate is 0.341. Suppose next that one can obtain two independent measurements X1 and Xz from the object to be classified. How do the allocation rules and the error rates change? Some easy calculations show that one should allocate to class 1 if X :::::; 12.0, to class 2 if 12.5 :::::; X :::::; 17.0, and to class 3 if X ~ 17.5, where X = (X1 + X2)/2. The revised class-wise success rates are pcc(1) = Pr{X:::::; 12.01 C = 1} = 0.843, pcc(2) = Pr{12.5:::::; X:::::; 17.01 C = 2} = 0.640, pcc(3) = Pr{X ~ 17.51 C = 3} = 0.806. The overall error rate has been reduced to 0.237.']\n",
      "[\"This technique is known as multiple 'hot deck' imputation in survey sampling. 2.1 Bayes rules for known distributions 23 Remarks The constant d in (2.2) acts as a safety threshold, and should in principle be specified by the user of the resulting classifier. The inconveniences caused by a reject have to be judged against the consequences of a misclassification. In a serious application where the classifier is meant to work routinely on future examples one would typically try several d values on a training set of vectors with known classes, and obtain estimates of misclassification and doubt rates (see Section 2.7) before a 'final value' is chosen. Plotting misclassification rate against d is useful (see Figure 3.5 on page 114). If d is near zero then 'doubt' is inexpensive . This will lead to low error rates but on few classified vectors and a high doubt rate. If on the other hand d ~ 1 -1/K then decision £0 is so expensive that it never will be used. There are no restrictions on the\", \"-1/K then decision £0 is so expensive that it never will be used. There are no restrictions on the type of densities Pl, ... , PK ; in particular they need not be densities with respect to Lebesgue measure. (For professional probabilists : as long as Pk = dPk/dJ.l for some a-finite measure J.l dominating the class distributions Pt, ... , Pk both (2.4) and (2.5) continue to hold. We may in fact take J.l = I:f=I Pk-) Thus some or all of the Pk 's may have discrete components, they may represent normal distributions with singular covariance matrices, and so on. The small piece of theory presented here is fairly standard, although the rigorous derivation of the optimal reject ('doubt') region, by means of the loss function, is less known. The most popular special cases of the optimal rule are the normal distribution cases with common or different covariance matrices; see the example above and those discussed in Section 2.2. Indeed, discriminant or classification analysis started with a\", 'and those discussed in Section 2.2. Indeed, discriminant or classification analysis started with a sample version of (2.9), in Fisher (1936). He derived the best linear rule in the two-class case but from a different perspective; see Section 3.1. Missing values Some problems (such as the Pima Indians data) have examples with missing values for some of the features. In principle these are easily accommodated; just compute the posterior probabilities p(c I x*) using the observed features x·. However, these may be difficult to calculate. One technique is to simulate the missing features from p(x I x*) and average p(c I x) over the simulated values. For this to be possible, the marginal density p(x) must be known. If there are several missing features, the Gibbs sampler (Section A.3) may be used to allow them to be sampled one at a time.']\n",
      "[\"24 2 Statistical Decision Theory For normal classes both procedures are easy, as the distribution of some of the features is again joint normal, so we find a modified linear rule in the observed features. The density p(x I x*) is a mixture of normal distributions (one for each class) and so is easy to sample from. Simpler procedures are often used, such as replacing missing values by 'typical' values, for example by the average over observed values. This is potentially dangerous, as the conditional density p(y I x*) of a feature y may have a very different mean from the unconditional density. Missing values have been largely ignored in the pattern recognition literature. They are common in medical diagnosis, but rare in domains where data are collected automatically . It is a subject which has been treated most extensively in the literature on sampling surveys (Little & Rubin, 1987). There the problem may be that 'missing' actually indicates a refusal to respond, and so is informative\", \"the problem may be that 'missing' actually indicates a refusal to respond, and so is informative about the features. This can also occur in medical diagnosis, where the medical practitioner may not order a test whose outcome appears certain or not relevant to the diagnosis. It could also be that a feature is missing because it proved to be too difficult to measure. Note that informative missingness of y is only a problem if it indicates a departure from the distribution p(y I x*). Thus a missing test whose outcome could be predicted from the remaining features would not be a difficulty (although the medic may be predicting from qualitative data which are not recorded). On the other hand, the refusal to answer a test may well be unpredictable and so informative. Where this is suspected, often the only possible action is to code 'missing' as a value of the feature, and somehow to find the densities required using the expanded feature(s). Outliers The concept of outliers does not fit\", \"the densities required using the expanded feature(s). Outliers The concept of outliers does not fit cleanly into the decision-theory framework; one is supposed to have described the whole problem, and 'outliers' suggest incorrect specification. So one way forward is to anticipate outliers and build them into the specification as a separate class, with a specified TC(J) and class density p(J)(x). Where might these come from? As outliers express surprise, the class density should perhaps reflect ignorance , and so be a suitable uniform distribution over PI. This is likely to cause difficulties, as for many feature spaces the uniform distribution is not normalizable to a probability distribution. These can be circumvented; for example for PI = JRP we could take a normal distribution with a very large variance. However, the difficulties\"]\n",
      "[\"2.1 Bayes rules for known distributions 25 persist, as both the 'shape' of the variance (for p > 1) and the scale of the variance can affect dramatically the reporting of outliers. An alternative to assuming ignorance for the class of outliers is to follow the procedure we will use for all the other classes, and estimate 1T.(!) and p(!)(x) from the training set. Sometimes this is feasible; for instance in reading Zip codes and in object recognition, data are sufficiently plentiful to enable a representative sampling of outliers. But such training sets are not commonplace, and often training sets are collected under carefully controlled circumstances where outliers are less common than usual (or even removed entirely). Once the outlier distribution is given, under loss (2.2) outliers are declared if 1T.(!) Pm(x) ~ (1-d) p(x), max 1T.k pk(x) k If PCP is 'uniform' this classifies as an outlier when both p(x) = 2: 1T.k Pk(x) and each component is small. Another way to view an outlier would\", 'when both p(x) = 2: 1T.k Pk(x) and each component is small. Another way to view an outlier would be as an observation x which was implausible under each of the class densities Pk or under all classes, that is under p(x) = 2: 1T.k pk(x). Note that these two concepts can be very different if the classes have very different prior probabilities; the second seems preferable as we would want to report as an outlier a mildly-unusual observation for a very rare class. Thus in this approach outliers are detected by first screening observations x and declaring those with small p(x). How small? This is the same scenario as a pure significance test in statistical hypothesis testing (Cox & Hinkley, 1974; Lehmann, 1986) and the same ideas apply. Typically we will fix a level rx of acceptable false detections of outliers, and fix a level Pc so that Pr{p(X) < Pc} ~ rx. However, the integration needed here will often be intractable, and in the examples we relate p(x) to its average value on the', \"here will often be intractable, and in the examples we relate p(x) to its average value on the training set. The two routes lead to the same practical conclusion; declare an outlier when p(x) is small. Note that this is one place where knowledge of the posterior probabilities is not sufficient. We have to be very careful to ensure that 'uniformity' is an acceptable assumption for PCP; as this is a density it will depend on the particular transformation of the features used. Often structural constraints on the features will rule out uniformity , and some other plausible guess at p(!) will be needed. The data on Cushing's syndrome shown in Figure 1.2 on page 11 provide an illustration of the difficulties of outliers in even a small number of dimensions. (Typically there are many more points in many more dimensions, so the data may be equally 'sparse'.) One\"]\n",
      "['26 2 Statistical Decision Theory of the unknown results seems a clear outlier, both for all three types individually and from the whole distribution, but so does one of the results of type c, and the latter is believed to be genuine. (However, there are only five patients of type c with known results.) Another ofthe unknown results looks like a marginal outlier. In this problem we will assume that a uniform distribution over log excretion rates is plausible, even though there must be an effective maximum and minimum, and we might perhaps expect the two rates to be correlated. Ignoring the possibility of outliers can lead to misleading results. In the early 1950s anthropologists were discussing recently discovered hominoid fossils, and in particular whether Australopithecus africanus should be classified as an ape or a human. Bronowski & Long (1951) considered a linear discriminant analysis of teeth between chimpanzees and Homo sapiens and found agreement with Homo but not chimpanzees;', 'of teeth between chimpanzees and Homo sapiens and found agreement with Homo but not chimpanzees; Rao (1960) pointed out that they thereby overlooked the fact that on the full set of variables the sample tooth of A. africanus was implausible for either population . Sometimes outliers are the main interest in a classification problem, in what is known in signal processing as novelty detection . For example in detecting tumorous tissue in mammograms, the tumours are so rare See the cover for an that what is required is to highlight unusual tissue for further inspection example. (Tarassenko et al., 1995). 2.2 Parametric models We have seen in Proposition 2.1 the central role of the posterior probabilities p(k I x), although the consideration of outliers showed that this is not universal. Since the posterior probabilities are in general unknown, we have to estimate them from the data, and to do so we use models. The difference between the parametric models we consider here and the', 'and to do so we use models. The difference between the parametric models we consider here and the non-parametric models we consider in Chapter 6 is less clear-cut than the terms would suggest: the real distinction is between families of probabilities which are quite constrained by having only a few parameters, and those which are so flexible that they can approximate (almost) any posterior probabilities. We first give some general comments about the use of parametric models in classification, including discussion of what the methods actually do when the underlying assumed models are incorrect. We then present classification rules based on some of the most important parametric models. The most theoretically satisfying approach comes in Section 2.4.']\n",
      "['Figure 2.1: It is not always necessary to model the class-conditional densities (upper figure) accurately, as the posterior probabilities in the lower figure are effectively unchanged by most aspects of modelling the right peak of the class-conditional density shown dashed. Only the densities in the interval [1, 2] matter. 2.2 Parametric models 27 \"! \\'\\'I \\' I I ~ CC! \\' \\' I ·;,n 0 \\' I \\' I c Q) \"C \"\\' 0 \\' 0 0 0 2 3 4 X 0 \\' \\' ~ CC! \\' :c 0 \\' \"\\' \\' .c \\'\"\\' \\' e 0 0. 0 \"\\' ·;:: 0 * N g_ c) 0 0 0 2 3 4 X Theoretical and practical issues related to debiasing of maximum likelihood density estimates, predictive classifiers and robust estimation are addressed in later sections. Our first approach is that of classical statistics, to model either the class densities (this section) or the conditional probabilities (discussed in Section 2.3). It will be helpful to distinguish clearly these two tasks, which Dawid (1976) calls the sampling and diagnostic paradigms. Both give a parametric model of the joint', \"Dawid (1976) calls the sampling and diagnostic paradigms. Both give a parametric model of the joint density p(x, c; 8) of a random sample (X, C) of a set of features and its (reported) classification. In the sampling paradigm, interest centres on Pk(x;8), and we have p(x,c;8) = ncPc(x;8), with the prior probabilities (nk) for the classes assumed to be either known or completely unknown. In the diagnostic paradigm, interest centres on the posterior probabilities p(clx;8), with p(x,c;8) = p(clx;8)p(x;8), but any information about 8 in the unconditional density p(x; 8) is normally discarded by conditioning on the observed x 's. In later chapters we will concentrate on the diagnostic paradigm, which is illustrated in Section 2.3. The sampling paradigm is considered in this section, Sections 2.4 and 2.5 and Chapter 6. In Chapter 8 (X, C) is modelled simultaneously without stressing the importance of the class C. Each of these approaches has strengths and weaknesses. As Figure 2.1 shows,\", 'of the class C. Each of these approaches has strengths and weaknesses. As Figure 2.1 shows, direct modelling of the posterior probabilities may need fewer parameters than modelling via the class-conditional densities, and as the']\n",
      "[\"28 2 Statistical Decision Theory main quantities of interest, p( c I x ), are modelled directly, the procedure will often be less sensitive to the modelling assumptions. However, the diagnostic paradigm does have some disadvantages . We have already seen that we need the marginal density p(x) to handle missing values and outliers, and will see that using unclassified observations is much easier in the sampling paradigm. Thus although users of the diagnostic paradigm almost invariably do not model p(x; 8), it is often wise to do so. General considerations The optimal classification procedure under loss function (2.2) is given in (2.5) when the class densities are known. It resulted in the Bayes risk K R(c) = EL(C, c(X)) = L nk[pmc 0(k) + d pd0(k)], (2.11) k=l featuring misclassification and doubt rates for procedure c. In practice the Pk 's are at least partly unknown, and the statistical task becomes one of providing good alternative procedures with Bayes risk as close to R(c) as\", 'task becomes one of providing good alternative procedures with Bayes risk as close to R(c) as possible. It is assumed in this section that the prior probabilities nk are known and that the class densities are modelled parametrically, say pk(x) = Pk(x;8) fork= 1, ... ,K, where 8 E E> is the vector of unknown parameters needed to describe the K class densities. Suppose a training set of the form (2.12) is available, with the nk Xk,j \\'s coming from class k. These give rise to an estimate Ok of 8k. A natural proposal is then the classification rule ~ { k ifp(k I x) = maxp(ll x) and this exceeds 1-d, c(x) = I g) if each p(k I x) ~ 1-d, (2.13) where parameter estimates are inserted in class densities to produce approximate posterior probabilities ~(k I ) = nk pk(x; 0) p X K ~. Ll=l n1 PI(x; 8) (2.14) The rule (2.13) is called the plug-in classifier. Some of the most widely used classification methods are of this form, as shown in the examples below. We would estimate 1tk by nk = nd L.:: \"j·', 'methods are of this form, as shown in the examples below. We would estimate 1tk by nk = nd L.:: \"j·']\n",
      "[\"Here consistency means almost sure convergence to the 'true' value. 2.2 Parametric models 29 It remains to decide exactly which estimator should be plugged in. The maximum likelihood (ML) estimator has been the most popular choice in statistical practice, together with modifications to reduce its bias. (It is defined and discussed below.) The widespread use of the plug-in rule with the ML estimator has been caused by the good general reputation the ML method enjoys and the fact that several pioneers in statistical classification theory have directly or implicitly reco!llmended it. The use has been rather uncritical, though. Although () may be excellent as an estimator of () there is no guarantee that pk(x; 0) is a good guess for Pk(x; 0), nor is c(x) necessarily a good approximation to c(x). The performance of plug-in rules and other procedures should really be judged by the criterion of total risk, R(c) defined in (2.3), if (2.2) is still considered to be the appropriate loss\", 'of total risk, R(c) defined in (2.3), if (2.2) is still considered to be the appropriate loss function, or by othe_E criteria more tied to classification accuracy than to the behaviour of (J as an estimator for e. For example, to apply Proposition 2.1 we only need to know which of the posterior probabilities is the largest (or which of a weighted sum is the smallest), which requires high accuracy of modelling only for some parts of the feature space. (If one posterior probability dominates, it does not matter if it is fitted as 0.999 when it is really 0.85.) We know of no work aimed at this aspect of the problem, although some approaches are closer than others to its goals. These questions and related problems are returned to later, but first we give some general comments pertaining to the use of parametric models in discriminant analysis. Bayes risk consistency A reasonably simple observation that has been taken as support for the use of plug-in parametric rules is the following: As', 'that has been taken as support for the use of plug-in parametric rules is the following: As the training s;:t increases, that is each of n1, ... , nK grows, then provided only each ()k is consistent and the class densities are continuous in their parameters, c of (2.13) becomes identical to the optimal c and its total risk R(c) converges to Bayes risk R(c). Many plug-in rules, corresponding to a large class of possible estimators 0, have this property; see Van Ryzin (1966) and Glick (1972, 1976). There is an important assumption behind this argument, that the class densities Pl, ... , PK in fact obey the parametric structure in question. As statisticians sometimes admit, their parametric models are only approximations to reality, implying in the present context that even when the size of the training set increases beyond bounds, c of (2.13) will become close to only an approximation to c of (2.5), and the']\n",
      "['30 2 Statistical Decision Theory total risk R(C) will converge to a number greater than R(c). Expressions for this limit can be found using the theory presented below. It is often possible to construct procedures that are Bayes risk consistent in the sense that the sequence of total risks converges to the Bayes risk R( c) when the training sets grow. Unless one firmly believes in a certain parametric model the Bayes risk consistent rules will necessarily involve non-parametric or very flexible parametric methods, a topic returned to in Chapters 4 and 6. These comments are not meant to imply that parametric models are useless; they may indeed constitute good and compact approximations to more complicated models. Classifiers built on parametric assumptions may work excellently. Non-parametric methods often demand for their successful application far larger training sets than parametric alternatives. Thus there is a trade-off between perhaps simple, easily implementable algorithms that', \"Thus there is a trade-off between perhaps simple, easily implementable algorithms that work well even for moderately sized training sets, and non-parametric ones that may behave awkwardly for small to moderate training sets. The non-parametric ones will nevertheless (nearly always) win if sufficient training data are available. Likelihoods and unclassified observations The likelihood for the training set ff is K nk t(8;ff) = ITITpk(xk ,J;8)nk(8) k=1}=1 and this applies whether the nk were fixed in advance or resulted from a random sample taken from the whole population . We will use L( 8; ff) = log t'( 8; ff) for most of our calculations. Conventionally likelihoods are only defined up to a factor which does not depend on 8 and hence log-likelihoods up to an additive constant. Here we assume that either nk(8) is known, hence does not depend on 8 and can be dropped from the likelihood, or completely unknown and forms part of the parameter vector, which is then really tp = (8, n1, ... ,\", 'completely unknown and forms part of the parameter vector, which is then really tp = (8, n1, ... , 7rK ). The maximum likelihood estimator of tp is the maximizer of the (log-)likelihood. We have L(8,(nk);ff) = LLlogpk(xk,J;8) + Lnklognk. k j k We can maximize first over the second term; after introducing a Lagrange multiplier for the condition L 1rk = 1 we find nk = nkf L nj.']\n",
      "['2.2 Parametric models Plugging this in gives the log profile likelihood L( 0, (nk); ff) = L L log Pk(xk,j; 0) + const k j which is the same as the log-likelihood knowing (nk). 31 In most cases we have to maximize over 0 directly (numerically or analytically). Sometimes the parameter 0 divides into separate parts for each class, in which case we can fit Pk(x; 0) to each class separately by maximum likelihood. If some of the features are missing, we replace Pk(Xk,j; 0) by pk(xZ,j; 0) for the observed features xk,j\" There are some problems in which observations X are cheap but classifications C are expensive, so we can envisage having a set of unclassified observations diJ = {xj} in addition to the training set. These must be regarded as independent samples from the mixture distribution p(x; 0) = Lk 1tkPk(x; 0), and the log-likelihood (or profile likelihood) becomes L(O,(nk);ff) = LLlogpk(Xk,j;O) + Llogp(xj;O) (2.15) k j j which will couple the class densities even if they could', 'LLlogpk(Xk,j;O) + Llogp(xj;O) (2.15) k j j which will couple the class densities even if they could previously be separated. Note that the extra observations may carry much useful information; consider classes with Np{Jlb ~k} distributions. Given enough unclassified data, we could estimate all the parameters Jib ~b 1tk as precisely as desired, except we would be unable to say which group applied to which class. The classified observations provide the information on this matching. Fitting parametric families when they are wrong We will give a brief discussion of the behaviour of ML estimates when the underlying parametric model is not necessarily true. Assume that X 1, ... , Xn are independent and identically distributed with a density p(x), and that the parametric model p(x; 0) = pe(x) is forced on the data, 0 being a q-dimensional parameter belonging to some open parameter set. The ML estimator e maximizes the log-likelihood function n (2.16) i=1 with respect to 0. By the law of', \"ML estimator e maximizes the log-likelihood function n (2.16) i=1 with respect to 0. By the law of large numbers n-1 Ln(O) tends to J p log Pe dx, the mean of log pe(Xi), with probability 1 (often termed 'almost surely').\"]\n",
      "[\"32 2 Statistical Decision Theory For many important parametric models this function has a unique maximum at a parameter value 8 = 8o. This 8o is not necessarily the 'true value' because we have not assumed that p belongs to the family of Po's. In a sense 8o is the value of 8 making Po closest to the true p, in that it minimizes the Kullback-Leibler divergence I p(x) d(p,p0) = p(x) log Po(x) dx. (2.17) This measure is not symmetric in its arguments and therefore not a distance in the usual sense. It is rather a 'directed' distance from the true density to the modelled density, and we think of 80 as_::he 'least false' parameter value. Under weak regularity conditions 8 ~ 8o almost surely (see for example: Huber, 1967; White, 1982) thus generalizing the classical consistency result for ML estimators. If the true density is in the parametric family, p(x) = p(x;8o) and d(p,po0) = 0. Applying this result to each of the parametrically estimated class densities we see that the ML plug-in rule\", 'this result to each of the parametrically estimated class densities we see that the ML plug-in rule c defined in (2.13) and (2.14) converges pointwise to a rule c• defined analogously to (2.5) but with posterior probabilities of the form p(k I x) = ;k pk(x; 8o) . Lt=l nt Pt(x; 8o) Furthermore, R(c) ~ R(c*) almost surely, and R(c*) > R(c). (2.18) The classical result on the limiting distribution of Jii(ii-80) may also be generalized to the present agnostic state of affairs where the parametric family does not necessarily contain the true p (Huber, 1967, p. 231; White, 1982, Theorem 3.2). Proposition 2.2 Under mild regularity conditions (2.19) where ~d denotes convergence in distribution and d K _ V o logp(Xi; 8o) an -arp iJ() . If the true density belongs to the parametric family, J = K.']\n",
      "['The deviance is defined in the glossary. Here the reference model is the true distribution. The second step uses the independence of X and e. 2.2 Parametric models 33 Proof: The ML estimator solves the vector equation un(i)) = 0, where Un(O) = 2:::~1 fa logp(Xi; 0). A Taylor expansion shows that where In(O) is the Hessian of the log-likelihood function and 7J lies on the vector between the ML estimator and 00. This implies JYt(iJ-Oo) = [-n-1In(O)r1n-112Un(Oo) -a J-1Nq{O,K} = Nq{O,J-1KJ-1 }. If the family contains the true model that J = K is well known (Cox & Hinkley, 1974, p. 108; Lehmann, 1983, p. 118). 0 The usual definition of the Fisher information matrix is K. The regularity conditions needed imply that J and K are positive definite. We also need to consider the effect of approximating the loglikelihood (M. Stone, 1977b; Murata et al., 1991, 1993, 1994). Proposition 2.3 Let D = 2 E (logp(X) -logp(X; B)], the expected deviance on a single test example. Then n x D = E deviance+ 2', \"E (logp(X) -logp(X; B)], the expected deviance on a single test example. Then n x D = E deviance+ 2 q* + 0(1/ y'Yt) (2.20) where q trace [K J-1]. If the parametric family contains the true density, q* = q, the number of parameters. Proof: Let i(x, 0) be the Hessian of the log-likelihood for just one sample. We approximate D via the Taylor expansion about 00 2logp(x; 0) ~ 2logp(x; Oo) + 2 (0-Oof a logp(x; Oo)jaO ~ T ~ + (0-Oo) i(x, Oo)(O-Oo) = 2logp(x; Oo) + 2 (0-Oo)T a logp(x; Oo)jae + trace[i(x,Oo)(O- Oo)(O-Oo)T]. We assume ad(p,p0)jae = Ealogp(X;O)jae = o at 00, so D ~ 2 d(p, Po0) -E trace (i(X, Oo)(O-Oo)(O-00) T] = 2d(p,po0) + trace[J Var(O)] = 2d(p,po0) +! trace(JJ-1KJ-1] n 1 = 2d(p,po0) +-trace(KJ-1]. (2.21) n\"]\n",
      "['34 2 Statistical Decision Theory For the training set we expand about (j : 2 I: log p(~i) . p(X;,Oo) I ~ 2 L log p(~;~ -L trace [i(X;, O)(e-Oo)(e-eof] i p(X;,8) i \"\"\"\"l p(X;) * d . • ~ 2 ~ og . ~ + q = evtance + q . p(X;,O) Checking the error terms shows the error to be 0(1/ Jii). D This is the basis of Akaike\\'s (1973, 1974) AIC and Murata et al.\\'s (1991, 1993, 1994) NIC criteria for model selection, which are of the form (deviance+2q*), with q* replaced by q for AIC. M. Stone (1977b) derived NIC while considering cross-validation and AIC, but did not comment that it might provide a better approximation. Moody\\'s (1991, 1992) Pelf is a more general version which we discuss in Section 4.3. To use (2.20) we replace the expectation of the deviance by the observed value. The main error comes in the fluctuations of the deviance at Oo, 2 I: logp(X;)/p(X; ; 8o), about its mean 2n d(p, po0), which by the central limit theorem (assumed applicable) will be of order Op(Jii), and we have n x D =', 'by the central limit theorem (assumed applicable) will be of order Op(Jii), and we have n x D = NIC + 0(1/ Jii) + Op(Jii). (2.22) (The notation Op() is explained on page xii.) Now consider comparing several models via their values of NIC, and choosing the model with the smallest. Equation (2.22) shows that for large enough n we will choose one of the models with smallest D. Of course, there may be many such models if we have a nested set, so NIC will there choose a model which includes the smallest true model, but not necessarily the smallest such model. One major source of the fluctuations in (2.22) is the variability of the training set (X;), and this is common to all models. However, the claim by Murata et al. (1994, §5) that for differences in NIC amongst nested models this fluctuation term in the differences is Op(1/ Jii) is false. Suppose we have nested models with q1 > q2, 6..q = q1 -q2 and the smaller model (and hence both) are true. Then as M. Stone (1977b) pointed out,', '= q1 -q2 and the smaller model (and hence both) are true. Then as M. Stone (1977b) pointed out, AICt-AIC2 = 2(LR test of 1 vs 2)-26..q \"\\'xiq-26..q for large samples, and the right-hand side has fluctuation Op(1). Even asymptotically we might find AICt < AIC2 and so choose the larger AI C was named by Akaike (1974) as \\'An Information Criterion \\', although it seems commonly believed that the A stands for Akaike. NI C is an abbreviation of \\'Network Information Criterion\\'. Some definitions of AIC and the definition of NIC divide by n, which is fixed.']\n",
      "['2.2 Parametric models 35 model. If the models are not nested and equally good we can have a fluctuation term in the difference of Op(.j1i); consider the perverse example of two models which choose the true family for the odd numbered Xi, fixed e for the even ones and vice versa, so the effective training sets are disjoint. To make use of these results in practice, we have to be able to estimate J and K. Now J is the expectation of the observed information, the Hessian of the negative log-likelihood, with the observed information evaluated at e0 and the expectation over the true distribution of examples. Thus we can form a reasonable estimate by replacing the expectation by the average over a training (or test) set, and replacing eo by 0. The same argument suggests estimating K by the variance of ologp(X,O)joe over the training or test set. If qjn is not negligible, there is a danger of bias here, especially in estimating K, and hence of underestimating q*. To see this, let U(x,e) =', \"bias here, especially in estimating K, and hence of underestimating q*. To see this, let U(x,e) = ologp(x;e)joe denote the scores. Then EU(X, e0) = 0 from the definition of e0, so K = EU(X, eo)U(X, eof. For a training set .l:::i U(Xi, B) = 0, which imposes q constraints on the scores, and the divisor in the variance should perhaps be n-q. For a test set it is perhaps best to use the variance with divisor n-1. Very little of the argument here depends on using a maximum likelihood estimator, and Huber's (1967) results hold much more generally. All we need is that 8 maximizes 2::::: tp(Xi; e) for a suitably smooth function tp playing the role of log p, and that a unique e0 minimizes E tp(X; e). Of course, the definitions of J and K change by replacing logp by tp. (We use this freedom on page 140.) Example Consider the normal distribution Nq{Jl, ~} as an approximation to a given density p on 1Rq. The density is given at (2.7). Some analysis (Huber, 1985, Lemma 12.4) shows that the\", 'p on 1Rq. The density is given at (2.7). Some analysis (Huber, 1985, Lemma 12.4) shows that the parameter values (JLo, ~o) that provide the best approximation according to the Kullback-Leibler criterion (2.17) are Jlo = EpX = j xp(x)dx and ~o = VarpX = j(x-JLo)(x-JLo)T p(x)dx. Thus when the normal model is used to describe data from a density p that perhaps is known a priori not to be normal and the ML estimators ji, !: are computed, the theory shows that what they really estimate']\n",
      "['36 2 Statistical Decision Theory are J-lo, I:o, the population mean and variance. It is worth pointing out that this was proved without using any explicit expressions for the estimates themselves. These are derived below, after which another and more direct proof of /1 ~ J-lo and ~ ~ I:o can be given. The normal model and the best linear rule Suppose Pk is the Np{J-lb I:} density for k = 1, ... ,K, as defined in (2.7). There we saw that the Bayes rule is a linear rule in the sense that we choose the maximum of K linear combinations, and for two classes we divide the linear combination (J-l2-J.1I}I:-1x (from (2.9)). The total likelihood for a training data set of form (2.12) is K nk II II II:I-112 exp[-~(Xk,j- }-lk)TI:-1(Xk,j-J-lk}]. k=1 j=1 This is maximized by flk = Xk = nk\"1 2::}~1 Xk,j and by (2.23) where N = 2::~=1 nk is the total training set size (and the maximum will be infinity unless N ~ p + K ). See, for example, Mardia et al. (1979, §4.2.2). The ML-estimated best linear rule', \"N ~ p + K ). See, for example, Mardia et al. (1979, §4.2.2). The ML-estimated best linear rule takes the form (2.8) with these estimates plugged in: . . . 2~T~-1 ~T~-1~ 21 k 1 K (2 24) mm1m1ze -J-lk ~ x + J-lk ~ J-lk-ognk over = , ... , . . For two classes this is Fisher's (1936) linear discriminant, derived from another criterion; this approach stems from Rao (1948). Often the bias-corrected estimator of I: with divisor N -K is preferred (and N -1 appears in at least one computer package). This makes no difference to the linear rule unless the prior probabilities differ, in which case the effect is to change the constant terms to reduce slightly the influence of the data term relative to the prior. The best linear rule for the data on Cushing's syndrome on page 11 is shown in Figure 2.2. The equal-covariance normal model does not seem appropriate for this dataset. The best quadratic rule Now let the model for class k be Np{J-lk, Lk}· The ML estimators can be found from a likelihood\", 'Now let the model for class k be Np{J-lk, Lk}· The ML estimators can be found from a likelihood expression as before, and since there are no']\n",
      "['Figure 2.2: The decision regions of the best linear rule for the data on Cushing\\'s syndrome, together with contours for p(x) at negative powers of 10 of the average for the training set. i! .. c • c ;; 2.2 Parametric models u b b b b 10 Tetrahydrocol\\'ti&one 37 / 50 parameters common to more than one class, Jik and ~k are found by maximizing the likelihood for class k separately. The result is nk Jik = Xk and ~k = : \\'L,(Xk,j- /ik)(Xk,j- Jik)T k j=1 (2.25) where we need nk ~ p + 1 for each class for a finite maximum of the likelihood. This produces the plug-in version of the best quadratic rule; . . . 1 1 I~ I 1 ( ~ )T~-1( ~ ) 1 m1mm1ze 2 og \"\"\\'k + 2 x -Ilk \"\"\\'k x -Ilk -ognk (2.26) over k = 1, ... ,K. The rule goes back to C. A. B. Smith (1947). The number of estimated parameters has increased dramatically from Kp + p(p + 1)/2 for the best linear rule to Kp + Kp(p + 1)/2, so parameter estimates may be rather variable for the quadratic rule. Even though this method is guaranteed to', \"estimates may be rather variable for the quadratic rule. Even though this method is guaranteed to outperform the linear rule for very large sample sizes, it can very well be outperformed by the linear rule for moderate sample sizes. Since it may be preferable to use a linear rule, we can ask which linear rule produces the smallest error rate. This has been considered for two classes by Riffenburgh & Clunies-Ross (1960), Clunies-Ross & Riffenburgh (1960) and Anderson & Bahadur (1962). (See Anderson, 1984, §6.10.2.) The optimal linear rule is not that derived by pooling the covariance matrices and using (2.24) (for example, with ~ the MLE or the average of ~i ), although the linear combination used does derive from a convex combination of the two covariances. In practice it may be better to take some intermediate position, and compromise between the linear and quadratic rules. This is discussed in Section 3.4. The data on Cushing's syndrome look suitable for quadratic discrimination,\", \"in Section 3.4. The data on Cushing's syndrome look suitable for quadratic discrimination, since although the numbers in the classes are very small,\"]\n",
      "['the covariance ellipsoids vary very considerably in orientation. The results are shown in Figures 2.3 and 2.4. The six unknown types are all given quite high posterior probabilities (the lowest is 70%, and the two apparent outliers have low values of p(x), roughly w-12 and w-3 times the average for the training set. Thus both are rated as outliers (and they were medically, the more extreme being due to difficulties in the measurement procedure, and the less extreme to another type not represented in the training set). It is possible that l:k is singular in one or more groups. (This happens in the forensic glass data-none of the samples of tableware contains any potassium, barium or iron.) A singular covariance matrix implies that the population for the class lies in a subspace of f£; equivalently it satisfies one or more linear constraints. Then a future example which does not satisfy those constraints does not come from the class, and one which does will come from this class (or any', 'constraints does not come from the class, and one which does will come from this class (or any other that has the same constraints).']\n",
      "['These equations follow from Huber (1981, §8.4). They extend in the obvious way to a common scale matrix for all groups. See also Kent et al. (1994) and Lange et al. (1989). Kent et al. (1994) show that the solution is unique for v > 1 and n~p+l. 2.2 Parametric models 39 Multivariate t models The univariate normal distribution is well known to have shorter tails than distributions which occur in applied problems, and a t distribution with a moderate number of degrees of freedom is often regarded as a better fit. The multivariate analogue of a t distribution is usually described by the analogue of the distribution of Student\\'s t statistic: the multivariate t with location vector 1-l and scale matrix ~ is the distribution of J-l +X /S where X \"\\' Np{O, ~} and vS2 \"\\' X~ (Johnson & Kotz, 1972, §37.3; Mardia et al., 1979, p. 57). (Unfortunately, several variant definitions exist in the literature, not all of which are actually densities!) With this definition, for v > 2 the. mean is 1-l and', \"not all of which are actually densities!) With this definition, for v > 2 the. mean is 1-l and the covariance matrix is v~/(v-2). The density (2.27) has elliptical contours with shape determined by ~ but which spread out more slowly than a normal distribution. The optimal classifier is minimize (v!p) log [1 + ~(x- J-lk)T~k'1(x-J-lk)] +!log l~kl-log 1tk. (2.28) If the prior probabilities are equal and the scale matrix is common to all groups we again have the best linear rule. The log-likelihood for the multivariate t is similar to that for the normal, except that the quadratic term Qi = (xi- l-lf~-1(xi -J.l) is replaced by (v + p)log(1 + Qifv). Thus the maximum likelihood estimators of 1-l and ~ are weighted versions of the mean and scale matrix, with weights wi(J-l,~) = 1/(1 + Qi/v): The effect of the longer tails of the t distribution is to down-weight observations which are far from the mean. The maximum likelihood estimators can be found by an iterative algorithm which updates the\", 'mean. The maximum likelihood estimators can be found by an iterative algorithm which updates the weights, although it would be wise to choose resistant estimates of the mean and covariance matrix (see Section 2.5) as starting points. Details of existence and convergence are a special case of arguments of Maronna (1976) and Huber (1981, §8.6). Note that as llxdl -+ oo its effect on the location estimate goes to zero, whereas on the scale estimate its effect remains bounded but does not vanish.']\n",
      "[\"One use of a multivariate t is as an agnostic model for distributions with elliptical densities with long tails, in the spirit of robust statistics (Huber, 1981; Lange et a/., 1989). In that setting it is interesting to consider what the least false parameter s are, as they indicate what the parameters measure in the population . They are weighted versions of the mean and variance, weighted by w(,uo,I:o) = 1/ [1 +(X-.uofi:01(X-.uo)/v]. Thus if the true density has elliptical contours, .uo will be the centre of the ellipses and I:o will be proportional to the moment matrix of the ellipses (with the constant of proportionality depending on the true density). The decision rule for multivariate t distributions on 5 degrees of freedom is shown in Figure 2.5 for the data on Cushing's syndrome. The number of degrees of freedom was chosen arbitrarily to give fairly 'fat' tails; despite this there is little difference from the best quadratic rule. Some of the difference is due to different mean\", 'is little difference from the best quadratic rule. Some of the difference is due to different mean and scale estimates,']\n",
      "['2.2 Parametric models 41 but the differences in the lower right reflect the tail behaviour. The much greater uncertainty shown in Figure 2.6 in the lower right also reflects the tail behaviour. A mixed model for discrete and continuous components Suppose X = (A, Yt, ... , Yp) where A is discrete and takes values in { 1, ... , m} and Y = ( Y1, ... , Yp) T has a continuous distribution. A simple and sometimes quite effective model for such feature vectors is to postulate Y I {A= a}\"\" Np{llk,a,L} while Pr{A =a I k} = gk(a) for class k (Olkin & Tate, 1961; Krzanowski, 1975). Many variations exist around this theme; the L matrix which is assumed common here can be taken to vary with either or both of a and k, for example. There is also a possibility of modelling gk(a) if the number of possible values for A is anything but small. (This is termed a conditional Gaussian distribution; see, for example, Edwards, 1995; Lauritzen, 1996, Chapter 6.) We shall be content here to illustrate the', \"for example, Edwards, 1995; Lauritzen, 1996, Chapter 6.) We shall be content here to illustrate the general principle with the simple model, sometimes called the 'location model'. The class densities are and so from Proposition 2.1 we find the class k maximizing when (a, x) is observed. We need to find and plug in the maximum likelihood estimators of the parameters. These are straightforward: Jik,a is the mean of observed X from class k with A = a, i: is the observed covariance matrix (with divisor n) of X -1-Lc,A, and gk(a) is estimated by the proportion of examples in class k with A = a. Finite mixture distributions We can consider larger parametric models, for example mixtures of normals which will allow us to model multi-modal class densities. As this is a way to fit quite general class densities, we defer the most of the details to Chapter 6. However, there is one quite commonly used 'trick' to fit class densities by mixtures, and that is to model sub-populations of the classes.\", \"'trick' to fit class densities by mixtures, and that is to model sub-populations of the classes. A rather extreme example is that of Oliver et al. (1979), who considered 13 cell types in cervical cytology, 5 normal and 8 abnormal. We have experienced several instances of feature distributions with a clear bimodal structure. Consequently histograms for even well chosen\"]\n",
      "['42 2 Statistical Decision Theory transformations of data are not well described by fitting a normal density. This suggests studying mixtures of two normal distributions as a means of describing class densities. The case of three or more normal components of a mixture is similar, but the number of parameters needed increases quite rapidly. We view the following mixtures of two normals for each class density as still being within the realm of parametric modelling. Let X~, ... ,Xn be a random sample from a density which we intend to describe parametrically by The important problem of fitting data to this class of densities is a difficult one and is perhaps not yet satisfactorily solved in the literature. The model is not properly defined until a restriction of the parameter set is made to avoid problems of identifiability; we may exchange (J.L1, I:l) and (J.l2, I:2) and rename q as 1-q to get two representations of the same density. The model is identifiable if one demands q ~ ~ or that', 'get two representations of the same density. The model is identifiable if one demands q ~ ~ or that the first J.ll component should be to the \\'left\\' of the first J.l2 component, for example. One may check by drawing graphs in the one-dimensional case, however, that curves with rather different sets of parameters may still come close to each other, making estimation of the parameters a more confusing and difficult task than usual. The density is not necessarily bimodal even when J.ll and J.l2 are different; see Eisenberger (1964). The maximum likelihood programme does not work as smoothly and automatically as in the earlier examples. First of all it does not exist in the usual sense, since the log-likelihood Ln is unbounded, with many singularities. For example, Ln ~ oo as J.ll = X1 and I:1 ~ 0, corresponding to the \\'explanation\\' q = 1 -~\\' X1 \"\" N{J.ll, 0}, while X2, ... , Xn follow N{J.l2, I:2}. Clearly this is not the solution we want. The Ln function will usually have several local', 'I:2}. Clearly this is not the solution we want. The Ln function will usually have several local maxima, and one of these corresponds to the nth element in a sequence of stationary points that converge almost surely to the true parameter values. A one-dimensional example of fitting two normals is given in Venables & Ripley (1994, Chapter 9) which illustrates some of the difficulties even in that case. They use direct maximization, with derivatives of the log-likelihood being found by automatic symbolic differentiation. Updating estimates from unclassified data We saw at (2.15) that we could include unclassified observations in the likelihood, and this opens the possibility of continuing to estimate the For univariate data, Hathaway (1985) establishes consistency for the global minimum under a constraint on the ratio of the variances.']\n",
      "['2.3 Logistic discrimination 43 parameters (and the prior probabilities (nk)) while the classifier is in routine use. This has two important implications: it enables work to start with a minimal training set, and it allows the classifier to adapt to slow changes in the class distributions over time. The log-likelihood (2.15) involves the marginal density p(x; e) which is a mixture of the class-conditional densities. Mixtures are discussed in more detail in Section 6.4, but we only need the application of the EM algorithm (Section A.2). Regard the true classes of the unclassified observations o/1 as missing data. Then we estimate the posterior distribution of the true class k as nk(x; e) oc nkpk(x; e) and nk as the average of the nk(x; e) over all observations. Hence e is estimated by maximizing a weighted log profile likelihood L(e) = L L:logpk(xk,j;e) + L L nk(xj;e)logpk(xj;e). k j j k The first term comes from the training set !T and the second from the unclassified observations o/1.', 'first term comes from the training set !T and the second from the unclassified observations o/1. This is an iterative process, in that e and (nk) are updated alternately with ( nk(xj; e)). We can apply this programme to the best linear and quadratic classifiers (Hjort, 1986, §7.2) and to multivariate t distributions. We need only keep the means and covariance matrices of the classified data, but since nk(x; e) will change as more examples are collected, all the unclassified data needs to be retained. Approximations for situations where retaining the data is computationally undesirable are discussed by Titterington et al. (1985, Chapter 6). 2.3 Logistic discrimination Let us return to the normal model for classes with a common covariance matrix given by (2.7) with Jl = Jlj for class j. If we compare class k with class 1 we have 21 p(klx) og p(11 x) = (x-JldT~-1(x-Jld-(x-Jlkf~-1(x-Jld + 2log nk 1rl = 2(Jlk- JldT~-1x-(Jlk + JldT~-1(Jlk-Jld + 2log nk 1rl = 2/3[ x + 2rxk say, a linear', '+ 2log nk 1rl = 2(Jlk- JldT~-1x-(Jlk + JldT~-1(Jlk-Jld + 2log nk 1rl = 2/3[ x + 2rxk say, a linear function of x. Thus the posterior probabilities obey a log-linear model of the form logp(k I x) = logp(11 x) + rxk + f3[ x (2.29)']\n",
      "['44 8 ,; 8 0 ci 2 Statistical Decision Theory u b b b b 10 Tetrahydrocortisone 50 which is also known as a multiple logistic model. The case of two classes is much simplified, as logitp(k I x) =a+ pT x (2.30) for the logit transform logit(x) = log(x/(1- x)). Thus (2.30) gives the posterior log-odds of class two versus class one, and is known as a logistic regression. Equation (2.29) is illuminating, as it expresses the posterior probabilities, the important quantities in a plug-in rule, directly in terms of the parameters. This suggests that rather than use maximum-likelihood estimation of Jlk, ~ and hence ak. fJk, we should estimate the latter directly. As (2.29) and (2.30) only concern the dependence of C on X, this is done by conditioning on X. For illustration we consider only the case of two classes here, and defer the general case to Section 3.5. However, comparing Figures 2.2 and 2.7 shows that the two estimates may give quite different classifiers when the common covariance', \"2.7 shows that the two estimates may give quite different classifiers when the common covariance model seems inappropriate. Conditional on X = x, the class C has a Bernoulli distribution with probabilities p(c I x). Thus if were-express the training set as .o7 = { ( Ci, Xi), i = 1, ... , n} the conditional log-likelihood for the parameters (}=(a, {J) is given by n n II p(ci I xi)= II p(21 xJ(c;=2)[1- p(21 Xi)]l-l(c;=2) i=l i=l and so if YJ = I(C1 = 2) the conditional log-likelihood is given by (2.31) Figure 2.7: The decision regions based on logistic discrimination for the data on Cushing's syndrome.\"]\n",
      "['The loss is even higher when there is very little overlap, but then both rules perform well. The ARE is the ratio in large samples of plug-in error rate minus Bayes risk; this is more relevant than the variability of the parameter estimate itself. 2.4 Predictive classification 45 Note that maximizing (2.31) will not give the same answer as plugging in the maximum-likelihood estimators to give 7J = 2-1(J1k -jll), a = log(n2jn1)-!(J11 + jl2)T2-1(jl2-jll) as here the likelihood is based on the conditional distribution. As they are based on less information, the direct estimators should be less efficient (that is more variable) although the standard large sample theory applies to show that the estimates are consistent and asymptotically normal. Efron (1975) demonstrates that this is the case, and the loss of efficiency can be appreciable when the class densities overlap, so the classification task is neither easy nor hopeless. If there are two classes with equal prior probabilities, the', \"task is neither easy nor hopeless. If there are two classes with equal prior probabilities, the asymptotic relative efficiencies are a function of the Mahalanobis distance (j between the class means, given by 0.5 1 1.5 2 2.5 3 3.5 ARE 1.00 0.99 0.97 0.90 0. 79 0.64 0.49 pmc% 40.1 30.8 22.7 15.9 10.6 6.68 4.01 (The values of pmc are computed from the arguments below (2.10).) On the other hand, the logistic form (2.30) assumes less and is therefore less likely to be biased. (We will see in Chapter 3 that (2.30) can arise from other models of the class densities.) Logistic discrimination is a very important template for many of the generalizations we will consider, much more so than linear discrimination. We take it up in Chapter 3 as a principle in its own right. 2.4 Predictive classification Next we discuss the predictive approach towards estimation of parametric densities and posterior probabilities. It is Bayesian in inspiration and flavour even though the 'vague prior' versions of\", \"probabilities. It is Bayesian in inspiration and flavour even though the 'vague prior' versions of the method can be used and motivated outside the Bayesian paradigm. Suppose that we have a parametric family p(x, c; e) for the joint distributions of the classes and features; this implies parametric models for Pk(x; e), nk(e) and p(k I x; e), although of course not all of these need actually depend on e. Assume also that we have a prior distribution p(e) for e. Then in principle (and sometimes in practice) we can calculate p(k I x) = Pr{k I x;Y}. The predictive approach then acts as if p(k I x) were the true posterior probabilities, and uses Proposition 2.1 to calculate the optimal rule, which we will call the predictive classifier. The crucial difference between the plug-in and predictive classifiers is that the former acts as if the estimated e was the true e whereas predictive methods average over the uncertainty in e.\"]\n",
      "[\"46 2 Statistical Decision Theory The predictive approach gains very little mention in even comprehensive texts such as Berger (1985) and McLachlan (1992). This may well be because it usually makes little difference within the tightly constrained parametric families we are considering in this chapter, but it will be important when we consider much larger families. The books of Aitchison & Dunsmore (1975) and Geisser (1993) are devoted to the approach. Both contain brief accounts of classification, in Aitchison & Dunsmore's Chapter 11 under the heading of 'diagnosis'. Within the Bayesian paradigm this needs no further justification: the prescribed way to handle unknown parameters is to integrate them out from the conditional distribution given the data. We may however ask whether the predictive classifier has any optimality properties in terms of risk. In one sense it cannot, for if we knew the true value of e we must do better. Suppose rather that we extend the framework so e is an\", 'knew the true value of e we must do better. Suppose rather that we extend the framework so e is an unobserved random variable. Let c(x, 3) be a classifier which is allowed to depend on the training set (2.12). Its risk function, the expected loss when using it, is R(c,k,e) = E [L(k,c(X,3)) 1 c = k,e] = pmc(k, e) + d pd(k, e), where Ek,8 denotes the expectation for class k and fixed e. As a function of e alone the risk function is R(c,e) = EeL(C,c(X,3)) = pmc(e)+dpd(e). The overall risk in this framework is R(C) = EL(C,c(X,3)) = j R(c, e) p(e) de= pmc + d pd, where pmc = J pmc(O) p(e) de and pd = J pd(O) p(O) de are unconditional misclassification and reject rates, averaged over the unknown e. Note that this criterion makes good sense with any reasonable weight function over the parameter space; it does not have to be interpreted as a prior density (although it can be). Proposition 2.4 The classifier that minimizes the overall risk under loss (2.2) is -( ) = { k ifp(k I x) = max1', '2.4 The classifier that minimizes the overall risk under loss (2.2) is -( ) = { k ifp(k I x) = max1 p(llx) and this exceeds 1 -d, c x ~ if each p(k I x) ~ 1-d, (2.32) where p(k I x) = Pr{ C = k I X = x; 3}. This is also the rule that minimizes the conditional risk given the training data. The extension to other loss functions follows Proposition 2.1.']\n",
      "['2.4 Predictive classification 47 Proof: We condition on the training set: R(c) = EL(C,c(x,.rn = Eg-E[L(C,c(x,.rn 1 .r] and the conditional expectation is the total risk in the conditional distribution. Now apply Proposition 2.1 to the conditional total risk, to show this is minimized by a classifier of the form (2.32). D The full expression for the predictive posterior distribution when the future observation is independent of the training set is p(klx) oc j p(klx;O)p(x;O)p(Oiff)dO (2.33) n n i=l i=l The posterior density and the integral here readily become intractable. Explicit expressions are given below for some important special cases, and approximations can be provided in other cases. Note that (2.33) and (2.34) depend on the parametrized marginal density of X; it is at this point that the simplicity of logistic discrimination loses out, unless the assumed form of the marginal density does not depend on e. Alternative expressions which simplify in that case are p(k I x) = j p(k I', 'does not depend on e. Alternative expressions which simplify in that case are p(k I x) = j p(k I x;O)p(O I ff,x)dO n p(O I ff,x) oc p(O)p(x; 0) IIp(ci I xi; O)p(xi; 0). i=l and p(O I ff,x) will not depend on x if p(x;O) does not depend on e. If we work with parametrized class densities in the sampling paradigm, it is easier to use Pk(x) = j pk(x;O)p(Oiff)dO. (2.35) It is helpful to remember that p quantities are just conditional densities given ff and so can be manipulated as densities. We consider later what happens if the prior (nk) is unknown. Suppose we have to classify m > 1 future examples with feature vectors x~, ... , x~. The approach so far will not be fully efficient if p(x; 0) really does depend on e, since all the xj can be used to increase our knowledge of e (Geisser, 1966). We should use n m p(O I ff,x~, ... ,x~) oc p(O) II p(ci I xi; O)p(xi; 0) II p(x;; 0) i=l j=l']\n",
      "['48 2 Statistical Decision Theory in the diagnostic paradigm, and use this omitting the current xj in (2.35). (This differs from the proposal of Geisser, 1966, 1993, which is to maximize the joint predictive probability of the m classifications. The latter is optimal under the loss structure that all predictions be correct rather than the number of misclassifications be small. The difference is important in statistical image analysis; Ripley, 1988, p. 114.) Example: Poisson distributed counts For a structurally simple example, suppose that a count variable X is modelled by Poisson distribution p(x; 8) = exp( -ewx I X!. Let e have a gamma (0(, p) prior distribution with density [p(X lf(O()]OIX-l exp( -PO) on [O,oo), which has mean rxiP and variance rxlfJ2• Assume that independent counts X1, ... ,Xn have been observed from p(x; 8). Then it is not difficult to show that e given the data has a gamma(O(+nO,fJ+n) distribution, where 0 = Xn is the ML estimate for e. Hence the predictive', 'has a gamma(O(+nO,fJ+n) distribution, where 0 = Xn is the ML estimate for e. Hence the predictive density is 1oo ex (p + n )tX+nO , p(x) = exp(-8)1 ~ etX+nB-l exp[-(p + n)8] de 0 X. r(O( +nO) 1 (p + n)tX+nO f(O( +nO+ x) = x! (fJ + n + l)a+nO+x f(rx +nO) When 0( and P are sent to zero the expression simplifies to 0 ~ p(x) = _!_ nn , f(n8 ~ x) x! (n + 1)n8+x r(n8) and when n is large this is close to exp( -0) ex I X!, the ML density estimate p(x; 0). The difference is shown in Figure 2.8. 0 Example: Normal Let p(x;J.L,r.) be the Np{J.L,r.} density, (2.7), and assume (at this stage) that r. is fixed. Then we can consider each class separately. Choose a prior Np{J.lo,A} for the J.l vector. The ML density estimate based on data X1, ... ,Xn from p(x;J.L,r.) is p(x;ji,r.) with ji =X. The posterior density is This follows from the fact that']\n",
      "['Figure 2.8: Plug-in (left) and predictive (right) probabilities for four samples with total 20 from a Poisson distribution. ll) -c:i 0 -c:i ll) 0 c:i 0 c:i I 0 5 2.4 Predictive classification ~ 10 15 0 -c:i 0 c:i l\" 0 49 k 5 10 15 combined with properties of conditional distributions for jointly normal vectors; see for example Mardia et al. (1979, §3.2). The predictive density (2.35) can be worked out from this, but gives quite lengthy expressions. It is usual to use instead the simplified version that comes from the uniform (and improper) prior on JRP, and which also corresponds to letting the matrix A tend to infinity (in the sense that all its eigenvalues tend to infinity). With this flat prior .u Iff is simply Np{/2, I:/n}. Calculations (Geisser, 1964) give p(x) = (2n)-p/21I:I-l/2(_n_)p /2 exp(-!_n_(x- /l)TI:-I(x- /2)] n+1 2n+1 = Np{/2, nt1 I:}(x). The difference from the plug-in estimate of the density is small when n is moderate or large. The optimal rule is still a linear', 'estimate of the density is small when n is moderate or large. The optimal rule is still a linear discriminant and has the same combinations of the variables, but the slightly increased variance will affect the cutpoints if the prior probabilities are unequal. Unknown covariance matrix/ ces The most important case is when the covariance matrices are also unknown. We start with one class and a non-informative prior of the type (.Uk, I:k) ,...., II:k 1-ao/2, where the symmetric covariance matrix is parametrized by its upper triangle, so this is a density on JRP+P(P+l) /2. In the quite lengthy calculations that are involved it is more convenient to work with Ak = I:;;1 instead; the density for (.Uk.Ak) is IAkl-a/2 with a = 2(p + 1) -ao.']\n",
      "[\"50 2 Statistical Decision Theory The choice of a non-informative prior here is not clear-cut, but the majority view is for ao = a = p + 1 with a minority supporting ao = 2p, a = 2. Even for p = 1, Jeffreys' (1961) information principle leads to an answer that he rejects, but the choice ao = a = p + 1 follows from assuming prior independence of J.lk and Lk then seeking a non-informative prior for Lk alone (Box & Tiao, 1973, pp. 425-426). This is Geisser's (1993) choice, following Geisser & Cornfield (1963), and that of Aitchison & Dunsmore (1975) who take a limiting case of a Wishart conjugate prior. Berger (1985, §6.6) advocates using right invariant Haar measures as non-informative priors, which he demonstrates for p = 1 gives ao = 2. For larger p the results do not follow directly from his work (since the necessary group isomorphism fails). Hjort (1986) was led to ao = 2p and a= 2 by this approach, a value Geisser & Cornfield (1963) derived from a Fisher-Cornish fiducial\", 'a= 2 by this approach, a value Geisser & Cornfield (1963) derived from a Fisher-Cornish fiducial distribution. On the other hand Villegas (1969) obtains ao =a= p + 1 from a fiducial argument, and this was derived by Fraser (1968) from structural probability. We can see the difficulties by examining p = 2 in more detail. Then l: can be specified by K; = (Jf, i = 1, 2 and the correlation p; its determinant is K1K2(1-p2). Jeffreys (1961, pp. 176, 187) variously advocates the priors d(J1d(J2dp/(J1(J2 and d(J1d(J2dp/(Jw 2(1 -p2)312, and the latter corresponds to jl:j-312. After extensive manipulation (Geisser & Cornfield, 1963; Hjort, 1986) we find -( ) -p/2( + 1)-p/2 r( !(nk + P-a+ 1)) I~ 1-1/2 Pk X = 1t »k 1 \"\\'-\\'k r{:z(nk- a+ 1)) 1 [ 1 T~ 1 ] -:z(nk+p-a+l) x 1 + nk + 1 (x-Jlk) 1:;; (x-jlk) . (2.36) Here Jlk and i:k are the usual ML estimators defined at (2.23) on page 36. Figure 2.9 displays three estimates of a one-dimensional normal based on a sample of size 10; the ML plug-in', 'displays three estimates of a one-dimensional normal based on a sample of size 10; the ML plug-in estimate, the estimate which is unbiased on log scale, and the predictive estimate. Note that these three do not estimate comparable quantities, as the predictive estimator takes the uncertainty of the parameters into account in a way that the other two do not, but all are potential estimates to be used in Proposition 2.1. For p = 1 the two approaches to choosing the prior agree. The predictive density is a (scaled) t distribution centred on }l.k. The general form is known as a multivariate t distribution on (nk + 1-a) degrees']\n",
      "['Figure 2.9: Estimates of the density based on a random sample of size 10 from N{O, 1 }. The \\'plug-in\\' estimate is shown with a solid line, the predictive estimate (2.36) with a dotted line, and the unbiased estimate on log scale (2.38) with a dashed line. The multivariate t is defined in the glossary (see \\'t distribution\\') and on page 39. Aitchison & Dunsmore (1975, p. 255) give a different parametrization which affects the interpretation of their results. 2.4 Predictive classification 51 \"\\' ci ~, I \\\\ I \\\\ I \\\\ ... ci I I I I I I \"\\' I I ci I: ·.I I: I 1.: I t.: I \"\\' 1: I ci I I I I. ··.\\\\ 1.: . \\\\ ( \\\\ ci I \\\\ I \\\\ .J \\\\· ·/ \\'\\\\> . . ·/ ............ ·\\'/ ,·. 0 ci -4 -2 0 2 4 of freedom with location vector Pk and scale matrix For the point of view of classification we assume independent noninformative priors for each class. Then the predictive class densitj_es have a larger spread than the normal distribution with variance :Ek, but the same shape of contours, and so the optimal classifier is a', 'distribution with variance :Ek, but the same shape of contours, and so the optimal classifier is a quadratic rule. The difference from the plug-in quadratic rule is to move the decision boundaries to be more nearly equidistant from the class centres (apart from allowing for unequal prior probabilities of the classes). Very similar ideas can be applied to the case of a common withinclass covariance matrix, leading to which is a multivariate t distribution on N -K -a + 2 degrees of freedom with location Pk and scale matrix (1 + 1/nk)N ~N-K-a+2']\n",
      "['52 ~ 8 .,; ~ iil 0 0 0 :!l 0 2 Statistical Decision Theory u b b b b 10 Tetrahydrocortisone 50 For equal group sizes and prior probabilities we exactly recover the best linear rule. The calculations here are from Hjort (1986); versions of these formulae are given by Aitchison & Dunsmore (1975) (up to the differences in the meaning of their multivariate t) and Geisser (1993). This approach is originally due to Geisser (1964, 1966). The differences between the predictive and plug-in approaches will be small or zero for roughly equally prevalent classes. In other cases, for example screening for rare diseases or when very few data are available, the differences can be dramatic as shown by the examples in Aitchison & Dunsmore (1975, §§11.5-11.6). The latter do have groups with nk only slightly greater than p, for example p = 8 and n2 = 11 when fitting a covariance matrix to each class, which would be seen as over-fitting in the plug-in approach. (Indeed, one might choose not to use all the', \"would be seen as over-fitting in the plug-in approach. (Indeed, one might choose not to use all the variables, or perhaps to restrict the class of covariance matrices considered.) Aitchison et al. (1977) conducted a small-sample simulation comparison of the plug-in and predictive methods for two multivariate normal populations. They were (correctly) criticized by Moran & Murphy (1979) for using the accuracy of the estimation of the log-odds as the basis of comparison rather than error rates, and for including mainly equal sample sizes of the two classes. Moran & Murphy's results show very little difference in the error rates, and show that for estimation of the log-odds the debiasing methods of Section 2.5 are effective in removing the dramatic optimism of the plug-in method where it occurs. Vlachonikolos (1990) extended these calculations to some simple cases of the 'location model' for mixed discrete and continuous data discussed on page 41. Figure 2.10: The decision regions of the\", \"mixed discrete and continuous data discussed on page 41. Figure 2.10: The decision regions of the predictive quadratic rule for the data on Cushing's syndrome, together with contours for p(x) at negative powers of 10 of the average for the training set.\"]\n",
      "[\"Figure 2.11 : The uncertainty of the predictive quadratic rule for the data on Cushing's syndrome. The greyscaJes represent the maximum posterior probability of a class, with light grey as one and black as zero. The Dirichlet distribution is defined in the glossary. 2.4 Predictive classification 10 T.....n.,.CIIOOOI1IIol lt 53 50 We return to the data on Cushing's syndrome shown on page 11. With the predictive estimates, the classification is less certain, especially of the two apparent outliers which are now both classified as c (with probabilities 61% and 68%). However, both still seem outliers, with values of p(x) roughly 0.2% and 6% of the average for the training set. These much less dramatic values and the tendency to classify both outliers as c reflect the great uncertainty in the class distribution for that class. Unknown class prior We need to consider the effect of unknown prior class proportions (nk) in (2.35) as we would need to justify plugging-in the 'obvious' estimates.\", \"class proportions (nk) in (2.35) as we would need to justify plugging-in the 'obvious' estimates. Under random sampling the observed numbers (nt, ... , nK) follow a multinomial (n, p1, ... , PK) distribution. The natural choice for a prior is a Dirichlet(a;) distribution . The posterior is a Dirichlet(n; + a1) distribution so say, since the priors and hence posteriors for e and (nk) are independent. Then so we act as if we had plugged in the estimate 1ik. The difference The MLE is it= ntfN. between this and plugging in the MLE will be negligible unless the class sizes are very small or the prior extremely strong. For two classes the Dirichlet distribution reduces to the beta distribution with parameters (a1,a2). We can ask how to represent ignorance\"]\n",
      "[\"54 2 Statistical Decision Theory by the Dirichlet parameter vector a. Three suggestions in the twoclass case are a; = 1 (used by Bayes and Laplace), a; = 0 (which is improper) and a; = 1/2 (Berger, 1985, §3.3.4; Geisser, 1984). Good arguments can be made for any of these; fortunately in our setting they will be very similar. This suggests the simplest choice for the Dirichlet of a; = 0 as a vague prior, which gives the simple plug-in rule 1ik = nk/ n. Hyperparameters In some circumstances the prior p( 8) is specified only up to a family p;.(8) of priors; 2 is known as a hyperparameter. This occurs most often in pattern recognition when the prior is used to express 'smoothness ' of the posterior probabilities p(k I x; (}) as a function of x, and 2 is then the degree of smoothness. (See, for example, Section 4.3.) How should 2 be chosen? Within the predictive Bayesian framework, the solution is clear; we give a prior to 2, called a hyperprior . If this contains parameters, they too are\", 'is clear; we give a prior to 2, called a hyperprior . If this contains parameters, they too are given a prior, and so on. This is sometimes known as hierarchical Bayes (Berger, 1985, §3.6, 4.6) and the analysis is in principle obvious, since the effective prior is p(8) = J p;.(fJ)p(2)d2. This may be awkward to use if p;.(8) has been chosen to simplify computation, and the integration over 2 may be postponed to a late stage in the calculation . Thus we may find Pr{k I x,ff,2} and then integrate this with respect to the density p(21 x, !T). Other approaches have been proposed, and in some cases advocated strongly. Empirical Bayes methods use the data to choose 2, which entails a data-dependent prior which purist Bayesians do not allow, but is sometimes seen as an approximation to hierarchical Bayes and sometimes as desirable in its own right. (Maritz & Lwin, 1989, is devoted to empirical Bayes methods; Berger, 1985, §4.5, gives references to many strands.) Let us consider empirical', 'Bayes methods; Berger, 1985, §4.5, gives references to many strands.) Let us consider empirical Bayes as an approximation to p(k I x) by Pr{k I x, 5\"\",1}. If p(21 x, 5\"\") is highly concentrated about one value 1(x, 5\"\"), and if we can estimate this value easily and well, empirical Bayes will provide a considerable comp~tational simplification. (Often x will not be at all informative, so 2 can be computed once the training set is given.) How could we find 1? Good (1965, 1983) (the latter a compendium of earlier work) calls one method \\'type II']\n",
      "['2.5 Alternative estimation procedures 55 maximum likelihood\\' or \\'ML-II\\'. This is to choose A. to maximize the marginal density of the data; if we ignore x this is m(§\"IA.) = j t(O;§\")pJc(O)dO. (2.37) Note that ML-II is equivalent to maximizing p(A I§\") oc m(§\"l A.)p(A.) if p(A.) is constant, and so its 1 is likely to be a good estimate of A.o(x, §\") if m(§\"l A.) has a sharp peak. Deely & Lindley (1981) discuss conditions for such approximations. In the usual empirical Bayes context the assumption is of many problems with different e but the same 2, so this condition follows from the asymptotic normality of maximum likelihood estimation of A. from independent samples from m(§\"l A.). This is not the usual situation in pattern recognition applications, where a single large training set allows increasingly precise inferences about e but provides just one sample to estimate A.. ~ The empirical Bayes methods usually ignore the variability in 2; PX(O) will be more concentrated than p(O I 3).', 'Bayes methods usually ignore the variability in 2; PX(O) will be more concentrated than p(O I 3). This may not matter in the centre of the distribution, but may be material in applications such as ours of finding p(k I x) since p(k I x; 8) is often a highly non-linear function of 8 and the empirical Bayes methods often tend to produce fitted probabilities which are too extreme. 2.5 Alternative estimation procedures The maximum likelihood estimator ek is not always the very best estimator of ek (we habitually use a different estimator of the variance of a normal population), and even in ~ses where it is well-chosen for ek the plug-in density estimate Pk(x; ek) is not necessarily the best estimate of Pk(x; ek). In view of (2.5) and (2.4) our interest lies more with the densities themselves than with the parameters that describe them, so here we consider alternative estimation procedures, principally within the sampling paradigm. Debiasing density estimates One route is to modify', 'principally within the sampling paradigm. Debiasing density estimates One route is to modify estimators so as to make them unbiased or less biased, if they are not unbiased in the first place. For example, the ~ estimator of (2.23) has expected value NNK r., and statisticians normally use the modified version with denominator N -K instead of N. The same remark applies to the modification of ~k of (2.25) which uses denominator nk-1 instead of nk.']\n",
      "[\"56 2 Statistical Decision Theory The densities themselves are more directly involved in the classification problem. It is possible to find an unbiased estimator of a normal density (Ghurye & Olkin, 1969), but it is more natural to find an unbiased estimator of the log density, and hence of differences in log densities. This is the appropriate plug-in estimator if the interest concentrates on the log-odds or on logp(k I x), as in the experi~ents of Aitchison et al. (1977) and Moran & Murphy (1979). Suppose ~ is the unbiased estimator of ~ based on m degrees of freedom (so m = nk -1 for group k, or m = N-K if a common covariance is assumed). Then logp*(x) = -~plog(2n)- ~[log 11:1 + Bp(m)J 1 [m-P-1 r-1 PJ -2 m (x-J1) ~-(x-ji)-~ is the unique unbiased estimator for log p(x; ,u, ~), where p-1 (2.38) Bp(m) = plog(~m)-L tp(~(m- i)) and tp(z) = r'(z)/r(z) i=O for the digamma function tp (Abramowitz & Stegun, 1965, p. 258). When n goes to infinity there is agreement with the plug-in estimator.\", '& Stegun, 1965, p. 258). When n goes to infinity there is agreement with the plug-in estimator. The proof depends on E 1:-1 = m~-1 /(m-p -1) and that jml:!/1~1 = IJg-1 Zi where Zi are independent X~-i random variables (Mardia et al., 1979, pages 85 and 73 respectively). Moran & Murphy (1979) give explicitly the effect of this bias correction on the linear and quadratic classifiers. The plug-in version of the two-class linear discriminant (2.9) is to allocate to class 1 whenever the estimate of logit p( 11 x) is positive, or (ji1 -ii2f~-1(x-~) + log(n!/n2) > 0. (2.39) Using the unbiased estimator of the log density gives the rule N-K-p -1 [ r-1 ~] p [ 1 1] N _ K (ji1-Ji2) ~-(x -71) + 2 n1 -n2 + log(n!/n2) > 0. (2.40) The effect of the data over the prior class probabilities is reduced, but the constant term will also be important if the class sizes are very different in the training set. For the quadratic discriminant, the effect of the debiasing is to increase the effective variance', \"For the quadratic discriminant, the effect of the debiasing is to increase the effective variance by a factor nkf(nk-p-1) over the usual nk/(nk- 1), and to add a constant which depends on nk. This debiasing is usually unimportant, but can make a difference if for some class(es) nk is only a little larger than p + 1, as Figure 2.12 shows for the data on Cushing's syndrome.\"]\n",
      "[\"Figure 2.12: The decision regions of the debiased quadratic rule for the data on Cushing's syndrome, together with contours for p(x) at negative powers of 10 of the average for the training set. 2.5 Alternative estimation procedures i! o. o a 10 Telrahydrooortlsone Robust estimation of parametric densities 57 50 The normal distribution is a convenient abstraction, but all careful studies show that real distributions do not quite follow a normal distribution but have slightly heavier tails. In addition we should consider the possibility of outliers, that is examples which do not belong to the class under consideration (for example, they might be wrongly labelled in the training set). This has led to discussion of robust estimators of the normal mean and variance for use in 'plug-in' linear and quadratic classifiers (for example, McLachlan, 1992, §5.7) Our classifiers should be different in the two scenarios. If the distributions are non-normal, then we need to take into consideration\", 'in the two scenarios. If the distributions are non-normal, then we need to take into consideration that the tails will be longer, and assuming a t distribution will be more appropriate. As we saw on page 39, this leads to robust estimators of the means and variances of the t distribution, but with a common covariance matrix and equal prior probabilities the Bayes rule is still the best linear rule. This suggests that for the linear rule it is reasonable to plug in robust estimators, whereas for the quadratic rule the rate of decay of the densities in the tails is crucial. On the other hand, if we believe that the true class densities are close to normal but that we have outliers, it will be desirable to plug in robust estimators, since the aim of robust estimation is to characterize the uncontaminated populations, and it is the latter we wish to use in the Bayes rule. Robust estimation in multivariate problems is trickier than appears at first sight, since simple extensions of', 'in multivariate problems is trickier than appears at first sight, since simple extensions of univariate methods (based on down-weighting extreme observations as we saw for fitting the multivariate t) can fail completely with a fraction 1/p of outliers (Huber, 1981, pp. 227-8). The most recent approaches (for example,']\n",
      "[\"58 2 Statistical Decision Theory Rousseeuw & van Zomeren , 1990) start by finding a central 'core' of the data, use the shape of this to identify outliers and then take the mean and covariance matrix of the 'cleaned' data (adjusting the scale of the covariance to compensate for the effect of cleaning). A specific method finds the minimum-volume ellipsoid containing L(n + p + 1)/2J data points. This ellipsoid is used to define a Mahalanobis distance, and all points within the 97.5% point of distance from the ellipsoid centre are retained. Finding the ellipsoid (even approximately) is time-consuming. Weighted estimation It is quite common in medical diagnosis for the abundance of the classes in the training set not to reflect their importance in the problem. Often when the training data are a random sample from the population, the vast majority of cases are 'normals' yet the cost of mis-classifying a diseased case as normal is t times higher than that of a false positive. In screening\", \"a diseased case as normal is t times higher than that of a false positive. In screening problems t can be ten or more. For clarity, suppose we have just two classes, 'diseased' , d, and 'normal', n. The effect of differential costs is to move the decision threshold , so we will declare a positive result when the odds in favour of 'diseased' are not too adverse (better than 1 : t). If we estimate the posterior probabilities p(k I x) from the training data by plug-in methods, we would expect to learn p( n I x) much more accurately than p(d I x). Under some circumstances this can lead to serious bias in the estimators. Consider two normal distributions within the sampling paradigm. For the best quadratic rule, we estimate the class-conditional density Pk(x) from the examples from class k. For the best linear rule, the common covariance matrix I: is estimated from both population s, and hence is principally determined from the sample of 'normals'. This is fine if the covariance matrix\", \"hence is principally determined from the sample of 'normals'. This is fine if the covariance matrix really is the same in each group, but can lead to biased estimates if the covariance matrix in the 'diseased' group is somewhat different from that in the 'normal' group. The effect of this bias on the posterior probabilities is much more pronounced when the classes are unequally represented . The biases in the diagnostic paradigm are often more serious. The plug-in decision rule is to declare a case 'diseased' if p( d I X; e) > c for c = 1/(1 +t) less than 0.5, often much less. We have already noted that plug-in rules tend to produce estimated posterior probabilities which are too extreme, and this will result in a bias when c is small. A further bias results from the disproportion of the two classes in the training set, resulting in underestimation of p(d I x) (since there are many more 'normal' cases to be fitted).\"]\n",
      "[\"The reduction in the size of the training set can have considerable computational benefits, so sampling might be preferred. 2.6 How complex a model do we need? 59 There are two ideas to alleviate these biases. A simple idea is to use a biased sample in the training set; this normally means randomly subsampling the 'normal' group. Let nk denote the numbers of the classes in the training set, and nk the proportions in the population. (We assume that these are known, for example estimated from the original training set.) The fitted probabilities p(k I X; e) then estimate quantities proportional to p(k I x)nk!nk> the posterior probabilities under biased sampling. Thus the decision rule is to declare a case 'diseased' if p(d I x; 8)ndlnd 1 --~:---->-p( n I X; e)nn Inn t or p(d I x; e) > 11(1 + tndnnlnnnd). If we under-sample the 'normal' group by a factor of t, this declares 'diseased' if the odds exceed one. This degree of bias in the training set puts the two groups on an equal footing in\", \"odds exceed one. This degree of bias in the training set puts the two groups on an equal footing in the parameter estimation, thereby reducing the estimation biases. When the biased training set is created by subsampling a larger training set, it seems wasteful to discard data on the 'normal' group. This suggests using weighting rather than sampling. In a weighted procedure, all the 'normal' examples are used, but their contributions to the log-likelihood are weighted by a factor w and, in the sampling paradigm, the size of the 'normal' sample is regarded as wnn. Taking w ~ 1lt will minimize the estimation biases, since the decision rule declares 'diseased' if p( d I X; e) > 1 I ( 1 + t (J) ). These palliatives can also be applied when there are several diseased groups. The formulae can be extended quite easily by working with the odds of each disease to 'normal'. 2.6 How complex a model do we need? Adequacy of a model is not usually an absolute criterion; rather we ask how complex\", 'do we need? Adequacy of a model is not usually an absolute criterion; rather we ask how complex the model needs to be within families of models. For example, we can ask how many input features to use for a linear classifier. In this section we concentrate on the adequacy of a parametric model for the class densities or posterior probabilities. In the next section we look more directly at the effect of model inadequacy on performance, and in the final section of this chapter we consider absolute bounds on performance, averaged over training sets. We have seen two distinct modelling problems. In Section 2.2 we modelled the class densities Pk(x; 8); in Section 2.3 we modelled the posterior probabilities p(k I x; 8). It is important to realize that a model']\n",
      "['60 2 Statistical Decision Theory may be adequate for the posterior probabilities without being adequate for the class densities (see Figure 2.1 on page 27). We will consider the traditional statistical approaches to model complexity which are applied to both problems. These fall into two camps. 1 Iterative selection of a model. For example, in choosing the number of features to use in a linear discriminant or a logistic discriminant there are many variants of stepwise procedures, modelled on those used for regression problems. Backward selection starts with all possible features, and drops them one at a time. Forward selection starts with no features, and adds one at a time. Stepwise procedures start somewhere (usually with all features) and at each step consider either adding or dropping a feature, and choose the best single step before iterating. There are a large number of families of models considered in later chapters with direct analogues of this, for example selecting centres in', 'models considered in later chapters with direct analogues of this, for example selecting centres in radial basis function models, the number of hidden units in a feed-forward neural network and the number of components in a mixture distribution. The distinctive feature of this approach is that the selection is made by a series of pairwise comparisons: is the larger model sufficiently superior to the smaller one? 2 Penalizing the fit by a measure of the complexity of the model. In this approach the search is in principle over all models within the family. Normally we would expect the largest models to fit best, but the penalty on size will tend to ensure that the smallest adequate model is chosen. In practice we may have to confine the search to only some of the models in the family: this could even be done by a stepwise search as in 1. The penalty is often motivated by predicting the degree of fit on a test set. We have left open the measure of fit to be used. The most common is the', \"of fit on a test set. We have left open the measure of fit to be used. The most common is the log-likelihood evaluated at the ML estimate. It is often more convenient to work with the deviance, minus twice the log-likelihood shifted to be zero for the 'perfect' model. In the classification context the perfect model has p(k I x) = 0 or 1, with 1 for the class which actually occurs. In iterative selection we can use likelihood-ratio tests, or equivalently differences in deviances. For a regular problem, the reduction in deviance on adding q further parameters has an asymptotic chi-squared distribution on q degrees of freedom provided the smaller model is adequate (Lehmann, 1986, §8.8). Iterative selection normally works by This may not be possible as a function of the features actually observed.\"]\n",
      "[\"Akaike (1985) reviews the rationale in a more leisurely way than the original papers. 2.6 How complex a model do we need? 61 choosing some conventional significance level (often 10%) to decide a companson. The penalization methods themselves have three schools. The most common one is based on the idea that the deviance will be smaller on the training set than on a test set of comparable size, since we actually chose the parameters to minimize the deviance on the training set. How large would the difference be on average (over training and test sets)? Akaike's (1973, 1974) AIC criterion is based on the answer 2p, where p is the number of free parameters. This does assume that the model is correct. The criterion NIC of Murata et al. (1991, 1993, 1994) is based on the answer 2p* where (2.41) and J and K are defined in Proposition 2.2. As pointed out there, J = K if the model is adequate, so p* = trace(/) = p. Both criteria follow from (2.20), and are based on asymptotic normality of the\", 'p* = trace(/) = p. Both criteria follow from (2.20), and are based on asymptotic normality of the parameter estimates. (Moody, 1991, 1992, uses his effective number of parameters in the same way.) Note that whereas the deviance plus 2p* may be a poor estimate of the mean test-set deviance because of the variability over training sets, differences of this measure between models may be acceptably good estimates of the differences in test-set deviances. Note the may in the previous sentence; the fluctuations are normally small enough to distinguish models whose mean test-set deviances differ appreciably, but they can dominate if the mean test-set deviances are nearly the same (see page 34). Asymptotically in the size of the training set, the use of AIC may choose a model which is at least as large as the correct one with probability one (see Shibata, 1976; Hannan & Quinn, 1979; M. Stone, 1979; this is established for the order of an autoregressive time series, for instance). Thus if', '1979; this is established for the order of an autoregressive time series, for instance). Thus if there is no correct model in the family, AIC will tend to choose larger and larger models as more training data becomes available. The fact that it tends to overshoot the correct size has led to modifications (BIC) which penalize complex models more severely (Akaike, 1977, 1978; Schwarz, 1978) and to justifications based on allowing the complexity of the true model to depend on n (Shibata, 1980, 1981). An alternative way to estimate the expected deviance on a test set of the same size as the training set is to use cross-validation and the other methods of Section 2. 7 on the deviance. A general programme for measuring and controlling the complexity of fitted models based on minimum description length (MDL) or minimum message length (MML) is given by: Rissanen (1983, 1987,']\n",
      "[\"62 2 Statistical Decision Theory 1989); Wallace & Freeman (1987); Barron (1990, 1994); Barron & Cover (1991). In this the deviance of a model is penalized via the minimum length of a binary code needed to represent it. Now in most cases we can only (over-)estimate this length by providing a specific encoding, and the extension to continuous parameters is via discretization. There are many variants within this programme. Vapnik's (1982) structural risk minimization is a similar idea, using a bound on test-set risk based on the work of Section 2.8 and discussed further there. The third idea is to estimate more directly the performance on a test set, by cross-validation and allied methods which we discuss in the next section. The predictive approach Bayesian methods provide an interesting view of these measures, as discussed by Smith & Spiegelhalter (1980) for linear models (but asymptotically most methods are locally linear). In the Bayesian formulation, models are compared via Pr{ M I\", \"most methods are locally linear). In the Bayesian formulation, models are compared via Pr{ M I Y}, the posterior probability assigned to model M, which requires a prior distribution (p M) over models and the ability to integrate out the parameters following the predictive approach : p(Y I M) = j p(Y I M, 8)p(8) de so the ratio in comparing models M1 and M2 is proportional to p(Y I M2)/p(Y I MI), known as the Bayes factor. Note that if the models are nested, the priors will correspond to a prior over the parameters of the larger model which gives positive probabilities to zero values of some of the parameters. Then model choice will involve the controversial testing of 'precise hypotheses', where classical and Bayesian methods are often in conflict (Berger & Delampady, 1987). Note that the predictive approach does not actually select a model, but averages the predictions of the models, with weights proportional to the Bayes factors. This seems not to be widely used, possibly for\", \"with weights proportional to the Bayes factors. This seems not to be widely used, possibly for computational reasons, but can be very effective. It is used for simple logistic models by Stewart (1987), and in time series prediction by West & Harrison (1989). Geisser (1993, §4.1) argues that the only possible loss function which would suggest choosing just one model is one which embodies an extreme principle of parsimony, that only one model is acceptable. Even those who argue for restricting the class of models (such as Madigan & Raftery, 1994) show that averaging is much better Cheeseman ( 1995) claims 'for most interesting problems achieving this goal is NP-hard', referring to finding the model with the minimum message length. We might want to restrict attention to one or a few classifiers for computational speed.\"]\n",
      "['L is the log-likelihood. Various results are known on the large-sample accuracy of (2.42), but our uses will be as approximations far from asymptotia. Kass & Raftery (1995) suggest that at least 5p and preferably 20p training samples are required. 2.6 How complex a model do we need? 63 than any single model. The posterior probability may be spread over many models: Moulton (1991) reports an example in which the top 800 models out of 212 = 4096 are needed to account for 90% of the posterior probability . In pattern recognition our sole concern is future decision making, but in other applications the use of a single model may be more acceptable or desirable (see, for example, the arguments of Geisser, 1987 and A. F. M. Smith, 1991). These ideas go back at least to Box & Tiao (1962). Bernardo & Smith (1994) give an overview of the current philosophical discussion, also to be seen in Draper (1995) and its discussion and references. We will see other ways to choose combinations of models in', 'and its discussion and references. We will see other ways to choose combinations of models in the next subsection, and examples of using estimated Bayes factors to combine posterior probabilities from different models in Section 5.5. Another idea is to use Markov chain Monte Carlo ideas such as the Gibbs sampler to integrate over both the model space and the parameter space, as considered by George & McCulloch (1993) and Madigan & York (1995). Suppose we just use the Bayes factor as a guide. The difficulty is in evaluating p(.r I M). Asymptotics are not useful for Bayesian methods, as the prior on (} is often very important in providing smoothing, yet asymptotically negligible . We will assume that p((} 1.r) is approximately normal with~ mean fJ and covariance matrix V. One approximation is to take (} as the mode of the posterior density and V as the inverse of the Hessian of -log p(B 1.r) (since for a _p.ormal density this is the covariance matrix); we can hope to find (} and V from', \"(since for a _p.ormal density this is the covariance matrix); we can hope to find (} and V from the maximization of log p( (} I .r) = L( (}; .r) + log p( 8) + const. Let E(8) = -L(8;.r) -logp(8), so this has its minimum at fJ and Hessian there of v-1. Then p(.r I M) = j p(.r I 8) p(8) de= j exp -E(8) de ~ exp-E(B) j exp[-~(8-fJ)Tv-1(8-B)] d(} = exp -E(B) (2n)P/21VI1/2 (2.42) from (2.7). (This is sometimes known as a saddle-point approximation or Laplace's method: Lindley, 1980; Tierney & Kadane, 1986. This and other approximate methods are discussed by Evans & Swartz, 1995.) Thus log p(.r I M) ~ L(B; .r) +log p(B) + ~log 2n + ~log I VI. (2.43) It may be feasible to use this directly for model choice, as was proposed for nested models by Kass & Vaidyanathan (1992).\"]\n",
      "['64 2 Statistical Decision Theory If we suppose e has a prior which we may approximate by N{Oo, Vo}, we have logp(ff I M) ~ L(ii;ff)-i(B-8o)Tv0-1(0-8o)-i log !Vol+ i log lVI and v-1 is the sum of v0-1 and the Hessian H of the log-likelihood at B. Thus logp(ff I M) ~ L(B; ff)-i(B-8o)rv0-1(B-Oo)-i log IHI. If we assume that the prior is very diffuse we can neglect the second term, so the penalty on the log-likelihood is -!log IHI. For a random sample of size n from the assumed model this might be roughly proportional to -( i log n) p provided the parameters are identifiable. This is the proposal of Schwarz (1978) derived following the ideas of Smith & Spiegelhalter (1980, §2.1). Others argue for retaining different terms in (2.43); for example Draper (1995, p. 57) retains the second term of (2.43), drops the third and replaces log I VI by -log I HI. The latter might be damaging (Raftery, 1993) and is often of no computational benefit. The assumption that the prior can be neglected is a', '1993) and is often of no computational benefit. The assumption that the prior can be neglected is a strong one, since we may not obtain much information about parameters which are rarely effective, even in very large samples. For example, suppose we have separate parameters for each class-conditional density Pk(x; 8). Then we will learn very little about the parameters of very rare classes, and the effective sample size in the expression for BIC for the parameters for class k will be nk not n. Since we would expect nk oc n, the leading term in n is still -( i log n )p, but as ( i log n) will be quite small for practical n ( 5.75 for n = 100 000 ), replacing i log IHI by (!log n)p can be quite misleading. We should be interested in comparing different models for the same n, and in many problems p will be comparable with n. It seems best to use (2.43) directly. Kass & Raftery (1995) review many of the approaches to approximating Bayes factors; Gelfand & Dey (1994) provide one of the', \"many of the approaches to approximating Bayes factors; Gelfand & Dey (1994) provide one of the clearest accounts of the multitude of variations which have been proposed for model choice. Improper priors over e (those that are not integrable) lead to difficulties, since p(ff 1M) will be unknown up to a constant factor, and might be infinite. It may be possible to resolve this by taking limits of results with proper priors (but often improper priors were chosen to make the integrations feasible). Other approaches are discussed by Kass & Raftery (1995, §5.3) including the device of an 'imaginary training sample' used by Spiegelhalter & Smith (1982). It is not clear This is often called BIC. Jeffreys (1961, pp. 248, 272, 277, 343) discussed special cases many years before in earlier editions.\"]\n",
      "['2.6 How complex a model do we need? 65 however that any of these methods is totally satisfactory, especially when the models under consideration have very different numbers of parameters with improper priors. The difficulty shows up in (2.43). With an improper prior we may treat logp(e) as constant, but there is no reason to suppose that it is the same constant for different models, although careless workers in the neural networks field often do so. We can relate these calculations to the derivations of AIC and NIC. The present derivation is less asymptotic and does take account of a prior. It produces a factor of log n (for some appropriate n) rather than 2 on the penalty for the deviance, which curbs the tendency of AIC to overshoot the true model size. (Remember the Bayes factor is intended for use in averaging not selecting models.) Note that the approach of this subsection necessarily assumes that the model is true while calculating p(ffl M). We defer further consideration of NIC', \"assumes that the model is true while calculating p(ffl M). We defer further consideration of NIC to Section 4.3, where we consider other methods of parameter estimation. Combining models We have already mentioned that the full predictive approach 1s to average models by p(k I x) = LPm(k I x) Pr{m Iff} m rather than choosing one model (unless our loss function includes costs on multiplicity of models). We now consider other ideas for combining models, using ideas which are developed in other contexts in Section 2.7. M. Stone (1974, pp. 126--7) mentioned the idea of using crossvalidation not to choose between models but to combine them. In our context this would amount to combining the posterior probabilities (either plug-in or predictive) from a series of M models. The predictive viewpoint motivates Stone's suggestion of (2.44) m for .a set of constants ( ocm), perhaps confined to a probability distribution and chosen by cross-validation. We could also allow the weights to depend on\", \"distribution and chosen by cross-validation. We could also allow the weights to depend on the class k. This idea has been developed (independently) and extended by Wolpert (1992), under the name of stacked generalization, and applied by Breiman (1992) in the regression context. Breiman's work is precisely within Stone's setting, and shows that in his simple examples it does indeed help to confine attention to non-negative weights (although he\"]\n",
      "[\"66 2 Statistical Decision Theory seems not to have considered a unit-sum constraint on the weights). The idea of averaging (both simple and weighted) for regression neural networks has been suggested many times. (A selection of references is Baxt, 1992; Benediktsson & Swain, 1992; Bridle & Cox, 1991; Hansen & Salamon, 1990; Lincoln & Skrzypek, 1990; Pearlmutter & Rosenfeld, 1991; Perrone & Cooper, 1993; Srihari, 1992; and Xu et al., 1992.) LeBlanc & Tibshirani (1993) took up the same thread, but also considered estimation by the bootstrap. Both cross-validation and the bootstrap can be seen as methods to correct the bias of the deviance (or other measure) on the training set as a function of ( ~Zm), and are used exactly as for the apparent error in Section 2.7. The biascorrected estimate of the deviance is then minimized over ( ~Zm). Their experiments considered non-negativity and unit-sum constraints, but not both together! Wolpert's ideas were much more general than those picked up\", \"constraints, but not both together! Wolpert's ideas were much more general than those picked up by later users. Although (2.44) is suggestive, we might only to want to use it locally in the feature space, and so could allow the weights to vary (slowly) with x. In his general scheme, we use the outputs of all the 'level 0' models (under leave-one-out cross-validation) and the true response as inputs to a 'level 1' procedure which then makes the final decision. At its simplest we could take the predictions from M classifiers and learn how best to combine them to give a single classification. But the outputs of the models can be their posterior probabilities, and we can also pass the inputs through a 'do-nothing' level 0 procedure. So stacked generalization includes any method of combining the outputs of the models, possibly varying with x. A similar approach is taken by Jacobs et al. (1991) in which they train all the classifiers simultaneously and the level-0 classifiers are not\", \"(1991) in which they train all the classifiers simultaneously and the level-0 classifiers are not required to work well over the whole input space. This approach is discussed in Section 8.5. 2. 7 Performance assessment The title of this section begs the question of what is meant by performance. Since we have identified the Bayes rule on the basis of total risk (expected loss) this seems a suitable basis for performance evaluation. When loss (2.2) is used, it is often helpful to plot the expected rate Figure 3.5 on page 114 of misclassification against the expected rate of 'doubt' classification, is an example. usually termed the reject rate. To be explicit, the error rate is the probability of making a definite erroneous classification (including outlier (!)) for a future randomly\"]\n",
      "[\"We use use ±2 standard errors as an approximate 95% confidence interval. 2. 7 Performance assessment 67 chosen sample, previously called pmc, and the reject rate is the probability of declaring doubt, previously denoted pd. Our discussion will concentrate on pmc, but entirely analogous statements follow about pd. Also, statements made about error rates can be replaced verbatim by ones about average cost for other losses. A later subsection on 'confusion matrices' discusses more detailed information on error patterns. The apparent error rate pjilC is the proportion of errors made when classifying either a training or a test set. If the training set is used, pjilC will (usually) be biased downwards. Error rate estimation The easiest way to assess the error rate is to choose a test set independent of the training set (and validation set if used), to classify its examples, count the errors and divide by the size M of the test set. The reject rate is estimated by the proportion of test-set\", 'divide by the size M of the test set. The reject rate is estimated by the proportion of test-set examples which are rejected. These measures are clearly unbiased estimates under all circumstances, but they can be highly variable, and having to use a test set may waste data which could otherwise have been used for training. The idea of a test set is sometimes called the hold-out method and goes back at least to Highleyman (1962a). Simple calculations show that the test set needs to be large for the error rate to be estimated at all accurately. The estimate pjilC = R/ M for an error count R has a binomial(M, pmc) distribution. Thus pjilC has variance pmc(1- pmc)/M ~ 1/4M. Suppose that pmc is around 5% and we wish to know it to around 1%. Then we will want 2J0.05 X 0.95/ M ~ 0.01 or M ~ 1900, which is considerable. (Here we use the normal approximation to the binomial, which is justified at such sample sizes.) Note that the task of comparing the error rates of two classifiers is rather', 'at such sample sizes.) Note that the task of comparing the error rates of two classifiers is rather easier as they use the same test set, a point often overlooked in the literature and taken up in a later subsection. We can also calculate the error rates conditionally for each class, just by counting within each class. If we know the prior probabilities nk, the estimator I: 1tk pjile(k) (2.45) k estimates pmc. This form is important when the test set is a deliberately biased sample, which can be a good idea when almost all the errors']\n",
      "['68 2 Statistical Decision Theory occur in uncommon classes. In general we would expect it to be less variable, but there is a problem that it will be undefined if nk = 0 for any class, which complicates the theoretical analysis. Risk averaging Suppose we knew the posterior probabilities p(k I x) and are using the Bayes rule. Consider a random pair (X, C) from the whole population. Then P(correct I X= x) = P ( C = argmaxp(c I x) I X= x) = maxp(c I x) c c and so 1-pmc = P(correct) = E [ maxp(c I X)]. c (2.46) Both I [ C = arg maxc p( c I x)] and maxc p( c I x) are conditionally unbiased estimators of P(correct I X = x), but the second averages over P ( C I X = x) and so has a smaller variance. Let Z = 1-maxc p( c I X) ~ 1-1/K. Then EZ = pmc and EZ2 ~ (1-1/K) EZ, so Var[maxp(c I X)] = Var(Z) ~ (1 -1/K)pmc- pmc2 c = Var(J [C = argmaxp(c I x)])-pmc/K. c Note that (2.46) does not depend on knowledge of the true class C, which can be useful if the authenticity of the classifications of the', 'of the true class C, which can be useful if the authenticity of the classifications of the test set is in doubt. This can also be an advantage if examples are cheap but accurate classification is expensive, as in almost any form of automated data collection which needs human classification . This approach is sometimes known as risk averaging. Of course, we only very rarely know the posterior probabilities, but (2.46) can be used if we believe we have accurate estimates of them. It will be much better to use predictive estimates p(k I x) rather than plug-in estimates p(k I x; (i) as the latter ignore the variability of e and so tend to underestimate small probabilities, often quite severely. The Compare Figures 2.4 estimate from (2.46) can be based on either the training or the test set; and 2.11. if the training set is used there will be some bias, the size of which will depend on the number of parameters. When the probabilities are estimated maxc p(c I x) will be biased, both because', 'of parameters. When the probabilities are estimated maxc p(c I x) will be biased, both because we cannot usually find unbiased estimators of p(c I x) and because the maximum is a non-linear operation. We would expect the bias to be small for a test set; calculations for one particular classifier are given in Section 6.2. If this method is to be used, it would seem desirable to check if the estimates p(k I x) are reliable, which is itself a check of the adequacy']\n",
      "[\"2. 7 Performance assessment 69 of the model. The methods are part of the more general methodology of verifying if probability forecasters (such as 'the probability of rain tomorrow is 60%') are well calibrated (see, for example, Dawid, 1982, 1986). The basic idea is that if p(k I x) = 11 say, amongst the examples for which we predict 17, the proportion which occur should be about 1'7· We can apply this either to the posterior probabilities of each class, or to the probability maxk p(k I x) of a correct classification. The test set then provides a set of independent events Ei and predicted probabilities fh We can test the calibration by using a non-linear logistic regression on Oi (say by the methods of Chapter 4) and test if the identity is adequate. If it is not, we can even use this regression to re-calibrate the probabilities (an idea for a linear logistic regression going back to Cox, 1958). Various methods for fitting such regressions will be described in Chapters 4 and 5, and an\", '1958). Various methods for fitting such regressions will be described in Chapters 4 and 5, and an example is shown in Figure 3.6 on page 115. There is also a literature on methods of numerically assessing probability forecasts. The most common measures are (half-)Brier scores (Brier, 1950) which is the sum of squared differences between the predicted probabilities and the indicator function that the event occurred (or, equivalently, the sum of (1-p)2 where p is the forecast probability of the positive or negative event which occurred), and logarithmic scoring (Good, 1983) which sums the negative log probability of the event which occurred. Note that logarithmic scoring computes the conditional log-likelihood as used in logistic regression. The effect of using risk averaging with inaccurate probability estimates can be severe; on p~ge 228 there is an example in which the error rate is underestimated by a factor of more than two. Cross-validation Often the use of a test set is regarded', 'by a factor of more than two. Cross-validation Often the use of a test set is regarded as too wasteful of scarce classified data. Can we avoid this by dividing the training set? If we divide the training set into two halves, we could train with one half and test with the other. As the halves are independent samples, the resulting estimator is unbiased. Furthermore, we can swap the halves and still obtain an unbiased estimator, and so the average of the two estimators remains unbiased. The drawback of this approach is that the estimate is an unbiased estimate of the performance using just half the data. Can we do better? Yes, at the expense of more computation. Suppose we randomly divide the training set into V pieces. Then we can use one piece to test the performance of the classifier trained on the remaining (V-1) pieces.']\n",
      "['70 2 Statistical Decision Theory This is again unbiased, and we can average the V such estimates. For moderate V such as 5 or 10, the loss of performance from a smaller training set will usually be small enough, at the expense of V times the computation (although this can be done in parallel if several CPUs are available). (A technical nicety is how to do the average; should we weight by the size of the pieces if they are unequal? Weighting is common practice, as it corresponds to counting the number of errors made by the cross-validated classifiers.) The extreme version of this strategy is to take V very large. Then each test set will contain zero or one examples. This suggests (but does not quite justify) the leave-one-out estimator, in which each observation is tested on the classifier trained on the remaining (N-1) observations, suggested by Mosteller & Wallace (1963), Hills (1966), Lunts & Brailovsky (1967) and Lachenbruch & Mickey (1968) and often rediscovered. The leave-one-out', 'Lunts & Brailovsky (1967) and Lachenbruch & Mickey (1968) and often rediscovered. The leave-one-out version of cross-validation apparently requires a large amount of computation, but for some classifiers this computation can be reduced to a similar level to classifying a test set of n examples (see pages 100, 184 and 200). The leave-one-out estimator is a balanced version of cross-validation in that the sets are chosen of exactly equal size. This can be applied to the V -fold version as well, and indeed we may choose to balance the subsets on other characteristics such as regions of the feature space or even the numbers taken from each class. Are these variants valid? A test-set error rate is an unbiased estimator of pmc provided that C is sampled from its conditional distribution given X, and that X is sampled with density p(x); no independence is needed to justify the unbiasedness of an average. This justifies all the approaches except choosing fixed numbers from each class, but', 'of an average. This justifies all the approaches except choosing fixed numbers from each class, but including the leave-oneout estimator. If we choose fixed numbers from each class, we obtain unbiased estimates of the class-conditional error rates pmc(k), and hence an unbiased estimate if the prior probabilities are known or estimated in the usual unbiased way (from other data). Choosing V = N should give the most accurate assessment, as the true size of the training set is most closely mimicked; it also normally involves the most computation. There is another argument in favour of smaller V. Dropping just one observation assesses the classifier via 0(1/ N) perturbations from the training set. On the other hand the sampling variations in the parameter estimates are (usually) 0(1/ JN), so for large N we end up extrapolating these from much smaller perturbations. Thus cross-validation estimates of performance for large V might be expected to be (and are often reported to be)', 'estimates of performance for large V might be expected to be (and are often reported to be) Leave-one-out cross-validation is also known as ordinary cross-validation. This is sometimes called stratified cross-validation. A good example is a classification tree, where dropping any single example might not alter the chosen topology, only the fitted probabilities at single leaf.']\n",
      "[\"Stone's argument was also given by Liu (1993, 1995). 2. 7 Performance assessment 71 rather variable; taking a smaller V can give a larger bias but smaller variance and mean-square error. We have concentrated on cross-validation for the error rates, but it is also possible to use cross-validation for the 'smoothed' measures such as 1-maxc p(c I X) discussed in the previous subsection. Estimation and model choice Cross-validation is also commonly used for model choice. (Cover, 1969, is the first advocate of which we are aware.) This can be used to estimate either a measure of performance (such as error rate) or a measure of model fit (such as deviance). M. Stone (1974) and Geisser (1975) pointed out that cross-validation could also be used for parameter estimation; just choose the parameter value which minimizes the crossvalidated measure of performance or fit. Then if we want to assess the performance of the resulting classifier by cross-validation, we have to do so by a double layer of\", 'performance of the resulting classifier by cross-validation, we have to do so by a double layer of cross-validation. M. Stone (1977a, b) considered various asymptotics for model selection by cross-validation. Consider first cross-validating the deviance (under ML estimation). This is a sum of terms D; over examples in the training set. We follow the usual notation in which (i) refers to a quantity based on the training set with the i th example deleted. From Taylor expansions we have 2: D;(8(i)) = D(8) + I:[8(i)- 8]T v;(e;) i 8(i)-8 = D\"(8;)-1 D\\'(8(i)) for 0;, 7J; convex combinations of 8(i) and 8 (and since L,Ni Dj(8(i)) = 0 by definition). Under consistency all the estimators converge to Oo, so l:D;(8u)) \"\\'D(8) + I:v;(Oo)TD\"(Oo)-1D;(Oo) = D(8) + trace[D\"(Oo)-1 I:v;(Oo)D;(Oo)T] and the limit of the second factor on the right-hand side is 2p• by the arguments in Section 2.2. Thus leave-one-out cross-validation of the deviance is asymptotically equivalent to using NIC to correct the', 'cross-validation of the deviance is asymptotically equivalent to using NIC to correct the deviance. This suggests that model choice by NIC and by leave-oneout cross-validation are asymptotically equivalent. (For finite classes of models this argument will prove so unless two or more models have the same D(Oo); all true models have the same value, zero.) M. Stone (1977a) gave heuristic arguments and examples for asymptotic consistency of cross-validatory assessment (which follows from']\n",
      "[\"72 2 Statistical Decision Theory unbiasedness and a law of large numbers) and asymptotic efficiency of cross-validatory estimation. Improving on cross-validation Cross-validation was used to estimate the performance (error-rate, loss, deviance) on a test set by constructing a pseudo test-set from the training set. We now take a different viewpoint , of accepting that the performance measure on the training set is biased, but trying to estimate that bias, and correct it using our estimate. For concreteness we will work with error rates (although the principles apply much more widely). We need to distinguish between the pmc, the true error rate for our classifier trained on this training set, E pmc, its average over training sets, and pmc0, the true error rate with the 'least false' parameter eo plugged in. We can then aim to correct the bias of pmc as an estimator of either pmc or pmc0. The first is most relevant for performance assessment of this classifier, the second if we use pmc0\", 'The first is most relevant for performance assessment of this classifier, the second if we use pmc0 as an upper bound for the Bayes risk (which for large parametric families may be close to the Bayes risk) or as a lower bound on the achievable performance within this parametric family. In either case pmc will be biased. The two biases are E[pmc- pmc] and E pmc-pmc0, and in each case we will correct pmc by subtracting an estimate of the appropriate bias. How do we estimate the biases? The method of Quenouille (1949), later termed the jackknife by Tukey, is sufficiently similar to leaveone-out cross-validation to have caused considerable confusion in the literature . Suppose en is an estimate of (} based on n observations, and that its mean has the expansion ~ a1 a2 E (}n = (} + - + 2 + · · · . n n Then each leave-one-out estimator e(i) has mean ~ al a2 E (}(i) = (} + n-1 + (n-1)2 + ... as does their average 0. Now consider ne-(n-1)0. This has mean ~ ~ 3 nE (}-(n-1)E (} = (}-n(n _ 1) +', 'does their average 0. Now consider ne-(n-1)0. This has mean ~ ~ 3 nE (}-(n-1)E (} = (}-n(n _ 1) + O(n-) and so much smaller bias for large n. Thus the jackknife estimator of the bias is (n-1)[0-e]. The most obvious application of the jackknife is to reduce the bias of pmc as an estimate of pmc0. The expansion needed is valid under']\n",
      "['2. 7 Performance assessment 73 very mild regularity assumptions for ML plug-in classifiers, and the biased-reduced estimator of pmc0 is --n-1\\'\"\\'--n pmc--n-L.....t pmc(i) i using E pmc = pmc0 + at/n + O(n-2). It is less obvious how jackknifing can be used to estimate the bias E[pmc- pmc]. Efron (1982, Chapter 7) sketches how to do so. The idea is to compare the error in predicting the omitted sample with that in predicting the (n-1) remaining samples. Let ei be the indicator of the error of predicting the class of Xi from §{i), and pmc(i) the apparent error rate on fitting to §{i)· Then the estimator of the bias is (n-1) --n m~an[pmc(i)- ei] where the scale factor is a sample-size adjustment. Under mild regularity conditions we would expect E pmc = pmc0 + bjn + O(n-2), and so Eei = pmc0+b/(n-1)+0(n-2) and E pmc-pmc = (al-b)/n+O(n-2). Then our bias estimator has mean n -1 [ a1 2 b 2] --pmc0 + --+ O(n-) -pmc0----O(n-) n n-1 n-1 = al -b + O(n-2) n as required. The complete jackknifed', '--pmc0 + --+ O(n-) -pmc0----O(n-) n n-1 n-1 = al -b + O(n-2) n as required. The complete jackknifed estimate of pmc is pmc + (1 -1/n) m~an [ei-Pri1Cu)1 ! which has bias O(n-2). In our current notation the leave-one-out cross-validated estimate of pmc is 2::: ei/n, and so it implies pmc -meani ei as its estimate of the bias. Efron (1982, Chapter 7) gives a suggestive argument why the relative difference between the two bias estimates might be Op(1/n), and hence there would be little practical difference. Our arguments show that if we drop the sample-size correction, thereby making a relative error of 0(1/n), the difference between the two bias corrections is pmc-meani pmc(i) which has a mean of O(n-2), and is often Op(n-2). Note that the computational effort of these two estimators of pmc is almost the same. The bootstrap is loosely related to the jackknife, and conceptually simpler. Suppose we make a new sample of size n by resampling with']\n",
      "[\"74 2 Statistical Decision Theory replacement from our sample, and calculate an estimate e• from the bootstrap sample (as it is called). Then the variability of e·-7i should mimic that ~ 7i-e, in particular the mean of the first should estimate the bias of e. This can be used by actually resampling B times and averaging, or sometimes by finding the mean analytically. In our problem we can bootstrap pmc to estimate pmc0, or bootstrap pmc -pmc to estimate the bias correction. This bias is the difference between the classifier's apparent error and true error, averaged over training sets. To bootstrap this we replace .r by .r· and the mean over x by the average over the points in .r. Thus the bootstrap estimate of the bias is the average (over bootstrap samples) of the error rate on the training set .r· minus that on the larger set .r. Of course, we will evaluate the error rate on the distinct members of .r• using weights for multiple members. Immediately we see a snag, as the example from\", \"members of .r• using weights for multiple members. Immediately we see a snag, as the example from .r being predicted may be in the bootstrap training set .r•, and if it is we may expect to predict it well, for some classifiers far too well. Efron (1983) proposed the '.632' bootstrap, which considers only the predictions of those members of .r not in .r•; specifically for each point x; estimate the error by averaging over those bootstrap samples not including x;, then average over points to get eo. The final estimate is then 0.368 pmG + 0.632 EQ. Here 0.632 is shorthand for (1 -1/e), the limit for large n of the probability that a given observation from .r appears in .r·. Bootstrap methods may also be used to estimate the precision of the apparent error rate pmc, using the variability of pmc • about pmc to estimate the variability of pmc about pmc, for example to estimate the variance of pmc by the variance of pmt•. But we have to be careful, as we should really be interested in the\", 'of pmc by the variance of pmt•. But we have to be careful, as we should really be interested in the mean square error of the bias-corrected estimator, not of pmc, and the bias correction is itself an estimate. Efron & Gong (1983) suggest the mean square error of the bootstrap samples used to estimate the bias gives a lower bound on the mean square error of the bias-corrected estimator. Introductions to the bootstrap are given by Efron (1982), Efron & Gong (1983) and Efron & Tibshirani (1993); Efron (1983, 1986) contain comparisons of error rate estimation methods including those based on the bootstrap. Other comparisons within the pattern recognition literature are given by Chernick et al. (1985) (for linear classifiers), Crawford (1989) (for classification trees) and Jain et al. (1987) and Weiss (1991) (for k-nearest neighbour classifiers). These show some Note that !T. contains some of the members of !T more than once, and (usually) some not at all, so as a set !T• is smaller.']\n",
      "[\"2. 7 Performance assessment 75 support for the '.632' estimator, but by no means universal improvement over leave-one-out cross-validation. The title of this subsection has been chosen in optimism, since the full power of the bootstrap (for example, used in conjunction with ideas such as (2.46)) seems not to have been fully tested. Confusion matrices Thus far we have concentrated on a single measure of performance, the overall error rate. This is natural within our decision-theory framework, but we may want more detail to help understand where a classifier is failing. The next level of detail is the class-conditional error rates previously termed pmc(k), that is the error rate amongst examples of class k. Further, we may want to know which classes are being confused, and so we may wish to know eiJ = Pr{ decision j I class i} which is sometimes called the confusion matrix. Note that the decisions can include 'doubt', ~-The most obvious way to estimate ekj is from the pattern of errors\", \"decisions can include 'doubt', ~-The most obvious way to estimate ekj is from the pattern of errors on a test set, and sometimes the term 'confusion matrix' refers to the matrix of counts of the events 'true class i decided as j'. Almost all the methods we have discussed apply equally to the classconditional error rates. We just consider only those examples with true class k, and in some cases (such as the jackknife) need to take the sample size as the number of class-k examples. Although Efron (1986) derived the '.632' estimator for the overall error rate for two groups, it extends readily to class-conditional rates (Hjort, 1986). Once the conditioning on class k is accomplished, estimating the confusion matrix merely amounts to accounting for which errors were made. The one method that has a less obvious extension is the use of the posterior probabilities at (2.46), since this looks at the predicted rather than true class. Extensions were considered by Schwemer & Dunn (1980),\", 'at the predicted rather than true class. Extensions were considered by Schwemer & Dunn (1980), Basford & McLachlan (1985) and McLachlan (1992). As at (2.46) we at first assume that the posterior probabilities are known. Then eij = Pr{ decision j I C = i} = Pr{ C = i, decision j} /ni = E{p(i I X)J[c(X) = j]} /rri = L 1tk Ek{p(i I X)J[c(X) = j1}. k 1tj We can form an unbiased estimator of eij by replacing the expectations in the final expression by averages over a test set. If the (rri) are']\n",
      "['76 2 Statistical Decision Theory unknown and are estimated as usual by (ni/n), the estimator simplifies to eij = _!_ :t p(i I X!) I [c(Xl) = j] ni 1=1 which can also be seen to be a ratio of unbiased estimators. This suggested to Basford & McLachlan replacing the conditional probabilities and the Bayes classifier c(x) by estimates. Then it may happen that L:j eij =I= 1, so their final estimator n n eij = LP(i 1 Xt)I[C(Xt) = j] / LP(i I Xz) ~1 ~1 is formed by re-normalizing the estimator to sum to one. Note that unlike the estimator of the unconditional error rate, this may be biased even if the posterior probabilities are correct. Comparing error rates A study to compare the error rates of difference classifiers is an experiment, and should be designed and analysed as such. There is much known from many years of theory and experiments in the statistics literature; an excellent basic reference is Box et al. (1978). Experiments in our field are computer experiments and have much in', \"reference is Box et al. (1978). Experiments in our field are computer experiments and have much in common with work in the field of simulation; Kleijnen & van Groenendaal (1992) provide a non-technical introduction in that context which is amplified in Kleijnen (1987). Important ideas from that field include importance sampling and stratified sampling, both of which can be used to design 'difficult' test sets and to compensate for the increased difficulty in estimating error rates. For example, we might arrange for rare patterns to be well-represented in the test set, but down-weighted (as in the study of Candela & Chellappa, 1993, Blue et al., 1994). It should always be possible to give some idea of the variability of a quoted performance estimate. For test-set error counts we can use the binomial distribution or the normal or Poisson approximations to it. For other measures such as the smoothed error counts based on 1-maxc p(x I x) we can use the sample variance, as each example in\", 'smoothed error counts based on 1-maxc p(x I x) we can use the sample variance, as each example in the test set is assumed to be an independent sample from the population on which we are trying to predict the error rate. However, a crucial observation is that since the same test set is used for each method, the comparisons between methods are usually much more accurate than the standard errors suggest. (In the terminology of the design of experiments we have a paired comparison, or a blocked']\n",
      "[\"Table 2.1: Test-set error counts (out of 120) for the Leptograpsus crabs example, from Ripley (1994c). This uses the variance of a binomial distribution . An exact test has precisely the distribution claimed under the null hypothesis. 2.8 Computational learning approaches 77 methods 4-way sex only sex only colour given yes yes no linear discriminant 8 8 8 linear discriminant on log variables 4 4 4 quadratic discriminant 11 9 8 quadratic discriminant on log variables 9 7 7 experiment if there are more than two methods.) Consider the first two lines of Table 2.1. As frequently happens, the 4 errors made in the second line are also made in the first line. The standard error of the difference between 4/120 and 8/120 assuming separate test sets is y'0.01642 + 0.02272 ~ 3.37/120 so a naive comparison would conclude that there was no significant difference. More appropriate methods are available, such as McNemar's test (Fleiss, 1981). Let nA and n8 be the number of errors made by method A and\", \"such as McNemar's test (Fleiss, 1981). Let nA and n8 be the number of errors made by method A and not method B, and vice versa, so in our example nA = 4 and nB = 0. Then McNemar's test (with continuity correction) refers InA -nBI-1 y'nA + nB to a N(O, 1) distribution, and an exact test refers nA to a binomial (nA + n8, 1/2) distribution. This suggests that we need nA ~ 5 for a significant difference (but this is only sufficient if n8 = 0 ). Thus large test sets are needed to distinguish between classifiers of similar performance; to detect a 1% difference in error rate needs at least 500 examples. So although the difference here is suggestive, the sample size is too small for a definitive conclusion . One pitfall to be avoided is to give too much emphasis to statistically significant results. In an experiment in which method A with error rate 29.8% is significantly better than method B with error rate 30.1 %, it is clear that the difference is unlikely to be of practical importance,\", \"with error rate 30.1 %, it is clear that the difference is unlikely to be of practical importance, especially if we estimate the Bayes risk as 6% by the methods of Chapter 6. 2.8 Computational learning approaches One recent strand of theory looks at what Valiant (1984) called 'the theory of the learnable' and has since become known as PAC-learning (for probably almost correct). Suppose we have a training set of n\"]\n",
      "[\"78 2 Statistical Decision Theory samples, and use these to fit a classifier g from a class !#' of possible classifiers. If the class !#' is not too large and includes the true classifier, we would expect that for large enough n the fitted classifier g would be 'close' to the true classifier f. Thus the theory addresses the question of how large the training set needs to be. How should we measure closeness? The obvious way is to compare the decisions made by f and g for a randomly chosen sample from g( x rl, where rl is the set of classes, and ask that they agree with high probability, that is Pr{g(X) -=/= f(X)} < c, (2.47) say, for some pre-specified E. (This implies that the true error rate of the classifier g exceeds the Bayes risk by less than c.) The left-hand-side of this statement is a random variable, as it depends on the training set. In PAC-learning we ask that (2.47) be true for a high proportion of training sets, say with probability exceeding 1 -b, for a sample size no more\", \"a high proportion of training sets, say with probability exceeding 1 -b, for a sample size no more than polynomial in 1/c, 1/b and that g be fitted in time polynomial in n. Here we are more concerned with the sample size than the computational complexity of finding g. In the 'noise-free' case when there is a f E !#' which correctly classifies any training set, Pr{g(X) -=!= f(X)} is the error rate of g, so (2.47) corresponds to low error rate. In the 'noisy' case we actually study the difference between apparent and true error rates. The bounds used in studying PAC-learning are often called worsecase bounds since they apply to any distribution over g( x rl, provided that both the training set and future samples are drawn (independently) from the same distribution. Much of the theory currently available applies only to two-class problems, and the results are most refined in the noise-free case. A warning. The results of this section are often misinterpreted. They apply over all possible\", \"case. A warning. The results of this section are often misinterpreted. They apply over all possible training sets !T, and assert that events occur for most (or few) training sets for a given model. As such they are subject to the usual criticism of frequentist methods, that we cannot know if our particular training set is an exception. But the difficulty here is particularly acute, as the model will have been chosen on the basis of the training set, indeed often on the basis of the sort of performances that these bounds guarantee. A typical claim is 'If our network can be trained to classify correctly a fraction 1-(1-y )E of the n training examples, the probability that its error-a measure of its ability to generalize-is less than € is at least 1 -o.' However, the probability is in fact guaranteed to be less than 1-b over all !T, including those our model will not fit well, not the conditional\"]\n",
      "['For each g with overall error rate at least E, the probability of getting n correct samples is at most (1-E)\", and one of at most r classifiers is chosen. 2.8 Computational learning approaches 79 probability asserted. To use these results (correctly!) in performance assessment , they have to be applied to the whole procedure including model choice for every problem, with any exceptions included in the probability b. (This is pointed out by Wolpert, 1994b.) We have never seen this done. The results are interesting theoretically, and perhaps useful in model choice, but are very conservative. Much better bounds should be possible using the knowledge of the world gained from the examples to fit a class of models. We give examples of the \\'dimensions\\' used in the results in later chapters. Finite sets of classifiers The simplest approach is to assume that there is a finite number r of distinct classifiers in ffi\\' (Blumer et al., 1987). Then the probability that a g chosen consistent with', 'classifiers in ffi\\' (Blumer et al., 1987). Then the probability that a g chosen consistent with (that is correctly classifying all of) a training set of size n yet having overall error rate at least E, is at most r(1-E)n. We can invert this bound to show that log r + log -} log r + log ! ----,,----,-:---\"- ~ ----\"--log(1- E) £: (2.48) training samples are enough to ensure that if we classify the training set correctly, the true error rate is less than £: with probability at least 1-b. Now suppose that we cannot find a classifier in our class correctly classifying all cases, so the apparent error rate pmc is non-zero. There are several possible bounds. The number of errors is a binomial ( n, pmc) random variable so the Bienayme-Chebychev inequality gives ~ pmc(1- pmc) Pr{lpmc(g)- pmc(g)l > £:} ~ 2 nE which, allowing for the r possible classifiers, gives r n ~ 4bE2 which is much worse than the noise-free bound for small £: or large r. The bound of Hoeffding (1963) gives for each', 'than the noise-free bound for small £: or large r. The bound of Hoeffding (1963) gives for each classifier Pr{pmc < pmc-e} ~ exp-2ne2 and twice this probability for a two-sided bound, so logr +log-} n ~ 2~:2']\n",
      "[\"80 2 Statistical Decision Theory suffices to bound the optimism in the apparent error rate by E for a proportion (1 -<5) of training sets. This is better, but still has rate O(c-2). To overcome this we have to look at relative error with Chernoff's (1952) bound: Pr{pmc < (1-y)pmc} ~ exp-!ny2pmc; this gives logr +log-} n ~ 2 2 y E if we confine attention to classifiers with pmc > E. Thus just as for the noise-free case we consider the case in which we are doing badly and are not aware of it from the fit on the training set. Taking y ---+ 1 gives double the previous bound on the sample size for a perfectly-fitted training set. A combination of absolute and relative error is obtained by using the metric d ( ) _ lr-sl v r,s ---'-----'----v+r+s used by Pollard (1986) and Haussler (1992). For large v this behaves like absolute error, for small v like relative error. Note that for arguments in [0, 1] (such as error rates) we have lp-ql v+2 ~dv(p,q)~lp-ql, and that de(P, q) > ! implies IP-ql >\", '1] (such as error rates) we have lp-ql v+2 ~dv(p,q)~lp-ql, and that de(P, q) > ! implies IP-ql > E, and is equivalent for p = 0. We have the bound (Haussler, 1992, Theorem 1) Pr{ dv(pmc, pmc) > oc} ~ 2 exp -nvoc2 which translates into the sample size bound Iogr +log~ n~ 2 voc To compare this bound with the previous ones, note that Pr{lpmc-pmcl >c} ~ Pr{dv(pmc,pmc)>c/(v+2)} ~ 2 exp -nvc2 j(v + 2)2 which is minimized by v = 2 as 2 exp -nE2 /8. For a Chernoff-like bound we have Pr{pmc < (1-y) pmc} ~ Pr{ dv(pmc, pmc) > Y pmc } v +pmc ~ 2exp-nv [ ypmc ]2 =2exp[-!ny2pmc] v +pmc The precise form of the bound used here is due to Angluin & Valiant (1979). In Bather (1996, p.340) Chernoff says the bound should be named after Herman Rubin.']\n",
      "['This concept is discussed very clearly by Pollard (1984). Anthony & Biggs (1992) give full but opaque versions of the proofs of Blumer et al. (1989) for the noise-free case, except for measurability conditions. 2.8 Computational learning approaches 81 on taking v = pmc. Note that none of the bounds in this subsection depend on the number of classes, as they work directly with error events. Infinite number of two-class classifiers A key quantity in the PAC-learning results is the Vapnik-Chervonenkis (or VC) dimension d of :F. Consider the set of functions {0, l}n -+ {0, 1} induced by evaluating functions in :F at the n points of the training set. (That is, the induced function gives the predicted class for each of the possible class assignments to the training set.) Let the number of distinct functions be A(n). In many cases this number will be 2n for small n, since all possible functions are induced. Let d be the largest value of n such that A(n) = 2n for some training set of size n,', \"are induced. Let d be the largest value of n such that A(n) = 2n for some training set of size n, or infinity if there is no such number. Then it turns out that for d < oo we have (2.49) the last inequality holding for n ~ d ~ 1 (Blumer et al., 1989, Proposition A2.1). This result is sometimes called Sauer's lemma; its history is traced by Assouad (1983). Note that these bounds apply to all training sets; from now on we will use A(n) to denote the maximum over training sets of size n, and to use the results we will replace it by one of the bounds in (2.49). We will give the most precise results available for two classes (from Blumer et al., 1989) then sketch how they are derived. (There are benign measurability conditions which we ignore.) As a simple example of the VC dimension, consider [!( c: Rm and classifiers of the form m sign(LaiXi >b) i=l which we shall meet under the name of perceptrons in Section 3.6. Cover (1965) showed that these rules have VC-dimension m + 1 (as follows\", 'in Section 3.6. Cover (1965) showed that these rules have VC-dimension m + 1 (as follows from Proposition 3.1 on page 119). On the other hand, for binary inputs, there are between 2m(m-l)/2 and 2m2 different functions generated by perceptrons (Muroga, 1971), so the bound given by {2.48) is of the form m2 log 2 + log t n ~ ------\"€ The following proposition usually gives considerably tighter bounds:']\n",
      "['82 2 Statistical Decision Theory Proposition 2.5 (Blumer et al., 1989) Let d denote the (finite) VC dimension of ff. (i) Given 0 < E\\' < 1, the probability that there is a classifier g E ff consistent with n training examples and with true error rate greater than E\\' is bounded above by (ii) If (4 2 8d 13) 4 [ 2 12] n ~max ; log2 b\\'-; log2-; or n ~; log2 b + dlog2-; the bound in (i) is less than (J. (iii) For given 0 < E\\' ~ 1/8, (J ~ 1/100, d ~ 2 and (1-E\\' 1 d-1) n < max -E\\'-log 2 b\\' 32E\\' fix an algorithm to select a classifier g for each possible training set of size n. Then there is a probability distribution on PI x { 0, 1} and a function f E ff which correctly classifies examples with probability one, but the probability exceeds (J that the algorithm gives a classifier with error rate exceeding E\\'. The lower bound in (iii) was shown by Ehrenfeucht et al. (1989) by exhibiting a suitably malicious distribution, which concentrates on d points and gives probability 1 -8E\" to one of them.', 'malicious distribution, which concentrates on d points and gives probability 1 -8E\" to one of them. The conclusion from the proposition is that to achieve a high-probability guarantee of an error rate of less than E\\' we must take the size of the training set to be at least of order d/E\" log(d/E). This is, however, very much a worse-case bound, and such empirical evidence as there is (such as Cohn & Tesauro, 1992) suggests that practical performance is closer to the lower bound given by (iii), and can even be well below that bound for any \\'normal\\' distribution of examples. Similar results are known for the case with no perfect classifier: Proposition 2.6 Let d denote the (finite) VC dimension of ff. (i) (Vapnik & Chervonenkis, 1971) For any E\\' > 0 Pr{ sup lpmc(g)- pmc(g)l > E\\'} ~ 4L\\\\(2n) exp[-nE2 /8] gES&\\' and the probability is less than (J if 16 [ 4 32e] n ~ E\\'2 log b + d log E\\'2 . (2.50) The constants in (2.50) can be improved. The factor 1/8 in the exponent can be removed (Parrondo', 'The constants in (2.50) can be improved. The factor 1/8 in the exponent can be removed (Parrondo & Van der Broeck, 1993) at the expense of increasing the constant: the claims of Vapnik (1995, pp. 66, 85) are not supported by the belated proofs in Vapnik (1998).']\n",
      "[\"2.8 Computational learning approaches 83 (ii) (Vapnik, 1982, with a slight improvement by Anthony & ShaweTaylor, 1993) For any 11. > 0 { pmc(g) -piTie(g) } Pr sup > 11. ::( 4Ll{2n) exp -ina2 g .jpmc(g) (2.51) and the probability is less than <5 if 8 [ 4 16e] n ~ 11.2 log J + d log 11.2 . (iii) (Blumer et al., 1989) Given 0 < E, y ::( 1, the probability that there is a classifier g E ~ with true error rate pmc exceeding E and apparent error rate piTiC < (1-y)pmc is bounded above by 4Ll(2n) exp -h2nE. This probability is less than (J if n ~max(+ log~, 1~d log 126) or n ~ --:.-[log~+ dlog 1~e] . YE u YE YE YE u YE (iv) (Haussler, 1992, Theorem 3) For any 11. > 0 Pr{ sup dv ( piTie(g), pmc(g)) > 11.} ::( 4L'l(2n) exp[-!nva2]. (2.52) gE.?F If we can find a classifier g E ~ which fits the training set exactly we could apply Proposition 2.5(i) or Proposition 2.6(iii) with y = 1, but the bound given by Proposition 2.5 is smaller. Part (iv) implies somewhat weaker versions of parts (i) and part\", 'by Proposition 2.5 is smaller. Part (iv) implies somewhat weaker versions of parts (i) and part (iii) with factors 1/16 and 1/8 in the exponent (using v = 2,11. = E/4 and v = E,IJ. = y/2 respectively). Proposition 2.6(i) has been used to give upper bounds for PAClearning by Pearl (1979) and Abu-Mostafa (1989). This bound can be improved for large nE2; Devroye (1982) has Pr{ sup lpiTie(g)- pmc(g)l > E} ::( 4e4€+4f\"2 Ll(n2)exp[-2nE2] (2.53) gE.fF and Alexander (1984) has Pr{ sup lpiTie(g)- pmc(g)l > E} ::( 16(ynE)4096(d+l) exp[-2nE2] (2.54) gE.fF for nE2 ~ 64, which translates to a guarantee for nE2 ~ [log ~6 + 1024(d + 1)log 2048~ + l)] ,64.']\n",
      "['84 2 Statistical Decision Theory So far we have considered these results as bounding the size of the training set. It is possible to change our point of view and seek upper bounds on the true error rate; in fact given the probability framework we will have (conservative) upper confidence limits corresponding to probability 1 -!J. From (2.48) we have an upper bound of the form (log r -log !J)/n if we can always fit n training cases exactly. From part (i) of Proposition 2.5, in the same case the upper bound is 2d 2ne 2 2 e ~ -; logz d + n logz b which shows convergence at slightly less than 0(1/n). On the other hand, if the Bayes risk is non-zero, we obtain convergence at rates around 0(1/ ..[ri) from Proposition 2.6 parts (i) and (iii). These give lpmc-pmcl ~ -d log -+ log -8 [ 2ne 4] n d lJ --· 4 [ 2ne 4] pmc -pmc ~ .JPrliC n d log d + log b . Devroye (1988) considered the expected maximal difference (over classifiers) between pmc and pmc rather than confidence limits for pmc, and also', \"difference (over classifiers) between pmc and pmc rather than confidence limits for pmc, and also looked at the direct calculation of d(m) for practical families of classifiers. One way to look at these results is in terms of empirical risk minimization. In the noise-free case we select a classifier which makes the minimum number (zero) of errors on the training set. For the noisy case it is convenient to choose a classifier with the same property, as then the upper confidence limits are tightest on the true error rate. This amounts to a parameter estimation strategy, although often it will not lead to a unique parameter estimate. If we then consider families §m of models of increasing flexibility, we expect to obtain a lower apparent error rate as m increases, but a confidence limit on the error rate which will decrease and then increase. Vapnik's (1982, 1992) structural risk minimization chooses the model class to minimize this bound. (Note that minimizing bounds, especially those\", 'chooses the model class to minimize this bound. (Note that minimizing bounds, especially those as loose as these appear to be, may not be a good idea!) Outline of proofs We will only give the main ideas of the proofs, omitting details of measurability. It may be puzzling that d(2n) appears in Propositions 2.5 and 2.6 since we have only n training samples. The reason is an idea']\n",
      "[\"2.8 Computational learning approaches 85 that Pollard (1984) calls symmetrization. Suppose we consider two independent training sets of size n. Let 11 be the true error rate and rfi be the apparent error rates on the two sets. Then Pr{sup 1111-111 > e} ~ 2Pr{sup 1111-1121 > !e} (2.55) gE.'F gE.'F for n > 2je2. This reduces the computation to comparing two independent training sets of size n, thus to events on a training set of size 2n. This will be used in the proof of (2.50). Proof of (2.55): Fix a classifier with true error rate 11 and consider n1Ji> which has a binomial (n, 11) distribution. By the Bienayme-Chebychev inequality, for n > 2/e2 we have Now condition on the first sample and choose a g which maximizes the left-hand side of (2.55). Then conditionally we have Pr{ I1J2(g)- Yf(g)l ~ !e I g} ~ ! since the second sample is independent of the first. Thus unconditionally and ~ Pr{ I1J1 (g)-Yf(g)l > e, I1J2(g)- Yf(g)l ~ !e} ~ !Pr{l171(g)- Yf(g)l > e} = !Pr{ sup ll11-111 > e}\", \"Pr{ I1J1 (g)-Yf(g)l > e, I1J2(g)- Yf(g)l ~ !e} ~ !Pr{l171(g)- Yf(g)l > e} = !Pr{ sup ll11-111 > e} gE.'F which suffices. D Now consider the class /Fe of classifiers with true error rate 11 at least e; we will show Pr{1J1 = 0 for some g E /Fe} ~ 2 Pr { 1J1 = 0, 1J2 ~ 11/2 for some g E /Fe} (2.56) for n > 8/e. Again condition on the first sample and choose g E /Fe which is consistent with it (if possible). Then conditionally (~ 1 I ~ =) p (I~ I 1 ) 4(1-'1) 4 1 Pr '72 < 2'1 g E :#'e ~ r '72-11 > 2'1 ~ ~ -~ 2· n17 ne\"]\n",
      "[\"86 2 Statistical Decision Theory Thus as before Pr{17t = 0,172 ~ !11 for some g E 37'e} ~ Pr{17t(g) = O,l72(g) ~ !11(g)} ~ !Pr{17t(g) = 0}. Consider the right-hand side of (2.56). We condition on the locations of the 2n training points, and consider only their order. Of the 22n possible assignments by classifiers in 37' of labels to points, at most ~(2n) will occur. Suppose there are t erroneous assignments of labels by an eligible classifier; we must have t > !n11 ~ !nf'. The probability (over permutations) that these all occur in the second sample is (;);e;) ~ 2-t ~ 2-ne/2, so the probability of the event {17t = 0, ll2 > 1112 for some g E 37'e} is at most ~(2n)2-ne f2, which with (2.56) establishes part (i) of Proposition 2.5 for nf' > 8, hence ~(2n) > 8. The (uninteresting) remaining cases can be proved by showing Pr{l72 < !11 }~! actually holds for n > 2/€. Part (ii) of Proposition 2.5 follows from part (i) using the second bound in (2.49). To cover all the cases, consider a bound\", \"follows from part (i) using the second bound in (2.49). To cover all the cases, consider a bound of the form A~(2n)e-Bn ~A (2~n) d e-Bn ~b. Note that logx ~ x-1 can be manipulated to give logx ~ Cx-logCe. Take C = r~.Bjd for 0 < r~. < 1. Then r~.Be rt.Bn ~ dlogn +log d' so it suffices to choose n satisfying A 2 (1- r~.)Bn ~log b + dlog r~.B· The inequalities come from r~. = 1/2 and 8/ log 2 ~ 12. We return to (2.55). We condition on the 2n examples and only consider their order. Let ei = I (error on sample i ). Then half the right-hand side of (2.55) is n Pr{ sup I L)ei- ei+n)l > nf'/2}. gE§ i=l Consider random permutations of the total sample. The terms ei -ei+n are bounded by ±1 and have a symmetric distribution with mean zero. We need only consider permutations which swap elements between the two sets, so consider Yi = ±(ei-ei+n) independently with probability\"]\n",
      "[\"2.8 Computational learning approaches 87 a half for each sign. Hoeffding's (1963) inequality asserts that the probability over these permutations is bounded by n Pr{jl:.::Yij > 1nc} ~2exp-2n(c/2)2/4=2exp-nc2/8 i=l and averaging over the remainder of the permutation distribution does not affect the bound. As before, allowing any g E .'F gives rise to at most ~(2n) assignments of errors. With (2.55) this gives (2.50). Part (iii) of Proposition 2.6 follows immediately from part (ii) on taking rx2 = y2c. (The reduction of the constant to 4 is from Anthony & Shawe-Taylor, 1993.) We will prove part (ii). Choose g E .'F as before, and assume 1J = pmc(g) > rx2 and nrx2 > 4, or the bound is trivial. Since i]2 has a binomial (n, 1J) distribution with n1J > 4, Pr{ih > 1J} ~ 0.32768 > 1/4. (The worst case occurs with 1J = 1 -1/n and n = 5, hence this value.) Now ift < 1J -rxJil and if2 > 1J imply if2 -ift > rxJ[1(ift + if2) J. (Show this by minimizing [if2-ift]/rxJ[1(ift + if2)] over ift, which is\", '-ift > rxJ[1(ift + if2) J. (Show this by minimizing [if2-ift]/rxJ[1(ift + if2)] over ift, which is clearly achieved by taking this as large as possible, and its bound is least restrictive if 1J = \\'h.) Thus ~ Pr{ift < 1J -rxJii} ~ Pr{ift < 1J -rxjii, if2 > 1J} ~ Pr{if2-ift > rxJ[1(ift + if2)]}. As before we consider random swapping permutations. If all Yi are zero then if2 = ift and the probability is zero, so suppose there is at least one Yi + 0. Hoeffding\\'s inequality (1963, Theorem 2) gives n 2n Pr{if2-if1 > rxJ[1(ift + if2)]} = Pr{L Yi > rxJ[1n Lei]} i=l i=l 2nrx21 L~~ ei S:: exp 2 z-l = exp \"\"\" 42:¥/ 2 \"\\'2n nrx L....i=l ei s:: exp _lnrx2 4l:IYil \"\"\" 4 More general problems The methods based on VC dimension are confined to two classes, since the VC dimension itself is. There are several extensions we might need to consider: 1 more than two classes; 2 loss functions other than the error rate;']\n",
      "['88 2 Statistical Decision Theory 3 characterizations other than the VC dimension, since this is either unknown or infinite for many of the classifiers we would want to consider in practice. These are beginning to be addressed, notably by Haussler (1992). An alternative approach for multiple classes is to use the VC dimension of the graph of the classifier, a subset of [!( x ~. as in Shawe-Taylor & Anthony (1991). The loss functions we consider will be bounded by 0 and 1. Since our decision space is finite, this amounts to a possible re-scaling, but excludes working with the deviance, for example, unless probabilities are bounded below. Techniques are available for unbounded loss functions (Pollard, 1990). The general technique to replace the VC dimension is one of approximating the infinite class of functions by a finite €-cover, that is a set of functions such that any function is within distance € of a member of that set. Let %(€) be the smallest number of points in a €-cover, which', \"distance € of a member of that set. Let %(€) be the smallest number of points in a €-cover, which is closely related to the maximum number At{€) of points at least € apart which can be packed in: in fact Jt(2€) ~ %(€) ~ At(€). Then the bounds will be in terms of the expected value of % applied to the family of functions evaluated at the training set. For example (Haussler, 1992, lemmas 13 and 14) Pr{ sup dv(R(g), R(g)) > o:} ~ 2E %(o:v /8) exp -o:2vnj8 gE$' where R and R are the risk and estimated risk respectively , and L1 distance is used between loss functions. Very similar ideas occur in the method of sieves (Grenander, 1981; Geman & Hwang, 1982). The concept of pseudo-dimension (Pollard, 1990; Haussler, 1992) generalizes the VC dimension and provides a convenient way to bound covering numbers. Consider orthants of IR.P, possibly shifted to a new origin. The pseudo-dimension is the largest n for which there is a training set of n points such that the loss functions evaluated at\", 'largest n for which there is a training set of n points such that the loss functions evaluated at those points meet all the orthants for some origin. Clearly for {0, 1 }-valued functions the pseudo-dimension is the VC dimension. If the set of loss functions for all rules under consideration has pseudo-dimension d then (Pollard, 1984, p. 27; Haussler, 1992, Theorem 6) [2e 2e] d .!V(€) ~ At(€) < 2 -;log-; Such notions are often referred to as metric entropy. An orthant is specified by giving the sign of each coordinate.']\n",
      "['2.8 Computational learning approaches 89 for any training set, where e is measured in an Lt distance. These two results combine to give the bound 8 [ 8 8e] n ~ -2-log ~ + 2d log -0( v u ocv to ensure that Pr{dv(R(g),R(g)) > oc} with probability at least 1-~-']\n",
      "[]\n",
      "['3 Linear Discriminant Analysis In this chapter we discuss methods which arise in statistics and in pattern recognition based on linear combinations of the feature vectors (so we assume that the feature space f!l is contained in 1lV ). These methods provide the templates for generalization to flexible non-linear methods discussed in the next two chapters as well as being of interest in their own right. We can identify three distinct ways in which the idea of approximating a function f from f!l to IRK can be used to produce a classifier, although all have variations on their themes. 1 Take fk(x) = p(k I x) = E[J(Y =k) I X= x] and f(x) = (fk(x)). The Bayes rule chooses a maximizer of fk(x). Define target tk to be the kth unit vector. Since IIYII denotes the norm llf(x) _ tk112 = -2fk(x) + 1 + llf(x)ll2 of a vector y . The L1 distance between x and y is Lilxi-yi[. the Bayes rule amounts to choosing the nearest target to f(x). This leads to ways to approximate by f(x; 8) based on choosing e', \"the nearest target to f(x). This leads to ways to approximate by f(x; 8) based on choosing e to make the predictions for the training set as close as possible to the targets. 2 Dietterich & Bakiri ( 1991, 199 5) consider coding the class targets tk to be widely spaced in :!l' = {0, 1 }m for m > k, and learning a function f from f!l to [0, l]m. The classifier then chooses the nearest target in :!l' to the prediction f(x) for a new example. The actual coding is done using error-correcting codes, and the distance is L1• We can view this approach as training a classifier for m pseudoclasses, and then mapping the distribution over pseudo-classes to the K real classes. 3 We have seen that the Bayes rule maximizes log p(k I x), and the multiple logistic model (2.29) is a linear model for these log posterior\"]\n",
      "['92 3 Linear Discriminant Analysis probabilities. Variants which are less principled but commonly used are separate logistic models of each class versus the rest or versus a reference class (see Section 3.5). We vary slightly our usual notation: there is a training set of n observations (or examples) ofa p-variate pattern, and these observations are classified into g groups. Note that the groups need not coincide with the classes, and in the less flexible methods of this chapter it may be desirable to split some of the classes. For example, in symbol recognition we might divide the class for sevens into crossed and uncrossed sevens. If we let the classifier choose the best group and then assign to its class we would be using a cost structure which penalizes the wrong choice of group rather than class. The cost structure based on groups corresponds to adding posterior probabilities over groups to form the posterior probability for the class, then choosing the class with the largest', \"groups to form the posterior probability for the class, then choosing the class with the largest posterior probability. Let X denote the n x p matrix of examples, and G the n x g matrix of indicator functions for the groups (i.e. gij = 1 if and only if observation i belongs to group j ). Note that GT G = diag (ni), where ni is the number of observations for group i. A typical example will Remember x is a row be denoted by x and is a row vector where necessary (as it is a row of vector. X ), and T denotes the transpose of a vector or matrix. 3.1 Classical linear discrimination We will normally assume (to ease the notation) in this section and in Section 3.2 that X has been centred; each feature variable has had its average subtracted. We saw on page 36 that if we assume the probability model in which the observations for group j are normal with mean l'j and common covariance matrix I:, the Bayes rule is to allocate a future observation x to the group for which -2 logp(j I x) =\", \"I:, the Bayes rule is to allocate a future observation x to the group for which -2 logp(j I x) = (x-P.j)I:-1(x-l'jf-2 log 1tj + const (3.1) is smallest. The first term on the right is known as the Mahalanobis distance from x to the group mean. Expanding this out we find -2logp(jlx) = -2xi:-1p.J +p.ii:-1p.J -2logni+const+xi:-1xT (3.2) which is a linear term in x plus a quadratic term which does not depend on the class. Since we wish to maximize p(j I x) or equivalently to minimize (3.2), we may as well maximize the linear terms LDAj = 2xi:-1 p.J-Jlji:-1 p.J + 2 log 1tj.\"]\n",
      "['3.1 Classical linear discrimination 93 The space f!{ = JR.P is partitioned by hyperplanes, another sense in which this is linear discrimination. For the special case of g = 2 groups, comparing LDA2 with LDA1 amounts to computing the linear function LDA2-LDA1 and choosing group 2 if and only if it is positive. More generally, the comparison can be done in a space of dimension at most g -1, and the distances computed in such a space, as we shall show more formally. In practice the population quantities /lj,\"f. are estimated by mj, W, where W is the within-group covariance matrix defined below, rather than the maximum likelihood estimates considered in Section 2.2. (Other estimates are considered in Sections 2.4 and 2.5, but very rarely used in applied statistics.) Fisher\\'s linear discriminant The classical method of linear discrimination was described by Fisher (1936) for two classes and extended to more by Rao (1948) (but sometimes attributed to Bryan, 1951). It uses a different', \"and extended to more by Rao (1948) (but sometimes attributed to Bryan, 1951). It uses a different criterion not based on the decision theory of Chapter 2; it seeks a linear combination xa of the variables which maximizes the ratio of its between-group variance to its within-group variance. This is appealing even if multivariate normality is implausible. These terms come from the analysis of variance and are defined for a variable y = (yi) as follows. Let mj denote the mean of y in group j, let m = (mj) and let [i) denote the group of observation i. The n x g matrix G indicates which group each observation belongs to, so gij = I (j = [i] ). Then the within-group variance is defined to be W _ Li(Yi- m[iJ)2 IIY-Gmll2 y-n-g -n-g and the between-group variance is B _ Li(m[iJ-:Yf = y-g-1 IIGm-ylf g-1 We can extend these definitions to the multivariate observations X by defining M as the g x p matrix of group means and W =(X -GMf(X -GM) n-g ' B = (GM -lxf(GM -lx). g-1 Then the linear\", \"g x p matrix of group means and W =(X -GMf(X -GM) n-g ' B = (GM -lxf(GM -lx). g-1 Then the linear combination xa has variances aT W a and aT Ba, and total variance Ts _ r(n-g)W+(g-1)B a ra-a 1 a. n-\"]\n",
      "['94 3 Linear Discriminant Analysis It may be that the training sample is known not to be a random sample from the underlying distribution, but that the numbers nj of observations from each group were pre-specified. (This is often done to ensure that sufficient information is obtained on rare groups.) Then better estimates are obtained by weighting the observations in group j by nnj/nj in forming B, W and x. The interpretation remains unchanged. (Note that the maximum likelihood estimates are not weighted in this circumstance; the weighted versions are thought to be more accurate when the group covariance matrices actually differ.) The classical computational approach is to seek a rescaling of the variables xS such that their within-group covariance matrix is the identity matrix I and then perform an eigenvalue decomposition of B expressed on these variables. It will suit some of our further computations better to rescale the variables so that the total variance is nl /(n -1). The', \"further computations better to rescale the variables so that the total variance is nl /(n -1). The rescaling is achieved by taking a matrix S such that srxrxs = nl. This can be done in a number of ways: a simple one is to use the QR decomposition (Golub & Van Loan, 1989) of X: QX = [ ~] where Q is a n x n orthogonal matrix and R is a p x p upper triangular matrix. Take S as the solution to RS = Jill. Then on the rescaled variables X' = X S we have X' T X' = nl. There will be difficulties if the covariance matrix does not have full rank, and this can be hard to identify numerically. (For example, a column differing only in the fifth significant digit could be constant up to rounding error or could be an extremely precise measurement of, say, refractance.) A fairly safe procedure is first to rescale all variables to unit variance (and detect any constant variables) then to use the singular value decomposition X = U A V T. Then small singular values correspond to nearly constant\", \"singular value decomposition X = U A V T. Then small singular values correspond to nearly constant combinations. We would take S = JiiV A -1, but small singular values should cause concern, since they correspond to linear combinations which are nearly constant and whose varia·nce is likely to be determined inaccurately from the training set. We now work with these rescaled variables. The matrix GT G is diagonal containing the numbers nj of observations on each group. Let T = diag ( y'nlnj) so TGTGT = nl. The group means are given by the g x p matrix M = (GT G)-1GT X= n-1 T2GT X. Since X has been centred the column sums of M See the glossary. Modified procedures are discussed in Section 3.4.\"]\n",
      "['3.1 Classical linear discrimination 95 (weighted by group size) are zero, hence M has rank r ~ min (p, g-1 ). Consider the singular value decomposition of r-1 M = U A V T. Since we do not wish to assume that either g or p is larger, we will assume that U is g x r, A is r x r and V is p x r. From the singular value decomposition we find (g -1)B = (GM)T(GM) = VATuT(TGTGT)UAVT = nVA2vT (n-g)W = XTX -(g -1)B = nl -nVA2VT = nV[I -A2]VT which incidentally shows that the singular values are at most one. (Note that one can occur; it corresponds to a linear combination which is constant within the groups but has different values on two groups. If so it is the desired linear combination.) The original problem reduces to finding a linear combination xa of the rescaled variables which maximizes the ratio aTV[I -A2]VTa· Let b = yT a. The ratio is I: Afhf I l.:(1-Af)bf, which is maximized by taking only b1 non-zero. Thus on the original variables a is proportional to the first column of SV. The', 'only b1 non-zero. Thus on the original variables a is proportional to the first column of SV. The linear combination is unique up to a scale factor unless A2 =AI. The linear combination found by this process is called the first linear discriminant or the first canonical variate. Subsequent columns of S V give further linear discriminants which maximally separate the group means subject to being uncorrelated with previous linear discriminants (since on the variables rescaled by SV both the W and B covariance matrices are diagonal). The first J ~ r linear discriminants maximize the ratio of the determinants of the between-group to within-group covariance matrices for J -dimensional linear transformations of the original variables. (This follows immediately since the determinant is the product of the eigenvalues.) The linear discriminants are usually scaled so that they have withingroup variance one (unless constant on groups). We have only defined r of them but a further p-r can be', 'variance one (unless constant on groups). We have only defined r of them but a further p-r can be chosen by taking further columns which are orthogonal to the columns of V. For the linear discriminant variables the group means differ only in the first r variables. The quantity (n-g)Af /(g -1)(1-Af) measures the ratios of the between- to within-group variances on the i th canonical variate. We can show graphically the difference between groups by plotting the data on the first few canonical variates, often the first two. Although the original probability model corresponds to classifying using all r dimensions, it may be better not to use canonical variates']\n",
      "['96 3 Linear Discriminant Analysis which have low discrimination between groups, so we may wish to use only those dimensions for which the ratio of between-group to within-group variance is appreciable. Let A be the matrix whose columns define the linear combinations for the canonical variates, specifically A = diag ( J(n-g)/n(l -~1)) S V. Then the transformed variables XA minus the appropriate group means are uncorrelated with unit variance (since AT W A = I). Thus on the canonical variates, Mahalanobis distance is Euclidean distance. Since the group means differ only on the first r variates, the Mahalanobis distances to the group means can be computed from the distance in the first r dimensions plus a quantity from the remaining dimensions which does not depend on the group. The Bayes rule minimizes the Euclidean distance to the mean in the first r dimensions minus 2log 1tj. With just two groups this process does find the linear variable LDA2-LDA1, for it computes distances only in', \"two groups this process does find the linear variable LDA2-LDA1, for it computes distances only in the first dimension, the only one on which the means differ. So far we have only considered finding the linear combination(s) required. Fisher's procedure cannot tell us the threshold between the two groups in classification. It seems common practice to classify by choosing the group whose mean is nearest in the space of canonical variates. Since in that space Euclidean distance is the within-group Mahalanobis distance, this corresponds to the Bayes rule if (and only if) the prior probabilities are equal. The problems of rank-deficiency and various solutions are discussed by Cheng et al. (1992); the solutions given here are much simpler and more transparent. Canonical variates and canonical correlation The name 'canonical variate' comes from a connection with canonical correlation analysis, which seeks linear combinations xa and yb of maximal correlation. Since X is centred we have\", \"which seeks linear combinations xa and yb of maximal correlation. Since X is centred we have bryrxa corr ( xa, yb) = -----,=::=============== jbTVar(Y)b aT(XTX)a JbT Var(Y)b nllall2 Now if we take Y = G we have GTX = (GTG)M = (GTG)TUAVT = nT-1UAVT. (3.3) Without loss of generality we can centre Y. Let b' = urr-1b. Then Var(yb) = IIGbll2 = n11T-1bf = nllb'll2 and bTyTxa = nb'T AVTa, This is a form of shrinkage: see Section 3.4.\"]\n",
      "['Figure 3.1 : Linear discriminant plots for the Leptograpsus crab data. The left plot is from the original variables and the right plot from variables on log10 scale. The blue species is shown by triangles, the orange species by squares, and the symbols for females are filled. \"\\' ... 1U \"E: ~ \"\\' ~ . § 0 c ~ \"\\' \"0 • c 8 9l ..,. 3.1 Classical linear discrimination o\\'bo 0 q, !; !; co \\'\"\\'c \\'b \\')~\\\\\\' ~~~ c c. o • ... ..... ·(·· ~· •t1· • ··\\'=\" •• . ·5 0 5 first canonical variate ... * \"E: ~ \"\\' ~ <: 0 g ~ \"\\' \"0 • <: 0 ~ ..,. . ..... I I ·-I o I :\\'\": o • I I \\' .. \\' • ~ ~ ~ ~ 0 2 4 6 first canonical variate 97 so the correlation is maximized by taking xa as the first canonical variate and b\\' proportional to the first coordinate· vector ( 1, 0, ... ) T. Then b is the first column of E> = T U and the correlation achieved is ,tt, Subsequent combinations with maximal correlation are given by subsequent discriminant variables and columns of E>, and achieve correlations A.;. We refer to the columns of', 'discriminant variables and columns of E>, and achieve correlations A.;. We refer to the columns of e as scores. Note for future use that (GE>f(GE>) = nl and so the scores are uncorrelated and have sum of squares n over the training set. Examples The crabs data are shown on the first two linear discriminants in Figure 3.1. As the measurements are lengths, we also considered taking logarithms, and as the figure shows this does increase slightly the separation between the groups. As there are four groups, the linear discriminants span three dimensions, and for the variables on log scale the ratios of between- to within-group standard deviations are 25.5, 16.9 and 2.9 on the three discriminants. Thus the first two linear discriminants explain 99.1% of the variance between the groups. Clearly the first linear discriminant expresses the difference between the species, the second that between the sexes. On log scale they are given by Lt = 72FL + 22RW + 22CL -151 CW + 41 BD L2 = -6 FL -56 RW', 'the sexes. On log scale they are given by Lt = 72FL + 22RW + 22CL -151 CW + 41 BD L2 = -6 FL -56 RW + 88 CL -49 CW + 13 BD which shows that the differences between the sexes are principally in the ratio of length to width, and that the blue form has a wider carapace than the orange form relative to its other measurements. Next consider the forensic glass dataset. Figure 3.2 shows the first two canonical variates which account for 93.14% of the between-group variation. The variables are not on a common scale, but we can rescale']\n",
      "['98 3 Linear Discriminant Analysis \"\\' \"\\' 1ij ..,. ·~:: \"\\' > ~ ·c: C\\\\1 0 \" ~ \"0 \" 8 0 \"\\' (/) 2 4 6 2421:42444 ~ 4 2 6 5 4 1 ~~2 2 2 2 2 2 65 2 1 ~· ~ 1 1,11 31!1 1 \\'!; 31 3 1 \\'l\\' e3 \\'I\" -5 0 5 first canonical variate each variable to have unit variance. The weights given by the first two canonical variates are then RI Na Mg Al Si K Ca Ba Fe 0.95 1.94 1.07 1.67 1.90 1.02 1.43 1.15 -0.05 0.09 2.58 4.31 0.86 2.33 1.21 3.38 1.71 0.02 so the amount of iron appears to be unimportant, and the two most important variables in the second variate are magnesium and calcium. However, as the sum of the compositions is close to 100%, the variables are highly collinear which does not help interpretation. The plot is dominated by groups 4 to 6, with suspicions that groups 4 (containers) and 5 (tableware) are not homogeneous. As the boxplots of Figure 1.5 on page 14 show, this problem has a far from normal distribution, and probably has mixed distributions for some of the composition variables (with a', 'distribution, and probably has mixed distributions for some of the composition variables (with a positive probability for zero). Linear discriminant analysis has a cross-validated error rate of 38%. All of the vehicle glass is misclassified as window glass, and there is considerable confusion between float and non-float window glass. The cross-validated confusion matrix is WinF WinNF Veh Con Tabl Head WinF 47 20 3 0 0 0 WinNF 20 49 0 4 2 1 Veh 11 6 0 0 0 0 Con 0 6 0 6 0 1 Tabl 0 3 0 0 5 1 Head 1 1 0 2 0 25 The predictive form of linear discriminant analysis (page 51) gives almost identical results. Figure 3.2: Linear discriminant plots for the glass fragments data. The coding is 1 = window float glass, 2 = window non-float glass, 3 = vehicle window glass, 4 = containers, 5 = tableware and 6 = headlamps.']\n",
      "['Figure 3.3: Density estimates of the non-diabetic (left) and diabetic group (right) on the canonical variate for the Pima Indians data. 3.1 Classical linear discrimination 99 Since the within-group Mahalanobis distance is Euclidean distance in the space of canonical variates, it is useful to scale plots so that the axes have equal scales. Then circularly symmetric scatter plots for each group indicate that the assumptions of normality and equal covariances are realistic (or not, as in this example). The Pima Indians diabetes data have two groups, so plots of the canonical variates are not very useful. The within-group covariance matrices are quite similar, although blood pressure and pedigree are uncorrelated in the non-diabetics group, and strongly negatively correlated in the diabetics group, which generally has slightly higher variability. The correlation is due to just one woman, who has the highest observed pedigree, and the second-lowest blood pressure. Standard linear', 'woman, who has the highest observed pedigree, and the second-lowest blood pressure. Standard linear discrimination makes 67/332 errors on the test set. Choosing a subset of variables by cross-validation on the training set suggests that no reduction is worthwhile . The predictive version makes one fewer error. Both are making most of their errors (42/109) on the group reported to have diabetes. In this example quadratic discrimination does significantly worse (84/109), making many more errors on the non-diabetics group. We can see something useful by plotting the first (and only) canonical variate, Figure 3.3, as there is a suspicion of multi-modality in the diabetics group, and skewness in the non-diabetics. The density estimates used were kernel methods (Section 6.1) with automatically chosen bandwidths . ... ci \"\\' ci \"\\' ci ci q 0~---- -------- -------------- ~ ·4 ·2 Variable selection It is sometimes desirable to consider only those variables which make a useful contribution to', 'It is sometimes desirable to consider only those variables which make a useful contribution to discriminating between the classes. For classification it may be desirable not to have to measure unimportant variables.']\n",
      "['100 3 Linear Discriminant Analysis For interpretation it may be easier to concentrate on the important variables. In either case we need a variable-selection procedure . McKay & Campbell (1982a, b) provide a comprehensive introduction to practical issues in variable selection. Computer programs are widely available to select feature variables to be used in linear discrimination. This is an example of our considerations of Section 2.6, and the simplest stepwise methods are commonly used. There are two main approaches. One is to use significance tests for the value of individual features under the normal model (there are many such tests; McLachlan , 1992, §12.2), the other is to use the error rate (Hermans et al., 1982). Cross-validation Linear discrimination is one of those classifiers for which leave-one-out cross-validation can be computed without complete re-fitting (Fukunaga & Kessell, 1971; Hjort, 1986, §12.1). Suppose we wish to re-train the classifier omitting example Xj which is', \"1971; Hjort, 1986, §12.1). Suppose we wish to re-train the classifier omitting example Xj which is of class c, and find its predicted class. To do so we need to update our estimates of the Mahalanobis distances Ll]k from Xj to the group means l'k· We find for k i= c. As these formulae do not involve new matrix operations, they can be computed relatively quickly. For the best quadratic rule, updating is somewhat easier, as we only have to update the Mahalanobis distances to the mean of class c plus the determinant of fc. We find hence which can be evaluated with modest additional computation.\"]\n",
      "['This follows from results of Efron (1975) discussed in Section 2.3. 3.2 Linear discriminants via regression 101 3.2 Linear discriminants via regression Consider first the case of two groups, and let Y be the class indicator for class 2. Then the posterior probability for class 2 is p(21 x) = E(Y I X = x). Remarkably, although the conditional mean is not linear in x, linear regression of y on x can be used to find the linear discriminant. With equal prior probabilities and group sizes, future observations can be classified by predicting via the linear regression and selecting group 2 if and only if the prediction exceeds 0.5. This was established by Fisher (1936) by a direct calculation, reproduced by T. W. Anderson (1984, §6.5.4) and Hand (1981). We will take another approach which gives greater insight and provides an extension to more than two groups. Thus for two groups, if the normal model for the population holds, it is more efficient to use linear rather than logistic regression,', 'model for the population holds, it is more efficient to use linear rather than logistic regression, even though the population regression E(Y I X = x) is logistic not linear. We can think of the linear regression as the best linear approximation to the posterior probabilities. As a principle of classifier design this has been used (Duda & Hart, 1973; Devijver & Kittler, 1982; Fukunaga, 1990) under the name of minimum (mean) square error classifiers. Unlike the linear discriminant (for more than two groups), that procedure classifies by the nearest target or equivalently the largest component. An alternative would be to find the linear classifier which minimizes the total risk (Highleyman, 1962b). This is much harder, and has only been achieved for two general normal populations (when the Bayes rule is quadratic); see Section 2.2. The manipulations which follow are of some interest, but would not be used to actually calculate a linear discriminant in preference to the methods of', \"but would not be used to actually calculate a linear discriminant in preference to the methods of Section 3.1. Their importance lies in the realization of Breiman & Ihaka (1984) that non-linear regressions could be used in place of linear regression, thus providing one way to use non-linear regressions for classification problems. Once again we assume that X is centred and that in the algebraic formulae we work with the rescaled variables xS, which have covariance matrix nl /(n-1). The derivation of the canonical variates via canonical correlations shows that the 'scores' for classes given by the columns of E> = T U have a special place, as these are best predicted by the corresponding canonical variate. However, we can first observe that if we regress the class indicators G on X using the rescaled variables we have regression\"]\n",
      "[\"102 3 Linear Discriminant Analysis coefficients using (3.3 ). Thus the predicted values are of the form x V AA for a fullrank matrix A and so span the same space as the linear discriminants. This implies that the regression will perform the reduction to r ~ min(p, g -1) dimensions. (Since X is centred, the predicted values cannot predict a constant term. If g > p+ 1 there will be no reduction.) If g = 2 then either r = 1 or the groups have the same mean, and we see immediately that linear regression will find Fisher's linear discriminant, up to a constant factor that is not needed for classification purposes. For more than two groups Breiman & Ihaka (1984) showed how to find 0 by minimizing the residual sum of squares over the scores as well as the coefficients, but it seems as easy to apply standard linear discrimination methods to the predicted values. That is, the data are replaced by the fitted values, classification is done by using Mahalanobis distances to the group means based\", 'the fitted values, classification is done by using Mahalanobis distances to the group means based on their within-class covariance matrix, and lower-dimensional plots can be made after a singular value decomposition. Note that finding the maximum of the linear functions xp does not give the linear discriminant classifier, for Pk = nk/(n -l)S:r1mk where ST is the total variance matrix, whereas LDAk uses W, the within-group covariance matrix. There are, however, ways to calculate the classifier in the space of predicted values of the regression. Two groups In the case of two groups we can achieve a worthwhile simplification. Unless the groups have the same mean, r = 1 and we need only consider ). = .A.1. There are only two possible scores with mean zero and sum of squares n over the data; e = ±(-~, vn;Jil2f. The coefficients for the regression of the score variable on X are (using (3.4)) and so the predicted values y are ). times the first column of (xS) V and have within-group variance', 'so the predicted values y are ). times the first column of (xS) V and have within-group variance W = nJ2(1 -J..2)/(n-2). The group means are M = TUAVT = 0AVT and these are mapped to MP = 0AA = ).28. The correlation achieved is J.., so the residual sum of squares is 1 -J..2 times the total sum of squares n, and the residual mean square s2 = n(1-J..2)j(n-2). A regression package will use divisor n -p for s2.']\n",
      "['3.2 Linear discriminants via regression 103 The linear discriminant chooses class 2 if and only if 2 2 LDF = ~( )m2-ml -m2-ml 1 7r2 0 Y x W 2 W + og n1 > · Now (3.5) Let us change y from the scores to be the indicator of class 2. This is a linear transformation, so we can linearly transform the fitted regression. After some manipulation we find LDF _ y(x)-1/2 (n-2)(n2 -nl) 1 n2 - - + og-s2 2n1n2 n1 which is directly computable from the regression. Note that if we use the proportions in the training set to estimate the prior probabilities, the constant term has leading terms in an expansion in powers of 1i2 -1/2 as 8(7i2-1/2) -32 (! -!) (7i2-1/2)3 n 3 n so the dividing point for two groups on y(x) will differ negligibly from 1/2 unless the proportions in the training set are very different. (Remember that s2 will often be very small since the targets are zero and one and the fit can be very good.) More than two groups For more than two groups the simplest procedure is to apply standard', 'good.) More than two groups For more than two groups the simplest procedure is to apply standard linear discriminant techniques in the space of fitted values. However, Breiman & Ihaka (1984) (who appear not to have realized this) extended some of the calculations to more than two groups. We work with the regression of G0 on X. Remember that we can replace the data X by the fitted values F of the regression on G on X, and perform a canonical correlation analysis on these to find 0, since the canonical variates are linear functions of F as well as of X. The predicted values are proportional to the canonical variates, so W is diagonal expressed in these variables, and the Mahalanobis']\n",
      "['104 3 Linear Discriminant Analysis distance is a sum of r terms similar to that for the two-group case. Let sr = n(1-.A})I(n-g) be the residual mean square for the ith regression. The linear discriminant between group t and group s becomes (using (3.5)) ~ [(~·( ) -eis + eit) eit-eis (n-g)(8ft-efs)J 1 1rt L.....t Yr X 2 2 + 2 + og i si n ns = ~ [ll(y(x)- ts)diag(1lsi)ll2-n: g 11tsll2] 1 [11 ~ d\" I 2 n-g 2] nr -2 (y(x)-tr) mg(1 Si)ll --n-lltrll +logns where ts is the row of E> corresponding to group s. Thus we choose the group t to minimize ll(y(x)-tr)diag(1lsi)f- n-g -2lognt (3.6) nr since lltrf = nlnr. (This is a formula of Breiman & Ihaka apart from a difference in divisor for W.) Since diag(1lsd = J(n-g)ln [1-A2r1/2 the first term is the Mahalanobis distance between the predicted value and the target in the space of predicted values for the scores E> (since we saw that (n-g)W = nV[I-A2]VT ). Note that (3.6) corresponds to a distance between the predicted values and targets (which are', '). Note that (3.6) corresponds to a distance between the predicted values and targets (which are the k th unit vector for class k ), with quadratic form [(n-g)ln]E>[1- A2]-1E>r. (This is not a metric since it ignores variation in the g th coordinate, corresponding to a shift in level of all the predicted values.) Thus the linear discriminant chooses the nearest target in this distance, adjusted by ( n -g) I nr + 2log nt. Breiman & Ihaka noted that the linear combinations E> can be found by minimizing the residual sum of squares, since for combination e this is where ( = ur r-1e, and so is successively minimized subject to II( II = 1 by taking ( as the coordinate vectors or e as the columns of T U = E>. The condition is that II GO II = 1, so that the scores for the groups have unit variance over the training set. Let F be the matrix of fitted values. Then the problem is to maximize IIF8f subject to II r-1811 = 1, or IIFTU(II2 subject to IIU(II = 1. The solutions are then the columns']\n",
      "['3.3 Robustness 105 of TV* where v· is the matrix of right singular vectors of FT. But it is simpler to consider the canonical correlations of the fitted values directly. Hastie et al. (1994) have independently re-interpreted the work of Breiman & Ihaka to the same conclusions, via an entirely algebraic route. 3.3 Robustness When the crabs data were re-entered by a clerk she made an error in row 98 which should have read sp sex FL RW CL cw BD Bl F 17.4 16.9 38.2 44.1 16.6 but was entered as sp sex FL RW CL cw BD Bl F 17.4 16.9 438.2 4.1 16.6 We expected this to be disastrous for the discriminant analysis, but in fact it turned out to make rather little difference to the plots on the first two canonical variates. The effect of the errors is to inflate the within-group variance for the variables CL and CW which are then heavily down-weighted in the second canonical variate. It happens that in this example there is enough structure for the remaining variables to show almost the same', \"that in this example there is enough structure for the remaining variables to show almost the same discrimination . This example does suggest that it would be wise to have a robust form of linear discrimination . The choice of canonical variates depends on the estimated within-group covariance matrix W and the matrix M of group means. Robust estimators of means and covariance matrices are discussed in Section 2.5, and our experiments used the minimumvolume ellipsoid method discussed there. The effect of using robust estimators can be seen for the glass fragments data by comparing Figures 3.2 and 3.4. Even more of the variation (97.83%) is explained by the first two canonical variates (using the robust measures of variance) and the central 'core' is more concentrated, with one example of class 2 being shown up as a considerable outlier, perhaps closer to class 4 (containers). But what this example shows is that the groups do not have a common covariance matrix. The cross-validated\", 'this example shows is that the groups do not have a common covariance matrix. The cross-validated error is much worse at 46.7%. We mentioned on page 99 that the covariance matrix for the Pima Indians data was influenced by one unusual observation, so we tried l']\n",
      "[\"106 3 Linear Discriminant Analysis .. 6 2 2 2 2 2~·~~4 2 2 5 1 5 5 2 -10 0 2 ~ 2 4 4 4 .. 6 2 4 25 first canonical variate 6 6 ' ' 10 robust estimation. This increased the test-set error rate for linear discrimination considerably (but not significantly) to 76/332. For quadratic discrimination there is a large increase to 100/332. Using multivariate t discrimination gives 77/332, better than any other form of quadratic discrimination. Robustification of discriminant analysis has been considered a number of times in the literature; we first saw it in Campbell (1980b, 1982). Broffit et al. (1980) used trimmed estimators of mean and covariance, but these are less satisfactory (as they are not affinely equivariant). 3.4 Shrinkage methods When discussing the best quadratic rule in Section 2.2 we mentioned that if the training set is not large, we might do better to use the best linear rule than attempt to estimate all the variance matrices ~kThere is evidence (for example, Marks & Dunn,\", 'than attempt to estimate all the variance matrices ~kThere is evidence (for example, Marks & Dunn, 1974) that if the true class covariance matrices ~k are similar, a linear discriminant may outperform the quadratic discriminant in small samples. This suggests other compromises. We might take some convex combination of the equal and unequal estimated variance matrices, say ~ ~ ~k(a) = (1-a)nk~k +an~ (1-a)nk +an (3.7) with parameter 0 < a < 1 chosen to maximize performance (using a validation set or cross-validation). We describe this as shrinking the covariance matrices towards a common value, and hope thereby to obtain a biased but less variable estimator. Figure 3.4: Robustified linear discriminant plot for the forensic glass data. Two points of class 4 at (-4.7,33.7) have been omitted. Compare this to Figure 3.2 on page 98.']\n",
      "['Some accounts just consider W + ).[, but finding the linear discriminants is unchanged by an overall scale change in w. 3.4 Shrinkage methods 107 We might wish to shrink our estimator of the common covariance matrix L. Recall that our algorithm for linear discriminant analysis was first to rescale each variable to zero mean and unit variance, and then seek a transformation to variables which are uncorrelated (using xS ). Suppose we use the singular value decomposition X = U AVT to do so; the variables XV are uncorrelated with variances proportional to .A.i, ... , A~, and trace(XT X)= L AJ = np. Thus if the original variables were positively correlated, there will be some linear combinations of high variance and some of low variance. The variance of the latter will not be determined at all precisely, so it is conceivable that we will find the first canonical variate taken in the direction of a combination of features that happens by chance to be nearly constant within groups. One way to', 'of a combination of features that happens by chance to be nearly constant within groups. One way to avoid this is to shrink the .At towards a common value, and this is precisely the effect of using the eigendecomposition of (1-y)Sr + yi. In linear discrimination it is more usual to shrink W, that is to use L(y) = (1-y)W + yi. (3.8) Of course, this is only appropriate if the variables have been rescaled to unit variance. Many accounts (for example, Campbell, 1980a) replace I by a diagonal matrix, but this is equivalent to rescaling the variables if we use the diagonal matrix of the variances of the variables. Campbell (1980a) suggested that finding the first canonical variate to be in a direction of low within-group variance was common in applications, and that shrinkage was important in interpreting the coefficients of the linear discriminants . Both aspects of shrinkage were considered by Friedman (1989) under the name of regularized discriminant analysis. He took a convex', 'considered by Friedman (1989) under the name of regularized discriminant analysis. He took a convex combination of the within-group and pooled covariance matrices to estimate Lk. and then shrunk that estimate, to obtain a shrunken quadratic discriminant analysis. Specifically, he used a plug-in quadratic rule with I:k(a, y) = (1-y)I:k{a) + 2:. trace[Lk{a)] I p I:k(a) = (1 -a)nki:k +ani:. (1-a)nk +an The parameters a and y are chosen to minimize the cross-validated error rate; Rayens & Greene (1991) point out that the minimum will often occur over a wide range of values of (a, y ). The largest value of (a, y) is chosen (in lexicographical order, so first the largest a for any y, then the largest y for that a).']\n",
      "['108 3 Linear Discriminant Analysis The idea of shrinkage is better known for regression under the name of ridge regression (Hoerl & Kennard, 1970a, b) where the (centred) matrix xr X is replaced by xr X + cD where c is an adjustable constant and D is a diagonal matrix which will be the identity if the variables are on a common scale, and otherwise could be the diagonal matrix of the variances of the variables. Given that we have seen that the linear discriminant may be computed by regression, we might wonder if the connection is exact: it is. Other forms of shrinkage are considered in regression (Sen & Srivastava, 1990; Frank & Friedman, 1993), such as dropping small principal components. There are other intermediate positions between linear and quadratic discrimination. We can restrict the class of covariance matrices Lk, for example to be proportional to each other (Owen, 1984; Flury, 1986; Eriksen, 1987) or to have equal correlation matrices (Manly & Rayner, 1987). More generally,', \"1986; Eriksen, 1987) or to have equal correlation matrices (Manly & Rayner, 1987). More generally, we can parametrize Lk = AkUkDkU[ where Uk are the eigenvectors, Ak the sum of the eigenvalues and Dk a diagonal matrix of the eigenvalues divided by their sum. The proportionality restriction forces equality of Uk and Db and commonality of correlations forces commonality of Uk. Other restrictions which might be imposed are commonality of 'shape' (equal Dk) and/or 'size' (equal Ak) as discussed by Banfield & Raftery (1993) and Flury et al. (1994). Thus far we have only considered shrinking W. We mentioned on page 96 using J < r canonical variates when allocating future examples, retaining only those dimensions in which the group means are well separated, say those for which the ratio of the between-groups to within-groups variance is around one or more. This is equivalent to setting the difference between the group means to zero in the dropped directions, and so is a form of shrinkage to\", 'between the group means to zero in the dropped directions, and so is a form of shrinkage to zero of the between-groups covariance matrix B; we could use a less extreme form of shrinkage in which these directions are down-weighted. However, we can downweight by either reducing B or increasing W, so this is merely a different perspective. The most common sources of strong correlations between the variables are when features are measuring essentially the same quantity and when the features are a discretized signal or image. In both cases we may be able to design a better composite feature, and we will certainly be able to design a better form of shrinkage. In the regression context for a discretized signal, we would assume that the quantity of interest was really an integral J {J(t)f(t) dt which is approximated by a sum at the reported values of t (Hastie & Mallows, 1993), and so would assume that fJ(t) was smooth. There penalized regression could be used (as in Section 4.3). In the', 'assume that fJ(t) was smooth. There penalized regression could be used (as in Section 4.3). In the discrimination context Hastie et al. (1995) This is a special case of the algebra of Hastie et a/. (1995).']\n",
      "['3.5 Logistic discrimination 109 apply this idea by modifying w to w + n for a pre-specified n and finding the equivalence to canonical correlation analysis and to ridge regression (for xr X + nn) in the same way as in Section 3.2. In practice in these high-dimensional situations it may be preferable to re-parametrize the signal, for example by a spline basis, and discard the fine detail before any computation is done. After all, these methods are still assuming normality, and that is likely to be a serious limitation. We will need to be very careful with shrinkage if some i:k is singular, since this is likely to make it easy to identify that class, and shrinkage may reduce the performance dramatically. 3.5 Logistic discrimination Logistic discrimination is important both as a more direct way of estimating posterior probabilities and hence the Bayes rule, and also as a method which is much easier to generalize. We saw in Section 2.3 that using a normal model for each class (or group)', 'easier to generalize. We saw in Section 2.3 that using a normal model for each class (or group) density with a common covariance matrix gave rise to (2.29): logp(k I x) = logp(11 x) + ak + xfh. More generally, we could model log p(k I x) -log p( 11 x) by some parametric family of functions, say gk(x; 8) (with g1(x) = 0 ). Then we estimate p(k I x) = exp gk(x; e)~ . L:j exp gj(x; 8) (3.9) (For the linear model we take the parameter vector e to contain all the ak and /h.) In the neural network literature this is known as softmax (Bridle, 1990a, b), and in the (earlier) statistical literature as a multiple logistic model. Classification is done by using the (estimated) Bayes rule, taking the maximum of p(k I x) (under the usual loss function (2.2)). This procedure is known as logistic discrimination. Several remarks are needed. First, (3.9) can arise from other probability models, sometimes with transformed x variables. Some of its earliest applications were to epidemiological studies', 'with transformed x variables. Some of its earliest applications were to epidemiological studies where such a direct parametrization was very natural, and other models leading to (3.9) are considered by J. A. Anderson (1972) and Cox & Snell (1989). Another has two classes and independent binary features (Minsky, 1961) with probabilities eik under class k, for then logp(21 x) -logp(11 x) = L Xi[log ei2 -log On] i + (1-Xi)[log(1- 8i2) -log(1- Oil)].']\n",
      "['110 3 Linear Discriminant Analysis It is normal to fit logistic regressions and log-linear models by maximum likelihood, but we have to consider carefully the likelihood to be used (as we do below). The standard methods (Cox & Snell, 1989; McCullagh & Neider, 1989) regard x as fixed and take the observed class y to have a Bernoulli distribution with probability distribution (p( 11 x), p(21 x)) for two classes, and a multinomial distribution for more. Thus the likelihood of the training set is t(8;ff)= rrp(c;lx;)= IT expgcj(x;;~~ (3.10) ; i L:j exp gj(X;, 8) with deviance (3.11) We note that if gk(x; 8) = ak + xfh, the log-likelihood is concave, so any local maximum of the likelihood is a global maximum. In the special case of two groups 0 and 1 the log-likelihood is L(8; ff) = L c; p(21 x;) + (1-c;) [1 -p(21 x;)] and hence the deviance is \" [ C; (1-C;) ] D(8) = 2 ~ C; log p(1l X;) + (1-c;) log (1 _ p(1l X;)) · (3.12) An alternative way to specify the probabilities p(k I x) is to give g', 'log (1 _ p(1l X;)) · (3.12) An alternative way to specify the probabilities p(k I x) is to give g -1 logistic models of the form logp(k I x) = logp(11 x) + gk(x; 8) (3.13) which is the same model as equation (3.9) (as replacing gk(x; 8) by gk(x;8)-gt(x ;8) leaves (3.9) unchanged). However, the models (3.13) could be fitted separately comparing each group with group 1, and this gives different maximum likelihood parameter estimates . The pairwise comparison uses less information and so will be less efficient, but for linear models Begg & Gray (1984) show the loss is often negligible. An approach which is common in neural networks is to consider a logistic model for each p(k I x) against the rest, that is ~ p(k I x) = exp gk{x; 8) ~ . 1 + exp gk{x; 8) (3.14) This has the disadvantage that there is no guarantee that the estimated probabilities will sum to one, and no compensating advantages. This model is however appropriate if the classes are not mutually exclusive, for example if they', 'This model is however appropriate if the classes are not mutually exclusive, for example if they indicate diseases which could conceivably occur together. This is a special case of the computations on page 152 which show that the Hessian is non-positive definite.']\n",
      "['The empirical distribution gives probability 1/n to each observed Xj. 3.5 Logistic discrimination 111 Likelihoods We derived likelihood (3.10) as a conditional likelihood for a training set of size n which was a random sample from the whole population. It will also be appropriate if the feature vectors are selected rather than sampled. We will lose information if we know p(x; 8) as a function of 8, so we are implicitly assuming that p(x) is completely unknown. The issue is more complicated when the numbers of examples in each group are fixed, which implies that we will not be able to estimate the prior probabilities of the groups. The natural model is for Pc(x) = p(x I c) not p(c I x). However logp(x I k; 8) = logp(k I x; 8) -log nk + logp(x; 8) = logp(11 x) + ock + xpk -log nk + logp(x; 8) and it is clear that we can only estimate (k = ock -log nk. The loglikelihood formed by conditioning on all the observed classes is L(8;ff) = L logp(xi I ci; 8) = \"L:logp(ci I xi; 8) -log ncj +', 'on all the observed classes is L(8;ff) = L logp(xi I ci; 8) = \"L:logp(ci I xi; 8) -log ncj + logp(xi; 8). If we assume that p(x; 8) is entirely unknown (apart from normalizing to a probability density), we can maximize over this as well as (k and Pk· The maximum likelihood estimate of p(x) is then the empirical distribution of the observed values (so not a genuine density), leaving the maximization of the profile likelihood for pk, L logp(xi I ci;O) = L logp(ci I xi; /3) + const. This is exactly what we get from the other forms of sampling, and so we will obtain the same maximum likelihood estimates of Pk· This is a streamlined version of the arguments of J. A. Anderson (1972) and Prentice & Pyke (1979). Note that we have maximized over an infinite-dimensional parameter p(x), so standard likelihood theory does not apply; Anderson avoided this by considering only discrete x, but Prentice & Pyke and Cosslett (1981) prove analogues of the standard asymptotic results. Scott & Wild (1986)', '& Pyke and Cosslett (1981) prove analogues of the standard asymptotic results. Scott & Wild (1986) also consider a different approach. Suppose the sample was chosen to have nk cases of class k, but the prior probability is known to be nk. A natural idea is to weight the cases of class k by Wk = Nnk/nk. that is to use a weighted log-likelihood of the form LWei logp(ci I xi). (3.15) This is precisely what we would use if combining identical cases in the training set, and is the common practice in survey statistics. Scott &']\n",
      "['112 3 Linear Discriminant Analysis Wild show that the profile likelihood approach is more efficient if the logistic model is true, but the weighted form may be preferable for estimating the least false parameter if the model is reasonable but not precisely true. Ordinal logistic models The approaches thus far are appropriate if the classifications are purely nominal. Some classifications are naturally ordered, such as grades of foodstuffs and the severity of a disease. For these a model which takes account of the ordering is desirable. McCullagh & Neider (1989, §5.2.2) advocate models for Yk = P(Y :( k I x), on logistic (log[yk/(1- Yk)]) or complementary log-log (log[-log(1- Yk)]) scales. Suppose there is an unobserved variable Z (say the true quality of the foodstuff) of which Y is a grouped version, so Y = k if and only if (k-l < Z :( (k. Suppose also that Z has a logistic distribution with mean IJ(x); then exp[(k -IJ(x)] Yk = P(Y :( k I x) = P(Z :( (k I x) = 1 [( ( )] + exp k-11 x', 'with mean IJ(x); then exp[(k -IJ(x)] Yk = P(Y :( k I x) = P(Z :( (k I x) = 1 [( ( )] + exp k-11 x which naturally gives rise to a logistic model logitP(Y :( k I x) = (k + IJ(X) for Yk that differs only in intercept for each category. Giving Z the extreme-value (or Gumbel) distribution rather than the logistic distribution leads to log{ -log[1- P(Y :( k I x)]} = (k + IJ(x). In both cases the linear model part is then IJ(x) = xp. If we regard the intercepts (k as unknown (but necessarily increasing) this analysis can be extended to grouped versions of </J(Z) for an unknown but monotonic transformation ¢(·), since this will be equivalent to grouping Z. The likelihood for the parameters (which are ((k) and the parameters in 11 ) then follows from our earlier considerations, since a model for P(Y :( k I x) implies one for p(k I x) = P(k-1 < Y :( k I x). In numerically maximizing the likelihood we will have to remember the ordering constraint on ((k). Anderson & Phillips (1981) illustrate', 'we will have to remember the ordering constraint on ((k). Anderson & Phillips (1981) illustrate the linear ordinal logistic model for data on severity of back pain. It does lack flexibility, for there must be a linear projection on which the class distributions occur in the assumed order for the fit to be adequate. If we allow a non-linear function 11 (Mathieson, 1996) this may be much easier to achieve. Higher efficiency as usual means smaller variance in large samples.']\n",
      "['3.5 Logistic discrimination 113 Infinite estimates A difficulty which arises with the conditional likelihood is that the maximum likelihood estimates may be infinite (or, as some prefer to say, fail to exist). Consider just two classes. If there is a direction such that the projection pT Xi completely separates the two classes, it should be clear that the posterior probabilities can then be made arbitrarily close to one for every example by taking fJ -+ oo with the appropriate sign. Indeed, the methods of Section 3.6 are designed to find such projections where they exist. Albert & Anderson (1984), Albert & Lesaffre (1986), Santer & Duffy (1986), Silvapulle & Burridge (1986) and Lesaffre & Albert (1989) consider this in detail (apparently unaware of the earlier parallel development given in Section 3.6). With more than two groups, some groups can be completely separable from others on a linear projection, or (quasi-complete separation) p(j I Xi; 8) ~ p(k I Xj; 8), k -:/= j for all xi', 'linear projection, or (quasi-complete separation) p(j I Xi; 8) ~ p(k I Xj; 8), k -:/= j for all xi from class j and some fJ. The maximum likelihood estimate has an infinite component. This occurs for the tableware group in the forensic glass dataset. We feel too much has been made of this. The difficulty is an inappropriate parametrization, and the limits for infinite II fJ II of the fitted posterior probabilities remain perfectly suitable fits, albeit sometimes predicting probability zero or one. Predictive approach We considered the predictive framework for the diagnostic paradigm at (2.33). Suppose we assume that we know nothing about the marginal density p(x). Then the posterior density is of the form logp(8 I 5\") = L(8;§\") + logp(8) + const. (3.16) Then for large n t~e posterior density p( 8 I 5\") will be approximately normal with mean 8 and covariance matrix V, say, and we can approximate these by the maximizer and the inverse of the Hessian of (3.16). The integration over 8', 'approximate these by the maximizer and the inverse of the Hessian of (3.16). The integration over 8 could be performed approximately (Aitken, 1978) or by simulation using importance sampling from the approximate normal distribution (Ripley, 1987, §5.2). Since the approximate normal distribution will have short tails, it is better to choose a longertailed distribution, for example by increasing the variance slightly or using a multivariate t distribution.']\n",
      "['114 3 Linear Discriminant Analysis If the maximum likelihood estimate is (partially) infinite, the local approximation will break down, but with a proper prior the predictive approach will give a sensible fit. Indeed, a proper prior which restrains the size of the parameter vector is a form of ridge regression (see Section 3.4) and has advantages even in non-Bayesian views. For example, it avoids the embarrassment of predicting probability zero for events that happen (as might happen with the plug-in rule if the training set is linearly separable but the populations are not). Bias correction of the parameters of a linear logistic discrimination has been considered extensively, but Byth & McLachlan (1978) showed that the bias of the plug-in estimates of the posterior probabilities p(k I x; 0) was of smaller order than 1/n and so much less important than for the best linear classifier discussed in Section 2.5. Examples First we consider the Pima Indians data. The simplest approach is a', 'in Section 2.5. Examples First we consider the Pima Indians data. The simplest approach is a direct logistic regression on the seven explanatory variables. This made 66 errors on the test set, an error rate of 19.8%. However, some of the features had insignificant coefficients, and a stepwise selection procedure to choose the fit with the smallest value of AIC dropped blood pressure and skin thickness. Its test set performance was also 66 errors (but not the same ones). The number of pregnancies varies from zero to seventeen, and this seems unlikely to enter linearly. To test this, we allowed separate coefficients for 0, 1, 2, 3, 4, 5 and 6+ pregnancies, but the fit was little better, and the predictions worse (69 errors). If we allow a polynomial in age, AIC chooses a cubic; the number of errors is increased to 69. \"\\' C\\\\1 0 0 C\\\\1 0 ~ error rate c:i ~ ~ 0 reject rate\\'\\',, I() 0 0 0 ci 0.30 0.35 0.40 0.45 0.50 doubt cost Figure 3.5: Error and rejection rates against doubt cost d for a', '0.35 0.40 0.45 0.50 doubt cost Figure 3.5: Error and rejection rates against doubt cost d for a logistic discrimination model for the Pima Indians data.']\n",
      "['Figure 3.6: Calibration plot for a logistic discrimination model for the Pima Indians data. The \\'rug\\' of ticks shows the events which occurred in the training set against the predicted probabilities. The smooth curve is a kernel regression (see Section 6.1). 3.5 Logistic discrimination 115 C! ·~ \"\\' 0 <0 0 \" 0 \"\\' 0 0 0 0.0 0.2 0.4 0.6 0.8 1.0 predicted probability We can use this example to illustrate some of the concepts we saw in Chapter 2. If we allow a low enough cost of \\'doubt\\', this will be chosen. Figure 3.5 (computed on the test set) shows a fairly small reduction in error rate with rejection in this example; if we allow 10% of the patients to be rejected, the error rate drops from 19.9% to 15.7%. We can also check if the predicted probabilities look well calibrated. Figure 3.6 was computed on the training set and shows no serious departures. For the forensic glass data the fitting algorithms converge slowly because of the partial separation of the classes, but produce', \"fitting algorithms converge slowly because of the partial separation of the classes, but produce satisfactory fitted probabilities and an error rate of 26.2% with confusion matrix WinF WinNF Veh Con Tabl Head WinF 50 17 3 0 0 0 WinNF 19 55 0 2 0 0 Veh 6 7 4 0 0 0 Con 0 2 0 11 0 0 Tabl 0 0 0 0 9 0 Head 0 0 0 0 0 29 The fit is rather better than linear discriminant analysis, not surprising given the very non-normal nature of these data. The cross-validated estimate of the error rate was 36% with confusion matrix WinF 46 19 5 0 0 0 WinNF 19 47 2 3 3 2 Veh 7 6 4 0 0 0 Con 0 3 0 9 0 1 Tabl 0 2 0 0 7 0 Head 0 2 0 2 1 24 since the cross-validation runs did not find the 'right' separation vectors.\"]\n",
      "[\"116 3 Linear Discriminant Analysis 3.6 Linear separation and perceptrons Historically , special attention has been given to situations such as Figure 3.1 on page 97 in which the two species are completely separated on the first linear discriminant. We say two groups are linearly separable if there is a linear function of the variables, say xa + b, which is positive on one group and negative on the other. A function which computes a linear combination of the variables and returns the sign of the result is known as a perceptron after the work of F. Rosenblatt (1957, 1958, 1962). (There are also publications by Block, 1962; Block et al., 1962.) Their interest now is in their continuing influence on the thinking in the field of neural networks. Let us add a column of 1's to x and add b to a. Let z = x/1/xll on the first group and z = -x/llxll on the second group. We then seek a linear combination a such that za > 0 for every example in the training set. Since the training set is finite, we\", 'a such that za > 0 for every example in the training set. Since the training set is finite, we can choose (j > 0 so that za > (j. Indeed, we can achieve this for any (j > 0 by rescaling a. One approach to the problem would be to choose a by least squares to make za as near one as possible, or to regress y = ±1 on x which as we have seen gives the linear discriminant up to a scale factor. (However, there is no guarantee that the linear discriminant will linearly separate the groups if they are linearly separable, and it is easy to construct examples in which it will not, Figure 3.7.) A more direct formulation is to minimize the number of errors, but as that is a discrete measure, the optimization is difficult. The sum of the degree of error will be zero if and only if linear separation can be achieved. This is equivalent to solving the linear programming problem zia ~ b, and linear programming methods can find a solution or show that none + + + + + + + Analytical evidence that', 'programming methods can find a solution or show that none + + + + + + + Analytical evidence that optimizing the number of errors made by a perceptron is hard is provided by Hoffgen et a/. (1995) who showed that the problem of determining if there is a solution with at most k ~ 1 misclassifications is NP-hard. (For k = 0 it is reducible to a linear programming problem, so solvable in polynomial time.) Figure 3.7: A dataset with the least-squares line (solid) and a linear separator (dashed).']\n",
      "[\"3.6 Linear separation and perceptrons 117 exists (Minnick, 1961; Muroga et al., 1961; F. W. Smith, 1968; Grin old, 1969). (We need to introduce ~ to avoid the trivial solution a = 0.) We introduce n artificial variables u; and also split a = a+ -a-into positive and negative parts. Then the problem becomes subject to and n min 'l:.:u; u;,a+,a-i=l which can be solved in a finite number of steps, and has a zero solution if and only if the classes are linearly separable. In the late 1950s a number of researchers were interested in simpler but iterative solutions, in which the value of a was adjusted after each example was presented. The derivative for the least-squares problem lly-Xaf is 2XT(Xa-y) and so a steepest descent procedure would be of the form (3.17) For small enough 11 this process converges to the space of least-squares solutions. Rather than compute the sum on the right-hand side and update a, we could update after each pattern was considered. This gives the rule (3.18) known\", \"and update a, we could update after each pattern was considered. This gives the rule (3.18) known as Widrow-Hoff learning (after Widrow & Hoff, 1960) or the delta rule. The patterns are presented cyclically until convergence, which will need 1J ---+ 0. Rosenblatt's perceptron learning rule replaced the term x;a in (3.18) by the output of the perceptron, the sign of xa. Thus a is changed only if the current pattern is misclassified, and so the rule is of the form a+---a+ 211zT I(z;a:::; 0). No generality is lost by taking 11 = 1/2, since we can rescale a. Rosenblatt showed that this rule will converge in a finite number of steps to a linearly separating combination if one exists. Let a• be a suitable combination chosen so that z;a• ~ 1 for all members of the training set. If the rule changes a we have z;a :::; 0 so !Ia +~a-a·ll2 = !Ia-a·ll2 + 2z;(a-a·)+ 1 :::; !Ia-a· f-1. This shows the rule terminates in at most !lao-a•ll2 steps.\"]\n",
      "['118 3 Linear Discriminant Analysis This result is known as the perceptron convergence theorem. Its limitations were explored by the first edition of Minsky & Papert (1988) published in 1969. They showed that the coefficients needed to achieve linear separation (with fixed {J) could grow very rapidly with the size of the problem and the finite number of steps needed by the perceptron rule could become very large. There is after all another rule which will terminate in a finite number of steps: try all integer-valued a in order of increasing length, and no one would advocate that rule. Minsky & Papert also considered the behaviour of the rule when the two groups were not linearly separable, and stated that llall would remain bounded. (Their proof was completed by Block & Levin, 1970.) Thus if the a belong to a fixed-precision set (as they will do in real computation) the rule will eventually cycle. In particular there is no immediate way to deduce whether the rule will ever terminate,', 'cycle. In particular there is no immediate way to deduce whether the rule will ever terminate, and cycling can be hard to detect, as the cycle length is unknown. There are a number of variants of the perceptron updating rule. For example, 11 can be chosen just large enough to correctly classify the current case. Ho & Kashyap (1965) have other algorithms, discussed in detail in Duda & Hart (1973, §§5.6-7). It is also possible to extend the procedure to K > 2 categories. In that case the natural classifier would be to choose the largest of K linear discriminants xak. Let a be the concatenation of the vectors ak. Then correct classification of pattern x in class k is equivalent to (0 ... , -x, 0 ... 0, x, ... O)a > 0 with the negative element in position j, for each j not equal to k. Thus each example x generates g-1 examples in the Kp-dimensional problem. Applying the perceptron updating rule to this problem is equivalent to the updating rule when pattern x is from class i, and j is a', \"rule to this problem is equivalent to the updating rule when pattern x is from class i, and j is a class with a larger value of xaj. Since this is the perceptron rule in the transformed problem, the convergence proof still holds. F. W. Smith (1969) extends the linear programming approach to more than two classes. The traditional simplex algorithm for linear programming has exponential worst-case behaviour, but recently a number of provably polynomial algorithms have been devised. Mansfield (1991) has implemented one of these, Khachiyan's algorithm, for perceptron learning. The algorithm is quite simple and related to quasi-Newton methods of optimization (Gill et al., 1981). It is guaranteed to find a solution, if there is one, in (p + 1)3log(p + 1) + (p + 1)2log[n(p + 1)] iterations with binary inputs, where an iteration involves finding a misclassified example and updating the perceptron weights. Prior to Minsky & Papert, Muroga et a/. (1961) had shown that the weights in a\", 'perceptron weights. Prior to Minsky & Papert, Muroga et a/. (1961) had shown that the weights in a linearly-separating perceptron with n binary inputs could be chosen to be integers less than (n + l)(n+l)/22-\", and Muroga (1965) showed that there were problems where integer weights of Q(2\") are required (The Q notation is defined on page 178.). Hampson & Volper (1986) showed that in some sense an average problem needs integer weights of Q(2\"12). More recently Hastad found an example which needs integer weights at least as large as n\"122-\"e0<205851. These results are reviewed and proved by Parberry (1994). This bound was reduced to O(p2log p) by Maass & Tunin (1994) using a more recent algorithm.']\n",
      "['3.6 Linear separation and perceptrons 119 It is possible to extend the ideas of separation by linear planes to other surfaces (for example conic sections) or to piecewise linear surfaces. These have been considered by Mangarasian (1968) and applied to the diagnosis of breast tumours in Mangarasian et al. (1990); Wolberg & Mangarasian (1990) and subsequent papers. Capacity questions We can ask how many random patterns a perceptron with p inputs can learn reliably; that is can be classified without error. There will be a finite limit since the patterns must be linearly separable; this is irrespective of the existence of an algorithm to learn the patterns. Cover (1965) showed that the asymptotic answer is 2p patterns. In other words, for large p we expect to be able to store most sets of up to 2p patterns without error, but attempts to store more than 2p have a very low probability of success. Proposition 3.1 The probability that N patterns randomly chosen from any continuous distribution', 'Proposition 3.1 The probability that N patterns randomly chosen from any continuous distribution in RP and randomly divided into two groups are linearly separable is one for N ~ p + 1 and in general is 21-N min~l,p) (N ~ 1) \"\\' <l> (-=-2p___,-=N-) i=O l JN for large N. Proof: Let C(n,p) be the number of assignments of classes 1 and 2 to n patterns in p variables which are linearly separable. We will show by induction that this does not depend on the patterns themselves provided they are in general position in RP (that is, not collinear). All the assignments (yi) are linearly separable for n ~ p + 1, since we can solve the system x;a + b = Yi of n equations in p + 1 unknowns to find a separating hyperplane. (This system will be singular only if the points are not in general position.) Thus C(n,p) = 2n for n ~ p + 1. Now consider adding an example to n linearly separable patterns. If the new point lies on the same side of every separating hyperplane, only one label for the new point', 'new point lies on the same side of every separating hyperplane, only one label for the new point gives a linearly separable set. In the other case, by continuity , there must exist a separating hyperplane passing through the new point and either label for the new point gives a linearly separable set. Of the C(n,p) sets of linearly separable patterns, precisely C( n, p-1) will pass through the new point (since this reduces the problem to one of dimension p -1 ). Thus C(n + l,p) = 2C(n,p-1) + [C(n,p)- C(n,p-1)] = C(n,p -1) + C(n,p).']\n",
      "['120 3 Linear Discriminant Analysis By induction we find the probability to be min(N-l ,p) (N 1) C(N,p)/2N = 21-N ~ ~ = P(X ~ min(N- 1,p)) where X has a binomial (N-1, 1/2) distribution. The approximation then follows from the normal approximation to the binomial for large N-1. D The approximation on the right-hand side goes rapidly from 1 to 0 as N increases through 2p, since for large n a binomial (N -1, 1/2) distribution is tightly concentrated about N /2. Support-vector machines If two classes are linearly separable, there will be a continuum of weight vectors a which give rise to separating hyperplanes . Amongst these we can choose a hyperplane with maximal distance to the nearest example, achieved by minimizing llall2 whilst insisting that za ~ 1. Finding this hyperplane is a quadratic programming problem, and the usual Kuhn-Tucker optimality conditions show that there will be a subset of examples Zi (known as support vectors) for which zia = 1 and that the optimal a is a linear', 'of examples Zi (known as support vectors) for which zia = 1 and that the optimal a is a linear combination of these zi. The advantage is choosing the optimal hyperplane is to reduce the VC-dimension of the space of solutions (which is proportional to a bound on llaf ). If the two classes are linearly separable then (Vapnik, 1995, Theorem 5.2) the expected error rate on future examples is bounded by the expected number of support vectors divided by n-1. Thus finding a small number of support vectors might indicate good generalization properties. Of course, linear separation in the original feature space is quite rare, but as for generalized linear discrimination (page 121) we can expand the feature space by using polynomials or even radial-basis function networks and sigmoidal functions. These can give rise to very large feature spaces, but generalization may remain acceptable if the number of support vectors remains small, which was the case in the experiments reported by Vapnik', 'number of support vectors remains small, which was the case in the experiments reported by Vapnik (1995, Section 5.7). By jointly minimizing the sum of the degree of error (page 116) and II a 112 these ideas can be extended to non-separable two-class problems (Cortes & Vapnik, 1995).']\n",
      "['4 Flexible Discriminants Linear combinations of the features will not always suffice to discriminate the groups. It is quite common to include ratios by including features on log scale as we did for the Leptograpsus crabs in Chapter 3. We can also allow non-linear functions of the features by including polynomial terms or dividing the range of the feature and including indicator terms for parts of the range. A more sophisticated alternative is to expand a feature on a spline basis such as B-splines (see below). These all amount to linear discrimination in a larger space of features, sometimes called generalized linear discrimination (Duda & Hart, 1973). We will continue to assume n cases from g groups, which may or may not be the classes. We saw at the beginning of Chapter 3 three main ways to use a family of functions f: fl£ ~ lRg to approximate the Bayes rule. All these ideas apply equally here, but we will now consider much more general and flexible classes of functions. Another', 'equally here, but we will now consider much more general and flexible classes of functions. Another large class is the subject of Chapter 5. The first approach was to estimate f(x) from the training set within our parametric family, and choose the class which maximizes fk(x) or has nearest target tk. (This includes the Dietterich & Bakiri, 1991, 1995, approach, which specializes the choice of targets.) As f(x) is a regression , it is natural to fit () by least squares. Since in the conventional version f represents (p(k I x)), the outputs are sometimes re-normalized to sum to one. The second approach was to fit f(x) within the parametric family, but then to use the predicted values as the variables in a linear discriminant analysis. (This appears to have been the motivation of Breiman & Ihaka, 1984.) This amounts to finding the nearest group mean in the Mahalanobis distance given by the within-group covariance matrix of the fitted values (or, equivalently, of the residuals). Note that', 'within-group covariance matrix of the fitted values (or, equivalently, of the residuals). Note that this differs from the first approach in using a different metric and in minimizing']\n",
      "['122 4 Flexible Discriminants distance to the group means rather than the targets. Formula (3.6) does allow us to consider distances to the targets provided the group sizes are equal, but the metric there is not the Mahalanobis distance. Thus the essential difference is the metric used. The second procedure appears preferable to the first if the predicted values are approximately normal with a common covariance matrix. However, in practice the fit is often very good except at a few points which therefore dominate the residuals and the estimate of the common covariance matrix. This suggests that it is desirable to use a robust discriminant analysis, and it may be better to accept the safe choice of the Euclidean metric. The third approach is to use the parametric family within a multiple logistic model of the form (3.9), which is often the most theoretically satisfying but needs the ability to fit by maximum likelihood rather than least squares. From the predictive viewpoint, these', 'to fit by maximum likelihood rather than least squares. From the predictive viewpoint, these methods are all (in principle) parametric, and so we need to average over the uncertainty in the fitted parameters . Since there will usually be many more parameters than for linear families, it will be more important to average over the greater uncertainty. In practice this can be nigh impossible, as in the highdimensional parameter space the integration is more difficult and it is unlikely that the asymptotics which suggest a normal approximation will be appropriate unless n is much larger than the number of parameters. We are only aware of such issues having been studied for neural networks, so discuss them in that context in Section 5.5. 4.1 Fitting smooth parametric functions We discuss some of the possible ways to describe more general functions of the feature variables. We consider first methods using univariate functions f : !![ ---+ 1R. Additive models and smoothers An additive model', 'methods using univariate functions f : !![ ---+ 1R. Additive models and smoothers An additive model is of the form p f(x) = rt + L gJ(XJ) (4.1) }=1 for smooth but unknown functions g1 (Friedman & Silverman , 1989; Hastie & Tibshirani, 1990), which could encompass the effect of transformations (such as square or log or even an arbitrary polynomial) of each feature.']\n",
      "['The notation [y]+ means max(y, 0). Sometimes the free parameters are reduced by end conditions ; for natural cubic splines the second and third derivatives vanish at the boundaries. Smoothing splines are a special case of regularization, to be considered in Section 4.3. 4.1 Fitting smooth parametric functions 123 One choice of the smooth functions g(x) of a single feature is to use splines. Splines are defined by M knots ~; which we can consider in increasing order. Then within an interval [~;, ~i+d a spline is a polynomial of degree d (often three) and at the knots the first ( d -1) derivatives are continuous. This can be written as d M g(x) = :~::::>x;xi-1 + LfJ;[x- ~;]~ i=O i=l which shows that there are M + d + 1 free parameters. There are other bases which have better numerical properties such as B-splines (de Boor, 1978; Green & Silverman, 1994 ). In any basis we can write M+d+l g(x) = L {J;c/J;(x). (4.2) i=l It remains to choose the parameters {3;. For a regression spline these', '= L {J;c/J;(x). (4.2) i=l It remains to choose the parameters {3;. For a regression spline these are chosen by least squares. Cubic smoothing splines are the solution to the minimization problem M L[y;- g(~;)]2 +A j g\"(u)2 du i=l and the parameters in (4.2) can be found by solving a sparse system of linear equations . Figure 4.1 shows the effect of the smoothness constraint on a smoothing spline: however many knots are included, over-fitting is prevented by the smoothness term. Thus smoothing splines are normally preferable to regression splines, except that the choice of A is computationally demanding , and .?. = 0 can be an adequate approximation with a small number of knots. Other smoothing algorithms are also used, for example the loess smoother (Cleveland et al., 1992) which uses robustly-fitted locallyweighted polynomials. Let h(x; ¢) be a polynomial of degree d (usually one or two). Then the fitted value g(x) is found by fitting h(x; ¢) in the ~eighbourhood of x and reporting', 'Then the fitted value g(x) is found by fitting h(x; ¢) in the ~eighbourhood of x and reporting the fitted value at x. Specifically, cPx is chosen to minimize and g(x) = h(x, \"¢x). The loss function p(u) could be u2 but might penalize large departures less severely, in the spirit of robust statistics. Finally, the parameter r controls the smoothness, and is chosen to']\n",
      "['124 0 ci \"\\' 9 0.0 0.5 4 Flexible Discriminants .. . .. .···~· . ······... . · .. .· .. 1.0 1.5 2.0 2.5 3.0 include a neighbourhood of cxn points at point x. This approach is easily extended to smoothing d dimensions, for small d. If the smooth functions in an additive model are written in terms of basis functions, as for polynomials and splines, we have p dfj f(x) = ex + L L f3 jk</J jk(x j ). (4.3) j=l k=l If the smoothing procedure chooses parameters by least squares, we have a linear regression in an extended space of features spanned by the functions </Yjk(Xj). On the other hand, if as in smoothing splines the functions minimize a sum of squares plus a penalty, we will have (approximately or exactly) a penalized linear regression. The method BRUTO used in the examples is described in Hastie & Tibshirani (1990, pp. 262-3). This adaptively chooses the smoothness ofthe splines and the number of terms in (4.1), including the possibility of linear functions and dropping features', 'the number of terms in (4.1), including the possibility of linear functions and dropping features completely. A general procedure to fit additive models is known as back-fitting (Hastie & Tibshirani, 1990). This holds all but one of the additive terms constant, removes that term and fits a smooth term to the residuals against the feature. In symbols, the model is f(x)-ex-L gj(Xj) = g1(x!) (4.4) HI and any smoothing algorithm (including loess) can be applied to the left-hand side. Smoothing is applied a feature at a time until the process converges (which it will under mild conditions). Figure 4.1: The effect of varying the number of knots in a smoothing spline for fixed A.. Based on an example of Wahba & Wold (1975). For 50 or more knots the curves are indistinguishable.']\n",
      "['4.1 Fitting smooth parametric functions 125 Thus far we have considered penalized least-squares fitting. Linear models can be used to fit a logistic regression by maximum likelihood via local linearization to give a weighted least-squares problem (McCullagh & Neider, 1989) and this is often solved iteratively in a handful of iterations. The extension to additive models is immediate (Gu, 1990; Wahba et al., 1995). An extension of this approach is to allow smooth functions of a small number of the features as terms in the additive model (for example, by C. J. Stone, 1985). Adding pairwise then three-term functions (and so on) is common practice in statistics, and these are known as interaction functions. Examples including functions of two features are given by Hastie & Tibshirani (1990, §9.5), Wahba (1995) and Wahba et al. (1995). Pima Indians diabetes In Section 3.5 we saw that a linear logistic discrimination model worked well for the data on diabetes amongst female Pima Indians, and', \"logistic discrimination model worked well for the data on diabetes amongst female Pima Indians, and that a polynomial in age improved the fit. We could consider if smooth non-linear terms in age or other variables such as plasma glucose levels and the body mass index might help the fit. This was tried with several smoothers. The improvement in fit as measured by AIC or NIC was marginal, but the test-set error rate was unchanged or increased. BRUTO (which fits by penalized least squares) selected only linear terms, dropping blood pressure and skin thickness. Wahba et al. (1995) built an additive model including an interaction term in glucose concentration and body-mass index and a categorical term for the number of pregnancies (0, 1 or 2, 3-5, 6 or more). In our experiments this did less well than the linear model. Projection pursuit regression Additive models do not allow interactions between the features in PI'. Perhaps the simplest way to allow interactions is through linear\", \"between the features in PI'. Perhaps the simplest way to allow interactions is through linear combinations (projections) of features: r f(x) =a+ L gj(tXj + PJ x) j=l (4.5) which is projection pursuit regression (PPR; Friedman & Stuetzle, 1981). Sometimes the components of ( 4.5) are called ridge functions because a peaked gj gives a topographic ridge in two dimensions.\"]\n",
      "[\"126 4 Flexible Discriminants This is a surprisingly general class of functions, as it can approximate uniformly arbitrary continuous functions over compacta (Diaconis & Shahshahani, 1984; L. K. Jones, 1987, 1992; Zhao & Atkeson, 1992). (This is sometimes referred to as the 'universal approximation' property.) As PPR encompasses feed-forward neural networks, these results follow from (but are a little easier than) those of Section 5.7 where the functions gj are restricted to one function, the logistic. However, ridge functions provide better (in the sense of fewer parameters) approximations to some functions than others (Donoho & Johnstone, 1989; Zhao & Atkeson, 1992), which Zhao & Atkeson express as working better for 'angular smooth functions' than for 'Laplacian smooth functions'. With multivariate regression we have to decide whether to use common non-linear terms for the different independent variables. This is usually done, so that for example for projection pursuit regression we\", \"variables. This is usually done, so that for example for projection pursuit regression we have r fk(x) = '1k + LYkjcPj(aj + 13[ x). (4.6) j=l This shows that the fitted values lie in a (r + 1 )-dimensional space. Since the scale of cPj is not otherwise fixed, we can choose cPj(aj + 13[ x) to have zero mean and unit variance over the training set. Algorithms The original algorithm for PPR has been superseded by SMART (Friedman, 1984). This constructs the (approximate) least-squares fit iteratively. A maximum value M for r is specified, and terms are added to ( 4.5) one at a time until M terms are present. Then at each step the least effective term is dropped and the model re-fitted, until r terms are left (and this process can also be used to help select r by looking at the fit). Some of the details can be changed by the user, and we only describe the 'highest' level of optimization. Backfitting is used to fit the model, and when the j th term is being considered, the direction 13j is\", \"is used to fit the model, and when the j th term is being considered, the direction 13j is optimized by a Gauss-Newton procedure (see Section A.5). This finds a local minimum of the least-squares criterion for a model with r terms. A new term is introduced by an initial direction 131, and the process continues until convergence. When M terms have been added, the least important (measured by Lk IYkjl) is dropped, the reduced model re-fitted and the process continued. The precise algorithm for scatterplot smoothing is not intrinsic to SMART, and we have also use spline smoothers. Friedman used his own 'super-smoother', which uses a local linear fit to k/2 data points\"]\n",
      "[\"See the glossary. 4.1 Fitting smooth parametric functions 127 each to the left and right of the point x at which g(x) is required. (The use of a rectangular window allows fast updating as x scans along.) The value of k is chosen from three possibilities by cross-validation at x, and then this choice is smoothly interpolated between the three smoothers. Hwang et al. (1991, 1992a, b, 1994a, b, 1996) replaced the supersmoother by Hermite polynomials, which tends to produce smoother functions, and Roosen & Hastie (1994) have also tried smoothing splines. Most of the examples were tried both with Friedman's supersmoother and our own implementation of smoothing splines. Hinging hyperplane s Breiman's (1993) hinging hyperplanes are the special case gj(x) = [x]+ = max(x, 0) of PPR. These suffice to approximate arbitrary continuous functions , by the results of Section 5.7. It is attractive to use the same projection direction for both positive and negative versions of the function, when we\", \"use the same projection direction for both positive and negative versions of the function, when we have g(a + f3T x) =a max(a + f3T x,O) + b min(a + f3T x,O). A little thought shows that this is a linear function plus a multiple of max(±(a + f3T x)), which suggests adding an overall linear function to f(x) to avoid a wasteful fit using a projection and two components to recover a linear term. These have universal approximation properties by the methods of Section 5.7, as using two such functions we can approximate a step ridge function. Further, we will have rate of convergence results for suitable smooth functions of order 0(1/ .jY). (Breiman's Theorem 3 is more restrictive than our Proposition 5.3 in assuming a higher degree of smoothness although it may thereby use a smaller r.) The attraction to Breiman of hinges was a fast way to fit one hinged hyperplane by least squares. This is used within the back-fitting approach of PPR. His reported CPU times seem comparable with the\", 'is used within the back-fitting approach of PPR. His reported CPU times seem comparable with the state-of-the-art in fitting neural networks, which have the advantage of using smooth (indeed, infinitely differentiable) functions. The fast algorithm is based on the idea that once we know which side of the hinge the points fall, fitting the hinged hyperplanes is simple (a linear least-squares problem, which can be updated as points change sets). Thus we choose an initial hinge, divide the points, fit, divide again on the fitted hinge and repeat. This gives a local minimum, so may be run from several starting hinges.']\n",
      "[\"128 4 Flexible Discriminants Breiman also considers variable selection, that is restricting {31 to pick just some features. This can ease the search (and could also be applied to full PPR) but does assume that the features have some individual meaning. MARS (multivariate adaptive regression splines) Friedman's (1991) multivariate adaptive regression splines allow for interactions more explicitly by M Km f(x) = rx + L f3m 11 cf>km(Xv(k,m)) (4.7) m=l k=l where and and tkm is an observed value of Xv(k,m)· The degree is the largest Km; if this is one the model is additive. The functions were chosen to allow fast least-squares fitting algorithms. (Terms cf>km and cf>k,m+l are added together.) The components are splines corresponding to a penalty on the first rather than second derivative, discussed further in Section 4.3. Once again back-fitting is used, and terms are only considered for a high-order product if they are interactions of terms which occur in the current fit. The precise\", \"a high-order product if they are interactions of terms which occur in the current fit. The precise details of the selection of terms are an 'engineering detail' discussed at length in Friedman's paper and its discussion. His algorithm has a forward phase followed by a backward phase. Terms are chosen to add or delete depending on a lack-of-fit criterion, which is the residual sum of squares divided by (1-Cjnf. Here n is the number of observations, and C is the number of parameters plus a multiple (2-4) of the number of terms M. The backwards elimination step aims to produce a model with comparable performance but fewer terms. In the additive case, over-fitting is avoided by reducing the number of knots rather than via a smoothness penalty. MARS produces fits which are continuous but not differentiable, which can be visually unappealing. Friedman suggests 'smoothing' out the piecewise linear functions cf>kJ• for example by replacing them by cubics (and re-fitting the coefficients).\", \"linear functions cf>kJ• for example by replacing them by cubics (and re-fitting the coefficients). PIMPLE Breiman's (1991) IT-method with program PIMPLE is another way to include interactions (and we will meet a third as interaction splines). As [y]+ means max(y,O). However, the splines are fitted by least squares, that is as regression splines rather than smoothing splines. This is related to the GCV penalty discussed in Section 4.3.\"]\n",
      "['Figure 4.2: Non-linear discriminants for the crabs data via projection pursuit regression. The left plot uses linear discrimination on the predictions, the right plot a robust version. 4.1 Fitting smooth parametric functions 129 for MARS, Breiman considers a sum of products of splines, this time cubic splines, but the method appears to be geared towards small numbers of products of quite accurate fits, rather than as in MARS large numbers of inaccurate products. Back-fitting is used. The variableselection strategy starts with a small number of knots in the splines, increases this and then considers deleting individual knots, using generalized cross-validation (Section 4.3) to decide when to stop adding and when to delete. Breiman suggests that in three or more dimensions the product terms will be identifiable (up to their ordering in the model) and so can be interpreted; his discussants are less confident. Examples We first consider the data on Leptograpsus crabs. BRUTO selected FL, CL', 'less confident. Examples We first consider the data on Leptograpsus crabs. BRUTO selected FL, CL and CW to enter linearly, with a slightly non-linear term for RW but a roughly parabolic term for BD. MARS of degree 1 chose a single break of slope for each feature (and a small departure from linearity) except for BD. If interactions are permitted, several two-term interactions and one three-term interaction are chosen. Although the models are quite non-linear, the discriminant plots are virtually unchanged. A difficulty (Ripley, 1994c) with our second approach (using LDA on the fitted values of a regression) is that a small number of outliers which are not fitted well can distort the within-group covariance matrix. Figure 4.2 shows the first two canonical variates for a projection-pursuit regression (with r = 3) for the crabs data, in which a number of outliers have appeared, together with bunching of points which are predicted particularly well. Sometimes using a robustified', 'with bunching of points which are predicted particularly well. Sometimes using a robustified discriminant analysis will help, as Figure 4.2 shows . ... D \" ~oJl\\'b 1U ,tiJ66 ·c \"\\' ~ D D o\" ~ 6 ~ 6 6 D D \" 0 • tt .. . 0 \"-l.\\\\...t \" \"\\' .... » .. <.> \" \\'l\\' :~~ AA ...... -: \" :-: -.1i) 0 l \\\\•. ~ ..,. • • . . • • I ••• . :· ~ ~· •.-... . : -6 -4 -2 0 2 4 6 -10 -5 0 5 first canonical variate first canonical variate Figure 4.3 shows a series of non-linear discriminant plots for the forensic glass data. The BRUTO fit is very similar to linear discrimination. When we examine the terms of the additive model, we find']\n",
      "['130 4 Flexible Discriminants co \"\\' \"\\' 0 0 ~ L---------~--~-- ----~ ~ L_--------~ ~--------~ -5 0 5 10 -5 0 5 (a) (b) . i\" 5 ... ... 4 f 2 i \"\\' ~ \"\\' tl I l\\'• 0 \\' \"\"\\' 2j 0 #\\': 42 # 2 .., \" 2 <)\\' \\') <)\\' \\'\" 3 \\\\ \\'.., 6 \\'\\' \\\\ -6 -4 -2 0 2 4 6 8 -6 -4 -2 0 2 4 6 (c) (d) that iron oxide has been omitted, and the term in calcium is slightly non-linear ; all the other terms are selected to enter linearly. The cross-validated error rate was 35%, a little better than the full linear discriminant analysis (but not significantly so). MARS with maximum degree 1 fits an additive modeL This introduces non-linear terms in RI, Mg, Si, K, Ca and Ba, and drops the rest. The fitted functions are shown in Figure 4.4. This achieves a cross-validated error rate of 32.2%. As more interaction terms are introduced in MARS these principally distinguish groups 5 (tableware) and 6 (headlamps) from the rest. As Figure 4.3 shows, the linear discrimination model on the transformed features becomes much less plausible.', \"4.3 shows, the linear discrimination model on the transformed features becomes much less plausible. The performance improved to 29% both with pairwise interactions and with unlimited interactions. Projection pursuit regression was also used to produce new features for use in linear discrimination. There are six classes here, so if we choose the number of ridge terms r < 5 there will be collinearity in the new features. It does seem desirable to choose at least five ridge functions, which must necessarily be rather smooth functions to avoid over-fitting. This was borne out by our experiments , which achieved a cross-validated error rate of 42% if Friedman's super-smoother was used, but 35.5% if smoothing splines were used with a relatively large value of A.. Using just three ridge functions made the performance Figure 4.3: Non-linear discriminant plots for the glass data: (a) using BRUTO; (b, c, d) using MARS with maximum degree 1, 2, and unrestricted.\"]\n",
      "[\"Figure 4.4: Fitted functions for the MARS additive model for the forensic glass data. The y scales are arbitrary. 4.2 Radial basis functions 131 I ~I ' Rl Mg ~I • ~I ' s; K ~I i ~I ,, Ca Ba considerably worse as the two types of window glass were barely separated. Pima Indians Fitting MARS and PPR models by least squares to the Pima Indians diabetes data did not improve the fit over linear methods, with a typical test-set error rate of 75/332. 4.2 Radial basis functions We return to ways of parametrizing f or the log probabilities as a linear combination of basis functions. For a one-dimensional x splines are a natural choice. For higher dimensions we could use multidimensional splines (Section 4.3), but radial basis functions or RBFs have been more widely advocated (Powell, 1987, 1992; Broomhead & Lowe, 1988; Lee & Kil, 1988; Moody & Darken, 1989; Poggio & Girosi, 1990a, b; Musavi et al., 1992). RBFs are approximations of the form y = o: + L /3jG(IIx- Xjll) j (4.8) for centres Xj.\", 'al., 1992). RBFs are approximations of the form y = o: + L /3jG(IIx- Xjll) j (4.8) for centres Xj. Examples of G proposed include the Gaussian G(r) = exp -r2 /2, the multiquadric G(r) = J(c2 + r2) (Hardy, 1971, 1990; Kansa, 1990) and the thin-plate-spline function G(r) = r2log r. (The RCU network of Reilly et al., 1982, has G(r) = I(r < ro).) It is easy']\n",
      "['132 4 Flexible Discriminants (but not as useful) to extend the definition to general kernels G(x-Xj) or G(x,xj)· For multivariate approximations we just take a and /3j to be vectors, that is we take different linear combinations of the same basis functions. When G is Gaussian, (4.8) can be seen as extending the notion of approximating a probability density by a mixture of known densities. The norm II II is unspecified, and could be Euclidean distance or a Mahalanobis distance, when the densities would have a common covariance matrix. In general we might want to consider different covariance matrices for each component, leading to the form y =a+ L /3jG(IIAj[x- Xj] II). j Girosi et al. (1995) call this form hyper basis functions. (4.9) A variant of radial basis functions which is sometimes considered (Moody & Darken, 1989; Xu et al., 1994) is the normalized form 2.:: · /3jG(IIx- Xjll) y = --===1=--------l.:jG(IIx-xjll) . (4.10) Approximation properties The class of radial basis functions', \". (4.10) Approximation properties The class of radial basis functions has similarly good approximation properties to those of ridge-function methods (such as projection pursuit regression and feed-forward neural networks). There are many possible cases to consider, depending on whether the centres and G are fixed or adaptive (chosen for each dataset). We will only give a flavour of the results. Park & Sandberg (1991) studied the subclasses of (4.9), Y =a+ Lf3jG(II[x-xj]ll/aj) j y = a+ L /3jG(II [x-Xj] 11/a) j (4.11) (4.12) in which G is fixed but the covariance matrices are proportional, including the special case in which the aj are identical. (This is still more general than (4.8).) Provided G is continuous, bounded and with a finite and non-zero integral, they show that the class (4.12) is dense in Lp for every p E [1, oo ), and can uniformly approximate continuous functions on compact sets. (This is sometimes referred to as the 'universal approximation' property.) Thus for any\", \"sets. (This is sometimes referred to as the 'universal approximation' property.) Thus for any function f(x) there is a set of centres (xj) and a a> 0 such that (4.12) is close to f\"]\n",
      "[\"Here H'·2 is the space of L2 functions all of whose derivatives up to order s are in L2 and so includes the Gaussian. 4.2 Radial basis functions 133 in the appropriate norm ( Lp or the maximum difference on a compact set). The uniform approximation on compact sets of continuous functions for Gaussian RBFs of the form (4.11) can be shown via the Stone-Weierstrass theorem (Girosi & Poggio, 1990; Hartman et al., 1990). Girosi & Poggio (1990, Appendix C) state a more general result for (4.8) and piecewise continuous G arising from (4.14) below, but their proof is of pointwise rather than uniform convergence. (The proof can be completed by consideration of the discretization error in a Riemann integral, using the modulus of (uniform) continuity.) Thus we still have uniform approximation with a fixed basis function G. Results on the rate of approximation are available for smooth enough target functions f (Girosi & Anzellotti, 1993), using the methods described in Section 5.7. They considered\", 'f (Girosi & Anzellotti, 1993), using the methods described in Section 5.7. They considered the class (4.8) for G E L2(JR.P) and the class of targets f which are in L2(JR.P) (for Lebesgue measure) and can be expressed in the form f(x) = { G(x-y) d.A.(y) JJR.P for a signed measure A of bounded total variation (that is, the difference between two finite measures). Thus functions in the class are at least as smooth as G; for example, for the Gaussian G the sharpness of peaks is firmly controlled. Then the L2 rate of convergence is bounded by II.A.II/ J1l for n terms, when G is scaled to II Gil = 1. Further, if G is in H8•2 for s > p/2, there is uniform convergence at rate 1/ Jll. As class (4.12) corresponds to the rescaled kernel G, the results of Girosi & Anzellotti also apply to target functions of their class for any rescaling of G (although the constant in the bound will vary with the target function). This class of functions is, however, strictly smaller than L2. (Consider the', 'the target function). This class of functions is, however, strictly smaller than L2. (Consider the function x-1/4 /(lxl < 1).) Fitting Finding the coefficients a and {31 in (4.8) to (4.12) is usually done by least squares and is easy; for fixed centres x1 and fixed scale parameters aJ these are linear regression equations, so least-squares fitting reduces to solving linear equations. This leaves the issues of finding the centres and any scale factors. One possibility is to take every training example as a centre, but this can lead to over-fitting. This suggests taking a representative collection of training examples, for example a random or stratified sample (e.g. Lee, 1991). There are other ways to take a representative collection of']\n",
      "[\"134 4 Flexible Discriminants points not necessarily within the training set. For example, the k-means algorithm (Section 9.3) of cluster analysis chooses k points in P£ to minimize the sum of squares from each training point to the nearest of the k points. These clustering algorithms do not take the classes of the training examples into account. Musavi et al. (1992) designed an agglomerative clustering algorithm (see Section 9.3) which only merges clusters of points with the same class. The cluster means then provide the centres for the basis functions. The algorithm will choose the number of clusters and hence the number of basis functions, but has a 'clustering parameter' 11 which controls the agglomeration process. Other ideas are the SOM (Section 9.4) and LVQ (Section 6.3) methods of Kohonen. LVQ aims for cluster centres which provide a good set for nearest-neighbour classification. Moody & Darken (1989) considered finding centres by k-means (using the method of (6.9) on page 202)\", \"Moody & Darken (1989) considered finding centres by k-means (using the method of (6.9) on page 202) and this has often been used. Note that this (and most other ways of choosing representatives) depends on the choice of a metric in P£, and so is most appropriate for (4.12) or (4.8) rather than for (4.9). Moody & Darken explored choosing the scale functions O'j in (4.11) from a heuristic using the P-nearest neighbour distance. Musavi et al. (1992) chose the matrices Ai in (4.9) (equivalently the covariance matrix of the Gaussian basis function) by fitting a maximal ellipsoid around the centre which includes no training examples of another class, and taking this as an isodensity surface containing 95% of the probability . Note that this procedure is very sensitive to outliers and faulty training-set classifications. Leonard et al. (1992) extend the Moody-Darken method by adding further outputs designed to signal extrapolation and to give confidence intervals for the predictions. It is\", \"designed to signal extrapolation and to give confidence intervals for the predictions. It is of course possible to minimize the least-squares fit over the parameters (J or (J j, and even over the centres of the basis functions. As the least-squares fitting is partially linear, this will be computationally quite feasible and seems to be the proposal of Poggio & Girosi (1990b), although considered and rejected as too demanding by Moody & Darken. Wetterschereck & Dietterich (1992) considered optimizing over both parameters O'j and centres. On their (single) example they found that optimizing over centres was particularly important to achieve a competitive performance by RBF methods, but that choosing the O'j by a local heuristic was counter-productive. Yet another idea is to choose cluster centres from a large class of candidates (for example, all training examples) by a stepwise regression\"]\n",
      "[\"4.2 Radial basis functions 135 procedure. This was the idea of Chen et al. (1991). Any sensible selection strategy could be used, and the method extended to choosing from a small number of scale factors at each candidate centre. Poggio & Girosi (1990b, §IV.D) consider alternatives to least-squares fitting for 'unreliable data'. They replace the square function in the sum of squares or in (4.13) below by with E positive and f3 large. This enforces a quadratic penalty only up to about ±-JE, and has the same effect as are-descending M-estimator in robust regression (Huber, 1981; Hampel et al., 1986; Rousseeuw & Leroy, 1987). This plethora of methods provides a difficulty in assessing RBF methods; no two workers use the same class of RBFs and method of fitting. There is a range of compromises being made between speed of fitting and accuracy of approximation. RBFs are often claimed to be much faster than ridge-function methods on the basis of their partial linear fitting, yet if the centres\", \"faster than ridge-function methods on the basis of their partial linear fitting, yet if the centres are varied (or regularization used; Section 4.3) their computational load seems as large as their competitors. However, there is considerable scope for inspired choices of centres in specific problems (as in Roberts & Tarassenko, 1995). Potential functions The concept of potential functions has a variety of meanings within the pattern recognition literature. To some users it is synonymous with kernel methods (Section 6.1). Its origins (Bashkirov et al., 1964; Aizerman et al., 1964a, b, 1965; Braverman, 1965; Arkedev & Braverman, 1966) are close to those of radial basis functions. Suppose we consider each observation X; as having some 'charge' q; and measure the 'potential' at another point x. It will be of the form and this provides another way to approximate by a smooth function (except perhaps at the data points). The 'potential' is to be chosen by the user, and so could subsume both\", \"perhaps at the data points). The 'potential' is to be chosen by the user, and so could subsume both kernel methods and radial basis functions . The potential-function classifier is trained to attempt to correctly classify all the samples of the training set by adjusting the qi by a\"]\n",
      "['136 4 Flexible Discriminants perceptron-like procedure. Indeed, it can be seen as applying perceptron ideas to generalized discriminant functions. Potential functions can also be used to approximate probability density functions (Kashyap & Blaydon, 1968; Tsypkin, 1966). The method is considered in more detail in books by Meisel (1972) and Young & Calvert (1974). It seems to have disappeared from view until revived as the study of radial basis functions. 4.3 Regularization An alternative to reducing the number of basis functions in fitting RBFs is to allow one per training example, but to control directly the smoothness of the fitted function, as is done for smoothing splines in one dimension. Indeed, since splines are so useful in one dimension, they might appear to be the obvious method in more. In fact they turn out to be rather restricted and little used. This section is dominated by least-squares fitting, since regularization has been most explored in approximation theory. We know', 'least-squares fitting, since regularization has been most explored in approximation theory. We know of no exact solutions (such as smoothing splines) for other forms of our problem such as fitting multiple logistic models by maximum likelihood. Bishop (1991) considers (4.12) with a centre at every example in the training set, but adds a penalty term when fitting. Let f(x) denote the approximating RBF. Then the term to be minimized is L IIYi-f(x;)ll2 + AC(f), i where i refers to the i th training example. This is an example of a general process termed regularization in which other penalties C(f) may C(f) is sometimes be considered. The parameter A controls the smoothness and degree called a stabilizer. of fit. For A = 0 the fits will usually be exact (from interpolation properties of RBFs) and as A ---+ oo the fitted function becomes flat. Bishop chooses A by trial and error. The sum over .examples can be seen as an approximation to an integral over fl£. Adding a penalty C(f) has a', 'over .examples can be seen as an approximation to an integral over fl£. Adding a penalty C(f) has a Bayesian interpretation. The first term is proportional to the log-likelihood if we assume that the noise variance a} in a regression is known, so if we take a prior over functions f which is proportional to exp -2Aa}C(f), minimizing a penalized sum of squares is equivalent to maximizing the posterior density over f. This is a MAP estimator (see Section A.1) and is widely used in image analysis (following Geman & Geman, 1984). However, the warnings about MAP estimation given in Section A.1 must be borne in mind.']\n",
      "[\"4.3 Regularization 137 Suppose we add a penalty of the form C(f) = II P f 112 for a differential operator P. We can then consider the function f minimizing the penalized sum of squares L IIYi-f(xdf + .?ciiP/112 (4.13) over all (smooth enough) functions f, not just those represented by a form of RBFs. Exactly as for smoothing splines, the general solution (Poggio & Girosi, 1990b; Wahba, 1990) is of the form (4.14) where G is the (symmetric) Green's function of P P, P is the adjoint differential operator, and n(x) is a function in the null space of P. If the operator is translation or rotation equivariant, so will G be. Thus equivariance under rigid motions leads to Green's functions of the radial basis function form. The coefficients Ci satisfy the linear equations (4.15) and n(x) is chosen by least squares. The Gaussian RBF arises from the (non-intuitive) penalty functional (Poggio & Girosi, 1990b, pp. 95-96), and the null space gives the constant IX. The penalty is more obvious when\", '1990b, pp. 95-96), and the null space gives the constant IX. The penalty is more obvious when expressed in the Fourier domain, and Girosi et al. (1995) consider the class of penalties C(f) = j J~s)J2 ds G(s) (4.16) for some positive symmetric function G that tends to zero as llsll ---+ oo. Here the tilde denotes Fourier transformation, and it turns out (Dyn, 1987; Madych & Nelson, 1990; Girosi et al., 1995) that G is the Fourier transform of the function G(x-xi) in (4.14). In this formulation the Gaussian RBF arises from G(s) = exp-,BIIsf. Although regularization is theoretically interesting, it demands the solution of large systems of linear equations (4.15). In the case of smoothing splines in one dimension this is a banded system and can be solved quickly, but in general it will take O(n3) operations for n']\n",
      "[\"138 4 Flexible Discriminants examples and so be prohibitively slow. It remains possible to use a smaller set of basis functions as in the previous subsection and to use a penalty functional to control the smoothness . We could use a generalpurpose optimizer to minimize the penalized measure of fit over all parameters rather than solve (4.15) within a loop. Although we have considered only least-squares problems in this subsection, similar considerations apply to other deviance functions, since they can be approximated locally at the optimum by a weighted sum of squares function, and this continues to have a solution of the form (4.14) with appropriate modifications to (4.15). Another approach to regularization is to add noise during training (see, for example, Sietsma & Dow, 1991). Suppose we add a moderate amount of white noise to the target values Yi· Then (Webb, 1994; Bishop, 1995b) a second-order Taylor expansion shows that the effect is approximately (despite Bishop's title) the\", 'a second-order Taylor expansion shows that the effect is approximately (despite Bishop\\'s title) the same as using the regularizer C(f) = ~ E [ ( 8~~~)) 2 + H.fi(X)-Yil ;;1~]. 1,] Bishop shows that for small added noise and a good fit the first term dominates (since E[fi(X) -Yil will be small), so the regularization is mainly by the expected squared length of the first derivative. With non-least-squares error functions, local linearization gives a weighted least-squares approximation and hence an expected weighted sum of (ofi(X)/8Xj)2. Multidimensional splines Smoothing splines in one dimension arise from the regularization penalty J g\"(u)2 du on the sum of squares at the data points. This penalty does not generalize immediately to higher dimensions, and a suitable generalization took some years to emerge. Thin-plate splines use the penalty !! a2j(x,y) 282j(x,y) 82f(x,y) d d ox2 + oxoy + oy1. X y on 1R.2 which is invariant under rigid motions, and m! j [ amj(x) ]2 a a dx 2: OCJ ! • • •', 'oy1. X y on 1R.2 which is invariant under rigid motions, and m! j [ amj(x) ]2 a a dx 2: OCJ ! • • • OCn! OX 1 • • • OXn\" tXJ+··+an l in JR.P. We have to take 2m > p to ensure this is a smoothing penalty, since a radially symmetric bump of width h will have penalty proportional to h2m-p which should go to infinity as h -+ 0.']\n",
      "['4.3 Regularization 139 The solution to the penalized least-squares problem is of the form f;.(x) = cp(x) + L c;G(IIx-xdl) i for a polynomial cp of total degree at most m -1 and G(r) = { r2m-p log r if 2m-pis even r2m-p otherwise (Duchon, 1977; Meinguet, 1979; Wahba, 1990, p. 33). Thus univariate smoothing splines and our two-dimensional example correspond to m = 2. The higher-dimensional cases lose the computational simplicity of smoothing splines since the matrices are no longer banded. Note that m = p = 1 gives a penalty of the integrated square of the first derivative, and the solution is piecewise linear, as used in MARS. We can only use a second-derivative penalty in p ~ 3 dimensions, and even in two dimensions there are edge effects to consider (Green & Silverman, 1994, Chapter 7). Additive models, including those with interaction terms such as MARS, can be put within the spline framework by choosing a suitable regularizer (Wahba, 1990, Chapter 10). Summing penalties gives an', \"framework by choosing a suitable regularizer (Wahba, 1990, Chapter 10). Summing penalties gives an additive model, and including terms which involve two or more variables gives rise to interaction terms, most simply for m = 1 (since for m > 1 there are interactions between polynomials and splines to consider). These are known as additive and interaction splines respectively; the latter are considered by Barry (1986), Gu & Wahba (1991) and Gu et al. (1989). Tensor product splines are similar to MARS in that the interaction terms are products of functions of a single feature. They arise from a penalty of the form (4.16) in which G is a product of functions of a single feature. Fitting additive models with interaction terms via splines gives rise to a potentially large number of parameters A to be considered, one for each term. For m = 2 there are 22 A's to choose even for p = 4, which is beyond current methods to select. Asymptotic theory Once a regularization term is added to the\", 'is beyond current methods to select. Asymptotic theory Once a regularization term is added to the fitting criterion, for example the deviance, the asymptotic distribution theory of the parameter estimate will be changed, whether or not the model is true, except in the unlikely event that the penalty is completely ineffective. From their intended purpose, we would expect the effect of regularization terms to']\n",
      "[\"140 4 Flexible Discriminants be to introduce bias, even asymptotically, but to decrease the variance. This raises the possibility of juggling the amount of the penalty so that the bias decreases as n --+ oo but the variance remains under control, indeed decreases to zero at a rate close to 0(1/n). This general programme is often considered 'non-parametric' since arbitrarily complex models will be needed to fit a true model outside the assumed class of functions . (For instance, consider Gaussian RBF functions with centres at data points, and approximating an exactly linear function.) The results of Section 2.8 provide sufficient bounds on the complexity of the model needed for n points which can allow the penalty to be varied with n, and a heuristic outline is given by Geman et al. (1992) with a more detailed account by White & Woolridge (1991) and White (1990). As an example of these techniques we can consider a projection pursuit regression with the number r of terms growing at\", 'techniques we can consider a projection pursuit regression with the number r of terms growing at O(n1-\") for E > 0, and control the smoothness of the terms to grow at O(log n), thereby achieving risk consistency. This asymptotic programme has little to do with the performance in realistic situations, since there is not usually a large enough training set relative to the complexity of the fitted classifier. For useful results we need an approximation, not a limit theorem. Moody (1991, 1992), Liu (1993, 1995) and Murata et al. (1993, 1994) attempt to provide this in a limited set of circumstances. They assume a regularization penalty proportional to n (unrealistic according to the previous paragraph, which suggests it should grow less rapidly, but much better than constant regularization). Then we can use the results of Propositions 2.2 and 2.3 in Section 2.2 with Lt = Et + A.Ct replacing -logp, where E1 is a term in the fit criterion (for example the contribution of an example to the', \"-logp, where E1 is a term in the fit criterion (for example the contribution of an example to the negative log-likelihood) and Ct = C/n. This leads to NIC = 2[E(O) + A.C(B) + p*] where p• = trace[KJ-1] and d K _ V oLt(8o;X) an -ar o8 . If the penalty A.C(O) measures the smoothness of the fitted function, it is the same for both the training and test sets, so the expected difference in E between test and training sets of size n is also p• (which is Moody's formulation). Sometimes if the penalty is an integral over the feature space of the form C(8) = Jq-G[f(x; 8)] dx it is replaced by C(l}) = * 2::: G[f(Xi; 8)], which has expectation C( 8) for both training\"]\n",
      "[\"4.3 Regularization 141 and test sets. Thus a penalty of the form C(O) can also be deleted from NIC. Moody (1991, 1992) calls 'the effective number of parameters', Pelf, the estimate of p• obtained by replacing the expectations in J and K by averages over the training set, and ()o by 0. There is a potential bias in using the training set to estimate p• since it shows how many effective parameters are needed to approximate the true distribution over a set of size n, and that could be much smaller than the number needed to approximate the whole distribution. Further, as we suggested in Section 2.2, a divisor n -p for the estimate of K would be more appropriate. We now return to the approximation of Bayes factors in Section 2.6. We can always regard p(()) oc -.IcC(()) as a prior density for the parameters (), although it may well be an improper prior (with infinite integral). Then choosing () to minimize the negative log-likelihod plus .IcC(()) is equivalent to MAP estimation, and we can\", \"() to minimize the negative log-likelihod plus .IcC(()) is equivalent to MAP estimation, and we can use (2.43) as an approximation to logp(:Y I M). Choosing A. To actually use regularization, we have to select all the A's. If they are derived from Bayesian priors as in Section 5.5, they are already determined, but otherwise they are treated as free parameters (and are also so treated in some empirical Bayes schemes). We can use the methods of Section 2.6, in particular cross-validation. However, this has two disadvantages. The first is computational unless updating schemes are known (as for univariate smoothing splines; Silverman, 1985). The second is that cross-validation is not invariant under orthogonal transformations of the data vector in a regression problem. Generalized cross-validation (GCV) computes the average adjustment of the fit on leaving each point out, rather than producing the adjustment for each point (Craven & Wahba, 1979). Thus in a regression problem we have\", \"the adjustment for each point (Craven & Wahba, 1979). Thus in a regression problem we have GCV(./c) = _.!._ l:f:t IIYi-JA(xi)ll2 N [1-trace(A(.Ic))/ N]2 where A,t is the matrix mapping the data vector to the vector of the fitted values. Where several A's are involved, A and GCV become a function of all of them, and a simultaneous minimization is needed; Wahba (1990) reports using up to 10 2 's for additive interaction splines. Except for univariate splines, the computation is dominated by the computation of A(./c). Hastie & Tibshirani (1990, §9.4.3) suggest approximating traceA(./c) by one plus the sum of the traces of the\"]\n",
      "['142 4 Flexible Discriminants matrices for the individual smoothers minus one, which seems ad hoc but is much faster. This is used in BRUTO. For non-least-squares problems there seems no general alternative to V -fold cross-validation. Hastie & Tibshirani ( 1990, §6.9) propose a version of GCV which weights the deviance by the divisor of the least-squares form, but this seems unsupported theoretically except as a local weighted least-squares approximation (given by Wahba, 1990, p. 113).']\n",
      "[\"What we denote by W;j, the weight on the link from i to j, is more often denoted Wji· 5 Feed-forward Neural Networks A great deal of hyperbole has been devoted to neural networks, both in their first wave around 1960 (Widrow & Hoff, 1960; Rosenblatt, 1962) and in their renaissance from about 1985 (chiefly inspired by Rumelhart & McClelland, 1986), but the ideas of biological relevance seem to us to have detracted from the essence of what is being discussed, and are certainly not relevant to practical applications in pattern recognition. Because 'neural networks' has become a popular subject, it has collected many techniques which are only loosely related and were not originally biologically motivated. In this chapter we will discuss the core area of feed-forward or 'back-propagation' neural networks, which can be seen as extensions of the ideas of the perceptron (Section 3.6). From this connection, these networks are also known as multi-layer perceptrons. A formal definition of a\", \"this connection, these networks are also known as multi-layer perceptrons. A formal definition of a feed-forward network is given in the glossary. Informally, they have units which have one-way connections to other units, and the units can be labelled from inputs (low numbers) to outputs (high numbers) so that each unit is only connected to units with higher numbers. The units can always be arranged in layers so that connections go from one layer to a later layer. This is best seen graphically; see Figure 5.1. Each unit sums its inputs and adds a constant (the 'bias') to form a total input Xj and applies a function fj to Xj to give output Yj· The links have weights Wij which multiply the signals travelling along them by that factor. The input units are there just to distribute the inputs, so have f = 1. Thus a network such as Figure 5.1 represents the function Yk = !k ( rxk + L Wjk/j( rxj + L. wijxi)) j~k I~] (5.1)\"]\n",
      "[\"144 5 Feed-forward Neural Networks Bias units Hidden layer(s) from inputs to outputs. The functions fJ are almost invariably taken to be linear, logistic (with f(x) = t(x) = ex /(1 +ex)) or threshold functions (with f(x) = I(x > 0) ). A variant is to take hyperbolic tangent units with f(x) = tanh(x) = (ex-1)/(ex + 1) = 2t(x)-1, but this only introduces a linear transformation which can be absorbed into the weights (except at the output units). Only threshold units give a genuine multi-layer extension of the perceptron, and such networks were considered in Rosenblatt's work. The general definition allows more than one hidden layer, and it also allows 'skip-layer' connections from input to output. If all units in a layer have the same function fh or f0, we have Yk = fo ( rxk + L WikXi + L Wjdh ( aj + L WijXi)). (5.2) i->k j->k i->j The bias terms can be eliminated by introducing a new unit 0 (the bias unit) which is permanently at + 1 and connected to all other units. We set wo1 = a1.\", \"0 (the bias unit) which is permanently at + 1 and connected to all other units. We set wo1 = a1. (This is the same idea as incorporating the constant term in the design matrix of a regression by including a column of 1's.) This is shown in Figure 5.1. The general form is then Yk = fo ( L WikXi + L Wjk/h (L WijXi)). (5.3) i--->k }->k i-+j Note that if we have logistic units in the hidden layer, adding skip-layer connections is not really more general, since we can add another unit per output in the hidden layer with input weights wik!G and output weight G to just unit k. Then for large G we only use the central, linear, part of the range of the logistic function. However, skip-layer connections can be easier both to implement and to interpret. Figure 5.1: A generic feed-forward network with a single hidden layer. To avoid over-crowding bias units are shown for each layer, but they can be the same unit.\"]\n",
      "[\"5.1 Biological motivation 145 A neural network with a single logistic output unit can be seen as a non-linear extension of logistic regression. With many logistic output units, it corresponds to linked logistic regressions of each class vs the others. The terminology of neural networks can be very confusing: Figure 5.1 is sometimes referred to as having three layers (which seems visually correct), two layers (as the input layer does nothing) and one hidden layer (as the states of the units in the central layer cannot be inspected from outside the 'black box'). We will refer to the inputs, the outputs and the hidden layer, since we will almost always have only one hidden layer. We will extend our notation to allow every unit j to have an input Xj and output Yj· The inputs to the whole network are the inputs to the input units, and the outputs from the whole network are those of the output units. The signal paths through the network are determined by the equations Xj = LWijYi· i-+j (5.4)\", 'units. The signal paths through the network are determined by the equations Xj = LWijYi· i-+j (5.4) We can even drop the condition on the sum by defining W;j to be zero for all non-existent links. When programming it is useful to number the units by layer, so all units in the first layer precede all those in the first hidden layer and so on. Then we know w;j = 0 unless i < j. We will briefly consider how such functions came to be suggested, and the theory which shows that they form large and flexible classes of functions. However, in practice the main issues are how the parameters, the weights, should be chosen, and how the architecture (the number of layers and the number of units in each, as well as which connections to include) is selected. 5.1 Biological motivation The original biological motivation for feed-forward networks stems from McCulloch & Pitts (1943) who published a seminal model of a neuron as a binary thresholding device in discrete time, specifically that nj(t) =I (L', 'model of a neuron as a binary thresholding device in discrete time, specifically that nj(t) =I (L Wijn;(t -1) > 8;) i-+j the sum being over neurons i connected to neuron j. Here n;(t) is the output of neuron i at time t and 0 < W;j < 1 are attenuation weights. Thus the effect is to threshold a weighted sum of the inputs']\n",
      "['146 5 Feed-forward Neural Networks at value ei. Real neurons are now known to be more complicated; they have a graded response rather than the simple thresholding of the McCulloch-Pitts model, work in continuous time, and can perform more general non-linear functions of their inputs, for example logical functions. Readers may worry that this model can only allow nonnegative weights. This is so, but neural systems have both excitatory and inhibitory connections, so whereas each connection can effectively have a weight of just one sign, it would be possible to envisage both an excitatory and an inhibitory connection, to simulate the effects of weights of either sign. There is also a wider motivation based on human abilities in pattern recognition. A driver can recognize that a traffic light has changed to red and take the appropriate action (or decide not to) in well under one second. Neurons are rather slow devices by the standards of electronic computers, with messages travelling at up', \"are rather slow devices by the standards of electronic computers, with messages travelling at up to 100m/s, and of low bandwidth-perhaps 100 bits/s (MacKay & McCulloch , 1952). This allows rather few steps in the computation, most famously expressed in Feldman's (1985) concept of a 'one hundred step program', since there is time for at most 100 steps within a human reaction time. Human brains must make up for this lack of speed by massive parallelism, and given the speed of messages this parallel computation must be highly distributed. Brain scientists currently envisage vision as being performed in a series of layers in the brain, which naturally suggests a layered architecture for the distributed computation. (There is some evidence for feedback from later layers to earlier layers in human vision, but this is recent and controversial.) Further, human beings can learn tasks such as driving; it seems extremely implausible that we are pre-programmed to recognize red traffic lights nor\", 'it seems extremely implausible that we are pre-programmed to recognize red traffic lights nor (except for advocates of Lamarckian evolution) that this is an inherited adaptation. Something in our distributed neural computing system changes through experience. This could involve adding or removing connections or units, but most emphasis has been on exploring the consequences of changes in the strengths of connections rather than their topology. The book of Hebb (1949) has been very influential in thinking about how connection strengths should be changed. His approach is not quantitative, and so is all-embracing. He considers that the connection strength between units should be increased if they are activated together, and this is often taken to suggest reinforcing the connection proportionally to the product of their simultaneous activations, that is']\n",
      "[\"Earlier, Cybenko (1988) had showed that two hidden layers suffice. Uniform convergence on compacta is defined in the glossary. 5.2 Theory 147 This is known as the Hebbian learning rule, widely used in other forms of neural network. This idea that there could be a simple adaptation rule for connection weights used universally in learning found fruit in the experiments of Rosenblatt (1957, 1958, 1962) and Widrow (Widrow & Hoff, 1960). As discussed in Chapter 3, they used very simple units, in one or few layers, with weights expressed by motor-driven potentiometers. Widrow-Hoff learning (also known as the delta rule) was an iterative algorithm of the reinforcement type to fit a linear regression or discriminant. This seems to have been the inspiration for the rules used for 'learning' in neural networks in their renaissance. Similar rules were proposed for learning in potential function systems by Aizerman et al. (1964a, b, 1965). 5.2 Theory Equations (5.1) and (5.2) are thus far merely\", 'by Aizerman et al. (1964a, b, 1965). 5.2 Theory Equations (5.1) and (5.2) are thus far merely suggestive ways to parametrize multidimensional input-output relationships. They are rather general classes of functions, something which took a long time to be appreciated. Cybenko (1989), Funahashi (1989), Hornik et al. (1989), Carroll & Dickinson (1989), Stinchcombe & White (1989) and many later authors have shown that neural networks with linear output units and a single hidden layer can approximate any continuous function f uniformly on compacta, by increasing the size of the hidden layer, and this implies many other types of approximation. There are also some results on the rate of approximation (how many units are needed to approximate to a specified accuracy), but as always with such results they are no guide to how many units might be needed in any practical problem. These results are in fact rather easy to prove, very much easier than most published proofs, so we give complete', 'are in fact rather easy to prove, very much easier than most published proofs, so we give complete proofs in Section 5.7. A heuristic reason why feed-forward networks might work well with modest numbers of hidden units is that the first stage allows a projection onto a subspace of :!{ of much lower dimensionality, within which the approximation can be performed. In this feed-forward neural networks share many of the properties of projection pursuit regression (PPR; Section 4.1). Indeed, for theoretical purposes the two are essentially equivalent. We consider only linear output units, as clearly logistic, softmax or other transforms can be applied to the outputs of either family. Clearly ( 5.1) is a special case of PPR ( 4.5), taking the smooth functions to be logistic functions. Conversely, if we have a general PPR Yk = IXk + L gj(!Xj + L /3j;X;) j i']\n",
      "['148 5 Feed-forward Neural Networks we can approximate each smooth function gj as a sum of shifted logistic functions (a special case of the results of Section 5.7), so gj(X) ~ LYjlt(bj[ + (j[X) I Yk ~ cxk + LYjd(cx}I + L f3}uxi) j,/ i and on writing j, l as a single index the right-hand side is seen as a special case of (5.1). 5.3 Learning algorithms Knowing that an approximation exists is useless without some way to find it, and it was this step which held up research in neural networks for many years. The original idea of the Rumelhart-McClelland school was to fit the parametrized function by least squares. Suppose we have examples (xP, tP), and that the output of the network is y = f(x; w). Then the parameter vector w is chosen to minimize E(w) = L IW-f(xP;w)ll2 p (5.5) as would be done in non-linear regression (Bates & Watts, 1988; Seber & Wild, 1989). (Note that this is a sum of squares over both output units and input examples.) As this is a minimization problem, we can use', 'squares over both output units and input examples.) As this is a minimization problem, we can use general algorithms from unconstrained optimization (Section A.5), and we shall see that this seems the most fruitful approach. Note that E(w) is a differentiable function only for differentiable units, and from now on we assume differentiable units, thereby excluding threshold units. Indeed, it was the change from the threshold units of genuine multi-layer perceptrons to logistic units which enabled effective algorithms to train these networks to be found. These algorithms all require the gradient of E(w) with respect to the weights. The Rumelhart-McClelland group used a form of steepest descent to reduce ( 5.5), with update rule (5.6) and since the partial derivative can be written in the form (see the next subsection)']\n",
      "[\"5.3 Learning algorithms 149 (originally with the sign of <5 reversed) this has become known as the generalized delta rule. (Here and later the superfix P refers to calculations involving example p.) Further, as the <5 'scan be computed from output to input across the network (see the next subsection) both the process of calculating the derivatives and the descent algorithm are known as back-propagation. Alternative discrepancy functions for logistic regressions have been considered in Sections 2.3 and 3.5; most ofthese have been re-discovered within the neural networks literature. The conditional log-likelihood (2.31) for a two-class problem has been widely suggested (Solla et al., 1988; Bichsel & Seitz, 1989; Hinton, 1989a; Bridle, 1990a, b; Holt & Semnani, 1990; Spackman, 1992; van Ooyen & Nienhuis, 1992). This is often summed over multiple logistic output units to give [ tp 1 tp] E = L L tf log ~ + ( 1 -tk) log -~ p k Yk 1-Yk (5.7) the terms in t log t being chosen so that E ~ 0\", '= L L tf log ~ + ( 1 -tk) log -~ p k Yk 1-Yk (5.7) the terms in t log t being chosen so that E ~ 0 with equality only for a perfect fit. The log-linear approach to classification gives rise to what Bridle (1990a, b) termed softmax. This is no different from the multiple logistic models considered several times in earlier chapters, but for convenience we will recap the notation here. We have (5.8) and 0e classifier chooses the class max1m1zmg p(k I x) and hence gk(x; 8). To use this with a neural network, we take the Yk to the outputs of a network with linear output units, and compute probabilities by expyk Pk = · l::jexp Yj (5.9) Minus the log-likelihood for the multinomial distribution is then summed as usual over examples. The targets tk will usually be one for the correct class, and zero for the others, and the probabilities are given by (5.9) computed from (yk). From now on we will assume that exactly one of the targets is one, all the others are zero, so L:j tj = 1. The outputs', 'assume that exactly one of the targets is one, all the others are zero, so L:j tj = 1. The outputs (Yk) can all be changed by the same additive constant without']\n",
      "['150 5 Feed-forward Neural Networks changing the probabilities or the fit, so there is a degree of redundancy. It is often convenient to take gk(x; 8) = 0 for some preferred class k. The error criteria of robust statistics (Huber, 1981; Hampel et al., 1986) may be used to replace least squares, as in Chen & Jain (1994). Liu (1994) uses a t distribution for regression errors, and hence a different robust error criterion. Back-propagation Recall that each unit has input Xj = Ei ..... j WijYi and output Yj = fj(xj). Each form of the fit criterion E is a sum over examples, so we calculate the derivatives of EP, which can then be summed over examples. For the rest of this subsection we consider one example and drop the superfix P. In these calculations we will take partial derivatives of E with respect to weights Wij and with respect to inputs Xi and outputs Yi of units. We have to make clear precisely what is kept fixed. (The literature with very few exceptions does not. Werbos, 1994, has a', 'what is kept fixed. (The literature with very few exceptions does not. Werbos, 1994, has a concept of ordered derivatives for this.) When we take partial derivatives with respect to weights, we regard E as a function of all the weights, so changes in a weight Wij affect the input and output of unit j and all units connected to j, including some output unit(s). When we take partial derivatives with respect to an input or output, we allow all other signals in the network which depend on the input or output to follow their usual dependence. Thus all weights and all inputs (and hence outputs) of other units in the same and earlier layers are kept fixed. We evaluate 8Ej8xj by noting that Xj only affects the outputs through yj, and this only acts through connections to output units. For the first derivatives we have aE aE axj oE , oE -8-= -8 -8-= Yi-0 = yJ1·(Xj)-0 = y/Jj (5.10) Wij Xj Wij Xj Yj if we define [Jj = 8Ej8xj. The first equality comes from the dependence Sometimes bj is given of', 'Yj if we define [Jj = 8Ej8xj. The first equality comes from the dependence Sometimes bj is given of E on the weights only through the outputs, the second from Xj = a sign change, to set 2: Wij Yi ( 5.4 ). We note that w;j .._ wij + 17 L yf bj For output units oEjoyj can be calculated directly from the form of E. It is customary to express fj(xj) in terms of yj; for logistic units']\n",
      "[\"The results here are only needed by the writers of programs, so others may wish skip to the next subsection. Our development follows Ripley (1994b) and extends that of Bishop (1992). 5.3 Learning algorithms 151 we have f'(x) = y(1-y). For an output unit o we have the expressions (jo = 2y0(1-Yo)(Yo-t0), logistic output unit, least squares (jo = (Yo -t0), logistic output unit, entropy fit (jo = 2(yo-to), linear output unit, least squares (jo = ( E1 t1) Po-to, softmax For units in earlier layers we have (jJ = f'/xJ) ~E = fj(xj) L WJk :E = fj(xj) L :: ~x~ YJ k:j->k Xk k:j->k k YJ = fj(xJ) L WJk(jk. (5.11) k:j->k the sum being over units k fed by unit j. (The first and last equalities follow from definitions; the second traces the effect of the output of an internal unit via the units to which it is connected.) Since the formula ( 5.11) for (); only contains terms in later layers, it is clear that it can be calculated from output to input on the network. This simple idea has been\", 'is clear that it can be calculated from output to input on the network. This simple idea has been re-discovered many times, and much credit has been given for it. Werbos (1974) had the idea of organizing such computations as a recursive computation, but its modern use stems from Rumelhart et al. (1986) and Rumelhart & McClelland (1986, Chapter 8). It is often discussed as a forward pass to calculate the outputs from the inputs, followed by a backward pass to calculate ([J;) and hence oEjow;1. In control theory (Bryson & Ho, 1969, §2.2) the idea occurs in a more general form if the weights are considered as control inputs to the layers. Second derivatives We can find the Hessian of the fit criterion E with respect to the weights by extending the derivation of (5.10). We use the symmetry of the Hessian matrix to require that j is never in a later layer than l in these expressions. (This ensures that x 1 is changed by w;1 but not by Wk/, although Xt could be changed by both.) Then we', 'ensures that x 1 is changed by w;1 but not by Wk/, although Xt could be changed by both.) Then we have H(w)iJkl = 82£ = Yi _!__ oE = Yi o(yk[Jl) , OWijOWk/ OXj OWkt OXj [ oyk 8(),] oyk = Yi (jl-+ Yk-= Yi (jl-+ Yi Yk h11 (5.12) OXj OXj OXj where hjl = 8[Jif8xj = o2EjoXj0XI = (j{jjjax,. The first term is zero unless j = k or there is a path from j to k.']\n",
      "['152 5 Feed-forward Neural Networks For a general network with a single hidden layer (allowing connections from input to output) the first term must be zero unless j = k is a unit in the hidden layer. Thus we have: 1 If both j and l are output units the first term is zero, and so we have a2E a a = Yi Yk hjl· Wij Wk[ 2 If j is in the hidden layer and l in the output layer a2E -~-= Yi (b1l(j = k)fj(xj) + Yk hjl] awijaWki (5.13) = Ydj(xj) [btl(j = k) + Yk L Wjmhmt] (5.14) J-+m on differentiating (5.11) with respect to x1• 3 If j and l are both in the hidden layer a2 E abt a [ , ] a a = Yi Yk -a = Yi Yk -a fL(xt) L W[mbm Wij Wk[ Xj Xj 1-+m = Yi Yk [l(j = l)fj(xj) ~ Wjmbm J-+m + Jj(xj)f/(xt) L L WjmWtnhmn] . (5.15) j-+m 1-+n These expressions only involve hjt for units in the output layer. If we differentiate the expressions given for b0 we find hoo = 2yo(1 -Yo)(2Yo -to+ 2toYo-3y;), hoo = Yo(1 -Yo), hoo = 2, hjt = pjl(j = l)-PjPl, logistic output unit, least squares logistic output unit,', '-Yo), hoo = 2, hjt = pjl(j = l)-PjPl, logistic output unit, least squares logistic output unit, entropy fit linear output unit, least squares softmax In the first three cases the off-diagonal terms of (hjt) are zero. The double sum in the third case simplifies for softmax, for if we define Hj = L.j-.m WjmPm we have = Yi Yk [1(j = l)fj(xj) ~ Wjmbm J-+m +Jj(xj)f/(xt){~ WjmWtmPm- HjHt}]. J-+m']\n",
      "['5.3 Learning algorithms 153 Buntine & Weigend (1994) give rules for finding second derivatives in more general networks, in particular those with more than one hidden layer, but the results here suffice for the networks used in practice. Several algorithms use the Hessian H(w) only to compute H(w)v for a few directions v (for example as part of a line search along direction v, although that only needs vT H(w)v ). Pearlmutter (1994) shows how to compute Hv without computing the whole matrix H, by a technique he calls 9P-backpropagation (and which also appears in Werbos, 1988). Define the operator a 9P[f(w)] = a,f(w + rv) (5.16) as the directional derivative in direction v. We want to compute Hv = (9l[oEjowiJ]). Let ~i = oE/oYi· We compute oE I OWij by i-+} ~} = LWJkbk }-+k after computing b0 from the formulae above (5.11). Applying the 9l operator using the normal rules for a derivative operator, we find (Hv)iJ = Yi9l[bj] + 9l[yiJbJ 9P[bj] = fj(xj)9l[~j] + fj(xj)9l[xj]~J 9l[~j] = L', 'operator, we find (Hv)iJ = Yi9l[bj] + 9l[yiJbJ 9P[bj] = fj(xj)9l[~j] + fj(xj)9l[xj]~J 9l[~j] = L Vijlh + Wjk9l[bk] }---+k 9P[yi] = J;(x;)9l[xi] 9P[xj] = L ViJYi + WiJ9l[yi] i---+j The last two equations then form a forward pass, the first three a backward pass (started by 9P[b0] for the output units o ). Whether these are easier to use than (5.13-5.15) will depend on the application, and in particular on how special v is. The classic algorithm The basic back-propagation algorithm (5.6) has been modified in many ways. In the original Rumelhart & McClelland experiments (1986,']\n",
      "[\"154 5 Feed-forward Neural Networks p. 330) 'momentum' was added, that is exponential smoothing was applied to the correction term, so we have Wij +-Wij -1} [(1-a):~} + e<(dwij)previous]. (5.17) They also considered the 'on-line' version of (5.17), that is Wij +-Wij -17'yf b)+ ct'(dWij)previous (5.18) and updating the weights after every example. This only makes sense if the examples are presented in a random or unstructured order, in which case the momentum creates an approximation to (5.6). In contrast, (5.6) and ( 5.17) are sometimes known as 'batch' algorithms. This algorithm can be implemented by a form of distributed computing on a (two-way) network, with outputs being passed forward and then b 's being passed back. There seem to be three motivations for the 'on-line' algorithm. One is the biological motivation of learning from every experience. Another is that it can converge faster than the batch version. Suppose that the training set contains large numbers of exact or near\", \"than the batch version. Suppose that the training set contains large numbers of exact or near duplicate examples. Then the average over a small proportion of examples will provide a good approximation to E or its derivatives, and we would expect the on-line methods with a small momentum term to do well. However, in that circumstance the alternative is to use a small sample of examples in the batch algorithm, at least in the early stages of training. The third is a belief that by introducing 'noise' into the algorithm (the random choice of which example to present) local minima in the optimization are more likely to be avoided. (We return to this on page 156.) Iterative algorithms need both a starting point and a stopping rule. The starting point is usually taken to be a random set of weights. Some care is needed that they are not taken to be too large, for if all the combinations ,E1 wiJxf are initially large, the hidden units start in a 'saturated' state (with outputs very near zero\", \"are initially large, the hidden units start in a 'saturated' state (with outputs very near zero or one). The stopping rule does need a form of central control. The earliest idea was to stop when (if) E became small. This is often fine in logical problems, where no example is ever mis-classified, but can result in poor generalization. Very many ad hoc stopping rules have been proposed. One which seems popular is to have a validation set, and stop training when the error measure on the validation set starts to rise. This is dangerous, as we have often encountered examples in which after an initial drop the error on the validation set rises slowly for a Exponential smoothing is a method of smoothing time series, taking an exponentially decaying weighted average over past values. We can compute Yt = (1-IX}l:::~ IXiXH by Yt = (1 -1X)Xt + IXYt-1· Generalization is defined in the glossary; it refers to the test-set performance.\"]\n",
      "['5.3 Learning algorithms 155 large number of iterations, then falls dramatically to a small fraction of its previous minimum. Thus one can never know if the minimum error on the validation set has yet been attained. It is also not uncommon to use the test set rather than a validation set, as the use of a validation set is thought wasteful. (One example in a textbook is in Thornton , 1992, p. 199.) The issue of when to stop is important , and we know of no satisfactory rule for this algorithm. Folklore suggests that disasters have been saved because its convergence is so notoriously slow that users cannot afford the computer time to overfit the training set. Much has been made of the idea of stopping before convergence (for example by Finnoff et a/., 1993). Note that the fitted weights will depend on the starting point if early stopping is used. This complicates the analysis of early stopping procedures. Wang et a/. (1994) attempt to study early stopping of a linear regression problem,', 'procedures. Wang et a/. (1994) attempt to study early stopping of a linear regression problem, which is a neural network with no hidden layer and a single linear output unit. The algorithm studied is (5.6), batch learning without momentum. (They also allow fixed transformations of the inputs, which adds nothing to the analysis.) They assume that the data were generated by random samples from a linear regression. Then there is an optimal stopping point before convergence, but this has a delicate dependence on the size n of the training set and the starting point; their actual results are useful only if the starting point is taken to converge to the true value as n increases. That the starting point must be critical can be seen by considering what happens if the initial weights happen to be the true weights, when the expected test-set performance will normally steadily decline during training. The batch version of the classic algorithm can converge for fixed 1J, but the on-line version', 'The batch version of the classic algorithm can converge for fixed 1J, but the on-line version will continue to wander unless 1J is reduced to zero. During training we want 1J to be large to approach the (local) minimum rapidly, but small to avoid large excursions about the local minimum. Both Amari (1967) and Heskes & Kappen (1991) studied the effect of the choice of 1J, specifically finding the covariance matrix of w at time t to be proportional to 1J, for large t and small 1J. From this various rules have been used to adapt 1J; Amari\\'s original suggestion was to increase 1J if successive steps had an angle of less than 90°, and to decrease 1J otherwise. White (1989b) uses results from stochastic approximation to give conditions under which convergence is bound to occur to a local minimizer ( 2:: 1Jn diverges, 1/1Jn -1/1Jn-1 be bounded and 2:: 1J~ < oo for some d > 1 ) which are satisfied by 1Jn oc n-\" for O<K~l.']\n",
      "['156 5 Feed-forward Neural Networks Variants of the classic algorithm A number of ideas have been proposed to speed up the convergence of (5.6). Many are reviewed by Jacobs (1988); for example, the constants rt and IX in ( 5.17) can be chosen adaptively for each weight Wij· Some further references are Schmidhuber (1989), Silva & Almeida (1990), Tollenaere (1990), Darken & Moody (1991), Salomon (1991) and Eaton & Oliver (1992). One scheme that is popular is Quickprop (Fahlman, 1989) which uses a crude line-search over rt for each parameter. It retains the immediate past value of the weight update, and fits a quadratic using the past and current derivatives. If this has its minimum at a sensible value the latter is used as the new weight, otherwise a number of heuristics are used. The details are given at the end of this subsection. Analogies have been drawn between the on-line algorithm and stochastic approximation (for example by White, 1989a, b, 1992), which can be seen as an algorithm', \"stochastic approximation (for example by White, 1989a, b, 1992), which can be seen as an algorithm of the form n s:n Wij +-Wij-1'ln Yi uj with rtn ~ 0 for a sequence of randomly chosen examples, and which then converges to a local minimum of the least-squares criterion. Injecting further noise (Kushner, 1987; White, 1989a; Styblinski & Tang, 1990; Gelfand & Mitter, 1991), for example for independent Gaussian en and rtn oc 1/log(n + 1), can lead to a global minimum, analogously to simulated annealing. However, See the glossary for stochastic approximation is not a very effective way to solve a least- simulated annealing. squares problem, not least because the magnitude of the current f(xP; w) is ignored. One of the difficulties encountered by the classic algorithms is that logistic units may become 'stuck' at the wrong extreme, in which case it takes many steps to change them to the opposite extreme, since in (5.10) we have f'(x) = y(l-y) which is small if y is very near zero or one.\", \"extreme, since in (5.10) we have f'(x) = y(l-y) which is small if y is very near zero or one. Fahlman (1989) has suggested an offset, using say f'(x) = 0.1+y(1-y) . van Ooyen & Nienhuis (1992) argue for the entropy fit because for the output units b0 does not go to zero for Yo tending to the extremes unless the fit is correct, and demonstrate that this leads to faster convergence in their examples. (However, saturation can still occur at the internal units.) This effect has led to a number of claims of algorithms reaching local minima when they are merely in very nearly\"]\n",
      "['For a regression neural network with no hidden units this will be equivalent to ridge regression. 5.3 Learning algorithms 157 flat regions. Genuine local minima do occur (Section 5.4), and this can be checked by considering the Hessian at a supposed minimum. One way to avoid saturation is to discourage large weights and hence large inputs to units. Weight decay (Hinton, 1986) modifies the classic algorithm to W;j +--Wij -17 LYf c5} -217AWjj p (5.19) which tries to reduces the magnitude of the weights at each step. We can see that (5.19) is steepest descent applied to E +A L w~ = E + J.C ij (5.20) say, a form of regularization. This will only make sense if the inputs and outputs have been (roughly) rescaled to the range [0, 1] to be comparable to the outputs of the hidden units. In other problems it may make sense to used a weighted sum of weights, and/or to use different weight decay parameters for groups of weights. Other functions C have been used, for example (5.22) on page 170.', \"parameters for groups of weights. Other functions C have been used, for example (5.22) on page 170. With linear output units and the least-squares error criterion, the selection of the output weights is a linear least-squares problem, and as in a regression or an RBF network this can be solved without iteration; this also applies to skip-layer weights. (In the parlance of non-linear regression we have a 'partially linear' problem.) This has been incorporated into a number of variant algorithms, for example by Shepanski (1987) and Hrycej (1992, Chapter 9). Another approach has been to turn the discrete-time update into the system of continuous non-linear differential equations dw;i 8E -=-17-dt n awij (5.21) (Owens & Filkin, 1989; Weiss & Kulikowski, 1991). Then the classic algorithm is seen as a very simple fixed-step Euler integrator for this system, and much more sophisticated integration schemes can be used, especially those which are designed for stiff systems, those in which the\", 'schemes can be used, especially those which are designed for stiff systems, those in which the Hessian has eigenvalues of very different magnitudes . Effectively these schemes allow long steps for some linear combinations of the weights, and short steps for others. A similar approach for the discrete-time update has been to use versions of the Kalman filter; see for example Singhal & Wu (1989), Ruck et al. (1992) and Chandran (1994).']\n",
      "['158 5 Feed-forward Neural Networks There have been numerous published comparisons of these variants, but these must be treated with circumspection, as the effectiveness of ad hoc devices can depend on the problem to be treated and how free parameters are chosen, as well as on the starting point and the quality of the implementation. Details of Quickprop Fahlman (1989) modifies the gradient 8Ej8wij both by including his offset and a small oo-4) weight decay; let the modified gradient be denoted g(k) at the k th iteration for weight Wij. We take a quadratic approximation along the line between the gradients g(k-1) and g(k) and look for its minimum. This amount to finding the zero for a linear approximation to the gradient, which occurs at w + a(k)8Ej8w(k -1), where g(k) a(k) = g(k-1)-g(k) This is replaced by 1.75 if it exceeds 1.75 or is uphill along g(k) (when the quadratic would give a maximum). A learning rate is needed to start, to re-start for a ~ 0 and is also used if the gradient', 'A learning rate is needed to start, to re-start for a ~ 0 and is also used if the gradient and the last update have the same sign. Thus the update rule for WiJ becomes WiJ +-Wij-0.55 I [gij(k)dwij(k- 1) > O]gij(k) + aij(k)dwij(k- 1). This can be seen as a combination of a line-search strategy through the dependence on the last step, and gradient descent to re-start when the the line search is close to a minimum. Beware that both the value of 17 = 0.55 and the weight decay are not scale-free in E, and so will need to be adjusted for regression networks. Other algorithms Many other algorithms have been proposed, but by far the most effective in our experience are those which treat the minimization of the fit criterion E or E + A.C as a general optimization problem. Steepest descent is generally regarded as a poor strategy for optimization, and the most widely used methods for optimizing differentiable functions are based on approximations by quadratic functions. Introductions to these', \"differentiable functions are based on approximations by quadratic functions. Introductions to these methods are provided in Section A.5 and by Fletcher (1987), Gill et al. (1981), Nash (1990) and Press et al. (1992). For realistic numbers of weights (up to thousands) quasi-Newton methods work well. For larger problems the storage of the approximate Hessian can be too demanding, and conjugate gradient methods or the limited-memory BFGS quasi-Newton method (see Section A.5) can The values of 'I and the weight decay are taken from Fahlman's publicly distributed code.\"]\n",
      "['The field of numerical optimization is as large as that of feed-forward neural networks and the practical experience very substantial. Two important issues are when conjugate gradient algorithms are re-started, and how the line searches are done. Fefferman & Markel (1994) consider more hidden layers. 5.3 Learning algorithms 159 be used. Our experience is that these all work well; a quasi-Newton algorithm was used for all the examples in this book. There are specialized algorithms for non-linear least squares, but these are designed for exactly-fitting functions and in general use are less effective. There are at least two reasons for the superiority of these algorithms. One is that they try to solve the original minimization problem, not the system of equations setting the derivatives to zero, and so are able to use the information provided by the size of the objective which is unavailable to equation-solving algorithms. Another is that a Taylor expansion will show that locally the', 'to equation-solving algorithms. Another is that a Taylor expansion will show that locally the objective is well approximated by a quadratic function, and this enables the algorithms to have super-linear convergence (see Section A.S). In practice this means that once they get close to a local minimum, they reach it to machine accuracy in a few iterations, this being especially effective with quasi-Newton methods. There is much collective wisdom in implementing these methods well, and the reader is advised to use a well-tested implementation from an expert. They are part of almost every package of numerical analysis procedures, and both Nash (1990) and Press et al. (1992) publish code. Unfortunately many of the published comparisons in the neural networks field have used their own implementations without fully understanding the issues nor documenting the precise procedures used. With algorithms that can actually solve the optimization problem to machine accuracy in a modest time, we can', 'that can actually solve the optimization problem to machine accuracy in a modest time, we can explore whether we have found a local minimum, by looking at the eigenvalues of the Hessian at the solution (which should all be positive, at least up to computing tolerances), and also if more than one local minimum exists. We find it to be the norm that choosing enough different starting values will lead to more than one local minimum being found. Now there will always be a number of local minima of the same value, since the hidden units are not identifiable, and can be permuted without changing the functional form. Further, the signs of all input and output weights to a single hidden unit can be reversed, and with suitable changes to the biases the fitted function is unchanged. Sussmann (1992) and Albertini et al. (1993) consider precisely when different sets of weights can give the same fitted function for a single hidden layer. However, we expect to find local minima with different', 'fitted function for a single hidden layer. However, we expect to find local minima with different values of the objective function. (Some simple examples are given by Gori & Tesi, 1992, and we will see examples in the next section.) Weight decay helps the optimization in several ways. When weight decay terms are included, it is normal to find fewer local minima, and as the objective function is more nearly quadratic, the quasi-Newton']\n",
      "['160 5 Feed-forward Neural Networks and conjugate gradient methods exhibit super-linear convergence from much farther from the local minimum and so converge in many fewer iterations. There seems no reason ever to exclude a regularizer such as weight decay. If no regularizer is used, the Hessian is usually almost singular at a local maximum, and this slows the convergence of second-order optimization methods (Saarinen et al., 1993). The idea of using general-purpose optimization algorithms is a very obvious one, much re-discovered for neural network fitting. Some early references are Watrous (1987), Battiti & Massuli (1990) and Beigi & Li (1990, 1993) for quasi-Newton methods and Kramer & SangiovanniVincentelli (1989), Makram-Ebeid et al. (1989) and Johansson et al. (1991) for conjugate gradient methods; Battiti (1992) gives a review. It is very easy to give an algorithm guaranteed to reach a global minimum (Baba et al., 1994), for example by taking a random step in the weight space from', \"a global minimum (Baba et al., 1994), for example by taking a random step in the weight space from a distribution with positive density (for example, any Gaussian) and accepting the step if the new weights are better than the old. Such algorithms will not be practical ones in the weight space of a non-trivial network. (The proof that this algorithm works is simple. Fix e > 0. We assume that there is a minimizing w, say w0, and E(w) is continuous . Then there is a ball around w0 with E(w) within e of the minimum. The random step will hit that ball with positive probability, and in an infinite sequence of steps will hit with probability one. The move will be accepted unless the current solution has E{w)-E(w0) <e.) 5.4 Examples We start with the data on Cushing's syndrome. If we add just two hidden units to direct input-output connections and use a softmax output layer, we find many solutions that fit the data exactly (that is, predict There are 21 weights. probability one for the\", \"solutions that fit the data exactly (that is, predict There are 21 weights. probability one for the observed class for each example). Figure 5.2 (a) and (b) show two such solutions; the lines are rough because the posterior probabilities vary so fast with x that the contouring routine's interpolation is inadequate. Adding even a minimal amount of weight decay produces a much smoother solution (see Figure 5.2(c)). With a weight decay of A.= 0.01 the solutions are quite smooth, but seventeen local minima with distinct values of E +A.C were found. Two commonly found solutions are shown in the figure, with values of E + A.C :::::: 4.18 and 5.93. The largest local minimum found had E + A.C :::::: 7.65. The Hessian at the minimum showed that in each of the seventeen cases\"]\n",
      "['Figure 5.2: Neural network fits to the data on Cushing\\'s syndrome. The network used had two hidden units and connections from the input to output layer. Figures (a) and (b) are two solutions without weight decay which fit perfectly. Figures (c) and (d) show two local minima each, for A= 0.001 and A= 0.01 respectively . The dashed lines correspond to the local minimum that fits less well. Figure 5.3: Neural network fits to the data on Cushing\\'s syndrome with A= 0.01. Part (a) had five hidden units, part (b) twenty. 5.4 Examples 0 0 \"\\' a a ~ a ub b a u :g 0 L---~~------------~ ~ on - 0 -b on \\' 0 a b on a \" \\' 0 0 a \\' 5 \"b b 5 10 (a) 10 (c) 50 50 g \"\\' g a a d a ub b a u :g 0 L---~--------------~ 0 0 \"\\' 0 b on 0 a \\' \\' \\' b on a \" \\' q a 0 5 \\' ~ ub b \\' 5 10 (b) b 10 (d) 50 50 161 these are well-defined local minima, since it was positive-definite with eigenvalues which were well away from zero. Adding more hidden units while keeping the weight decay constant makes only a little difference to', 'Adding more hidden units while keeping the weight decay constant makes only a little difference to the solution, as Figure 5.3 shows. 8 8 cc \"\\' \"\\' ~ 0 0 on 0 a ub b on 0 a ub b a \" a \" on on 0 0 0 0 5 10 50 5 10 50 (a) (b) For the Pima Indians diabetes data we tried fitting a neural network with a single logistic output unit, using the conditional likelihood (2.31 ). Omitting the hidden layer is equivalent to fitting a logistic discriminant. Adding weight decay (up to A = 0.01) changed the performance marginally. Adding even one hidden unit increased the error rate to the mid 70s/332, and no non-linear neural network fit approached the (linear) logistic discrimination. Forensic glass We have been assessing the performance of classifiers on the forensic glass data by 10-fold cross-validation. Since we will expect multiple local minima to occur, we will have to be careful to define precisely']\n",
      "['162 5 Feed-forward Neural Networks what procedure is to be cross-validated. We could choose to start fitting at a random set of weights (the same random set for each cross-validation experiment, or a different one for each) or to start from the fitted weights to the whole dataset. The latter will bias the results slightly, but may mean that the fitting can be done slightly more quickly. We chose to use the same initial random weights for all the crossvalidation runs, in part so we could explore the effects of different starting points. We know that tableware and headlamp glass can be linearly separated from the remaining classes, and this leads to very slow convergence if weight decay is not used. To avoid this, the smallest value of weight decay used was A. = w-4. The quasi-Newton optimization procedure used took around 4 times as many iterations (BFGS updates) as the number of weights, and about 1.2 function evaluations per iteration. With no hidden layer and A. = 0.001 the', 'weights, and about 1.2 function evaluations per iteration. With no hidden layer and A. = 0.001 the cross-validated error rate was 37.8%, slightly worse than that of multiple logistic regression (the same procedure without the weight decay). Adding two hidden units reduced this to about 30.4 to 34.1 %, and four and eight hidden units gave solutions in the range 24.8 to 29.9% depending on the starting point. Clearly the different local minima have quite different performances, and in some cases the best fit to the training set corresponds to the worst cross-validated performance. The simplest way to overcome this is to average across different solutions. We saw in Chapter 2 that the best quantities to average are the posterior probabilities, so we averaged these across fits from ten separate starting points for each cross-validation run. The table shows the error rates (% ). 0.0001 0.001 0.01 # (hidden units) 2 4 8 30.8 23.8 27.1 30.4 26.2 26.2 31.8 29.9 29.9 This hows that only a small', '# (hidden units) 2 4 8 30.8 23.8 27.1 30.4 26.2 26.2 31.8 29.9 29.9 This hows that only a small amount of regularization by weight decay is needed in this dataset; this is related to the near-linear separation of some of the classes. These results are the best for fitting a flexible discriminant model (and by far the most time-consuming). The sharp variation in performance with A. and the number of hidden units in this example is unusual, and can be traced to the success in separating the rare classes. This averaging is quite time-consuming since 100 fits are required; it took about an hour per run on a Spare 20 workstation.']\n",
      "['5.5 Bayesian perspectives 163 This is the best performance found in this example for a flexible discriminant method, but it is comparable with the much simpler nearest neighbour rule of page 201, which took around a second. The cross-validated confusion matrix is quite different: WinF WinNF Veh Con Tabl Head WinF 56 11 3 0 0 0 WinNF 10 59 3 3 0 1 Veh 8 1 8 0 0 0 Con 0 4 0 8 0 1 Tabl 1 0 0 1 7 0 Head 1 1 0 1 1 25 and different decisions were made in 47 examples. This suggests that combining the two classifiers might well improve the overall performance, but it did not do so appreciably. 5.5 Bayesian perspectives The Bayesian view of decision theory can add considerable insight to the fitting of neural networks, although this insight has sometimes been clouded in the literature by the confusion of poor approximations with exact calculations. Setting the weight decay An important question when using weight decay is how to set the parameter(s) A. A Bayesian perspective (Buntine & Weigend,', 'using weight decay is how to set the parameter(s) A. A Bayesian perspective (Buntine & Weigend, 1991; Ripley, 1994b) helps. Suppose E is the negative log-likelihood, up to a constant, or half the deviance. Then if we take a prior distribution over weights with density p(w) oc exp -AC(w), the minimizer of (5.20) will maximize the posterior density for the weights. For the weight decay of (5.20) this prior corresponds to independent Gaussian weights with mean zero and variance 1/2A. As the logistic function saturates for inputs beyond around ±3, the standard deviation of the total input might be expected to be around 2. (Remember that one motivation for weight decay is to avoid unnecessary saturation of logistic units.) If there is a small number of inputs scaled to the range [0, 1], this suggests that the standard deviations of the weights should be around 5, which corresponds to A = 1/50. This argument is rather conservative , as we do want some weights to saturate, but suggests the', 'This argument is rather conservative , as we do want some weights to saturate, but suggests the range A~ 0.001-0.1 as a basis for exploration. Experience shows that A is not critical to within a factor of 5.']\n",
      "['164 5 Feed-forward Neural Networks For the sum-of-squares error criterion, E is not half the deviance, and must be rescaled. In that case the deviance is of the form P log(E/P) for P examples. Suppose u; expresses the value of E/P that we expect to achieve. Then P log(E/P) = P log(u;) + P log(E/Pu;) ~ P log(u;) + P[E/Pu; -1] so an appropriate scaling is E /2u;. This suggests choosing A. in the range (0.002-0.2)u;. When least-squares fitting is used with outputs in the range [0, 1], this suggests values of u; ~ w-4-10-2 might be appropriate. With the softmax criterion (5.9) we will lose the symmetry of the classes if we set the output for one class to zero, so it usual to include all classes in the network. The weight decay resolves the redundancy over shifting all outputs, and gives a local minimum of E + A.C (rather than a saddle point). One criticism of weight decay emerges from this interpretation. Because it implies independence and the number of weights is often large, the opinion', 'Because it implies independence and the number of weights is often large, the opinion expressed about S(w) = 2: w~ is strong, being a rescaled chi-squared distribution with degrees of freedom the number of weights. It is therefore potentially dangerous if A. is set incorrectly. The predictive approach The Bayesian perspective goes much deeper, and has been the subject of partial implementations and considerable controversy (Buntine & Weigend, 1991; MacKay, 1992a-e; Wolpert, 1993). We will confine attention here to classification problems. Our aim is to model p(k I x) by a K-output neural network. For K = 2 we will use one logistic output unit to model p(21 x), and for K > 2 unordered classes we will use a multiple logistic model, also known as softmax. The weights are parameters, and are given a prior. It is usual to use the weightdecay prior. This itself has parameters, one or more A., often called hyperparameters. The predictive Bayes approach approximates p(k I x) by averaging p(k', 'often called hyperparameters. The predictive Bayes approach approximates p(k I x) by averaging p(k 1 x; w) over the posterior distribution for the weights w. This is of the general form (2.34 ), that is n p(w Iff) oc IT p(yP I xP; w)p(xP; w) p(w), p(w) = j p(w; A.) p(A.) dA.. p=l Note that we are assuming no model for the marginal distribution p(xP; w), and so assume that x carries no information about w. (If this is false or unreasonable it can lead to fallacies.)']\n",
      "[\"5.5 Bayesian perspectives 165 Such formulae can be misleadingly simple. For the moment consider fixed A. Fitting a neural network by minimizing E + AC is equivalent to maximizing the posterior density over w. Since we normally find several quite sharp local minima for E + AC, the posterior density will normally be sharply peaked at more than one point. Averaging the non-linear function p(k I x; w) over such a density is computationally difficult. As we saw in Chapter 2, the 'plug-in' approach ignores this difficulty, and uses p(k I x; w) for one fitted set of weights. A more general way to approximate the posterior is to take a multivariate Gaussian distribution about each local maximum of p(w Iff) (Buntine & Weigend, 1991; Ripley, 1994c). At each local minimum of E +AC we can find the Hessian H(w), and take a local Gaussian approximation to the likelihood surface, of the form N(w, H(w)-1 ). The total mass of that Gaussian is then proportional to IH(w)l-1/2 exp -E(w) (2ntw12 where nw\", \"). The total mass of that Gaussian is then proportional to IH(w)l-1/2 exp -E(w) (2ntw12 where nw is the number of freely-varying weights. Normalizing over all the local minima found gives a mixture of Gaussian distributions as an approximation to the posterior distribution of w. (This is closely related to the single Gaussian approximations discussed in Section 2.6.) We can average p(j I x; w) for future x by simulating w from this posterior density. (Often the effect of the spread about the peaks is so small that it is sufficient to average over the peaks. The weights given to the peaks can be radically different from their relative heights.) More exactly our mixture of Gaussians can be used as a density for importance sampling (Ripley, 1987, §5.2) in integrating p(j I x; w), since p(w Iff) can be calculated (up to a constant) from the fit of the network at w. Finally, a general approach is Monte Carlo integration which can be very inefficient unless the sampling is chosen to 'fill'\", \"is Monte Carlo integration which can be very inefficient unless the sampling is chosen to 'fill' the space of w effectively (as in Neal, 1993, 1996), and only very small examples have been demonstrated at a very large computational cost. Example We can compare the seventeen local minima found for the Cushing's syndrome data with two hidden units and A= 0.01. Six of them carry 5% or more of the total mass, with fitted values and weights E+AC % 4.544 24.4 5.928 21.1 6.198 17.5 4.502 6.9 4.269 6.1 4.180 5.7 The second and sixth are shown on Figure 5.2, and the (approximate) predictive classifier is shown in Figure 5.4.\"]\n",
      "['166 5 Feed-forward Neural Networks The choice of priors In the last subsection a weight decay prior was used with a A. which was assumed known. If we have a hyperprior for A., the extension is quite easy: we can sample from this hyperprior, apply the procedure for each sample based on p(w; A.), and average over samples. (It may be better computationally to use a weighted average over a very coarse grid of values of A..) Two other approaches are to use a vague prior for A. (Buntine & Weigend, 1991) and to estimate A. by ML-II empirical Bayes (Berger, 1985, p. 99) as advocated by MacKay (1992a-e). The usual vague prior for a (squared) scale parameter such as A. has a uniform density on log scale and so has improper (un-normalizable) density 1/ A on (0, oo). This can be integrated out, to give the prior density [\"\"\"\\' ] -nw /2 p(w) oc L...t wl which is the vague prior on S(w) = I:: wl and is again improper. Effectively there is no regularization assumed, since this density for S(w) has all', 'again improper. Effectively there is no regularization assumed, since this density for S(w) has all its mass at infinity. This makes intuitive sense, since 1/2..1. is the prior variance of the parameters, and if we express no opinion about it, it will be allowed to be as large as needed. This does not accord with our prior beliefs. Whereas fixed A. is a strong opinion, perhaps too strong, a vague hyperprior is too weak an opinion. A more sensible choice of hyperprior would be a gamma distribution for A. Note that a gamma hyperprior for 1/ A. would be a scaled chi-squared distribution for the variance of the weights, and so the prior distribution of the weights would be a multivariate t distribution centred at zero. Although MacKay sometimes claims to use a uniform prior for log A., he uses ML-II empirical Bayes (which omits p(A.)) to choose 1 and so is effectively using a uniform hyperprior on (0, oo) for A. The usual asymptotic justification for ML-II empirical Bayes is missing here,', \"on (0, oo) for A. The usual asymptotic justification for ML-II empirical Bayes is missing here, but the assumed prior independence of Wij may allow the posterior for A. to be quite concentrated, since we have nw pieces of information Figure 5.4: Predictive neural network fit to the data on Cushing's syndrome with two hidden units and A.= O.ol. This hyperprior was also proposed by Neal ( 1996). nw is the number of weights.\"]\n",
      "['5.5 Bayesian perspectives 167 about A.. However, prior independence is a dubious assumption , and if nw is large and comparable with n (which it often is) there is no reason to suppose that p(A. I§\"\") will be highly concentrated about one point. Our experiments showed this often not to be so. The use of the weight decay prior is convenient, but our prior beliefs are really on the functions represented by the network and not on the parameters (weights) per se. This is the approach of the regularization penalties, which can be alternatively expressed as priors over the family of functions realized by the network. These have been most explored for regression networks, but Buntine & Weigend (1991) do give a small example for a classification network. Bishop (1993) uses the regularizer discussed in Section 4.3 for RBFs, This is intended for regression networks , with linear output units. (The implied prior is exp -A.C(f).) For this choice of regularizer Bishop shows that ac 1 awij can be', 'implied prior is exp -A.C(f).) For this choice of regularizer Bishop shows that ac 1 awij can be calculated quite easily via the chain rule. For classification problems such a regularizer might be appropriate when applied to the total inputs to the logistic or softmax output stage. Nowlan & Hinton (1992a, b) motivate a prior for the weights which is a mixture of Gaussians by its encouragement for similar values of the weights. This had proved useful in networks with a hand-tuned architecture (but not for arbitrary groupings of weights). They choose a fixed number of mixture components (not necessarily centred at zero) and optimize the penalized log likelihood over the parameters in the prior as well as the weights. MAP estimation MAP is a common abbreviation for maximum a posteriori, summarizing a predictive distribution by its mode. Most of the controversy in this area comes from inappropriate use of MAP estimates. The predictive approach is quite clear; we should average over the', 'use of MAP estimates. The predictive approach is quite clear; we should average over the hyperprior (if any) and prior on the weights. This is done by mapping the posterior distribution over weights to one over fk(x) = p(k I x; w), and averaging over the weights w and hyperparameters . Finally we maximize the expected cost over decisions. As we have seen, this programme is computationally daunting, and early work (such as Buntine & Weigend, 1991) implicitly or explicitly approximated the posterior density p(w I §\"\") by a mode.']\n",
      "['168 5 Feed-forward Neural Networks This approximation is often not appropriate and can be misleading. It needs to be stressed that plugging in the MAP estimator of the weights does not give the MAP estimator of the function f from inputs to outputs. If it is really necessary to use MAP as an approximation, it would be better to take the MAP estimate off= (fk) rather than that of w. Wolpert (1994a) gives a computational programme for finding a correction term to the posterior density of w to skew the mode towards the MAP estimate off, but the problem of multiple maxima remains, and in practice it appears to be easier to average, as well as being theoretically correct. One difficulty with approximating distributions by their modes is that the mode depends on both the underlying measure for the density and the parametrization. For example, in the weight-decay regularization log A and 1 /2A. (the implied variance) are equally plausible parameters, yet their modes will not map to the mode', \"(the implied variance) are equally plausible parameters, yet their modes will not map to the mode of A. Indeed, approximating by the mode seems safe only when the parameter itself has a physical meaning and so a natural scale, or when the posterior distribution is so concentrated that the parameter is effectively constant. Optimization over both the weights and hyperparameters has been advocated by MacKay (1992b) and Nowlan & Hinton (1992b). This is further from the full Bayesian procedure, and so seems even more likely to mislead. 5.6 Network complexity One of the simplest and most commonly asked questions is 'How many hidden units should I use? Are there any rules of thumb?' There are rules of thumb, but these are as unreliable as those for the complexity of a multiple or polynomial regression. The answer depends on the unknown underlying function f which the neural network f(x; w) is approximating. There are three ways to control the complexity of the functions represented by a\", 'is approximating. There are three ways to control the complexity of the functions represented by a network: to cut out links, to change the number of hidden units, and to change the regularization (such as weight decay) parameter. We consider each in turn. The statistical background in Sections 2.2 and 2.6 is needed to appreciate these methods. Many of those results can be extended to methods of parameter estimation which minimize some criterion E (including any penalty A.C) and so in particular to penalized likelihood methods and the Bayes MAP estimate. However, the extensions are not']\n",
      "[\"Wald tests are large-sample equivalents of likelihood ratio tests: see Cox & Hinkley (1974) or Lehmann (1986). See the glossary. 5.6 Network complexity 169 very useful, as asymptotically the criterion E which grows proportionally to n (the size of the training set f7) will swamp any penalty and so not alter the results. Since the point of a penalty is to produce better behaviour for moderate n, a different asymptotic regime is needed. It follows from what we have said about the prevalence of local minima that the asymptotic theory is not normally relevant when fitting neural networks; n is often not much greater than the number of parameters. Neural networks are 'black box' models used for prediction. For prediction performance, it is almost always better to make smooth changes (shrinkage or regularization) than to select parts of the model: it is often computationally preferable too. It is important to remember the merits of combining models discussed in Section 2.6. Pruning networks\", \"is important to remember the merits of combining models discussed in Section 2.6. Pruning networks The usual selection procedures can be used, including stepwise selection and AIC. However, it makes little sense to set individual weights to zero, and whole internal units and the weights on their connections are added or deleted. Often some form of cross-validation (Section 2.6) is used to decide how many hidden units to use. The neural network community has developed some fanciful names for these ideas. Optimal Brain Damage (Le Cun et al., 1990b) and Optimal Brain Surgeon (Hassibi & Stork, 1993; Hassibi et al., 1994; Buntine & Weigend, 1994) are both methods to 'prune' weights, that is to choose to set some of the weights to zero. Both are approximate versions of the Wald test, which considers the ratio of a parameter to its standard error. (The large-sample theory computes the standard error as the appropriate diagonal element of the Fisher information matrix K.) The methods differ\", 'error as the appropriate diagonal element of the Fisher information matrix K.) The methods differ in how crudely the standard errors are estimated. OBD uses just the inverse of the diagonal of the observed information matrix. OBS replaces the Fisher information matrix by the covariance matrix of the scores oL(fi;Xi)/oe. The empirical covariance matrix can then be inverted incrementally by the Sherman-MorrisonWoodbury formula. It seems simpler to compute and invert the Hessian matrix, and Proposition 2.2 gives us more accurate asymptotic formulae which do not assume the model to be true. Reed (1993) gives a partial survey of pruning algorithms in the neural network literature. There are well-known difficulties in setting parameters to zero. Nearcollinearities can mean that if one weight is set to zero, the standard errors for others are drastically reduced, so it can be unsafe to set more than one weight to zero at a time. For classification problems the Wald']\n",
      "['170 5 Feed-forward Neural Networks test is known to have little power when the true weight is large (Hauck & Donner, 1977), and so can be most misleading. Another approach is to encourage small weights during training, so these can subsequently be set to zero, and perhaps hidden units and all their connections removed thereby. This is done by an extension of weight decay, for example with penalty w?.jW2 c = L 1 +1~2.jW2 (5.22) lj (Weigend et al., 1990, 1991, 1992) (which corresponds to a very improper prior with free parameter W ) or to base the penalty on the total squared weights reaching a hidden unit (Chauvin, 1989; Hanson & Pratt, 1989). Levin et al. (1994) extend the idea of principal components regression, in which the inputs are first linearly transformed to their principal components and then only some of the principal components are used in the final regression. Like all pruning methods, this reduces the variance of predictions at the expense of bias. This idea is applied to', 'methods, this reduces the variance of predictions at the expense of bias. This idea is applied to the input layer, reducing the inputs to the principal components, and then to subsequent layers in the neural network. Thus no weights nor units are actually removed, but the weights are restricted to lie in a lower-dimensional space. Selecting the number of hidden units Selecting the number of hidden units in a neural network is in principle no different from selecting regressors in a linear regression or the order of a polynomial regression. The main ideas that have been developed in that field are pruning by small steps (backward selection) discussed in the last subsection, incremental construction (forward selection; the next subsection), and minimizing some measure of performance over the class of possible models. All the candidate measures aim to predict the performance on a test set, and so to select the model with the best performance on the test set. The most general idea is to', 'and so to select the model with the best performance on the test set. The most general idea is to use cross-validation (Section 2.7). There is a particular difficulty in cross-validation for neural nets. How do we move from the whole set to a subset for training? Do we start at the fitted weights for the whole data? This could bias the procedure . If we start at another random starting point, we could end up at a very different solution even with the whole data, let alone with a subset. This shows that the learning procedure for a neural network is not well defined, as there are often multiple local minima of rather different performance.']\n",
      "['The motivation of Moody & Utans (1995) seems to have been to save time by re-training from a local minimum on all the data. For the forensic glass example with 10-fold cross-validation we found re-training took at least half as long as training from scratch. 5.6 Network complexity 171 Moody & Utans (1992, 1995) suggest viewing each local minimum as a separate model. This is appealing, but flawed. Consider a real example we encountered of around 100 examples and a binary classification. All but two of the examples could be fitted very well by a simple logistic regression model with no hidden units, but the other two examples had apparently been attributed to the wrong class. Fitting a net with two hidden units did much better overall, but had at least ten non-equivalent local minima. Now dropping either of the badly-fitted examples changed the nature of the fitted function completely. If we cannot track the local minimum over dropping just one example, a local minimum cannot be', 'If we cannot track the local minimum over dropping just one example, a local minimum cannot be sufficiently well defined to be used within a cross-validation procedure. The approach taken in Section 5.4 of averaging across the local minima is much stabler, and only a little more time-consuming . It is important to realize that penalty terms such as weight decay and regularization change the problem completely , as they often impose a limit on the complexity of the fitted functions irrespective of the number of hidden units. The use of splines for smoothing in one dimension (Section 4.1) provides graphic evidence of this. The smoothness of the fit can be controlled either by restricting the number of knots or the degree of regularization; when regularization is •1sed (that is, smoothing splines) the fitted function will remain smooth however many knots and data points are taken. Figure 5.3 shows a similar phenomenon for fitting a neural network with weight decay. The NIC penalty of', \"5.3 shows a similar phenomenon for fitting a neural network with weight decay. The NIC penalty of Murata et a/. (1991, 1993, 1994) and Moody's (1991, 1992) Pelf discussed in Section 4.3 were developed for this application, but do assume a strong single local minimum. For the fits of Section 5.4 for the data on Cushing's syndrome this condition is not met. The fits shown with weight-decay constant A, = 0.1 have Pelf in the range 2-5 depending more on the local minimum than on the number of hidden units. There is too much uncertainty in Pelf for it to be useful in such a small example. Ripley (1995) found it quite effective in a regression problem with one input and a training set of 100 examples. Incremental network construction There have been a number of ideas to grow networks incrementally, by adding hidden units one at a time in the same or extra layers. Many of the ideas were first developed for perceptron (threshold) units and are surveyed in Gallant (1993, Chapter 10). The\", \"developed for perceptron (threshold) units and are surveyed in Gallant (1993, Chapter 10). The 'pyramid' algorithm of Gallant (1990) adds units one at a time in a new layer, each unit being connected to all previous units. Frean's (1990) 'upstart' algorithm is l I\"]\n",
      "[\"172 5 Feed-forward Neural Networks for binary outputs. It starts with a perceptron, then adds two hidden layer units to attempt to correct (separately) the positive and negative mistakes. As these algorithms are of very limited scope (binary outputs, threshold units), we refer the reader to the references for detailed descriptions. Other early construction algorithms are described by Ash (1989) and Moody (1989). Moody & Utans (1995) call 'SNC' the heuristic construction algorithm which adds units to the hidden layer in groups of C units, and first trains the new units before re-training all the units. They implemented this for C = 1, when it is an example of back-fitting (described in Section 4.1). Cascade correlation (Fahlman & Lebiere, 1990) is a particular way to grow a 'pyramid' network, and seems the only iterative construction algorithm that is at all widely used. Initially just input-output connections are used. At each subsequent stage a new unit is added which has as inputs\", \"input-output connections are used. At each subsequent stage a new unit is added which has as inputs the original inputs and the outputs of all the previous units, so effectively a further hidden layer containing just one unit is added, with a prescribed set of skip-layer connections. However, only the additional weights (the input and output weights of that unit) are fitted at that step; weights to hidden units once fitted are never altered. Further units are added until some pre-specified measure of fit is achieved. (Their examples are of noiseless classification, and units are added until the whole training set is correctly classified.) This is called by its authors a cascade architecture. The 'correlation' in the name of the algorithm comes from the way that the new unit's weights are selected, although this involves a covariance not a correlation. If there is just one output unit, the weights are selected to maximize the absolute value of the covariance (over training-set\", \"unit, the weights are selected to maximize the absolute value of the covariance (over training-set examples) between the output value of the unit and the prediction error before that unit is added. If there are multiple outputs, the sum of this measure over outputs is used. In what can be seen as a means to avoid bad local minima and 'flat spots', several attempts are made to maximize this objective from random starting points, and the best taken. Then all the weights to the output units (including that from the new unit) are re-trained. It is illuminating to contrast this algorithm with the SMART algorithm for projection pursuit regression (page 126). Cascade correlation uses a layered architecture; SMART uses only the original inputs but also fits to the residuals. In cascade correlation no global optimization is done, whereas in SMART all the weights are periodically re-fitted, and a pruning stage removes units one at a time. This re-fitting of\"]\n",
      "['See Section 2.6. 5. 7 Approximation results 173 weights will take more CPU time, but is likely to produce a smaller network with better generalization properties. This is borne out by the empirical comparisons of Hwang et al. (1994b) on the example used by Fahlman & Lebiere, which shows that projection pursuit regression produces a smoother fit and hence a better generalization. Lee et al. (1990) take an approach they term structure-level adaptation. This adds units to the network during training where they appear to be most useful, and also removes units which appear not to be effective. The rules to do so are ad hoc; for example a new unit is added in parallel to one whose input weights appear to fluctuate continually during on-line training based on the heuristic idea that that unit is trying to follow two (or more) different features. Bayesian model choice Ripley (1995) found approximate Bayes factors by importance sampling using the multiple Gaussian approximation to the posterior', \"Bayes factors by importance sampling using the multiple Gaussian approximation to the posterior density p(w Iff) discussed on page 165. This was for a curve-fitting problem (one input, one output) selecting both the number of hidden units and a single weight decay parameter A.. The results are similar to those using NIC. 5.7 Approximation results We first prove the 'universal approximation ' result. We are concerned with functions f:lR.n ~ JRP for n inputs and p outputs. We wish to approximate a given function f by g from some specified class of functions , such as (5.1). Uniform approximation on compacta means that given a compact set K c 1R n and e > 0 we can find a function g within our class such that llf(x)-g(x)ll < e for all x E K. We do not write out all the fine details, on the understanding that readers who are interested will be familiar with simple arguments in mathematical analysis. By 'ramp units' we mean g(x) = max(O, min(x, 1)) which can be made up of two of Breiman's\", \"analysis. By 'ramp units' we mean g(x) = max(O, min(x, 1)) which can be made up of two of Breiman's (1993) hinge functions. If we can approximate the posterior probability function fk(x) = p(k I x) uniformly on compacta, we certainly will have enough to solve any practical classification problem. The restriction to compacta obviates the need to extrapolate correctly. It is reasonable to assume that this f is continuous , but it need not be (consider the task of classifying real numbers as rational or irrational).\"]\n",
      "[\"174 5 Feed-forward Neural Networks Proposition 5.1 Any continuous function f: 1R n -+ .IRP can approximated uniformly on compacta by functions of the form (5.1) with linear output units and logistic units in the hidden layer, and also by networks with threshold units or ramp units in the hidden layer. Proof: Our proof proceeds by building up the class of functions we can approximate (uniformly on compacta). (a) Our first step is to take n = p = 1. A compact set is contained in a bounded interval, say [a, b], and any continuous function f can be uniformly approximated on [a, b] by a step function with steps of size less than c /2. (For each x E [a, b] define the interval I(x) = (t(x), u(x)) where t(x) = max{y < x : lf(y)-f(x)l ~ c/4} and u(x) = min{y > x : lf(y)-f(x)l ~ c/4}. The open sets I(x) cover [a,b], hence so does a finite collection J(x;). Sort the x; into increasing order, and let g take the value f(x;) on [t;, u;) = [u(x;_l), u(x;)). Then ldg(t';)l = lf(x;-1-f(x;)l ~\", 'and let g take the value f(x;) on [t;, u;) = [u(x;_l), u(x;)). Then ldg(t\\';)l = lf(x;-1-f(x;)l ~ lf(x;-1-f(t\\';)l + lf(x;)-f(t;)l ~ 2e/4.) A step function is in class (5.1) for threshold units, and sums of logistic or ramp functions can approximate a step function arbitrarily closely except at the steps, and certainly to within one half of the largest step size. (b) We then extend the result to trigonometric functions of the form ll?=1 cos(w;x+tp;) for any n. By repeated use of the cos(A+B) formula, this can be written as a sum of the form I:j aj cos(wjx + tpj). Each term in this sum is continuous, and so can be approximated by step (a); hence the whole sum can, as well as linear combinations of these functions, including arbitrary trigonometric polynomials on .IR\". (c) Any continuous function f:.IR\" -+ 1R can be approximated by a trigonometric polynomial. This is a well-known result in Fourier theory, but we give an elementary proof as Proposition 5.2. (d) Fix the compact set K and c', 'Fourier theory, but we give an elementary proof as Proposition 5.2. (d) Fix the compact set K and c > 0. Each component function fj of f is continuous, so we can find functions gj within our class such that sup 1/j(x)-gj(x)l < c/ JP xEK and so sup llfj(x)-gj(x)ll <c. xEK The function (g1(x), ... ,gp(x)) is within the class (5.1), since we can take separate groups of hidden units for each coordinate function. D The idea of this proof is based on that of Diaconis & Shahshahani (1984), who used it to show the universal approximation property for projection pursuit regression.']\n",
      "['5.7 Approximation results 175 We can easily extend the proof to other types of unit in the hidden layer; all we need is the ability to approximate a step function in the sense used in the proof. It is clear that, for example, any cumulative distribution function could be used. The literature sometimes refers to such functions as sigmoidal, particularly if they are continuous, although that term is also used for just the logistic and hyperbolic tangent functions. We now give a probabilistic proof of the only slightly tricky step. This is an easy consequence of the (advanced) Stone-Weierstrass theorem (Simmons, 1963, p. 160). Proposition 5.2 Any continuous function f: 1R n ~ 1R can be uniformly approximated on compacta by trigonometric polynomials. Proof: Fix the compact set K. By an affine transformation of the coordinate system we may ensure that K c [0, 1]n. Now re-parametrize each coordinate invertibly by Pj = sin(xj/max{2,n}), and use the result of the second half of the proof to', \"invertibly by Pj = sin(xj/max{2,n}), and use the result of the second half of the proof to approximate h(p) = f(x) by a polynomial hN(P) on the simplex 9 = {p I Pi ~ 0, PI + · · · + Pn ~ 1} · Then f N(x) = hN(P) is a trigonometric polynomial which approximates f to the required accuracy. Let Y = (Yt, ... , Yn) be a sample of size N from a multinomial distribution with parameters p = (Pt •... , Pn) E 9. (As the probabilities need not sum to one, we complete the definition by taking a category n + 1 which can occur with probability 1-EPi·) Define We will show that hN converges uniformly to h on 9. Let M = maxpE.9' lh(p)l < oo and choose b > 0 so that which we can by continuity and compactness. Then hN(P)-h(p) = Ep[{h(Y/N)-h(p)}I(IIY/N-pll <b)] + Ep[{h(Y/N)- h(p)}I(IIY/N- Pll ~b)]\"]\n",
      "['176 5 Feed-forward Neural Networks gives n E \"\"\\' n ~ 2 +2M~ (j2NPi(1- Pi) i=l E n ~ 2 +2M b2N < E for large enough N. (The fourth step uses the Bienayme-Chebychev inequality.) 0 Uniform convergence on compacta is a strong form of convergence, and implies many others. In particular, it implies L2(f.l) convergence for any probability measure f.l on IR\". For the purposes of classification this is also sufficient, since it implies that decisions made by the classifiers derived from f and from g agree with high probability (except in artificial cases where several decisions are equally good). Kurkova (1991, 1992) gives uniform approximation results for networks with two hidden layers for which only the output weights are varied, including bounds on the numbers of units needed. Hornik et al. (1990) give approximation results for continuous functions and their derivatives; these would take us too far into approximation theory even to state. Stinchcombe & White (1990) give approximation', 'us too far into approximation theory even to state. Stinchcombe & White (1990) give approximation results for networks with bounded weights (for a fixed bound but using very many hidden units). Barron (1993) gives results on the rate of convergence in Lz(f.l) for some (but not all) functions f: IR\" ---+ IR; this overlaps work by L. K. Jones (1992). A typical result is Proposition 5.3 (Barron, 1993, Proposition 1) Suppose f:IR\"---+ IR has a Fourier representation of the form with Ct = { llwllf(w)dw < oo. }F_n This says that the variance of the difference (f-g)(X) goes to zero as g approximates f.']\n",
      "[\"5. 7 Approximation results 177 Thenfor each N, f can be approximated by afunction oftheform (5.1) with N hidden units (of logistic, threshold or ramp form), in L2(J.l) on Br = {II x II < r} with error at most (2rC f)/ JN. Further, we can take a0 = f(O) and l:j lwjol ~ 2rCJ for the hidden-to-output weights. Proof: For rigour, a lot of details are needed in the calculation, for which we refer the reader to Barron's paper. However, the main ideas are quite simple. By the choice of a0 we can assume f(O) = a0 = 0. Since f is real, it can be represented in the form f(x) = r [cos(wT X+ b(w))-cos(b(w))] if(w)i dw J{wfO} and so is a probability integral of functions of the class Gcos = {II: II [cos(wT x +b)-cos(b)] j IYI < rCJ} (with pdf if(w)illwll/rCJ ). Thus f is in the closure of the convex hull of such functions. Each of these is approximated by convex combination of single-step functions of height less than 2rC1, and then the step may be approximated by a logistic or ramp unit. This shows\", 'height less than 2rC1, and then the step may be approximated by a logistic or ramp unit. This shows that the approximation is possible. The rate of convergence then comes for free, by a lemma attributed to Maurey. Our class of approximating functions is the convex hull of Gt = {/U(a + bT x) I IPI < 2rCJ} and each member has norm at most (2rC1 f. Fix N, and suppose 7 = 2:;:1 p;g; is a function within the convex hull of Gt within distance ~ of f. We consider randomly selecting (with replacement) N members (say g*) from {g;} with probabilities {p;}, and take their (unweighted) average f*. Then Ef* = 7 and This shows that there must be a realization f* with and b was arbitrary. L. K. Jones (1992) gives a constructive version of this lemma. []']\n",
      "[\"178 5 Feed-forward Neural Networks This result has often been regarded as surprising in that the rate of convergence does not vary with n; typically the rate of convergence in approximation theorems is O(N-cfn), with c depending on how smooth f is assumed to be. It has been said to break the 'curse of dimensionality'. This is not correct; the conditions do depend on n and impose increasingly strict smoothness on the class of functions as n increases. First, Br becomes much smaller as n increases; the radius needed to include the unit hypercube is Jii. (Other forms of the result involve L1 norms, but with the same rate of increase.) Second, the integral for Ct is dimension-dependent; indeed considering radially symmetric functions f(x) = f(llx!l) suggests that normally Ct grows exponentially fast in n. Results of DeVore et al. (1989) show that to approximate all functions with r bounded derivatives to accuracy 1/ JN in L2(J.t), at least Q(Nn/2r) units are needed, so the 'curse of\", \"derivatives to accuracy 1/ JN in L2(J.t), at least Q(Nn/2r) units are needed, so the 'curse of dimensionality' cannot be broken by neural networks (nor by any similar non-linear method). The condition Ct < oo is not immediately interpretable. It does imply that f is continuously differentiable, with a gradient whose Fourier transform is integrable. Inspection of Gcos shows how the functions are actually restricted; Girosi & Anzellotti (1993) point out that functions with Ct < oo are precisely those which can be expressed as a convolution with II x 111-n, an increasingly restrictive constraint as n increases, and one which Barron (1993, p. 941) shows is satisfied iff has ln/2J + 2 continuous derivatives. Mhaskar & Micchelli (1992) showed that n(mn/r+(n+2r)/r2) units are needed for n ~ 2 to approximate all continuous functions with r bounded derivatives on the unit cube in 1R n uniformly to within distance 1/m. It is easy to extend the approximation results to functions with a bound on\", 'to within distance 1/m. It is easy to extend the approximation results to functions with a bound on b = (wij) for each hidden unit j, just by checking how well such functions can approximate step functions. If we impose the limit llbll ~ B, we increase the approximation error by less than 1 + 2log(rB) rB (Barron, 1993, pp. 936-937). This is an increasingly stringent restriction on b as n increases. Inverse functions The universal approximation results for single-hidden-layer networks apply to continuous functions f. In some applications we need to Often c is the number of continuous derivatives of the function. Here y = Q(x) means y / x is bounded below for all x > 0.']\n",
      "[\"5. 7 Approximation results 179 approximate the one-sided inverse of such a function, that is given e > 0 and an compact set K within the range of f, we need to find a function <J>:JRP -:IR.n such that 11/(<J>(x))- xll < e for all x E K. This arises in control theory, where f describes how the plant responds to control inputs, and 4> is the control mapping needed to achieve (approximately) a feasible output of the plant. Sontag (1992) points out that not only may 4> need to be discontinuous, but it can be outside the class of functions which can be approximated by single-hidden-layer networks, even those with threshold units. However, networks with two hidden layers and threshold units do suffice, as they can approximate the indicator function of any polyhedral region, which cannot be achieved with only one hidden layer. Dimension bounds To use the results of Section 2.8 we need to know (or bound) the 'dimension' of families of neural networks. A few results are known. First consider\", \"(or bound) the 'dimension' of families of neural networks. A few results are known. First consider threshold units and one output (so we can consider the VC dimension). Suppose there are M computational units and W weights in toto. Baum & Haussler (1989) showed that for any number of hidden layers the VC dimension is bounded by d :=:;; 2Wlog2eM and for a single hidden layer of H units we have d ~ 2plH /2J ::::::pH= W ___E_2 p+ where p remains the number of inputs. The upper bound follows from M .1(m) :=:;; IJ .1i(m), i=l where .1i(m) :=:;; (emjdi)d; refers to unit i with ki inputs and VC dimension di = ki + 1. Since W = 2:: di, if m = 2 W log2 eM M .1(m) :=:;; IJ(em/di; :=:;; (MemjW)w <2m. i=l The lower bound comes from a construction of Baum (1988) which shows that a single-hidden-layer net with 2j hidden units can separate 2jp vectors in JRP.\"]\n",
      "['180 5 Feed-forward Neural Networks Shawe-Taylor & Anthony (1991) extend the bound d ~ 2W log2 eM to multiple (threshold) output units, where d is now the VC dimension defined via graphs of functions. Other bounds are given by Bartlett (1993). Maass (1994a, b) shows that VC dimensions of order Q( W log W) can be achieved with two hidden layers, but it is not known if the true order is O(W) or O(W log W) for a single hidden layer. (These results apply to networks where both the number of input units and the number of hidden units are allowed to increase.) Sakurai (1993) has Q(W log W) results for networks with one hidden layer and real inputs (whereas Maass considered binary inputs). If we allow the units to be logistic, the VC dimension increases, but its value is not known; indeed only recently has it been shown to be finite (Macintyre & Sontag, 1993), by decidedly advanced methods from mathematical logic. Indeed, this result is subtle, since with cos units (and hence projection', 'from mathematical logic. Indeed, this result is subtle, since with cos units (and hence projection pursuit regression) the VC dimension is infinite. For sigmoidal neural networks, Karpinski & Macintyre (1995a, b) showed that the VC-dimension is 0( W4), and Koiran & Sontag (1996) showed Q(W2). For more than one output and for non-threshold units, the pseudodimension is more appropriate. Haussler (1992) gives bounds for networks with bounded weights. A specialization of his Theorem 11 to logistic units (including output units) is: Proposition 5.4 Suppose there are d ~ 0 hidden layers and a total of W adjustable weights. Suppose that the average of the input (non-bias) weights to units in layer i is bounded by bi. Then Proof: Haussler (1992, Theorem 11). D Bartlett & Williamson (1996) bound the VC-dimension for a singlehidden-layer network by 2 W log2(24e W D) if the inputs are restricted to {-D, ... ,D}. Karpinski & Macintyre give an explicit bound for a single-output network which has', 'to {-D, ... ,D}. Karpinski & Macintyre give an explicit bound for a single-output network which has a leading term of (W M)2 /2 where M is the number of sigmoidal units']\n",
      "['6 Non-parametric Methods We have seen that the Bayes rule is based on p(k I x) oc nkp(x I C = k). We usually estimate the nk from the proportions in the training set, and we have considered parametric models for Pk(x) such as the multivariate normal distribution, In this chapter we consider nonparametric estimates of the class distributions. We have also seen that classification can be done by choosing the largest of fk(x) = p(k I x), and that these can be estimated by regression methods, so in this section we also consider, briefly, non-parametric regression methods. We end with a discussion of the use of mixture distributions which, while parametric, is designed to approximate arbitrary class distributions. The methods of this chapter are illustrated on small problems. They can be applied to larger problems in many dimensions (and nearest neighbour methods are often very successful) but by their very nature there is nothing to illustrate except performance figures. Nearest neighbour', \"by their very nature there is nothing to illustrate except performance figures. Nearest neighbour methods are within the diagnostic paradigm; other methods which work within the sampling paradigm and aim to model the classconditional densities in many dimensions need a very large training set to be successful. 6.1 Non-parametric estimation of class densities Most non-parametric methods are based on the idea that a function is locally constant, and much of the difficulty in their use is deciding what is meant by 'local' in the high-dimensional space q'. We will start by considering kernel methods. A kernel K is a bounded function on q' with integral one. Suitable examples include probability density functions such as the multivariate normal. We assume that K is in some sense peaked about 0. We then use K(x-y) as a measure of the proximity of x and y. (This suggests that we should take K ( -x) = K (x)\"]\n",
      "['182 6 Non-parametric Methods and this requirement is commonly imposed.) The empirical distribution of x within a group k gives mass 1/nk to each of the examples. This suggests that a local estimate of the density Pk(x) can be found by summing each of these contributions with weight K (x -x;), that is (6.1) and this can also be interpreted as an average of kernel functions centred on each example from the class. We have ~(k I ) = 1rkPJ(x) = ~ L[i]=k K(x-x;) p X \"\"\\' ~ ( ) ::!1 . ( 6.2) Ltk 1rJPJ X Li n1;1K(x- X;) When the prior probabilities are estimated by nkfn, (6.2) simplifies to \"\"\\' . K(x-x·) p(k I x) = Lt[z]=k z L;K(x-x;) \\' (6.3) the weighted proportion of points around x which have class k. The difficulty with kernel methods is the choice of K. Suppose we make the very reasonable choice of a multivariate normal density. Clearly the mean should be zero. How do we choose the covariance matrix? We have seen in discriminant analysis the importance of choosing the right metric via the', 'matrix? We have seen in discriminant analysis the importance of choosing the right metric via the within-group covariance. The case of univariate x has been studied in most detail (Silverman, 1986; Hardie, 1990; Wand & Jones, 1995), and Scott (1992) also considers the multivariate case. Even in one dimension it seems that adaptive methods are needed, that is those which change the spread of the kernel over the space fl£. In our context we want to identify correctly those regions in which p(j I x) is maximal, and near-equality will normally occur in the tails of the densities, so it is particularly important to estimate the tails correctly. This seems almost completely ignored in the density estimation literature. For example, plots of the estimated densities on log scale show how rough they are in the tails (e.g. Duda & Hart, 1973, Figures 4.1 and 4.2), and discrimination is based on differences in log densities. In the two-class case, Hall & Wand (1988) suggest estimating', \"based on differences in log densities. In the two-class case, Hall & Wand (1988) suggest estimating 7r2P2(x)-n1Pl(x) directly by a kernel estimate. (With estimated prior probabilities this amounts to a kernel estimate for the whole sample, but counting samples from class 1 with a negative kernel.) Figure 6.1 shows the estimated class densities for the data on Cushing's syndrome, using a normal kernel with bandwidth chosen by a standard reference (Venables & Ripley, 1994, Chapter 5). The decision regions of the resulting classifier are shown in Figure 6.2. Remember that [i] is the group of training case i.\"]\n",
      "[\"Figure 6.1: The class densities for the data on Cushing's syndrome, estimated by kernel methods with a bivariate normal kernel. Figure 6.2: The decision regions for the data on Cushing's syndrome, using the estimated class densities shown in Figure 6.1. 6.1 Non-parametric estimation of class densities 8 :;' ~ 0 ;:; 8 .,; 0 ci :g ci ;.51 OJ u b b 10 50 T etrahydrocortisone b b 10 50 T etrahydrocortisone 183 Note that if we consider (6.3) with a normal kernel with covariance matrix KL, as K --+ 0 the posterior probabilities concentrate on the class of the training-set example nearest to x in Mahalanobis distance. On the other hand, the tail behaviour of the kernels is critical in determining the relative balance of the prior proportions of the classes and the effect of the training data at points x well outside the training set. It is quite common practice to use kernels with bounded support, so the density estimate at x could be zero for one or even all classes. Kernel discriminant\", 'so the density estimate at x could be zero for one or even all classes. Kernel discriminant analysis is the subject of monographs by Hand (1982) and Coomans & Broeckaert (1986). The ALLOC80 computer package (Hermans et al., 1982) for kernel discriminant analysis is widely used. Kernel methods can be used to estimate regression surfaces, by averaging the values of y attached to the nearby data points. We']\n",
      "['184 6 Non-parametric Methods obtain the Nadaraya-Watson non-parametric kernel regression ~( ) 2::::; y;K(x- x;) y X = . l:;K(x- x;) (6.4) Now suppose we use (6.4) to estimate fk(x) = p(k I x). Then y IS the indicator function for class k, and (6.4) reduces to (6.3). This form of classification fits into the framework of Chapter 4. (The nonparametric kernel regression can also be derived by taking a kernel density estimator in the space :1£ x 1R and evaluating E[Y I X= x].) Kernel methods are known in the pattern recognition literature as Parzen windows after Parzen (1962). There was earlier work on the method by M. Rosenblatt (1956) and, in passing, Fix & Hodges (1951). The extension to more than one dimension is usually attributed to Cacoullos (1966) and Murthy (1966). Kernel methods are readily updated for use in leave-one-out crossvalidation if we retain the numerator and denominator of (6.3). To find p(k I Xj, ff\\\\ {xj}) we only have to subtract K(O) from the denominator, and from', 'of (6.3). To find p(k I Xj, ff\\\\ {xj}) we only have to subtract K(O) from the denominator, and from the numerator when k is the true class of Xj. Specht (1990a, b, 1991) has re-labelled these methods as neural networks (without any apparent biological motivation); (6.3) he calls a probabilistic neural network and ( 6.4) a general regression neural network. Kernel methods require the whole training set to be retained. Methods which select a smaller set of centres are considered in Section 6.4. An alternative approach advocated by Specht (1967a, b) is to approximate p(x) for Gaussian kernels as the product of a multivariate normal density and a polynomial, using a Taylor expansion. (Details are given by Duda & Hart, 1973, pp. 106-107, and Wasserman, 1993, pp. 46-51.) This is more in the spirit of of the next subsection. We stress in Section A.1 that a density is defined with respect to an underlying measure, and can be changed by changing that measure. Thus if we know that Pj is near', 'an underlying measure, and can be changed by changing that measure. Thus if we know that Pj is near some density po, we should choose that density as the underlying measure, and so estimate Pj by an estimate of the ratio Pj/Po times po. For a kernel estimator this becomes pj(x) = Po(x)_!_ LK(x- x;)/po(x;). nj i We call p0 a fixed start density; p j will not integrate to exactly one. We can apply the same procedure with a density Po estimated from a parametric family (for example the family of normal distributions) by using a parametric start (6.5) A closer connection to neural networks is made for rotationally symmetric normal kernels and features of unit length, for then K(x-xi)= f(xT x) where f(t) oc exp(l -t)/2a2.']\n",
      "['6.1 Non-parametric estimation of class densities 185 However, the standard theory no longer applies. Hjort & Glad (1995) show that the bias and variance are (asymptotically) essentially unchanged from those of the fixed start estimator using the (unknown) best possible fixed start within the parametric class. Hjort & Jones (1996) reverse the roles of the parametric and nonparametric parts of ( 6.5) by estimating the parameters of the parametric family locally (as defined by the kernel). More precisely, the density at X is estimated by f(x; fl(x)) for a parametric family f(·; ()),where (}(x) is chosen to maximize a local log-likelihood ~LK(x-xi)logf(xi;{})-I K(t-x)f(t ;())dt. In both versions the hope is that the parametric family will capture the broad features of the density, and allow a kernel with a much larger spread to be used. This may make the methods feasible in more dimensions than the simple kernel method, but the issues of the choice of kernel remain. These methods will be', \"than the simple kernel method, but the issues of the choice of kernel remain. These methods will be most effective in a small number of dimensions . A more constrained form of correction to a parametric start is discussed later under 'projection pursuit density estimation'. Orthogonal expansion estimators A general approach towards non-parametric density estimation is via expansions in orthogonal basis functions, estimation of necessary coefficients, and a rule to decide when to stop including terms in the expansion. The expansion approach has some advantages over kernel methods in statistical pattern recognition problems. This approach often yields a compact representation of the estimates of class densities, with a low number of coefficients describing the estimate. Most texts on density estimation mention the approach , whereas Tarter & Lock (1993) consider only orthogonal expansions. We start with a general orthonormal set of basis functions 1/)k(x) on f!( with respect to a\", 'We start with a general orthonormal set of basis functions 1/)k(x) on f!( with respect to a suitable weight function w, that is, I 1pj(y)1pk(y)w(y)dy = I{j = k}. Examples of such structures abound, see for example Abramowitz & Stegun (1965, Chapter 22) or Thisted (1988, §5.3.2). We will exemplify the method by Fourier series, for f!( = [0, 1] and 1/)k(x) = exp 2nikx. (It is always worth bearing in mind that the feature space may profitably be transformed before density estimation; for example the transformation']\n",
      "['186 6 Non-parametric Methods <l>(x) will map JR ~ [0, 1] and turns near standard normal densities into near uniform ones.) Other examples we shall meet in Section 9.1 are polynomial expansions (Hermite polynomials) orthogonal with respect to a uniform weight on [-1,1] and with respect to 4>(x) (the normal density) on JR. Suppose we wish to approximate a density function f by a series expansiOn f(x) = L:k CklJYk(x). The best choice of coefficients q IS given by ck = j lJYk(Y)f(y)w(y) dy = E 1pj(X)w(X) in the sense of minimizing the mean integrated squared error MISE = E j(f -f)2w. The infinite series I:;:o q1pk(x) defined in this way converges pointwise to f(x) if f is continuous. We estimate Ck by ck = I: lJYk(Xi)w(Xi)/n. With estimated coefficients we do have to stop the expansion at some term m. More generally, we could use 00 f(x) = L bkck1pk(x) k=l for some suitable tapering sequence (bk). Let us explore Fourier series a little. Since the Fourier coefficients are defined for both', '(bk). Let us explore Fourier series a little. Since the Fourier coefficients are defined for both positive and negative integers, our sums should extend infinitely in both directions (or be truncated at ±m ). We can always write a Fourier series estimator as a kernel estimator on taking K( ) L b 2 .k sinn(2m+ l)x x = k exp nz x = --.--\\xadsmnx lkl~m if we truncate at ±m. This is the Dirichlet kernel shown in Figure 6.3; it has a narrow peak and slowly decaying oscillations about zero. Thus with this tapering 7 can take negative values, but for the tapering sequence bk = max[O, 1-k/(m + 1)], the kernel is the Fejer kernel K(x) = _1_ [sinn~m+ 1)x]2 m + 1 smnx ~ and so f is non-negative (Figure 6.3). A number of stopping rules have been proposed. It is fairly easy to show that including term k will decrease MISE if jqj2 > 1/(n + 1) (Tarter & Lock, 1993, §4.2) and with a bias-correction argument this suggests including term k only if lcki2 > 2/(n + 1). Unfortunately this will lead to the', 'this suggests including term k only if lcki2 > 2/(n + 1). Unfortunately this will lead to the inclusion of an infinite number of terms, so it is']\n",
      "['Figure 6.3: Kernels for Fourier series estimator. Left: Dirichlet kernel. Right: Fejer kernel. Ridge functions are constant orthogonally to one direction. 6.1 Non-parametric estimation of class densities 187 lll lll N N 0 0 N N ~ ~ ~ ~ lll lll 0 0 u;> u;> ·0.4 ·0.2 0.0 0.2 0.4 ·0.4 ·0.2 0.0 0.2 0.4 preferable to include terms until this test is failed for 2-4 consecutive terms. Diggle & Hall (1986) give a similar but more complex alternative. An alternative approach is to allow the data to choose the tapering sequence. Tarter & Lock report good results with the choice Orthogonal series estimators meet similar difficulties to kernel density estimators once we move to IR d, d > 1. We can, for example, use a multidimensional Fourier series, but these are tied to a particular coordinate system, and the stopping rule has to be defined for a dtuple index. For Hermite polynomials invariance to rotations can be achieved by including all terms up to degree m, but there are many of these for', 'to rotations can be achieved by including all terms up to degree m, but there are many of these for moderate m and we will need moderate m to approximate all but the simplest functions. Series estimators are particularly useful for low-dimensional projections. Parametric starts can be used with orthogonal series as well as kernel density estimates; for some univariate examples using a normal start and Hermite polynomials see Buckland (1992a, b). Projection pursuit density estimation Projection pursuit density estimation (Friedman et al., 1984) is the application of projection-pursuit ideas (Section 9.1) to density estimation, and so is appropriate when the variation in the densities is concentrated in a linear subspace of f!C. It estimates a density by the formula M PM(x) = Po(x) IJ qm(a~x) (6.6) m=l where Po is an initial density (perhaps an appropriate multivariate normal distribution) and the qm are multiplicative corrections which are ridge functions. We will consider its', 'and the qm are multiplicative corrections which are ridge functions. We will consider its application to n samples x;, perhaps the training samples for a single class.']\n",
      "['188 6 Non-parametric Methods The corrections in (6.6) are chosen recursively. At stage m we have Pm-1 and choose am and qm to maximize the goodness-of-fit of Pm as measured by the Kullback-Leibler divergence E log Pm(X) for X drawn from the true density. Given am = a it is easy to show that this is maximized by choosing qm to be the ratio of the densities p and Pm-1 projected onto the direction a. This is estimated by the ratio of a univariate density estimate for the data points (aT xi) projected on a to the projection of Pm-1 (the integration to find the marginal distribution along aT x being done by Monte Carlo methods). Rather than retain the full density estimates, Friedman et al. approximated the estimate of qm by a cubic spline. This method gives us an estimate of Pm given a. Since Elogpm(X) = Elogqm(aTX) + Elogpm-1(X), we choose am = a to maximize E log qm(a rx) and estimate this by its sample version ~ 2:: logqm(a T xi). This is maximized numerically. It remains to choose the', \"by its sample version ~ 2:: logqm(a T xi). This is maximized numerically. It remains to choose the number of terms M. Standard methods such as cross-validation can be used. Often examining qm (which shows the ratio of the two densities) against the projected data (aT xi) will indicate if a worthwhile improvement can be made. The density estimation strategy of Friedman (1987, §4) is 'backward' rather than 'forward' in flavour. Exploratory projection pursuit is used to find a direction a such that a rx is maximally non-normal. We then remove the marginal structure in direction a by cp(aTx) p(x) +-p(x) ( T )' Parx a x that is by adjusting the marginal density to be standard normal (see page 297), and repeat the process. Eventually the exploratory process will be unable to find an interesting projection, and the remaining density can be fitted by a normal distribution. Reversing the process reveals a density for X which is a normal density times a series of correction terms, and the\", 'reveals a density for X which is a normal density times a series of correction terms, and the corrections are ridge functions. The marginal densities can be fitted by any one-dimensional estimation method, including splines, kernel and orthogonal series methods, but the compact representation of orthogonal series will be especially useful. It would be possible to use q-dimensional correction terms, for small q. Discrete distributions Thus far we have implicitly assumed that we have continuous features, so !!{ c lR.P. We can consider non-parametric estimation of class Friedman et al. (1984) use crude histogram estimators of the densities, but kernel methods could be used.']\n",
      "['xc denotes the values of X; for i E C The empty set 0 is included to give a constant term. 6.1 Non-parametric estimation of class densities 189 distributions in the discrete case too. For simplicity let us assume that we have p discrete features. Then a distribution is specified by a p-way table of probabilities of all possible combinations, and the natural nonparametric estimator is to use the frequencies of the cells of the table in the training set !Y. Of course, if p is large and !Y is not enormous, most of the cells will be given zero probability. When we come to classify future cases, we are likely to find that the estimated class probabilities are zero under all classes. Thus it is essential to smooth the observed frequencies before using them in a plug-in classifier. We will consider how to do so, but point out that it will normally be better to use a logistic regression than the methods considered here, as that uses the data to model the quantities of direct interest, the', 'the methods considered here, as that uses the data to model the quantities of direct interest, the posterior probabilities. The natural idea for statisticians would be to build a contingency table model of the joint distribution of the features (McCullagh & Nelder, 1989). The most common choice would be a log-linear model, in which the log probability logPr{Xt = Xt, ... ,Xp = xp} = LA.c(xc) c (6.7) is expanded over subsets C c {1, ... ,p}. When the terms of (6.7) are restricted, for example by omitting A.c(xc) for large C, the family of probability distributions is restricted, but the coefficients A.c may be fitted by maximum likelihood. Choosing the appropriate restricted model is an art, and can be considered within the graphical framework of Chapter 8. The most extreme restriction of ( 6. 7) is to omit all sets C of two or more features, so Pr{Xt = Xt, ... ,Xp = Xp} = e;.0 IT e).;(x;) i which amounts to assuming independence of the features. Thus (6.7) can be seen as an expansion', 'i which amounts to assuming independence of the features. Thus (6.7) can be seen as an expansion away from an independence model. Other expansions have been used for binary data. Suppose each feature takes the values 0 and 1, and that feature i takes the value 1 with probability Pi· Let and consider the 2P polynomials y~1 • • • Yl· These are orthogonal with respect to the independence model, so we can take an orthogonal series']\n",
      "['190 6 Non-parametric Methods expansion. If we estimate p; by the frequency Pi with which feature i takes value 1 in the training set, we have the Bahadur-Lazarsfeld expansion (Bahadur, 1961a, b; Lazarsfeld, 1961) Pr{Xt = Xt, ... ,Xp = xp} p = rrg1(1-p;)1-Xj[1+ L:cijYiYj+ I: CijkYiYjYk+ .. l (6.8) i=l i<j i<j<k The estimates c... are just the sample moments of Yi for the appropriate indices. Often just the first correction term is used, which is a correction for correlations only. Kernel methods For binary features, Aitchison & Aitken (1976) proposed the kernel smoothing where K(x, y) = hP-d(l -h)d and d = llx-yll2 is the number of disagreements between x and y. Here 1/2 < h ~ 1 is a smoothing constant; for h = 1 there is no smoothing. The kernel gives weight hP to cell x and weight hP-k(l-hl to cells that differ in k features. Note that the kernel is a product over the features, since d is a sum over features. This product kernel can be extended naturally to categorical data with ki ~', 'is a sum over features. This product kernel can be extended naturally to categorical data with ki ~ 2 possible outcomes (give probability h to the observed outcome, (1 -h)/(k;- 1) to all others), to ordered categorical data (spread the probability 1-h over adjacent outcomes) and to features which are counts (Aitken, 1983). For mixed continuous and discrete features we can take a product of an appropriate kernel for each feature. For binary and categorical features the kernel estimator takes a convex combination of the frequencies with a uniform distribution. This has been considered in its own right as a method of smoothing (for example, Fienberg & Holland, 1973). The smoothing constant h can be chosen separately for each feature. Choosing an appropriate degree of smoothing remains a difficult problem (P. Hall, 1981; Tutz, 1986, 1988, 1989). Averaging with a uniform distribution gives a different perspective which suggests other ways to choose the degree of smoothing (Fienberg &', 'a different perspective which suggests other ways to choose the degree of smoothing (Fienberg & Holland, 1973; Titterington, 1980; Brown & Rundell, 1985).']\n",
      "['Choosing the neighbourhood using all the classes may not be a good idea if the class densities have very different structure. See the glossary. This was reproduced in Agrawala (1977), Dasarathy (1991) and with a commentary by Silverman & Jones (1989). 6.2 Nearest neighbour methods 191 6.2 Nearest neighbour methods One simple adaptive kernel method is to choose K to be constant over the nearest k examples and zero elsewhere. This does not in fact define a density as the estimate of p(x) (its integral is infinite) but (6.2) suggests a simple estimate of the posterior distribution as the proportions of the classes amongst the nearest k data points. This is a piecewise constant function over f!l, and gives a classifier known as the k-nearest neighbour rule. This differs from using the k-nearest neighbour density estimate for each class, as we choose the neighbourhood from the k nearest points of any class. If the prior probabilities are known and the proportions of the classes in the', 'points of any class. If the prior probabilities are known and the proportions of the classes in the training set are not proportional to nk, the proportions amongst the neighbours need to be weighted (Brown & Koplowitz, 1979). (We ignore this in the theory below.) The version with k = 1 is often rather successful. This divides the space f!l into the cells of the Dirichlet tessellation of the data points, and labels each by the class of the data point it contains. We can also consider the analogue of (6.4) which gives a locally constant non-parametric regression surface, and once again corresponds to the k-nearest neighbour estimate of the posterior probabilities. Both the k-nn method and kernel discrimination were first given in an unpublished report by Fix & Hodges (1951). There is a very extensive literature on nearest neighbour classifiers, much of which is reviewed or reprinted in Dasarathy (1991). Ties in the distances can occur with finite-precision data (or if the underlying', \"Dasarathy (1991). Ties in the distances can occur with finite-precision data (or if the underlying distribution has a discrete part). One solution is to include all patterns as near as or nearer than the k-nearest neighbour, and take a majority vote amongst them. Nearest neighbour rules can readily be extended to allow a 'doubt' option by the so-called (k, t)-rules (Hellman, 1970), called in this field a 'reject option'. These take a vote amongst the classes of the k nearest patterns in f!l, but only declare the class with the majority if it has t or more votes, otherwise declare 'doubt'. Indeed, if there are different error costs, we may want to allow the minimum majority to depend on the class to be declared. Properties of this class of rules are discussed by Devijver & Kittler (1982) and Loizou & Maybank (1987), but for large samples. The k-nn rule can be critically dependent on the distance used in the space f!l, especially if there are few examples or k is large (Figure 6.4).\"]\n",
      "[\"192 6 Non-parametric Methods 0 a 0 0 0 a 0 0 c c c c 0 0 0 0 .,; .,; c b b a b a b b b a ubb 50 T etrahydrocortisone Cover & Hart (1967) gave a famous result on the large-sample behaviour of the nearest neighbour rule. Note that the expected error rate is always bounded below by E*, by the optimality of the Bayes rule. Proposition 6.1 Let E* denote the error rate of the Bayes rule in a Kclass problem. Then the error rate of the nearest neighbour rule averaged over training sets converges in L1 as the size of the training set increases, to a value E 1 bounded above by E* (2-~E·) K-1 . Proof: Let X1 be the nearest neighbour to X, a randomly sampled pattern with class C. The (rather technical) arguments of C. J. Stone (1977) and Devroye (1981a) show that Now Eip(k I XI)-p(k I X) I ~ 0. Pr(C1 -=/= C I X= x) = L p(i I x) E [p(j I XI) I X= x] if.j Figure 6.4: Decision boundaries of the 1-nn rule for the Cushing's syndrome data. The left plot uses Euclidean distance on our usual plot, the\", \"rule for the Cushing's syndrome data. The left plot uses Euclidean distance on our usual plot, the right Euclidean distance on log10 excretion rates.\"]\n",
      "['/\\' and It\\' denote monotone convergence from below and above, respectively. and so 6.2 Nearest neighbour methods EIPr(Ct =I= C I X)-L p(i I X) p(j I X) I ~ 0. i#j 193 Thus E1 = Eet(X) where e1(x) = L:i-hp(iJx)p(jJx) = 1-\"L:;P(ilxf The conditional Bayes risk r*(x) is 1 -maxi p(i I x) = 1 -p(k I x), say, so by the Cauchy-Schwarz inequality and (K-l)_Lp(ilx)2 ~ [_Lp(ilx)f =r·(x)2 i=fk i=fk (K-1) L p(i I x)2 ~ r*(x)2 + (K-1) (1-r*(x)2) i 1-LP(i I x)2 ~ 2r*(x)- K ~ 1 r*(x)2. i On taking expectations we obtain E ~ 2E*-~E (r*(X)2] ~ 2E*-~(E*)2 1\" K-1 \" K-1 using E(Y2) =VarY+ (E Y)2 ~ (E Y)2 0 It is easy to see that the upper bound is attained if the densities Pk(x) are identical and so the conditional risks are independent of x. For the k-th nearest neighbour rule detailed results are only available for two classes. Intuitively one would expect the 2-nn rule to be no improvement over the 1-nn rule, since it will achieve either a majority of two or a tie, which we will suppose is broken at', \"rule, since it will achieve either a majority of two or a tie, which we will suppose is broken at random. The following result supports that intuition. On the other hand, we could report 'doubt' in the case of ties (the (2, 2)-rule). Proposition 6.2 Suppose there are two classes, and let Ek denote the asymptotic error rate of the k-nn rule with ties broken at random and E£ if ties are reported as 'doubt'. Then Proof: We rely on L1 convergence results such as those quoted in the proof of Proposition 6.1 to show the existence of the asymptotic results and to replace p(k I Xr) by p(k I X) in deriving the asymptotic results. Let rk(x) denote the (limit of the) probability of misclassifying x. We can think of this as taking k + 1 samples from p(·l x) and finding that the first belongs to the minority group.\"]\n",
      "['194 6 Non-parametric Methods Temporarily denote the posterior probabilities of the two classes by p = p(11 x) and q = p(21 x), and let ~ = pq. Consider ak. the probability that the first of k samples is in a strict minority, and let b; = (~;) ~; be the probability of exactly i examples from each class. First consider k odd. On adding a point, we can create a tie but not a minority, so k-1 (2k-1) k-1 k k-1 (2k-1) k k-1 alk-1 -alk = P 2k -1 k p q + q 2k -1 k p q k -1 (2k -1) k k -1 = 22k -1 k ~ = 2k-1 bk. If k is even, adding a point can create a minority but not a tie so a2k+1 = a2k + !bk 1 k -1 1 a2k+1 = a2k-1 + 2bk-2k _ 1 bk = a2k-1 + 2(2k _ 1) bk. Clearly a1 = 0, so by induction k 1 a2k+1 = L 2(2i-1) b;. 1=1 In terms of these quantities, we have r~dx) = a2k+1 r2k-1(x) = a2k + bk r2k(x) = r~k(x) + !bk = a2k + !bk From these formulae r2k+1-r2k-1 = (2~-!)bk ~ 0. Direct calculation shows that r1(x) = 2~ and r~(x) = ~ = !r1(x). On taking expectations this establishes the hierarchy on each', 'r1(x) = 2~ and r~(x) = ~ = !r1(x). On taking expectations this establishes the hierarchy on each side. Now Note that lim r2k-1 (x) = lim r~k(x) = ~ 2(2 .1 1) b;. k-HXJ k--+00 L.....t l -i=1 r(x) = min[p(11 x),p(21 x)] = ! [1-J1- 4~(x)] has the Taylor series expansion • 00 1 (2i -2) i 00 1 r (x) = L i i -1 ~ = L 2(2i -1)b;. 1=1 1=1 The proof is completed by taking expectations and using the monotone convergence theorem. D']\n",
      "['Figure 6.5: Large-sample risk rk ( k odd) or rk ( k even) of k-nn rules against the Bayes risk r\\' in a two-class problem. 6.2 Nearest neighbour methods 195 \"\\' ci ... ci \"\\' ci ~ 0 ~ \"! 0 ;; :; 0.0 0.1 0.2 0.3 0.4 0.5 Bayes risk This result is from Cover & Hart (1967); the formulae are given without proof by Devijver & Kittler (1982) and Fukunaga (1990). Figure 6.5 shows rk{x) as a function of r*(x); this shows the agreement is excellent for moderate r*(x) even for small k (but not k = 1 ). Comparable results for the (k, t)-nn rule were first considered by Tomek (1976a). Later Loizou & Maybank (1987) showed that for t > k/2 we have where E*(d) refers to the Bayes rule (if any) with doubt cost d such that its doubt rate is equal to the asymptotic doubt rate of the (k, t)-nn rule, and c(k,t,K) is a computable constant. Since E*(d) ~ E*, this can be used to give asymptotic lower bounds for E*. Clearly the left-hand side increases with t. Another version of the asymptotics is to allow k to', 'Clearly the left-hand side increases with t. Another version of the asymptotics is to allow k to increase with the size n of the training set. C. J. Stone (1977) showed that provided k --+ oo and kjn --+ 0, the risk for the k-nn rule (not averaged over training sets) converges in probability to the Bayes risk. For k/ log n --+ oo, Devroye (1981b) strengthened this to almost sure convergence. The same methods allow the variance over training sets for fixed k to be estimated. All these performance measures are asymptotic and they do not apply to finite samples; for example the error rate does not decrease monotonically with odd k. Notice that the results do not depend on the metric, whereas in practice the choice of metric is often very important. There are few finite-sample results. Cover (1968) is widely quoted as showing that the risk of the 1-nn rule converges to E1 at rate O(n-2), but his results only apply to the bias for one-dimensional f£. Fukunaga']\n",
      "[\"196 6 Non-parametric Methods & Hummels (1987a, b; see also Fukunaga, 1990, §7.3) consider an expansion of the mean difference between the error rates of the k-nn rule and its asymptotic version. They found a leading term of O(n-21P) for p-dimensional f!l', and for the 2-nn with ties reported as 'doubt', the rate O(n-41P). These are very slow rates of convergence for moderate p, but since that to E~ is faster, they suggest that for two classes it is better to estimate £1 by doubling the estimate of E~ (as suggested by Fukunaga & Flick, 1985). These results only concern the bias of the true error rate based on a training set of size n as an estimator of Ek. The variability must also be taken into account if these results are to be used to bound the Bayes risk. We have seen that for odd k, Ek-l,rk/21 = Ek-1 ~ E*, which r l is defined on suggests bounding the Bayes risk by the achieved performance of the page xii. (k-1, fk/21)-nn rule. However, recall from Section 2.7 that we can achieve\", 'of the page xii. (k-1, fk/21)-nn rule. However, recall from Section 2.7 that we can achieve lower variability by not using the class labels, but averaging 1 -max p(k I x) over a test or training set. If we observe ki neighbours of class i, this suggests estimating the error rate by E, the average of min(k1,k2) jk. In Section 2.7 we viewed E as a biased estimate of Ek. Under our large-sample assumptions for the training set but averaging over a test set, it can be shown that (for k odd) E E = Ek-l,rk/21, but that E has a lower variance than the test-set error rate of the (k-1, fk/21)-nn rule (Devijver & Kittler, 1982, §10.8). These results are confined to two classes. For K ~ 2 we have Proposition 6.3 In the large-sample theory the means of the risk-averaged (3, 2) -nn rule and the error rate of the (2, 2)-nn rule are equal and provide a lower bound for the Bayes risk. The risk-averaged estimator has smaller variance. Proof: Condition on x, and draw three samples from 1J = p(·l x), as', \"estimator has smaller variance. Proof: Condition on x, and draw three samples from 1J = p(·l x), as the observed point and its two neighbours. Let ( be the probability that two are from one class, one from another; unless this occurs both rules score zero. The (2, 2)-nn rule scores one if the observed point is in the minority, so has conditional mean and mean square of ( /3. The risk-averaged 3-nn scores 1/3 under all assignments of the three samples, so its conditional mean and mean square are (/3 and (/9. Conditionally and hence unconditionally the means are the same and the risk-averaged estimator has smaller variance. Now (/3 = 'L,k(IJ~-IJt) = Ll, say. We will show that Ll ~ 1-maXIJk and average over x to bound the Bayes risk. Suppose IJl ~ IJk for k > 1. Consider increasing IJi by [) and decreasing IJj by [) for i, j > 1. If IJi + IJj < 2/3 we can increase Ll by zeroing the smaller of IJi and Check the derivative .\"]\n",
      "[\"6.2 Nearest neighbour methods 197 ru; thus we can zero the smallest Y/j > 0 in turn, transferring the mass to the next smallest. If rt1 > 1/3, we can make just one Y/j > 0, which shows that d ~ ytf(l -ytt} + (1-rtd2111 = Y/1(1-ytt} < 1 -'11· If yt1 ~ 1/3 we can make just two Y/i, Y/j > 0; if Y/i > 1/3 the case already proved applied to (Y/i, Y/1, Y/j) shows d < 1-Y/i < 1-'11· We can show that the result holds for three Y/i = 1/3 by direct calculation. 0 This suggests estimating a lower bound for the Bayes risk by running the 3-nn classifier on the training set and reporting 1/3 the number of occasions on which the neighbours are two of one class, one of another (and of course one of the neighbours will be the training-set example itself). If the distances are tied, we can average over ways of breaking the tie, since this will be equivalent to averaging over perturbations of the points. Choice of metric We have not avoided the choice of a metric on !!l, and this can again cause\", 'Choice of metric We have not avoided the choice of a metric on !!l, and this can again cause difficulties. In practice Euclidean distance is normally used, but after a suitable scaling of the variables. It may make sense to use an (estimated) Mahalanobis distance if the within-class distributions are roughly normal and of similar covariance matrix. For two classes, Short & Fukunaga (1980, 1981) looked at a local metric with the aim of minimizing the mean-square error between the finite-sample risk and the asymptotic risk (which we have seen does not depend on the metric used). They show the metric should be of the form d(x, y) = IP(11 x)-p(11 Y)l, expand this about x and estimate the coefficients of the expansion. Short & Fukunaga (1980) and Myles & Hand (1990) experiment with extensions to several classes. Fukunaga & Flick (1984) suggest choosing a global quadratic metric with the same aim (and again with two classes). Their metric is a Mahalanobis distance and so they choose the', 'aim (and again with two classes). Their metric is a Mahalanobis distance and so they choose the inverse covariance A to obtain d(x, z) = V(x-z)T A(x-z). They compute the value of A which approximately minimizes the error rate, and then estimate the quantities involved (the underlying densities) by k-nn density estimation. One of the most important steps in choosing a metric may be to exclude features which have little or no relevance. The features']\n",
      "['198 6 Non-parametric Methods selected by classification trees (Chapter 7) can be a very useful guide (Ripley, 1993). An appealing idea is to combine the features of kernel methods and k-nearest neighbour methods by distance-weighting the classes of the neighbours in reaching a decision. This has proved controversial (Dudani, 1976; Bailey & Jain, 1978; Morin & Raeside, 1981; MacLeod et al., 1987; Parthasarthy & Chatterji, 1990) in that asymptotically for fixed k the distance-weighting does not help. However, this is not the correct basis for the comparison, since using distance-weighting may allow a larger value of k to be used for a given size of training set. Data editing One common complaint about both kernel and k-nn methods is that they can take too long to compute and need too much storage for the whole training set. The difficulties are sometimes exaggerated, as there are fast ways to find near neighbours (for example Friedman et al., 1975, 1977; Fukunaga & Narendra, 1975;', 'ways to find near neighbours (for example Friedman et al., 1975, 1977; Fukunaga & Narendra, 1975; Kalantari & McDonald, 1983; Kamgar-Parsi & Kanal, 1985; Preparata & Shamos, 1985; Ruiz, 1986; Kim & Park, 1986; Niemann & Goppert, 1988; Bryant, 1989; Jiang & Zhang, 1993) and fast approximate ways to find kernel density estimates by binning (Hardie, 1991; Scott, 1992). However, in many problems it is only necessary to retain a small proportion of the training set to approximate very well the decision boundary of the k-nn classifier. This concept is known as data editing. It can also be used to improve the performance of the classifier by removing apparent outliers. There are many editing algorithms: the literature on data editing is extensive but contains few comparisons. (It is surveyed in Dasarathy, 1991.) The multiedit algorithm of Devijver & Kittler (1982) can be specified as follows (with parameters I and V): 1 Put all patterns in the current set. 2 Divide the current set more or', '(with parameters I and V): 1 Put all patterns in the current set. 2 Divide the current set more or less equally into V ~ 3 sets. Use pairs cyclically as test and training sets. 3 For each pair classify the test set using the k-nn rule from the training set. 4 Delete from the current set all those patterns in the test set which were incorrectly classified. 5 If any patterns were deleted in the last I passes return to step 2. The edited set is then used with the 1-nn rule (not the original value of k ). Devijver & Kittler indicate that (for two classes) asymptotically the']\n",
      "['Figure 6.6: Reduction algorithms applied to Figure 1.3. The known decision boundary of the Bayes rule is shown with a solid line; the decision boundary for the 1-nn rule is shown dashed. (a) multiedit. (b) The result of retaining only those points whose posterior probability of the actual class exceeds 90% when estimated from the remaining points. (c) condense after multiedit. (d) reduced nn applied after condense to (a). 6.2 Nearest neighbour methods 0 D 0 D C D \\\\ Ltl or:Z,fl 0~ 0 co d\\'Bif.:>\\\\, a -o<»>loa ~ 0 ~0 I \\\\ df e e 0\"\" D 0 0 ~ I e \\\\ ~D D ., ... \\' -... ,_ :!. .. / • •• ~.tl • ••• A. • ·~ .. ·-. ... -... . ... . . ., ·\\' ..•.. (a) I D D I • I D \\' I 0 \\' (c) \\' \\' ..... / .... ____ _ (b) . .. , . . . • . I I o I • I o \\' -. (d) 199 1-nn rule on the edited set out-performs the k-nn rule on the original set and approaches the performance of the Bayes rule. (The idea is that each edit biases the retained points near x in favour of the class given by the Bayes rule at x, so eventually', 'the retained points near x in favour of the class given by the Bayes rule at x, so eventually this class dominates the nearest neighbours. This applies to any number of classes.) Figure 6.6(a) illustrates the multiedit algorithm applied to the synthetic dataset shown in Figure 1.3 on page 12. The Bayes rule is known in this example (since it is synthetic). In practice multiediting can perform much less well and drop whole classes when applied to moderately sized training sets with more dimensions and classes. Another idea (Hand & Batchelor, 1978) is to retain only points whose likelihood ratio Py(x)/pi(x) against every class i -=/= y exceeds some threshold t. (The densities are estimated non-parametrically.) It make more sense to retain points for which p(y I x) is high, for example those which attain a majority t in a (k, t)-rule for a larger value of k. This is illustrated in Figure 6.6(b) for the synthetic example using the (10,9)-nn. Earlier editing algorithms were given by Wilson', 'for the synthetic example using the (10,9)-nn. Earlier editing algorithms were given by Wilson (1972), Wagner (1973), Tomek (1976b) and Penrod & Wagner (1977). The multiedit algorithm aims to form homogeneous clusters in fl£. However, only the points on the boundaries of the clusters are really effective in defining the classifier boundaries. Condensing algorithms']\n",
      "['200 \"\\' 0 0 0 -1.0 6 Non-parametric Methods -0.5 0.0 0 • 0 .r:J Cl [l It • r::. IJ • . \\'c .. 0.5 1.0 aim to retain only the crucial exterior points in the clusters. For example, Hart (1968) gives: 1 Divide the current patterns into a store and a grabbag. One possible partition is to put the first point in the store, the rest in the grabbag. 2 Classify each sample in the grabbag by the 1-nn rule using the store as training set. If the sample is incorrectly classified transfer it to the store. 3 Return to 2 unless no transfers occurred or the grabbag is empty. 4 Return the store. This is illustrated in Figure 6.6(c). A refinement, the reduced nearest neighbour rule of Gates (1972), is to go back over the condensed training set and drop any patterns (one at a time) which are not needed to correctly classify the rest of the (edited) training set. As Figure 6.6(d) shows, this can easily go too far and drop whole regions of a class. Other attempts at condensing and reducing are discussed by', \"far and drop whole regions of a class. Other attempts at condensing and reducing are discussed by Swonger (1972), Ullmann (1974), Ritter et al. (1975), Tomek (1976c), Chidananda Gowda & Krishna (1979) and Fukunaga & Mantock (1984). Nearest neighbour methods will give low apparent error rate (zero for 1-nn) so it is essential to use other forms of performance assessment. Fortunately the leave-one-out cross-validated error rate can be computed as easily as the apparent error rate by finding the k neighbours of x excluding x itself. Essentially the same ideas have been considered within the field of machine learning, known as 'memory-based learning' (for example, Stanfill & Waltz, 1986). In its simplest form this is just a 1-nn classifier based on storing all the examples. Editing and condensing techniques Figure 6.7: The result of the reduced nearest neighbour rule of Gates (1972) applied after condense to the unedited data of Figure 1.3.\"]\n",
      "['6.3 Learning vector quantization 201 have also been considered (E. E. Smith & Medin, 1981; Kibler & Aha, 1987). These seem re-discoveries of the simplest such algorithms. Examples The variables in the Pima Indians diabetes example were on very different scales, and so were each scaled to have range about one over the training set. The initial choice of k was made by both leave-one-out and 10-fold cross validation. For the latter the error rates were 57, 65, 57, 54, 51 and 55 (out of 200) for k = 1, 3, 5, 7, 9, 11; for leave-one-out we obtained 58, 65, 60, 55, 49 and 41. On the test set the numbers of errors were 98 for k = 1 and 82 for k = 9, out of 332. This is an example in which there is considerable mixing between the classes, and local methods will be unable to pick up the broad trends. Using the 3-nn rule on the training set suggested a lower bound of about 15% for the Bayes error in this problem; linear methods achieve about 20%. For the forensic glass data, there are many', 'in this problem; linear methods achieve about 20%. For the forensic glass data, there are many rational ways to choose the metric, since the features are eight proportions plus the refractive index. We chose to rescale the refractive index to about ±10 but not to scale the compositional features. With this metric, risk-averaging the 3-nn rule suggests a lower bound of 14% for the Bayes risk. Each of the 1-nn, 3-nn and 5-nn rules had a similar level of performance; the 1-nn rule had a cross-validated error rate of 23.4% and confusion matrix WinF WinNF Veh Con Tabl Head WinF 59 7 4 0 0 0 WinNF 12 59 3 2 0 0 Veh 2 5 10 0 0 0 Con 0 2 0 8 1 2 Tabl 1 0 0 2 6 0 Head 1 3 1 1 1 22 Comparing this with the confusion matrices for larger values of k shows the effect of very different proportions for the six classes; as k increases fewer errors are made on the more abundant classes, whereas more are made on the rare classes. 6.3 Learning vector quantization The refinements of the k-nn rule aim to', 'made on the rare classes. 6.3 Learning vector quantization The refinements of the k-nn rule aim to choose a subset of the training set in such a way that the 1-nn rule based on this subset approximates the Bayes classifier. It is not necessary that the modified training set']\n",
      "[\"202 6 Non-parametric Methods is a subset of the original and an early step to combine examples to form prototypes was taken by Chang (1974). The approach taken in Kohonen's (1988a, b, 1990a, b, 1995) learning vector quantization is to construct a modified training set iteratively. Following Kohonen, we call the modified training set the codebook. This procedure tries to represent the decision boundaries rather than the class distributions. Once again the metric in the space P£ is crucial, so we assume the variables have been scaled in such a way that Euclidean distance is appropriate (at least locally). Vector quantization The use of 'vector quantization' is potentially misleading, since it has a different aim, but as it motivated Kohonen's algorithm we will digress for a brief description. Vector quantization is a classical method in signal processing to produce an approximation to the distribution of a single class by a codebook. Each incoming signal is mapped to the nearest codebook\", \"of a single class by a codebook. Each incoming signal is mapped to the nearest codebook vector, and that vector sent instead of the original signal. Of course, this can be coded more compactly by first sending the codebook, then just the indices in the codebook rather than the whole vectors. One way to choose the codebook is to minimize some measure of the approximation error averaged over the distribution of the signals (and in practice over the training patterns of that class). Taking the measure as the squared distance from the signal to the nearest codebook vector leads to the k-means algorithm which aims to minimize the sum-ofsquares of distances within clusters (Section 9.3). An 'on-line' iterative algorithm for this criterion is to present each pattern x in turn, and update the codebook by me ~ me+ a(t)[x-me] if me is closest to x (6.9) mi ~ mi for the rest of the codebook. Update rule (6.9) motivated Kohonen's iterative algorithms. Note that this is not a good algorithm for\", \"rule (6.9) motivated Kohonen's iterative algorithms. Note that this is not a good algorithm for k-means; better algorithms are discussed in Section 9.3. Max (1960) and Zador (1982) have pointed out that choosing the average r th power of the distance as the measure of approximation error amounted to choosing the density of the codebook vectors to approximate the dj(d + r)th power of the true probability density in d dimensions. Thus for d large the k-means procedure codes an approximation to p(x). This is an asymptotic result for large codebooks, Gersho & Gray (1992) is a reference text on vector quantization; a short introduction is given by Gray (1984).\"]\n",
      "['6.3 Learning vector quantization 203 but does indicate that a codebook produced by vector quantization for each class might be a good initial reduction of a very large training set. Iterative algorithms Kohonen (1990a) advocated a series of iterative procedures which has since been modified; our description follows the implementation known as LVQ_PAK documented in Kohonen et al. (1992). A initial set of codebook vectors is chosen from the training set. (We discuss later precisely how this might be done.) Each of the procedures moves codebook vectors to try to achieve better classification of the training set by the 1-nn rule based on the codebook . The examples from the training set are presented one at a time, and the codebook is updated after each presentation. In our experiments the examples were chosen randomly from the training set, but one might cycle through the training set in some pre-specified order. The original procedure LVQ 1 uses the following update rule. A example x is', \"pre-specified order. The original procedure LVQ 1 uses the following update rule. A example x is presented. The nearest codebook vector to x, me, IS updated by me -me + et(t)[x- me] me -me-et(t)[x-me] if x is classified correctly by me if x is classified incorrectly (6.10) and all other codebook vectors are unchanged. Initially oc(t) is chosen smaller than 0.1 (0.03 by default in LVQ_PAK) and it is reduced linearly to zero during the fixed number of iterations. The effect of the updating rule is to move a codebook vector towards nearby examples of its own class, and away from ones of other classes. 'Nearby' here can cover quite large regions, as the codebook will typically be small and in any case will cover f!l rather sparsely. Kohonen (1990a) motivates this as applying vector quantization to the function ln1P1(x)-n2P2(x)l for two classes (or the two classes which are locally most relevant). A variant, OLVQ1, provides learning rates Cte(t) for each codebook vector, with an updating\", 'A variant, OLVQ1, provides learning rates Cte(t) for each codebook vector, with an updating rule for the learning rates of Cte(t-1) Ctc(t) = 1 + (-1)/(classification is incorrect) Ctc(t -1)\" (6.11) This decreases the learning rate if the example is correctly classified, and· increases it otherwise. Thus code book vectors in the centre of classes will have rapidly decreasing learning rates, and those near class boundaries will have increasing rates (and so be moved away from the boundary quite rapidly). As the learning rates may increase, they']\n",
      "['204 0 ci -1.0 6 Non-parametric Methods + + + + -0.5 0.0 0.5 1.0 are constrained not to exceed an upper bound, often 0.3. Practical experience shows that the convergence is usually rapid, and LVQ_PAK uses 40 times as many iterations as codebook vectors. An explanation of this rule is given by Kohonen et al. (1992) and Kohonen (1995, p. 180) which we interpret as follows. At all times the codebook vectors are a linear combination of the training set vectors (and their initializers, if these are not in the training set). Let s(t) = (-1)/(classification is incorrect), so we can rewrite (6.10) as mc(t + 1) = mc(t) + s(t)ex(t)[x(t)- me] = [1 -s(t)ex(t)]mc(t) + s(t)ex(t)x(t) = [1-s(t)ex(t)][1- s(t-1)ex(t-1)]mc(t-1) + [1-s(t)ex(t)]s(t -1)ex(t- 1)x(t -1) + s(t)ex(t)x(t). Now suppose x(t-1) = x(t) and the same codebook vector is closest at both times (so s(t-1) = s(t) ). If we ask that the multiplier of x(t) is the same in both terms, we find [ 1 -s( t )ex( t)] ex( t -1) = ex( t) which gives', \"of x(t) is the same in both terms, we find [ 1 -s( t )ex( t)] ex( t -1) = ex( t) which gives (6.11). This adaptive choice of rate seems to work well, as in our examples. The procedure LVQ2.1 (Kohonen, 1990b) tries harder to approximate the Bayes rule by pairwise adjustments of the codebook vectors. Suppose ms, IDt are the two nearest neighbours to x. They are updated simultaneously provided that ms is of the same class as x and the class of mt is different, and x falls into a 'window' near the mid-point of ms and IDt. Specifically, we must have . (d(x,ms) d(x,mt)) 1-w mm , >--d(x,mt) d(x,ms) 1+w Figure 6.8: Results of learning vector quantization applied to Figure 1.3. The initially chosen codebook is shown by small circles, the result of OLVQl by + and subsequently applying 25,000 passes of L VQ2.1 by triangles. The known decision boundary of the Bayes rule is also shown.\"]\n",
      "['6.3 Learning vector quantization 205 for w ::::::: 0.25. (We can interpret this condition geometrically. If x is projected onto the vector joining ms and mt, it must fall at least (1 -w)/2 of the distance from each end.) If all these conditions are satisfied the two vectors are updated by ms ~ ms + or:(t)[x- ms], mt ~ mt -or:(t)[x- mt]. ( 6.12) This rule may update the codebook only infrequently. It tends to overcorrect, as can be seen in Figure 6.8, where the result of iterating LVQ2.1 is to push the codebook vectors away from the decision boundary, and eventually off the figure. Thus it is recommended that LVQ2.1 only be used for a small number of iterations (30--200 times the number of codebook vectors). The rule LVQ3 tries to overcome over-correction by using LVQ2.1 if the two closest codebook vectors to x are of different classes, and (6.13) for E around 0.1-0.5, for each of the two nearest codebook vectors if they are of the same class as x. (The window is only used if the two', 'nearest codebook vectors if they are of the same class as x. (The window is only used if the two codebook vectors are of different classes.) This introduces a second element into the iteration, of ensuring that the codebook vectors do not become too unrepresentative of their class distribution. It does still allow the codebooks to drift to the centre of the class distributions and even beyond, as Figure 6.9 shows. The recommended procedure is to run OLVQl until convergence (usually rapid) and then a moderate number of further steps of LVQ1 and/or LVQ3. de Sa & Ballard (1993) motivate a variant of LVQ2.1 by applying stochastic approximation to a kernel regression estimator of ln1P1 (x)-7!2P2(x)l for two classes. This normalizes the step size (replacing [x-ms] by [x-ms]/llx- msll) and reduces the window size as well as or:(t). Initialization The package LVQ_PAK chooses the initial codebook vectors from amongst the training set vectors. It is desirable that the initial vectors lie inside', \"vectors from amongst the training set vectors. It is desirable that the initial vectors lie inside the Bayes boundary for their class, as otherwise they end up representing 'islands' in the classification induced by the 1-nn rule based on the training set which are merely noise. Thus the candidates for initialization are screened to ensure that they are correctly classified by a k-nn rule for moderate k (default 7).\"]\n",
      "[\"206 6 Non-parametric Methods q - . + 00 + + 0 0 .. o+ It) c) + + .. + 0 + + 0 c) -1.0 -0.5 0.0 0.5 1.0 Kohonen (1990a) advocates a more constructive approach to the initial codebook, for example using a vector quantization of the whole training set (ignoring class) or his own 'self-organizing map' method (Section 9.4). A further question is how many codebook vectors to use, and how they should be partitioned amongst the classes. The literature seems to assume that the larger number the better, as this will enable a better piecewise linear approximation to the decision boundaries of the Bayes classifier. However, this argument assumes that the iterative algorithms can make a good job of distributing the codebook, and as Figures 6.9 and 6.10 show, this is not necessarily so. We may do better to start with a better-designed codebook such as the result of editing procedures. It is unclear how many codebooks vectors should be selected for each class, since the number needed depends as much\", 'many codebooks vectors should be selected for each class, since the number needed depends as much on how well they are employed as on the proportions of the class. Kohonen et al. (1992) suggest using equal numbers per class initially, and altering Figure 6.9: Further results of LVQ with a larger codebook. This time the triangles show the results from LVQ3.']\n",
      "['6.4 Mixture representations 207 the proportions so that the median of the nearest neighbour distances between vectors within a class is roughly similar across classes. Examples For the Pima Indians data we used OLVQ1 plus LVQ2 or LVQ3 with 2-4 vectors per class, and found a test-set error rate of around 70/332. Increasing this to 10 vectors per class increased the error rate to about 75/332. A procedure consisting of initializing, running OLVQ1 to convergence then 10,000 iterations of LVQ3 was run under the standard 10-fold cross-validation for the forensic glass data. The estimated error rate was 30.4%. Assigning equal numbers of vectors to each class (4 each) reduced this slightly, to 29.9%, making less errors in the containers and tableware groups, and more for non-float window glass. 6.4 Mixture representations Another way of looking at kernel density estimation with a non-negative kernel is that it represents a probability density by p(x) = L wif;(x) i where the densities f;(x) =', 'is that it represents a probability density by p(x) = L wif;(x) i where the densities f;(x) = K (x -x;) and the weights are uniform (or mr[i]/n[i] for known class probabilities). Then pj(x) is of the same form, but giving weights 1/nj to points from class j and zero to the others. Vector quantization uses a somewhat more general uniformly-weighted mixture. This suggests representing densities by a mixture of a fixed set of densities, so p(x) = ~ [ ~ 1tjWij] f;(x) I } (6.14) and \"( .1 ) n j 2::::; Wijj;(x) p} X = . L; [I:j 1tjWij]f;(x) (6.15) We may also want to allow parameters within J;, for example the means and in the covariance matrix of a normal density, so we will write f;(x; 8;). We considered the case of a two-component normal mixture in Chapter 2 on pages 41-42. Mixture densities of this sort have been considered occasionally as general models for density estimation (for example by Roeder, 1990).']\n",
      "[\"208 6 Non-parametric Methods We could estimate the class-conditional densities PJ by choosing the WiJ on the basis of the training samples labelled by class j and then use (6.15) for classifying future samples. It is also possible to use unclassified cases to assist in the estimation process, as proposed in this context by Traven (1991). Sebestyen (1962) proposed an iterative process of approximating the class-conditional densities by Gaussian mixtures; two recent accounts also using Gaussian mixtures are Chou & Chen (1992) and Streit & Luginbuhl (1994), both of which regard this as a 'neural network' method. There is an extensive literature on the estimation of mixture distributions surveyed by Redner & Walker (1984), Titterington et al. (1985) and McLachlan & Basford (1988). It is straightforward to write down the likelihood for the observed training patterns (xP, yP) and any unclassified patterns zu as II 1tyP L WjyP fi(xP; (}i) II L n jWijfi(zu; ei) p i u i,j but finding a maximum\", 'patterns zu as II 1tyP L WjyP fi(xP; (}i) II L n jWijfi(zu; ei) p i u i,j but finding a maximum may be another matter. Indeed, it is possible that the maximum may occur with the components f;(x;8i) degenerating around observed points. This can be avoided by suitably constraining the parameters ei. One favourite device for numerically finding a maximum is the EM algorithm (Section A.2 has the details). This pretends the example really did come from one of the components fi(x; (}i), say I, but this is unobserved. The posterior probabilities of I = i given xP are then used to weight the various components in the mixture; for example for a general Gaussian mixture we use weighted mean and covariance estimators for each mixture component. Of course, the posterior probabilities depend on the parameters , so the process must be iterated. The convergence of the EM algorithm is notoriously slow (Redner & Walker, 1984), and it may be better to use a conventional numerical optimization', 'slow (Redner & Walker, 1984), and it may be better to use a conventional numerical optimization technique. It is unclear whether this reputation is justified, as it may be much easier to find a nearly-optimal solution (in the sense of high log-likelihood) than to find the maximizing parameters precisely. (For pattern recognition purposes, only a good approximation to the mixture density is needed.) There are a number of ways to find good starting points for the optimization , for example by ad hoc partition of the space !!{ and fitting a component to the patterns falling in each partition. To emphasize the importance, note that if the EM algorithm is applied to each class of the synthetic data of Figure 1.3 for a two-component mixture with equal covariance matrices (the truth), it becomes trapped in a poor local minimum from starting']\n",
      "[\"This update formula is not quite correct, as fi depends on the new Iii· 6.4 Mixture representations 209 points which do not separate the group means sufficiently on the x axis. Ingrassia (1992) demonstrates that both the standard EM algorithm and his implementation of simulated annealing can find an inappropriate local minimum in univariate multimodal normal mixture problems with quite high probability. A Bayesian approach will have a prior on the parameters (8;) and on the mixing proportions wij. These are generally taken to be independent, and the proportions for each class given a Dirichlet distribution. The EM algorithm can be used to find a posterior mode for the parameters, but the predictive distributions p(j I x) can only be found by the iterative simulation methods discussed in Section A.3 (Diebolt & Robert, 1994; Gelman et al., 1995). The simplest way to apply the Gibbs sampler will be in its 'blocked' form, simulating alternately all the components JP for the examples given\", \"will be in its 'blocked' form, simulating alternately all the components JP for the examples given the component parameters, then the component parameters given (JP). In the spirit of neural networks, an 'on-line' approach has been sought, in which the parameter estimates are adjusted whenever an example is presented (and the training set will be presented many times). Traven (1991) considers an on-line approximation to the EM algorithm for general Gaussian mixtures (with separate covariance matrices). The current estimates of the means and variance are found as weighted means and variances. Those weights depend on the current parameter estimates, so the estimates cannot be updated exactly when a new example is presented unless the data are retained. When a new example x is observed, Traven uses /i; +---/i; + 1'f;(x-/i;) ~ ~ T ~i +---~i + q;(x-/i;)(x-ji;) for each component, where (q;) = p(i I x)/[p(i I x)+ L.f=l p(i I xP)]. This is unavailable, and is approximated by p(i I x)/(N+1)p;\", \"p(i I x)/[p(i I x)+ L.f=l p(i I xP)]. This is unavailable, and is approximated by p(i I x)/(N+1)p; on the assumption that the xP were a random sample from the class and N is large. Using a constant rather than N will allow 'forgetting' of examples. The only advantage of this procedure seems to be to avoid storing the data, and if the dataset is very large it may well be sufficient to use a smaller sample to estimate the parameters. An alternative way to estimate p(j I x) would be to regress the class indicator on the variables f;(x), which will differ from (6.15) and which for radially symmetric component functions has been discussed in Section 4.2. As there, we have avoided the question of choosing the number of component densities which can in general only be done\"]\n",
      "['210 6 Non-parametric Methods using knowledge of the complexity of the densities p;(x) or by crossvalidation . Vector quantization can be seen as a special case of a finite mixture, in which the components are constant densities over the tiles of the Dirichlet tessellation formed by the codebook. There have also been methods to design modified training sets for use with kernel methods, and these can be seen as using finite normal mixtures (or normalized Gaussian radial basis functions) with equally weighted components. For example, Specht (1991) describes a simple clumping method to select cluster centres, and Burrascano (1991) uses LVQ to select a set of centres for the normals. Hastie & Tibshirani (1996) explore normal mixtures with a common covariance matrix :E for all components in all classes. This is a rather restrictive assumption, but does allow all the components to be rescaled simultaneously to Np{J.l,l} as in linear discriminant analysis. Each class is then represented by a', \"simultaneously to Np{J.l,l} as in linear discriminant analysis. Each class is then represented by a distribution over component means rather than a single mean, but the between-groups covariance matrix can still be decomposed to find 'canonical variates' on which the data may be displayed . This model is similar to using LVQ with the Mahalanobis distance for :E, as the latter can be considered to be using equallyweighted mixtures. The LVQ model has the advantage of choosing the codebook vectors for good discrimination, and mixture models for the classes suffer from modelling the class populations accurately in regions where this is not needed. Choosing the number of mixture components is notoriously difficult (McLachlan & Basford, 1988; Peck et al., 1989; Furman & Lindsay, 1994). Examples Finding a good local maximum for maximum likelihood fitting of a mixture of normals is difficult in practice, and we found a wide range of fits from different starting points. The k-means algorithm\", 'practice, and we found a wide range of fits from different starting points. The k-means algorithm of Section 9.3 was used to initialize the means, with the covariance matrices started at the within-cluster covariance matrix. However, k-means is itself a random algorithm subject to local minima, so the whole procedure was run several times and the best fit selected. The k-means procedure depends on the distance used in f!l; Euclidean distance was used after careful scaling of the features. Figure 6.11 shows the plug-in classifiers for normal mixtures fitted to the synthetic dataset (which was generated from a normal mixture). In this case unequal mixtures were used (to avoid biasing the fit too']\n",
      "['Figure 6.11: The decision boundaries for the plug-in Bayes classifier for mixture models fitted to the synthetic dataset. The dashed line corresponds to fitting two normals with unequal covariances to each class; the dotted line to fitting five components to each class, whose means are shown by +. 6.4 Mixture representations 211 -1.0 -<l.S 0.0 0.5 1.0 much to the correct model). Fitting five rather than two components per class decreased the deviance by about 20 but used 30 extra parameters . The use of AIC strongly suggested two components per class, but the presence of many local minima makes this theoretically dubious (and we know that the global maximum has infinite log-likelihood). For the Pima Indians data we saw a suspicion of bimodal distributions on page 99. Using two normal components per group with a common covariance matrix (between as well as within groups) gave a test-set error of 64/332, a negligible improvement over linear discriminant analysis. Allowing different', 'error of 64/332, a negligible improvement over linear discriminant analysis. Allowing different common covariance matrices within classes achieved 84/332, comparable with quadratic discriminant analysts. For the forensic glass data some of the classes are too small to fit even a single normal density, so we need to use a common covariance matrix over all the classes and components. Some trials suggested that three components per class was a reasonable compromise , for which the cross-validated error rate was 30.8%, more than for LVQ.']\n",
      "[]\n",
      "['7 Tree-structured Classifiers The use of tree-based methods for classification is relatively unfamiliar in both statistics and pattern recognition, yet they are widely used in some application s such as botany (Figure 7.1) and medical diagnosis as being extremely easy to comprehend (and hence have confidence in). The automatic construction of decision trees dates from work in the social sciences by Morgan & Sonquist (1963) and Morgan & Messenger (1973). (Later work such as Doyle, 1973, and Doyle & Fenwick, 1975, commented on the pitfalls of such automated procedures .) In statistics Breiman et al. (1984) had a seminal influence both in bringing the work to the attention of statisticians and in proposing new algorithms for constructing trees. At around the same time decision tree induction was beginning to be used in the field of machine learning, which we review in Section 7.4, and in engineering (for example, Sethi & Sarvarayudu , 1982). The terminology of trees is graphic, although', '(for example, Sethi & Sarvarayudu , 1982). The terminology of trees is graphic, although conventionally trees such as Figure 7.2 are shown growing down the page. The root is the top node, and examples are passed down the tree, with decisions being made at each node until a terminal node or leaf is reached. Each non-terminal node contains a question on which a split is based. Each leaf contains the label of a classification. A subtree of T is a tree with root a node of T; it is a rooted subtree if its root is the root of T. A classification tree partitions the space f£ of possible observations into sub-regions corresponding to the leaves, since each example will be classified by the label of the leaf it reaches. Thus decision trees can be seen as a hierarchical way to describe a partition of f£. We could give the botanist a description of each species and ask for the description which matches the current specimen. Even in small domains this can be too difficult, and a decision tree', 'matches the current specimen. Even in small domains this can be too difficult, and a decision tree provides a structured description of the knowledge base. Often the same information can be structured in']\n",
      "['214 7 Tree-structured Classifiers 1. Leaves subterete to slightly flattened, plant with bulb 2. Leaves flat, plant with rhizome 4. 2. Perianth-tube > 10mm I. x hoUandica Perianth-tube < 10mm 3. 3. Leaves evergreen I. xiphium Leaves dying in winter I. latifolia 4. Outer tepals bearded I. germanica Outer tepals not bearded 5. 5. Tepals predominately yellow 6. Tepals blue, purple, mauve or violet 8. 6. Leaves evergreen I. foetidissima Leaves dying in winter 7. 7. Inner tepals white I. orientalis Tepals yellow all over I. pseudocorus 8. Leaves evergreen I. foetidissima Leaves dying in winter 9. 9. Stems hollow, perianth-tube 4-7mm I. sibirica Stems solid, perianth-tube 7-20mm 10. 10. Upper part of ovary sterile 11. Ovary without sterile apical part 12. 11. Capsule beak 5-8mm, 1 rib I. enstata Capsule beak 8-16mm, 2 ridges I. spuria 12. Outer tepals glabrous, many seeds I. versicolor Outer tepals pubescent, CHew seeds I. x robusta other ways. Many botanical trees amount to a set of rules', 'tepals pubescent, CHew seeds I. x robusta other ways. Many botanical trees amount to a set of rules describing one class, so each class is eliminated in turn. Another research area in machine learning has been to induce sets of rules from a training set, either directly or via an induced tree (e.g. Michalski, 1980; Quinlan, 1987a, b, 1993). The idea of tree induction is to construct a decision tree from a set of examples, which is how humans construct trees. It is usual to do so by growing the tree, that is by successively splitting leaves. Tree construction is easiest when there is an exact partition of !!l, that is one which classifies every example correctly. The alternative, in which the distributions of observations from the classes overlap, is often called a noisy classification problem. For the exact case, we need to continue to grow the tree until every example is classified correctly. In a noisy problem to do so would over-fit the examples at hand, and the two possible', 'correctly. In a noisy problem to do so would over-fit the examples at hand, and the two possible strategies are to stop growing the tree early, or to prune the tree after constructing, closely analogous to forwards and backwards selection in regression. Figure 7.1: Key to British species of the genus Iris. Simplified from Stace (1991) p. 1140, by omitting parts of his descriptions. Confusingly, these strategies are sometimes called pre-and post-pruning.']\n",
      "['Table 7.1: Example decisions for the space shuttle autolander problem, from Michie (1989). Figure 7.2: Decision tree for shuttle autolander problem. The numbers m/n denote the proportion of training examples reaching that node which are misclassified. 7 Tree-structured Classifiers stability error sign wind magnitude visibility decision any any any any any no auto xstab any any any any yes noauto stab LX any any any yes noauto stab XL any any any yes noauto stab MM nn tail any yes no auto any any any any Out of range yes noauto stab ss any any Light yes auto stab ss any any Medium yes auto stab ss any any Strong yes auto stab MM pp head Light yes auto stab MM pp head Medium yes auto stab MM PP tail Light yes auto stab MM pp tail Medium yes auto stab MM pp head Strong yes noauto stab MM pp tail Strong yes auto ~ ~moo \"~ 0 0/128 17/125 stability:stab stability:xstab 17/61 error:MM,SS ~ error:LX,XL I nMuto I 0/32 magn:Light,Medium,Strong ~ ma~ ~ 0/8 error:MM ~ err[fu ~ 0/12 ~·· 0/3 ~', 'error:LX,XL I nMuto I 0/32 magn:Light,Medium,Strong ~ ma~ ~ 0/8 error:MM ~ err[fu ~ 0/12 ~·· 0/3 ~ magn:Light,Medium ~ mag~ 0/4 ~ wind:head ~ wi~d:~to I 0/1 0/1 I nMuto I 0/64 215']\n",
      "['216 7 Tree-structured Classifiers The main differences between algorithms for tree construction are the pruning strategy used and the exact rule for splitting nodes. Many algorithms only allow binary splits, that is to divide a node into two; a few allow multi-way splits (for example by flower colour). Note that these are just algorithms; there are only very simple models and no deep theorems in this field. There are two types of optimality to be considered. One is optimality of the partition of fl£, which can be judged by the error rate achieved. In principle we could seek an optimal partition amongst all prescribed partitions of fl£, for example those representable by a set of decision rules splitting on a single feature. This is a computationally infeasible procedure for all but the smallest problems, but the stepwise construction of a partition by a decision tree can be seen as an approximation to finding the optimal partition. The other sense of optimality is to represent a', \"an approximation to finding the optimal partition. The other sense of optimality is to represent a partition by a tree in the best possible way. The most obvious criterion is to use the minimal expected number of tests. Hyafil & Rivest (1976) showed this particular problem to be NP-complete; Payne & Meisel (1977) give an algorithm to construct optimal trees with respect to fairly general cost functions. There are a number of partial surveys of the literature . Dietterich (1990) covers 'recent developments in practical learning algorithms'. Safavian & Landgrebe (1991) is wide-.ranging but shallow. Quinlan (1986, 1990, 1993) surveys the machine-learning approaches within his own school. 7.1 Splitting rules In this section and the next we consider the component pieces of currently favoured tree-construction algorithms. Some historical alternatives are mentioned in Section 7.4. Note that the number of possible trees is vast, so there is no question of an exhaustive search over trees.\", 'the number of possible trees is vast, so there is no question of an exhaustive search over trees. Consider first splitting a leaf. There is a set of features from which to construct splitting attributes. For binary features we will clearly consider the binary split on that feature. For categorical features with L > 2 levels we can either consider an L-way split, or consider binary splits dividing the levels into two groups. (There will be 2L-l -1 nonempty pairs of groups, so this generates many attributes for large L.) For ordered features the natural splits are binary of the form x ~ Xc; this applies both to continuous measurements and to ordered categories. Some systems also consider linear combinations of continuous features and Boolean combinations of logical ones. (See Section 7.5.)']\n",
      "[\"7.1 Splitting rules 217 Each leaf will have a set of attributes A on which it might be split. How should we consider the value of the split? There have been many suggestions from several different viewpoints. Consider first a population viewpoint. That is, there is a known probability rri is the set of classes. distribution over f!£ x '(} of examples which would reach that leaf. This gives a marginal probability distribution Pk over '(}. Consider splitting on attribute A which has levels a1, ... , am. There is then a probability distribution Pik over attributes and classes, and the child leaf A · denotes summation corresponding to A= ai would have probability distribution p(k I ai) = over that index. Pik/Pi· over classes k. We can then ask if the child nodes are on average 'purer' than their parent. A measure of impurity should according to Breiman et al. (1984, p. 24) be zero if Pj is concentrated on one class, and maximal if Pj is uniform. Two commonly used measures of impurity are\", \"concentrated on one class, and maximal if Pj is uniform. Two commonly used measures of impurity are the entropy i(p) =-L Pj logpj j (where 0 log 0 = 0) and the Gini index i(p) = LPiPj = 1-LPJ. ii=j j One interpretation of the Gini index is the expected error rate if the label is chosen randomly from the class distribution at the node. (It may be better to use this than the error rate from the Bayes rule at the node since it gives an element of 'look ahead'. Quite often no feasible split reduces the error rate, yet after two or three splits large reductions in error rate emerge; see the right-hand branch of Figure 7.2.) The decrease in average impurity on splitting by attribute A is then m i(pc)-L Pi· X i(p(c I ai)). i=l A common approach is to choose the split that maximizes this. Since this will in general favour many-valued attributes, Breiman et al. and many others confine attention to binary attributes. (See Section 7.4 for adjustments for multi-way splits.) Breiman et al.\", 'to binary attributes. (See Section 7.4 for adjustments for multi-way splits.) Breiman et al. preferred the Gini index. The entropy index has been used widely, for example by Sethi & Sarvarayudu (1982) and Quinlan (1983) in the engineering and machine learning literature respectively. The premise of the following proposition holds for both the entropy and Gini measures of impurity. Part (ii) reduces the number of attributes which need consideration for two classes from 2L-l-1 to L-1, but']\n",
      "[\"218 7 Tree-structured Classifiers it has no simple extension to three or more classes. (The result is due to Breiman et al., but the very much shorter proof is original.) Proposition 7.1 Suppose i(p) is strictly concave. (i) The decrease in impurity is non-negative, and zero if and only if the the distributions are the same in all children. (ii) Suppose there are two classes. For a categorical feature, order the levels in increasing p(11 x = x;). Then a split of the form { x1, ... xt}, { Xt+l• ... , xL} maximizes the reduction in average impurity. Proof: (i) We have by Jensen's inequality L p;.i(p(c I a;)) ~ i(L p;.p(c I ai)) = i(pc) i with equality if and only if p(c I a;)= p(c) for all i and c. (ii) With just two classes we can regard i(p) as a function of p1 only; it remains strictly concave. Consider dividing into two groups by allocating to group 1 with probability a; when x = x;. Then (a) the average impurity of the two groups is minimized by taking a;= 0 or 1 by concavity, and\", 'Then (a) the average impurity of the two groups is minimized by taking a;= 0 or 1 by concavity, and (b) the partial right derivative of the average impurity with respect to a; (which exists by concavity) at a; = 0 is of the form p(X = x;)[Ap(11 x = x;) + B] for constants A and B, and so is positive (when the optimal solution is to allocate x; to group 1) for all i ~ t or all i > t for some t. and both examples lead to the postulated form of split since which group is labelled 1 is arbitrary. D Another way to look at this approach IS to define the average impurity of the tree as I(T) = L qti(p(c It)) leaves t where qt is the probability an example reaches node t. The decrease in I on splitting the node is then qt times the decrease in node impurity we considered before, so that strategy is equivalent to splitting the node to minimize the average tree impurity. See the glossary.']\n",
      "[\"This paragraph is technical and not needed elsewhere. 7.1 Splitting rules 219 Of course, to use this population approach we estimate all the probabilities by frequencies in the training-set examples reaching the node. Ciampi et al. (1987) and Clark & Pregibon (1992) take another approach, viewing the tree as a probability model for the training set. For each node t there is a probability 1tte that an example reaches that node and is of class c, which can in principle be computed from the distribution over f£ x C(} by partitioning. Suppose we condition on the features for all the examples in the training set. We then know the number nt of examples which will reach leaf t, and the numbers nte of each class at that node will have a multinomial distribution with probabilities nelt = ntclnt·· The conditional likelihood is then proportional to II II n,c nelt leaves t classes e and this allows us to write a deviance for the tree probability model as D(T) = L Dt' Dt = -2 L nte log 1telt·\", 'us to write a deviance for the tree probability model as D(T) = L Dt\\' Dt = -2 L nte log 1telt· classes e leaves t (This is a deviance, since in the perfect model nelt = 1 whenever nte > 0 at a leaf t.) If we estimate 1tte by the maximum likelihood estimate nelt = ntclnt, the maximized deviance is D(T) = 2[Lntlognt- Lntelognte]· t t,e The splitting strategy is to choose the attribute which maximizes the deviance. Now consider the average tree impurity for the entropy measure. When the probabilities are estimated this is \"\\'\"\\' nt . \"\\'\"\\' nt net net l(T) = ~ -l(nte!nt) =-~--log-n n nt nt t t,e = -L net lognelt = D(T)/2n n t,e so the splitting strategies for deviances and for entropy-based impurity are identical. We can take this duality of approaches further. Many impurity measures can be written as a sum over examples: for example the Gini index is the sum of (1 -Pe)/n where c is the class of the example. Suppose there are nc examples of class c. Then e']\n",
      "['220 7 Tree-structured Classifiers and this is minimized over (pc) by taking Pc: = nc/n. Thus the use of the Gini index can be considered as a probability model with a different measure of goodness-of-fit. Chou (1991) considers a larger class of measures of the form I(T) = L i(t) = L ~E [t(Y,p(t)) It] leaves t t where Y is the class of an example, t is a loss function and p(t) is chosen to minimize the conditional expectation. The Gini index then arises from t(Y, p) = llind(Y)-pll2 = 1 + llpf-2py (and ind(Y) is the K -tuple of indicators Y = k) and the entropy from t( Y, p) = -log py. Clearly l(T) is always reduced by a split (since there is a p(t) for each child to minimize over). Further, let d(t,p(t)) = E [t(Y,p(t)) It]-i(t). Then if we consider a binary split over values of a categorical split, it is optimal only if for each category x assigned to the left child tL d(x, p(tL)) ~ d(x, p(tR)), and conversely for the right child tR (Chou, 1991). This extends Proposition 7.1, at least', \"p(tR)), and conversely for the right child tR (Chou, 1991). This extends Proposition 7.1, at least for impurity indices of Chou's form. Priors, weights and costs There are several assumptions made so far which we may wish to relax. Quite often the training set is not a random sample from the whole population , but chosen to disproportionally represent the classes, especially to over-represent rare classes. Suppose that the classes are known to have probabilities (nk) in the population, but have nk representatives out of n in the training set. The population approach works with (Pk), the population distribution of classes within the node t. Clearly n1k/n1 is no longer an appropriate estimate of Pk. and we would use the probability vector proportional to n1k/n1 x mrk!nk. Another extension is to allow weights to be attached to the examples. The most obvious reason is that we have an integer number wi of examples like this one, and wish to avoid the overhead of computing with many copies.\", 'number wi of examples like this one, and wish to avoid the overhead of computing with many copies. This suggests interpreting nck and n1 as the sum of weights, not merely counts of examples. Note that we can incorporate priors for the classes via weights, by giving all examples in class k a weight nnk!nk (or multiplying the current weight by this factor). Suppose we wish to attach costs to different misclassifications, say the cost Cij of misclassifying examples of class i as class j. One']\n",
      "[\"There are l1.5028369 t J rooted subtrees of a binary tree with t leaves; Breiman et a/. (1984, p. 284). 7.2 Pruning rules 221 approach is to say that the tree construction is merely modelling the posterior probabilities p(k I x), and the costs should be used to choose the classification at each node, but not otherwise. However, differential costs suggest that we would like a more accurate model for some classes than for others. Breiman et al. (1984, §4.4) suggest that this can sometimes be incorporated into the impurity index. For example, the interpretation given for the Gini index suggests the modified form i(p) = L Cij PiPjifj Unfortunately, this effectively symmetrizes the costs (since the coefficient of PiPj is Cij + Cji) and so is completely ineffective in two-class problems. It can also fail to be concave, and so give rise to splits with negative 'decreases' in impurity. For two classes there is a simple approach to misclassification costs. Each example in class 2 costs a factor\", \"there is a simple approach to misclassification costs. Each example in class 2 costs a factor C2I!C12 more to misclassify than an example in class 1, which suggests weighting the examples in class i by Cij for j '/= i. This will also be appropriate for more classes if the misclassification costs depends only on i and not on j '/= i. How about the deviance approach? We use the weights for each example to weight the log-likelihood ; the deviance becomes D(T) = L De, leaves t D1 = -2 L nee log 1tcle· classes c where nee now represents the sum of the weights of examples reaching leaf t of class c. (This is certainly appropriate if weights represent multiple examples.) We will once again estimate nclt by neclne, but this is an estimate of the biased posteriors, and will be adjusted to be proportional to n1c/nc x nnkfnk to estimate the posteriors in the population. Note that the latter is what we get if we weight examples in class k by nnkfnk. 7.2 Pruning rules The number of rooted subtrees\", 'we get if we weight examples in class k by nnkfnk. 7.2 Pruning rules The number of rooted subtrees of a binary tree is very large so we need a way to navigate this family efficiently. Cost-complexity pruning The best-known procedure for tree pruning is that proposed by Breiman et al. (1984). Let R(T) be a measure of a tree formed by adding the']\n",
      "['222 7 Tree-structured Classifiers contributions from the leaves. One obvious candidate is the number of misclassifications on the training set or a test set; another is the entropy or deviance of the partition. Let the size of a tree be the number of leaves. (For a binary tree the total number of nodes is twice the size minus one.) Then Breiman et al. (1984) proposed choosing a rooted subtree T of the full tree To which minimizes Ra(T) = R(T) + e~:size(T). We can also consider Ra(T) as the sum of R(t) + tX over the leaves of T. This can be seen as using a Lagrange multiplier for size, so finding the minimizing trees for all tX is equivalent to finding the trees with minimum R(T) for each size. (Our results are equally valid for other measures of size such as the total costs of the tests at the nodes.) When using the apparent error rate on the training set we will want to choose a positive e~: to penalize size, but our results also apply to e~: = 0 which would be appropriate with the', 'e~: to penalize size, but our results also apply to e~: = 0 which would be appropriate with the error rate on a test set. Ciampi et al. (1987) consider pruning with the Akaike Information Criterion which corresponds to taking R(T) as the deviance and tX = 2(K -1). (The AIC penalizes minus the log-likelihood by the number of parameters. Estimating the probability distribution within a leaf takes K -1 parameters. This count ignores parameters in the splitting attribute and the selection of the attribute itself; it is unclear how these should be counted.) Breiman et al. showed that there is a nested family of subtrees Tk of To = T such that each is optimal for a range of e~:, and so there are values -00 = CI:Q < (X t < ... 00 such that Ti is an optimal tree for e~: E [e~:i, tXi+t). Further, they gave an algorithm to construct the tree sequence (Tk). Often tXt ~ 0, for example if R(T) is the measure used to grow the tree (such as deviance or Gini) or the error rate on the training set', 'the measure used to grow the tree (such as deviance or Gini) or the error rate on the training set (from Proposition 7.5). However, tXt = 0 is quite common. We will now prove these results. There can be a number of trees with the same value of 14( T); we will consider only one which is a subtree of all to be optimal, and if this exists we call it T(e~:). Consider a non-trivial tree T, and for any non-terminal node t let Tr be the subtree rooted at that node. Let ( T) = R(t)-R(T1) g t, size (T1)-size (t) If we have weights, we would use these in calculating R(T). To handle missing values we will need a modest extension, in which R(T) also contains contributions from all nodes. Precisely, R(T) is assumed to be a sum over leaves plus a sum over non-leaves, the summands being different in the two cases.']\n",
      "[\"This was proposed as a new algorithm by Gelfand & Delp (1991), Gelfand et al. (1991) and Guo & Gelfand (1992), the latter including a more complex proof. However, the algorithm is implicit in earlier work, and explicit, without proof, in Quinlan (1987a), under the name of reduced error pruning. It follows immediately from Theorems 10.7 and 10.10 of Breiman et al. (1984). 7.2 Pruning rules 223 which compares the reduction in R by including the subtree with the increase in size. Note that g(t, T) > rx if and only if Ra(t) > Ra(Tt). The effect of pruning at node t is to replace T1 by t. Proposition 7.2 Suppose we number the nodes of a tree T so that each node precedes its parent. If we visit the nodes in this order (bottom-up) and prune at node t if Ra(t) ~ Ra(T!) for the current tree T', the result is T(rx). Proof: We will establish by induction that when node t is considered all the branches at t are optimally pruned. This is clearly true for the leaves. At node t we either prune with\", 'at t are optimally pruned. This is clearly true for the leaves. At node t we either prune with value Ra(t) or not with value Ra(T{) = Lbranches B Ra(T~) if this is strictly smaller. If there is a subtree T\" rooted at t with a smaller value of Ra it must be non-trivial , and there must be a branch B with Ra(T~) < Ra(T~) and so T~ is not optimally pruned, a contradiction. Now suppose there is another subtree with the same value of Ra. Then each of its branches (it must have some) will have the same value of Ra as the corresponding branch of r: and so include that branch. Thus after node t is considered , the current r: is optimally pruned. When the root is reached the current tree is optimally pruned, so is T(rx). D This gives an algorithm to find T(rx) for a single rx. We now show how to find (rxk) and the tree sequence Tk. From now on we assume that size is increasing, that is adding nodes increases (we.akly) the size. Proposition 7.3 Let rx1 be the smallest value of g(t, T) for any', \"increases (we.akly) the size. Proposition 7.3 Let rx1 be the smallest value of g(t, T) for any nonterminal node t of T. The optimally pruned tree is T for rx < cq, and T1 = T(rxt) is obtained by pruning at all nodes t with g(t, T) = rx1. Further, g(t, Tt) > rx1 for all non-terminal nodes of Tt. Proof: The optimality of T for rx < rx1 is immediate from Ra(t) > Ra(Tt) and Proposition 7.2. Consider rx = rx1, and pruning by Proposition 7.2. Whenever the tree is pruned, Ra(Ts) is unchanged for all nodes s of the new tree. Thus Ra(t) ~ Ra(T!) for the current tree T' if and only if Ra(t) ~ Ra(T1) if and only if g(t, T) ~ rx1. Then for a retained node t, Ra1(t)-Ra1(Ttt) = Ra1(t)-Ra1(Tc) + [Ra1(Tr)-Ra1((Tt)t)] = Ra1 (t)-Ra1 (Tr) = g(t, T)[ size (Tt)-size (t)] > rx1 [size(Tt)- size(t)] ~ rx1 [size((Tt)c)- size(t)] so g(t, Tt) > rx1. D\"]\n",
      "[\"224 7 Tree-structured Classifiers Proposition 7.4 For f3 > a T(/3) is a subtree of T(a) and is the result of {3-pruning of T(a). Proof: We will show by induction that Tt(f3) is a subtree of Tt(a) and conclude that T(/3) is a subtree of T(a). This is true at the leaves. At node t we compare Ra(t) to Ra(Tt(a)) and Rp(t) to Rp(Tc(f3)) and in each example prune if the first is (weakly) smaller. We must show that if Ra(t) ~ Ra(Tc(a)) then Rp(t) ~ Rp(Tt(f3)). Now since Tc(/3) is a candidate for a-pruning of the tree rooted at t, we have Rp(t) = Ra(t) + ({3-a)size(t) ~ Ra(Tt(a)) + ({3-a)size(t) ~ Ra(Tt(f3)) + ({3 -a)size(t) = Rp(Tc(f3))- ({3-a)[size (Tc(/3))- size (t)] ~ Rp(Tc(f3)). Since T(/3) minimizes Rp(T') over all rooted subtrees T' of T and is a subtree of T(a), it also minimizes Rp(T') over rooted subtrees of T(a). 0 The algorithm of Proposition 7.3 can be applied to the new tree T1 = T(al) to find a2 > a1 (since g(t, Tl) > a1 for all non-terminal nodes of T1) and T2 = T(rx2) and so\", 'T(al) to find a2 > a1 (since g(t, Tl) > a1 for all non-terminal nodes of T1) and T2 = T(rx2) and so on until Tk is the trivial tree, the root of To = T. From Propositions 7.3 and 7.4, T(rx) = T1 for rx1 ~ rx < rx2 and T(rx2) = T2. Repeating the process gives T(rx) for all rx ~ rxk. and Proposition 7.4 shows that the trivial tree is optimal for rx ~ rxk. This completes the algorithm to find the tree sequence: 1 Set k = 0 and write out To= T. 2 Set a= oo. 3 Visit the non-terminal nodes t in bottom-up order and calculate R(Tt) and size(Tc) by summing over the descendants (and including any contribution at t ). Set ( ) = R(t)-R(Tt) g t size (Tt)-size (t) and rx =min( a, g(t)). 4 Visit the nodes in top-down order and prune whenever g(t) =a. 5 Set k = k + 1 and write out ak = rx and Tk = T. 6 If T is non-trivial go to 2. We could visit the nodes in any order at step 4, but a top-down order avoids considering nodes which themselves will be pruned away.']\n",
      "['In fact Breiman et al. averaged only at the values Cjct.krxk+ 1 ) for the sequence rxk for the original tree, but this saves but little effort. 7.2 Pruning rules 225 It remains to choose the particular tree within this sequence. If we have a validation set we can use its error rate with rx = 0. Otherwise Breiman et al. propose selecting the value of rx using cross-validation. The training set is split into V parts; Breiman et al. (1984, pp. 11-12) seem to prefer 3 but later users (e.g. Clark & Pregibon, 1992) recommend 10. For each of the parts, a tree sequence is constructed from the remaining V -1 parts of the training set, and its measures R(Tk) calculated, to give a piecewise constant function for R(T(rx)). This is averaged over all V parts, and rx chosen to minimize the function. Because the training set is disjoint from the test set in each of the V cross-validation experiments, we can expect to form a reasonably unbiased estimate of R(T(rx)). If V is small we have used a', \"we can expect to form a reasonably unbiased estimate of R(T(rx)). If V is small we have used a considerably smaller training set, and so might expect to overestimate the error rates, but this does not necessarily mean that the relative values of R(T(rx)) for different rx are seriously biased. In practice the estimates of R(T(rx)) are highly variable over the choice of parts of the training set, and the estimated function may have no minimum within the range of rx considered, or a very broad minimum. Breiman et al. suggest choosing the largest value of rx with the cross-validated R(T(rx)) just above the minimum (the 'one SE' rule). The standard error can be estimated from a binomial distribution for error-count pruning, or a chi-squared distribution for deviance pruning. There is a difficulty with cross-validating deviance measures R(T) not found with error rates nor the Gini measure. Suppose that at some leaf t a class c occurs in the test set but not in the training set. Then the\", 'Suppose that at some leaf t a class c occurs in the test set but not in the training set. Then the fitted probability lite = 0 and so the deviance at that leaf is infinite. (The other measures give a unit penalty.) This might be thought appropriate, and will certainly lead to that leaf being pruned, but makes it difficult to average R(T(rx)). There are several ad hoc solutions, all of which involve altering the fitted probabilities. One we have used successfully is to give a prior of one example per class at each node so lite = (ntc + 1)/(n1 + K) for K classes, which is never zero, but can approach zero if a class does not occur in a large number of examples. This is one of a family of shrinking approaches. Bahl et al. (1989), Chou (1991) and Buntine (1992) each smooth at all splits, not just the leaves, taking the fitted probabilities to be a convex combination of those of the parent and the frequencies in the child node. (Clark & Pregibon, 1992, also propose this.) It remains to', 'and the frequencies in the child node. (Clark & Pregibon, 1992, also propose this.) It remains to choose the convex combination, and indeed to decide if it should be the same at each']\n",
      "['226 7 Tree-structured Classifiers node. Chou uses leave-one-out cross-validation at each node; Bun tine uses a combination which depends on the sample size (see Section 7.7). Clark & Pregibon use a constant factor over the tree, chosen by crossvalidation, and see this as an alternative to pruning. Another approach to pruning Gelfand et al. (1991) and Gelfand & Delp (1991) point out that the optimally pruned tree with respect to the true misclassification rate R(T), were this available, need not be within the family T(cx) pruned with respect to the apparent error rate. We have already seen how to prune with respect to an honest measure of error rate. Gelfand et al. propose a pruning algorithm based on dividing the training set into two and alternating the role of the halves. Initially the tree is grown (and nodes labelled) using one half and pruned using the error rate on the other half. The tree is then re-grown from the pruned tree using the previous test half and pruned using the', 'The tree is then re-grown from the pruned tree using the previous test half and pruned using the previous training half to estimate the error rate. This is repeated until the tree size is unchanged. The pruned subtrees are nested and increasing, and if a node is terminal at two successive steps growth from that node can be stopped. Long formal proofs are given in Gelfand et al. (1991) for the Gini measure of impurity. We can give a short and general argument. It is important here that ties are broken consistently when labelling leaves; we need to choose the class of the parent node if this is a contender. Proposition 7.5 Suppose a training set is partitioned, and the whole set and each cell of the partition are labelled by a class with the highest frequency within it. Then the apparent error rate is decreased by partitioning, strictly so unless the whole partition is given the same class. Proof: Let the frequency of class c within cell i be nci· Then the success count before division', \"Proof: Let the frequency of class c within cell i be nci· Then the success count before division is maxc nc., maximized by k, say, and after division is 2:::::; maxc nci ~ maxc 2:::::; nci = nk· with equality only if k maximizes nci for each class. If the error rate is the same, the class of the parent is a contender for the class of each cell. D Suppose the two halves of the training set are .'T 1 and .'T 2, and let R(il(T) denote the number of errors for test set .'T;, using the labels assigned when the tree was grown. Let T* denote the tree grown using .'T 1 and optimally pruned using R(2l. Proposition 7.6 The tree T* is unchanged under optimal pruning using R(1l.\"]\n",
      "[\"7.2 Pruning rules 227 Proof: From Proposition 7.2 it suffices to show that R(ll(Tt) < R(1l(t) for each interior node. Now Tt corresponds to a partition of the examples of .r-1 reaching node t, so by Proposition 7.5 R(ll(Tt) ~ R(ll(t) with equality only if Tt gives the same partition as t and hence R(2l(Tt) = R(2l(t) which would contradict the optimality of the pruning ofT*. 0 Now suppose a tree S is grown starting from T* using §' 2 and optimally pruned to s· using R(1l. Since S contains T*, Proposition 7.6 shows that s· contains T* (since pruning is monotone on trees). If a leaf in T* remains a leaf in s· the growing and pruning process to form T* will be repeated at the next step, and so that node will always remain a leaf. As there are only a finite number of examples and empty leaves will never be generated, the process must stop. Gelfand et al. (1991) propose reporting an error rate based on classifying at each leaf the examples from the half of the training set other than the one\", \"based on classifying at each leaf the examples from the half of the training set other than the one on which that leaf was labelled (when it was grown). They recommend this procedure only for large datasets (since it works with half the data at a time), and we have found it unsatisfactory for moderately sized datasets, in which T* is often just the root subtree. 'Pessimistic' and 'error-based' pruning Quinlan (1987a, 1993) introduced two much cruder ideas for pruning. In the first approach, he proposes a continuity correction for costcomplexity pruning, so that the number of errors on the training set at each node is increased by one half. The idea was to better estimate the true rather than apparent error rate. (This idea is exactly equivalent to taking rx = 0.5, since R(t) is increased by one half.) He compares the error rate of the tree Tt with the error rate at node t (after a 'continuity correction' adding one half to each error count). Rather than prune only if the adjusted\", \"correction' adding one half to each error count). Rather than prune only if the adjusted error rate for node t is smaller, he proposes to prune unless it is somewhat larger, specifically when error rate for t < error rate for Tt + std. dev.( error rate for Tt ). (Quinlan is vague about how to calculate the last term; his example appears to use a binomial formula with a common probability, but it would be better to calculate the standard deviation within each leaf.) This looks like an approximation to a significance test, except that the variability of the left-hand side is not taken into account. Again the details are vague, but Quinlan states that all subtrees are considered as\"]\n",
      "['228 7 Tree-structured Classifiers candidates for pruning (unlike Proposition 7.2). As there are very many such subtrees, this seems unlikely. A much larger adjustment of the apparent error rate is proposed in his 1993 book. Suppose a leaf t covers N examples, J of which are misclassified. Then R(t) is taken to be the 87.5% point of a binomial (N, J / N) distribution . This could be calculated exactly (and is in the C4.5 program) but given the approximate nature of the justification, using a normal approximation to the binomial to give R(t) = J + 1.15 X J J(1-J IN) seems perfectly adequate . Our understanding is that the algorithm of Proposition 7.2 is used. Examples The data on diabetes amongst Pima Indians have provided a difficult example for many methods, and is typical of the difficulty of using treebased methods. It is easy to grow a tree (using the deviance/entropy approach) with many nodes: our initial tree has 22 nodes, shown in Figure 7.3. (One of the splits has another', \"with many nodes: our initial tree has 22 nodes, shown in Figure 7.3. (One of the splits has another attribute with exactly the same split.) Four nodes can be pruned without changing the classifications at all; the error rate on the test set is 81/332. This is just about significantly worse than the logistic regression with 66/332, as the McNemar statistic is 1.96. Figure 7.4 shows the difficulty in choosing the size by 10-fold crossvalidation of error-rate pruning. There is little variation with the size of tree down to size 3, which suggests the latter should be adopted. This gives the rule that diabetes should be predicted if the plasma glucose level exceeds 123.5 and the diabetes pedigree function exceeds 0.31. This rule has a test-set error rate of 90/332, worse than the unpruned tree. However , the difference is not statistically significant, for McNemar's test statistic is [129-201 -1]/ ,J29 + 20 ~ 1.14. In this circumstance we should not use risk averaging, as most examples are\", \"-1]/ ,J29 + 20 ~ 1.14. In this circumstance we should not use risk averaging, as most examples are predicted with maxp(c I x) = 1, and so the true test-set error rate is dramatically underestimated (11% instead of 24.4%). Quinlan's 'pessimistic' pruning removes just 3 nodes, and AIC removes just one. Indeed, if we grow an even larger tree by allowing smaller populations within the leaves, both Quinlan and AIC select a tree with about 30 nodes. Such large trees have a test-set error rate of about 99/332. Using the Gini index rather than entropy to grow the tree produced a similar but not identical tree, with splits occurring in a slightly different\"]\n",
      "['Figure 7.3: The classification tree grown for the Pima Indians diabetes data based on a training set of size 200. Growth was stopped at leaves with 10 or fewer examples. Figure 7.4: Number of errors vs size for error-rate pruning of the tree of Figure 7.3. 7.2 Pruning rules 229 ~c,-~ / .81200 glu<123.5 / glu>123.5 / ~ s;o\\\\ ~Yes\\\\ I .5/YO I 38791 /age<2B.5 age>28.5 ~~pad-<O.S095 ped>0.3095. -- \\\\__ ;~· No\\\\ No\\\\ No~ Ye\\\\ ;-4tl4 ?73 2.!3 15756 npreg<.2.5 glu<90 glu<166 bmi<28.65 1 npre~5 ~ glu~: 1 g~ ! bml>2\\\\: b1T5B~ npl(:~ ~9 bmi<l~\\\\ pedl:\\\\ ~8 ageE~~ ped<E:5\\\\ ,11 ~L I \"1-;t ~,~ ~\\'~~; ~~~-rr; ~I ~J~pedi 0/53 1/5 316 0/10 /2/10 /7/16 /6/19 0/8 0/6 215 /7/28 0/17 771 bp>71 bmir·::,,35:_85 bmr::,,_r_· r I_ @ ck ~ Jo~ @ ~N~ ~ rr;e\\' ~5 015 0/6 Ts1ro 016 I fiils ~\" 17,,-J4.5 ped<0.1 8 age<40 npre >4 5 I ped>0.18 J age>40 ~l L~ ~ C,Ni~ ~ 01s 21s 11s 3/B /4110 1n ped<0.393 ckpe~3 2/5 1/5 alpha -lnf 0.00 0.67 0.75 1.00 1.50 5.00 15.00 ,----0 ,._ Ill rD \"\\' \"\\' \"\\' ~ 0 ·e rD Ill Ill I 0 Ill 20 15 10 5', '0.00 0.67 0.75 1.00 1.50 5.00 15.00 ,----0 ,._ Ill rD \"\\' \"\\' \"\\' ~ 0 ·e rD Ill Ill I 0 Ill 20 15 10 5 size']\n",
      "[\"230 7 Tree-structured Classifiers order. The Gelfand et al. (1991) procedure depends on the (random) division into two sets, but normally produced a tree with around 15 nodes, and a test-set error rate of around 85/332. For the forensic glass data, growing an initial tree using the entropy/deviance measure and using cost-complexity pruning on the cross-validated error rate gives the plot shown in Figure 7.5. This suggests choosing a tree of size 12, or 9 using the '1 SE' rule as these counts are approximately Poisson and so have a standard error of about 8. alpha -lnf 0.0 0.5 1.0 2.5 4.7 8.0 27.0 20 15 10 5 size It is tedious to cross-validate this choice of tree size within a crossvalidatory assessment of performance, and we introduce a slight bias by not doing so here. (The cross-validation partition chosen for costcomplexity pruning was not the same as that used for assessment.) The performance of the trees pruned to size 9 and 12 were almost identical; size 9 gave the estimated\", 'performance of the trees pruned to size 9 and 12 were almost identical; size 9 gave the estimated confusion matrix WinF WinNF Veh Con Tabl Head WinF 55 13 2 0 0 0 WinNF 14 50 6 3 2 1 Veh 5 8 4 0 0 0 Con 0 3 0 9 0 1 Tabl 0 1 0 1 5 2 Head 2 2 0 2 1 22 and an error rate of 32.2%. The whole Gelfand et al. (1991) procedure was cross-validated. On the whole dataset it grew a tree with 6 nodes that did not classify as container or tableware at all, just using the refractive index and magnesium and calcium oxides. Under cross-validation the tree size Figure 7.5: Cross-validated error count us tree size for the forensic glass data.']\n",
      "['Figure 7.6: Pruned classification tree for the forensic glass data. 7.3 Missing values Mg<2.695 Mg>2.695 Al<1.42 Al>1.42 ~~~~;I RI<·O cb \";~; K<Or:\\\\ I K>0.29 c;J~ Mg<f~\\\\ I Mg>3.75 c;J Jw~~;J 231 varied widely between subsets; the assessment of the error rate was 42%. The Quinlan pruning procedure tended to prune lightly; its crossvalidated error rate was 31%. 7.3 Missing values One attraction of tree-based methods is the ease with which missing values can be handled. Consider the botanical key of Figure 7.1. We only need to know about a small subset of the 10 observations to classify any example, and part of the art of constructing such trees is to avoid observations which will be difficult or missing in some of the species (or, as in the case of capsules, for some of the examples). However, missing values may be unavoidable , and there are several approaches to handling them. 1 A general strategy is to \\'drop\\' an example down the tree as far as it will go. If it reaches a leaf we can', \"strategy is to 'drop' an example down the tree as far as it will go. If it reaches a leaf we can predict y for it. Otherwise we use the distribution at the node reached to predict y, as shown in\"]\n",
      "['232 7 Tree-structured Classifiers Figure 7.2, which has predictions at all nodes. However, examples which are stopped high up the tree have little of the available information used. The pruning algorithms will have to be interpreted carefully, since R(T1) must include examples which reach node t but do not reach the leaves. This can easily be achieved by summing over both children and the examples which are not passed on. This strategy can also be used in tree growth. The deviance approach will automatically weight the value of a split by the proportion of non-missing values in assessing which attribute to use. 2 An alternative strategy is used by many botanical keys and can be seen at nodes 9 and 12 of Figure 7.1. A list of characteristics is given, the most important first, and a decision made from those observations which are available. This is formalized in the method of surrogate splits in which surrogate rules are available at nonterminal nodes to be used if the splitting', 'splits in which surrogate rules are available at nonterminal nodes to be used if the splitting attribute is unobserved. Breiman et al. (1984, §5.3) choose the attribute which maximizes the probability of making the same decision at the node as the primary split. 3 It is possible to split examples with missing values between the branches. In principle this should be done using the conditional probabilities of left and right splits given all the observed information. In general that probability is unavailable. What we can estimate easily is the probability of going left or right given the attributes used in earlier splits, from the frequencies of complete examples at the node. In this approach each example is split into a probability distribution over leaves; each time a missing value is encountered the current fractional example is subdivided. (There is a potential problem here, especially if it is the same feature under consideration as a higher split, since different conditional', \"if it is the same feature under consideration as a higher split, since different conditional distributions will be used each time the example is split. If an example has already been split, the imputed values at earlier splits also have to go into the conditioning.) The obvious way to produce a classification for a split example is to combine the posterior probabilities in the leaves reached by its fractions using the probabilities assigned to leaves, and then assign the class with the highest overall posterior probability. However, Quinlan's (1993) C4.5 system takes the simpler but less rational approach of weighting the leaf classifications, and choosing the class with the overall highest probability. Quinlan (1986) attributes to Alen Shapiro the idea of building a tree to estimate the conditional distribution of the missing value.\"]\n",
      "[\"7.3 Missing values 233 4 Another possibility is to take 'missing' as a further level of the attribute (e.g. Kass, 1980). For methods which allow multi-way splits this has the disadvantage of increasing the number of levels, so that making some values missing can increase the gain in impurity (Quinlan, 1986). This can be circumvented by allowing only binary splits, or by penalizing multi-way splits. In most approaches tree construction is based on the examples without any missing observations. Where missing values are very frequent this may be unacceptable or even impossible. Quinlan (1986) suggests replacing missing values by the distribution within the class at that node when computing the expected value of a split. On the other hand, Quinlan (1993) multiplies the impurity gain calculated on known examples by the proportion of missing values (as implied by the deviance approach) and his C4.5 system uses fractional examples throughout tree construction . However, note that in the\", \"his C4.5 system uses fractional examples throughout tree construction . However, note that in the deviance approach if example splitting is used the partitioning is no longer recursive (as the fitted probability for such examples depends on each branch). All of these ideas have merits and demerits, depending on how common missing values are and whether they are missing at random. For example, in medical diagnosis the absence of a test might well carry information, and examples with the value of an attribute missing could be very different from those with a recorded value. On the other hand, if missing values are rare, there will not be enough information to usefully treat 'missing' as a separate attribute value. Sometimes features are not missing but also not known exactly; for example a continuous feature may only be known to lie within an interval, or a test may indicate a 80% chance of being positive. Such information is best handled by splitting examples. Example The Pima Indians\", 'of being positive. Such information is best handled by splitting examples. Example The Pima Indians diabetes data has many missing values, so we tried out the value of splitting examples, with a training set that had 200 complete examples and 100 partially missing examples. Growing and pruning a tree on this augmented training set led to a slightly larger tree shown in Figure 7.7, which makes 74/332 errors on the test set. This highlights the role of the plasma glucose level and the body mass index. Figure 7.8 shows the training-set examples on those two features, which indicates that the separation is rather weak.']\n",
      "['234 >< Q) \"0 .5 Ill Ill al E >-\"0 0 .0 0 I!) 0 \"It 0 (\\') 0 C\\\\1 7 Tree-structured Classifiers ~ glu<127.5 ~ glu>127.5 ~ Yes 321177 49/123 bmk28.75 @ 7/28 12132 + glu>166 ~ 2112 + +-++ +it-+ ------+ + - -*+ ; t + + * + + + + + --+ ---+ + + + + -+! +-_--+ + --++ +:-+ + +\" -+ - +:~ -:.. :_ ;+--+-+ + + + + ... t. + + + ++ : T ~ i-+ - + ++ + \"\\'\"+ -:.+ - ;r : --+-+ + 60 80 100 120 140 160 plasma glucose + - + + + + • + + 180 200 Figure 7.7: The classification tree grown and pruned for the Pima Indians diabetes data based on a training set of size 300. Figure 7.8: The presence or absence of diabetes against plasma glucose and body mass index for the training set of the Pima Indians data.']\n",
      "[\"7.4 Earlier approache s 235 7.4 Earlier approaches Quinlan (1986) provides an historical overview of developments in the field of machine learning. He considers the TDIDT family of algorithms (Top Down Induction of Decision Trees) to stem from Hunt's Concept Learning System (Hunt et al., 1966), via his own ID3 (Quinlan, 1979) and the ACLS system of Patterson & Niblett (1983). Another branch is the ASSISTANT systems of Kononenko et al. (1984) and Cestnik et al. (1987). Quinlan calls his own descendant of ID3 C4.5. Many of the later systems are commercial and so not documented in the scientific literature. There was a family of CLS systems; the last, CLS-9, chose the split which maximized the number of examples correctly classified over the new leaves. All of these systems (except CLS) are based on the entropy measure of impurity. ID3 examines all candidate attributes and chooses that with the largest 'information gain', which is what we called the reduction in average impurity at the\", \"the largest 'information gain', which is what we called the reduction in average impurity at the node. All the probabilities are estimated from frequencies in the training set. Most of these systems allowed only two classes. What attributes are allowed? In ID3 only categorical features were considered and the split is into all levels of the feature. Both ACLS and ASSISTANT use a binary division of the feature. Continuous features could in principle be divided into ranges or split as in Figure 7.1. If multi-way splits are allowed, they would be expected to have greater information gain; indeed they may have a large information gain even if the attribute has no predictive power. Quinlan (1986, 1988) suggests guarding against this by comparing the information gain to what he terms the 'information value' IV of the attribute A, that is the entropy of the distribution of attribute values at the node. He suggests choosing the attribute which maximizes gain (A)/IV over attributes 'with\", \"at the node. He suggests choosing the attribute which maximizes gain (A)/IV over attributes 'with average-or-better gain amongst all tests examined' (itself a size-biased selection criterion). One feature of the original ID3 was that it works with a 'window', that is a subset of the training data. This is initially chosen as a random subset and the tree grown on it. The rest of the training set is tested, and a selection of incorrectly classified examples is added to the window. The tree is extended and the process repeated. In a logical domain this can reduce the computation, but has not been used in the descendants of ID3 (except C4.5).\"]\n",
      "[\"236 7 Tree-structured Classifiers The main differences between these algorithms come in their stopping rules. The original ID3 had no stopping rule. Quinlan (1983) proposed a chi-square test for the value of the split on the chosen A, that is a test of independence in the' A cross class' table. Niblett (1987) suggested Fisher's exact test for the same purpose. The ASSISTANT system compared a cross-validation estimate of the error after splitting at the node with the apparent error at the node, and stopped if the former was worse. Its successor, ASSISTANT86, computed the node size times the information gain divided by the entropy and stopped if this was smaller than a preset threshold (for example 4%). So far we have assumed that no attributes have missing values. ASSISTANT either used the proportions of the attribute amongst examples of the same class at that node to fill in the most frequent value (as used by CN2, Clark & Niblett, 1989) or used fractional examples to express the\", 'frequent value (as used by CN2, Clark & Niblett, 1989) or used fractional examples to express the distribution over values. Many of the general schemes discussed above have been used. Suppose there are K classes. The Laplacian error estimate replaces the error rate ed ni at a leaf by [ei + K -1] I [ni + K], in a crude attempt to compensate for the optimistic bias of the re-substitution estimator of error. Niblett & Bratko (1986) pruned the tree at node t if the naive Laplacian error estimate at that node is less than that for the subtree rooted at t. (This is described in detail in Niblett, 1987, and also used by Casey & Nagy, 1984.) Bratko & Kononenko (1987) give a number of comparisons for domains of medical diagnosis. Their results show that binary trees are generally smaller and have slightly lower error rates than multiway trees, and that stopping early slightly improves the error rate but markedly improves comprehension. One early strand of work in statistics was given by', 'error rate but markedly improves comprehension. One early strand of work in statistics was given by Kendall & Stuart (1966, §44.30-32) and Richards (1972). They consider continuousvalued attributes and two classes. Suppose for a feature X that one class, say class 1, has generally smaller values than the other. The split is then ( -oo, min2 Xi) to class 1, (max1 Xi, oo) to class 2 and the overlap [min2 Xi, max1 Xd is passed to the next level. The attribute is chosen for which this rule decides the most examples, and the process repeated at the next level. The system THAID of Morgan & Messenger (1973) was one of the first statistical applications of decision trees. Its splitting criterion was the error rate. A descendant, CHAID (Kass, 1980) chooses the split with the highest significance in the A cross class table. However, it found the (approximately) most significant table including amalga-']\n",
      "['See Section 9.3. 7.5 Refinements 237 mating categories of multi-level attributes. The stopping rule was again based on significance, with some allowance for selection. Mingers (1987) also based the choice of split on the highest statistical significance of the contingency table of A cross classes. Ciampi et al. (1987) merge leaves in a post-processing step if their populations are sufficiently similar. An agglomerative clustering algorithm is applied to the leaves with a dissimilarity measure computed from a log-likelihood-ratio test of the difference in within-leaf class distributions. We do not see this as preferable to pruning methods. The work in the engineering literature is diverse and wide-ranging; we will only highlight a few ideas. (Safavian & Landgrebe, 1991, catalogue many more.) Henrichon & Fu (1969) set up a tree with a linear combination at each node whose range was partitioned into positive, negative and undecided; the undecided examples are passed to the next layer. The', 'into positive, negative and undecided; the undecided examples are passed to the next layer. The partitioning criterion was to approximately minimize the error rate. Swain & Hauska (1977) suggested minimizing the sum of measurement and error costs using a 1-step lookahead. Again for a two-class problem, Friedman (1977) and Rounds (1980) used the Kolmogorov-Smirnov distance between the distributions of the two classes to choose the feature, and split at a maximum of the distance. Multi-class problems can be considered by building a tree contrasting each class with the first, and combining information in the leaves of the K -1 trees to decide between the K classes. Sethi & Sarvarayudu (1982) took an information-based approach identical to that which was emerging in machine learning. Engineers have continued to be active in this field, for example Argentiero et al. (1982), Casey & Nagy (1984), Dattatreya & Sarma (1981, 1985), Goodman & Smyth (1988), Kurzynski (1983a, b), Li & Dubes', '(1984), Dattatreya & Sarma (1981, 1985), Goodman & Smyth (1988), Kurzynski (1983a, b), Li & Dubes (1986), Schuermann & Doster (1984) and Wang & Suen (1984, 1987). 7.5 Refinements A modest amount of progress towards more efficient algorithms has been made since Breiman et al. (1984). Chan & Bao (1991) and Fayyad & Irani (1992) noticed that we can restrict the set of cut-points tried for a continuously-valued attribute A with the entropy measure of impurity or equivalently using deviances. The empirical distribution of A jumps only at observed values, so clearly the optimal cut-point will be one of the observed values. What these authors proved is that the cut-point always occurs on the']\n",
      "['238 7 Tree-structured Classifiers boundary between two classes, so we do not need to consider observed values if those to the immediate left and right correspond to examples of the same class. How much of a saving this produces depends on the number of classes (clearly it is best if this is small) and on the degree of overlap of class-conditional distributions of A. The results of Fayyad & Irani showed a very modest speed-up (less than two overall), and our experiments showed even less. Chou (1991) provided a partial extension to part (ii) of Proposition 7.1 based on the ideas discussed earlier. This result corresponds to finding a locally optimal partition of an attribute with L levels in the sense of reduction in average impurity or deviance, in linear expected time in the number of examples. It is only locally optimal, a point Chou glosses over in his title and description. Crawford (1989) considered alternative estimators of the error rate R(T) to be used in cost-complexity', '(1989) considered alternative estimators of the error rate R(T) to be used in cost-complexity pruning, based on the bootstrap (Section 2.7). The idea of the bootstrap is to resample with replacement a sample of size n (the original size) from the training set. (Clearly each of the original examples will occur an integer number of times in the bootstrap sample.) A tree sequence can be grown from each bootstrap sample, and the bias in the error rates for the bootstrap samples used to estimate the bias of R(T(a)). That is, for each of B bootstrap samples we grow and prune a tree to find Tb(a) and evaluate the difference between the error rate for the real training set and the bootstrap sample. The average of this quantity over the B samples, @(a), is the bootstrap estimate of the bias of R(T(a)), so finally a IS chosen to minimize R(T(a)) +@(a). Breiman et al. (1984, p. 312) give some calculations which suggest that the bootstrap estimator of the bias R(T) will systematically', \"some calculations which suggest that the bootstrap estimator of the bias R(T) will systematically underestimate the bias. This property is not shared by Efron's (1983) .632 bootstrap (Section 2.7). Crawford (1989) reports experiments on pruning via both bootstrapped and the .632 bootstrap estimators of the error rate, generally preferring the .632 bootstrap to both the ordinary bootstrap and cross-validation. Incremental learning Thus far we have assumed that the whole training set is available initially. It is easy to envisage situations in which the training set becomes available from an on-line process, and it is desired to maintain an up-to-date decision tree.\"]\n",
      "[\"7.5 Refinements 239 Quinlan (1979) originally envisaged building a decision tree by ID3 incrementally, but this was as a computational shortcut in a noisefree problem where it might be hoped that a small subset of examples would induce the broad shape of a suitable tree. Incremental tree induction has been taken up by Schlimmer & Fisher (1986), Utgoff (1988a, 1989, 1990), Utgoff & Brodley (1990) and Van de Weide (1989, 1990). The difficulty with the incremental growth of trees is that early decisions on which attribute to split were based on few examples and so are likely to be wrong. Utgoff (1988a) allows his procedure ID5 to recover by testing the current optimality of a split, and if it is sub-optimal to re-order the subtree if the optimal split occurs within the sub-tree. Utgoff (1990) gives a modification which is guaranteed to recover the tree grown by ID3. Van de Weide's objective is to grow the smallest possible tree. All this work was for noiseless problems. Crawford (1989)\", \"is to grow the smallest possible tree. All this work was for noiseless problems. Crawford (1989) considers incremental tree growth for noisy problems, carrying out the whole procedure (growth and pruning) on a subtree when a new example shows that the split at the root of that subtree is sub-optimal and will affect the path of the new example through the existing subtree. Subsequently he used bootstrap resampling to estimate if the gain by re-growing the subtree was significant. Hybrid methods We have mentioned that some systems allow linear combinations of continuous variates or Boolean combinations of binary ones at each node. Some of these have been termed hybrid by Utgoff (1988b), and are discussed by Dietterich (1990). The STAGGER system of Schlimmer & Granger (1986) combines a 'Bayesian weight-learning algorithm with a method for constructing Boolean expressions'. The FRINGE algorithm of Pagallo (1989), Pagallo & Haussler (1989, 1990) builds on STAGGER, and post-processes trees\", \"of Pagallo (1989), Pagallo & Haussler (1989, 1990) builds on STAGGER, and post-processes trees constructed by ID3 to include new attributes constructed as Boolean combinations of existing ones. Soft splits A classification tree makes hard splits; for example in Figure 7.1 completely different paths are taken if we measure the perianth tube as longer or shorter than 10 mm. We might be worried if we measured 9.9 mm, and test both possibilities. An automated system will not do that unless it is enhanced by soft splits, of the form 'branch right with\"]\n",
      "[\"240 7 Tree-structured Classifiers probability a(x)'. Hitherto a(x) = l(x > xo), but we can envisage a smoother transition, and average the predictions by splitting examples as for missing values. Carter & Catlett (1987) used a piecewise linear a, linearly interpolating between (xL, 0), (xo, 0.5) and (xR, 1) for XL < x0 < XR. They and Quinlan (1993, §8.1.2) suggested choosing XL and XR (in various ad hoc ways) after the main cut-point xo has been chosen in the usual way. Training set examples can then be divided if their value of x falls in the range [xL, XR], and the tree growth continued. It would not be difficult to choose XL and XR as well as xo to maximize the reduction in impurity at the split. We could use a logistic a, but an asymmetric smoothing of the split may be desirable. 7.6 Relationships to neural networks Two distinct relationships between neural networks and decision trees have been pointed out. The first is that the splitting mechanism invoked at a node is a way to\", 'have been pointed out. The first is that the splitting mechanism invoked at a node is a way to split optimally the examples reaching that node into two (or more), and a neural network could be used to select the attribute to be used. The simplest network would split a linear combination of the variates, and this gives the perceptron trees of U tgoff (1988b) and neural trees ofSankar & Mammone (1993) (and Stromberg et al., 1991). However, Breiman et al. (1984) had already considered allowing linear combinations of variates when setting out the list of attributes at each node, so this gives no added generality. (The idea goes back to at least Henrichon & Fu, 1969.) The growth procedures are different, in that both Utgoff and Sankar & Mammone use the perceptron learning rule (Section 3.6); Utgoff also used incremental induction for a noiseless problem. Breiman et al. used a gradient descent method to find a local maximum in the change in average impurity. An obvious extension is to allow', 'method to find a local maximum in the change in average impurity. An obvious extension is to allow a non-linear discrimination rule at each node. Indeed, we can avoid the combinatorial search over the set of attribute splits at a node by seeking a non-linear combination of the features as a new feature and splitting on that. Many smooth and non-linear regression techniques could be used, including feed-forward neural networks as considered by Guo & Gelfand (1992). They found difficulty in extending minimizing the impurity to non-linear functions, and instead used standard least-squares neural network methods to train the function to discriminate between two groups of classes. With more than two classes they need a rule to choose the partition of the']\n",
      "[\"7. 7 Bayesian trees 241 classes; one idea for a moderate number of classes is to compare the impurity change for each partition of the classes. The other relationship which has been explored is to use the decision tree to guide the design of a neural network. Brent (1991) considers perceptron trees with t splits and so t + 1 leaves dividing f£ into t + 1 regions with piecewise linear boundaries. There is an immediate correspondence between such trees and a neural network with threshold units and two hidden layers of sizes t and t + 1, the first hidden layer corresponding to non-terminal nodes of the tree and the second hidden layer to a path from the root to a leaf with weights zero or ±1. This can be used as a starting point for optimizing the neural network, perhaps with sigmoidal units. Brent minimizes the deviance of a split (in fact using a slightly different form similar to Fisher's exact test), but also uses the fact that the threshold units can be approximated by sigmoidal\", \"exact test), but also uses the fact that the threshold units can be approximated by sigmoidal units of high gain to allow optimization methods on the neural network to approximate finding an optimal split in the sense of deviance reduction. Sethi's (1990, 1991) entropy nets have essentially the same idea. The first hidden layer again computes the splitting functions at the nodes. The second and third layers are of AND and OR nodes respectively, and are wired to produce the same partition as the decision tree. (Note that AND and OR can both be produced by a perceptron.) The second layer corresponds to ANDing conditions down each path, and the OR layer collects paths with the same terminal class. Again, the threshold nodes may be relaxed to have sigmoidal response functions. Notice that although standard neural-network training algorithms could be used, Sethi points out that the derived neural network will be sparsely connected and of a special form. He proposes the use of a simple\", 'neural network will be sparsely connected and of a special form. He proposes the use of a simple reinforcement learning rule. Each node in the AND and OR layers is associated with identifying a single class, since it is part of one path through the tree, and only the weights to these layers are trained, reinforcing the strongest signals amongst nodes associated with the same class. 7.7 Bayesian trees A full Bayesian approach to tree construction will be stymied by the vast number of possible trees, each of which should appear in the posterior average. The best that is possible is to average over a few good trees, which is the approach taken by Buntine (1992).']\n",
      "['242 7 Tree-structured Classifiers The prior has to be given in several steps. A tree is fully specified by its topology (hitherto what we have meant by T ) and the specification of the conditional probabilities. Buntine chooses independent and identical Dirichlet priors at each node, principally for computational convenience. For topologies, one can put a uniform distribution over all possible trees, or over tree shapes (ignoring the choice of attributes), or code for the complexity of the tree. Given the prior and a dataset, it is in principle possible to find the posterior distribution over trees, and sum out over the topologies, giving a posterior distribution over classes for any future example. In practice this is impossible, and Buntine uses a variety of heuristics to grow trees which look to be of high posterior probability, and then averages over those trees. Let B(tiJ, ... ,tiK) = IJr(tij)/r(l::tij) be the normalizing constant in the density of the Dirichlet distribution. Then', \"= IJr(tij)/r(l::tij) be the normalizing constant in the density of the Dirichlet distribution. Then P(T I . . ) P(T) IT B(ntl + tiJ, ... , ntk + tiK) trammg set oc ( ) I B tiJ, ... ,tiK eaves t (7.1) P((ntc) IT, training set) oc IT ( 1 ) IT n~~c+IXc-1. B tiJ, ... ,tiK leaves t classes c Then (ntc) has mode at (ntc + tic)/(nt + 2::: tic), which is (for tic= 1) the smoothing we proposed earlier. Buntine uses (7.1) to suggest an heuristic for measuring the quality of a split, P( ) IT B(ntl+tiJ, ... ,ntk+tiK) test x . . d B(tiJ, ... ,tiK) chil ren Note that this would be appropriate if (7.1) was a product over paths, not leaves. Similarly the shrinking proposed is towards the parent node, whereas (7.1) suggests a shrinking towards tic/ 2::: tij, and this is set from the overall distribution of classes in the training set. Perhaps the most interesting heuristic is the use of 'option trees', in which more than one attribute at each node can be considered, but not simultaneously. Thus not\", 'in which more than one attribute at each node can be considered, but not simultaneously. Thus not only the best one-step lookahead split is chosen, but the best few are kept in play, allowing a range of trees with high posterior probability to be generated. An earlier idea along similar lines is that of Kwok & Carter (1990).']\n",
      "['8 Belief Networks The supervised methods considered so far have learnt both the structure of the probability distributions and the numerical values from the training set, or in the case of parametric methods, imposed a conventional structure for convenience. Other methods incorporate non-numerical \\'real-world\\' knowledge about the subject domain into the structure of the probability distributions. Such knowledge is often about causal relationships, or perhaps the lack of causality as expressed by conditional independence. These ideas have been most explored within the field of expert systems. This is a loosely defined area, and definitions vary: \\'The label \"expert system\" is, broadly speaking, a program intended to make reasoned judgements or to give assistance in a complex area in which human skills are fallible or scarce ... .\\' (Lauritzen & Spiegelhalter, 1988, p. 157) \\'A program designed to solve problems at a level comparable to that of a human expert in a given domain.\\' (Cooper,', 'to solve problems at a level comparable to that of a human expert in a given domain.\\' (Cooper, 1989) \\'An expert system has two parts. The first one is the knowledge base. It usually makes up most of the system. In its simplest form it is a list of IF ... THEN rules: each specifies what to do, or what conclusions to draw, under a set of well-defined circumstances. The second part of the expert system often goes under the name of \"shell\". As the name implies, it acts as a receptacle for the knowledge base and contains instruments for making efficient use of it. These include a short-term memory, tree-searching machinery and a user interface.\\' (Crevier, 1993, pp. 156-7) The last definition is the traditional one in AI, but excludes expert systems based on probabilistic knowledge by assuming that the knowledge base is made up of \\'if ... then rules\\'. Another aspect of expert']\n",
      "[\"244 8 Belief Networks systems which is often stressed is the availability of facilities to provide explanations, usually in the form of a chain of deductions which lead to the conclusion. In the development of probabilistic reasoning by Pearl (1986, 1988, 1993a), Lauritzen & Spiegelhalter (1988) and co-workers, the structural division is slightly different. The knowledge base is represented by a qualitative description of the dependencies (more accurately, lack of dependence) between the variables in the system, and the quantitative description of the numerical values of those dependencies. The role of the 'shell' is taken by the set of algorithms which manipulate the probabilities in an automatic way to present conclusions, such as the posterior probabilities of the various classes. Systems based on these ideas have many names: they have been called Bayesian expert systems, Bayes(ian) net(work)s, belief net(work)s, causal (probabilistic) networks, probabilistic expert systems and\", \"net(work)s, belief net(work)s, causal (probabilistic) networks, probabilistic expert systems and probabilistic reasoning on causal graphs. A very simple example may help to fix ideas. Suppose the input x is a set of m features x1, ... , Xm. As usual, we wish to find the posterior probabilities p(k I x) to classify a future case. The rule called naive or idiot's Bayes (Warner et al., 1961; Titterington et al., 1981) takes m p(k I x) oc nk II p(xi I k). i=l (8.1) One derivation of (8.1) is to assume p(x I k) = f1 p(xi I k ), that is that the features are conditionally independent given the class, from which it is immediate that m p(k I x)p(x) = p(x, k) = p(x I k )nk = nk II p(xi I k ). i=l The qualitative part of the knowledge base is then the assumption of conditional independence, and the quantitative part is the specification of the probabilities nk and Pk(xi) = p(xi I k). The shell is the set of rules for manipulating probabilities (essentially Bayes' formula) plus a Figure 8.1: The\", \"the set of rules for manipulating probabilities (essentially Bayes' formula) plus a Figure 8.1: The causal graph for the idiot's Bayes rule. The arrows represent causal influence. This list of names is not exhaustive.\"]\n",
      "[\"Recent books include Almond (1995), Jensen (1996) and Shafer (1996). 8 Belief Networks 245 user interface. The qualitative part of naive Bayes can be expressed graphically as in Figure 8.1. In the rest of this chapter we will develop more complex networks than Figure 8.1 and corresponding stylized ways to apply Bayes' formula to derive the posterior probabilities, and to modify them as more information becomes available. Overviews of this area with various applications are provided by Spiegelhalter et al. (1993), Andreassen et al. (1991), Charniak (1991) and Neapolitan (1990), as well as by papers within the collections edited by Oliver & Smith (1990), Shafer & Pearl (1990) and Gammerman (1995). Applications in computer vision are described by Agosta (1990), Binford et al. (1989), Levitt et al. (1990) and Rimey & Brown (1992). Various commercial and free shells are available, including BAlES (Cowell, 1992, 1995), Hugin (Andersen et al., 1989), IDEAL (Srinvas & Breese, 1990) and PRESS\", 'BAlES (Cowell, 1992, 1995), Hugin (Andersen et al., 1989), IDEAL (Srinvas & Breese, 1990) and PRESS (Gammerman et al., 1995). Although we work with probabilities, the same calculations can be applied to other measures of belief which satisfy certain axioms (see Section A.4)-see Pearl ( 1988), Dempster & Kong (1988), Shafer & Shenoy (1986), Shenoy et al. (1988), Shenoy (1989) and Shenoy & Shafer (1990)-and also to consistency calculations in computer databases (Fagin, 1977). The methods of this chapter are more complicated than, say, classification trees, so it is worth asking if the ability to feed in qualitative knowledge actually improves the accuracy of classification. Several of the discussants of Spiegelhalter et al. (1993) asked this, specifically in the context of medical diagnosis. Their answer (page 280) is equivocal. Belief networks are designed and trained to answer more than just the question of classifying future cases. They are able to give a much higher level of', 'than just the question of classifying future cases. They are able to give a much higher level of explanation, including exploring what were important input features in reaching the conclusion and whether the input data were in some sense in conflict. To do so they model the whole joint distribution. Although there will be an advantage in using qualitative knowledge (at least if it is a reasonable approximation to reality), the need to model the whole distribution makes more demands on limited data resources. Two restrictions need to be noted. Most of the development of belief networks has been restricted to categorical variables, that is discrete random variables with a finite (and usually small) number of levels; some developments using continuous variables are being made (such as Lauritzen, 1992, and Gammerman et al., 1995). Second, the problem of conditioning belief networks on observations is in general NP-hard (Cooper, 1990) and so the methods described here are potentially']\n",
      "['246 8 Belief Networks prohibitively slow. Fortunately, in many real systems the networks are sparsely connected, and the shells do seem to work quite fast enough. There has been a parallel (and until recently completely separate) development of methods within the field of pedigree analysis in genetics (Spiegelhalter, 1990; Cannings et al., 1978; Cannings & Thompson, 1981; Thompson 1985). Belief networks are often associated with notions of causality. Opinions on the usefulness of this vary from complete scepticism (Speed, 1990) to enthusiasm (Pearl, 1993b, 1995). Since our purpose is prediction of pattern classes, we will avoid discussion of causality except to use known causality to help us specify probability models. In the past directed graphs were widely used because there was perceived to be a problem with zero probabilities in graphical models on undirected graphs. This is inaccurate; these problems disappear when special distributions or (especially) special graphs are', 'inaccurate; these problems disappear when special distributions or (especially) special graphs are considered. Thus we derive most of the methodology in the context of decomposable (undirected) graphs after considering the simple case of a directed tree. 8.1 Graphical models and networks Graphs such as Figure 8.1 are used to represent conditional independence properties on a collection of random variables. It will be important to keep a clear distinction between directed graphs which can represent causality, and undirected graphs without arrows which represent dependence without specifying a causal direction; some terms are used for both with subtly different meanings. Throughout this section we will assume we are given a finite collection of random variables Xv, v E V, and we wish to describe qualitatively the dependencies between these random variables. In our applications these random variables will include the features in the pattern x and the class C. However, they may also', 'random variables will include the features in the pattern x and the class C. However, they may also include unobserved features. For example, an important extension to Figure 8.1 is given in Figure 8.2 where the class is not assumed to be reported accurately, as may be common where diagnosis is difficult. To describe dependence we will use the language of graph theory. This is usually self-explanatory, but more formal treatments can be found in many basic accounts of theoretical computer science, including Knuth (1968), Cormen et al. (1990) and Sedgewick (1990), as well as the specialist books by Berge (1973) and Golombic (1980). Maier (1983) gives a different perspective, that of database theory.']\n",
      "[\"Figure 8.2: The causal graph for the idiot's Bayes rule with inaccurate reporting of classes. Another convention is to call complete subgraphs cliques, when a clique is maximal if no vertex can be added without making the subgraph incomplete. Pedants will call these acyclic directed graphs. 8.1 Graphical models and networks 247 A graph is a collection of vertices and edges. The vertices will represent the set of random variables (hence the use of V to denote the set). The set of edges is a set of unordered pairs of distinct vertices; if an edge is present it is indicated on a diagram by a line (without an arrow) joining the pair of vertices. A path on a graph is list of vertices for which each successive pair is joined by an edge. A subgraph is a subset of vertices together with those edges both of whose vertices are in the subset. A subgraph is said to be connected if there is a path joining every pair of vertices, and complete if every possible edge is present. The maximal complete\", 'every pair of vertices, and complete if every possible edge is present. The maximal complete subgraphs of a graph are called its cliques. A cycle is a path which returns to its origin and visits no vertex more than once. A connected graph with no cycles is called a tree. Where necessary, we will refer to graphs as undirected graphs. Directed graphs also have a set of vertices and edges, but the edges are ordered pairs of vertices, and are represented on a figure (such as Figure 8.2) by lines with arrows from the first vertex to the second vertex. The first vertex is often called the parent and the second (marked by the arrow) the child. The notions of paths and cycles extend immediately to directed graphs. We will make frequent use of directed acyclic graphs, DAGs, that is directed graphs without cycles. A directed tree has the properties that it has one vertex, the root, such that a (directed) path leads from the root to any vertex, and any other vertex has precisely one incoming', '(directed) path leads from the root to any vertex, and any other vertex has precisely one incoming arrow. An ancestral subgraph of a directed graph contains all the ancestors of its vertices; for a directed tree ancestral subgraphs are rooted subtrees. A polytree is a singly-connected DAG, that is a DAG in which at most one path exists between any two vertices (see Figure 8.3).']\n",
      "['248 8 Belief Networks Markov networks The usual way to interpret the (in)dependencies represented by an undirected graph f§ is what Pearl (1988) calls an !-map. Given three subsets A, B, C of vertices, we say C separates A and B in f§ if every path from (a vertex in) A to (a vertex in) B goes through (a vertex in) C. For any subset A of V let XA denote the collection of random variables associated with the vertices in A. Then we consider whether XA and XB are conditionally independent given Xc, which we write as XA ...JL XB I Xc or sometimes A ...JL B I C. We say the graph f§ is an I-map of the distribution if this is true whenever A and B are separated by C. We say the distribution is global Markov with respect to f§ if separation implies conditional independence. Note that the complete graph on V will be an· I-map of any probability distribution on X v, since then there will never be any separating C. This shows that there may be subsets A, B and C with the conditional independence', 'any separating C. This shows that there may be subsets A, B and C with the conditional independence property A ...JL B I C, for which not every path from A to B goes though C. If it is also true that all conditional independencies are represented by separation, Pearl calls the representation by this graph a perfect map. An I-map f§ is called minimal if removing any edge from f§ makes it no longer an I-map. If there is a unique minimal I-map, it is called the Markov network of the distribution. Markov properties of distributions on graphs have been studied in the areas of random fields (Preston, 1974, 1976) and image analysis (Geman & Geman, 1984; Geman, 1990; Isham, 1981). Several Markov Figure 8.3: An example of a polytree. The two concepts are the same, but the graph varies for an 1-map and the distribution for global Markov. Distributions for which there is a Markov network that is perfect are often called graphical models, although this term is also used more loosely.']\n",
      "[\"This example is from Pearl (1988, p. 135). The set V \\\\ {a,b} can be any pair of vertices, and knowledge of any pair of random variables determines which of the three outcomes occurs. Thus conditionally Xa and Xb are constant. 8.1 Graphical models and networks 249 properties have been defined. Denote by oA the boundary of A, the set of vertices in Ac which have a neighbour in A (so all neighbours of points in A are in A U oA ). Then the most important Markov properties for a given graph r'§ are global For any disjoint subsets A, B and C such that C separates A and B (all paths from A to B contain a member of C) we have XA ...JLXB I Xc. local The conditional distribution of Xa given X V\\\\{a} depends only on Xa{a}• or equivalently X a ...JL X V\\\\[{a}uo{a}J I Xa{a}· This is probably easier to describe in words: the random variables at a and those at vertices not connected to a by an edge are conditionally independent given those which are so connected. pairwise Xa and Xb are conditionally\", 'conditionally independent given those which are so connected. pairwise Xa and Xb are conditionally independent given all the other random variables if there is no edge from a to b. These properties allow us to read off successively weaker conditional independence statements from the graph; the global Markov property is equivalent to the graph being an I-map. The three properties can be strictly different. Consider the four discrete random variables with joint distribution a b c d Pr 0 0 0 0 1/3 0 1 1 1 1/3 1 1 0 2 1/3 These are taken as the random variables at the vertices of a graph. The pairwise Markov property is satisfied by any graph on the vertices, even that with no edges. On the other hand, the four random variables are far from independent, and the conditional distribution of Xd given Xa, Xb and Xc depends on at least two of the conditioning random variables. For the local Markov property to hold we have two possible minimal graphs: b c and for both the global Markov property', 'property to hold we have two possible minimal graphs: b c and for both the global Markov property holds. Thus in this example there are two distinct minimal I-maps.']\n",
      "[\"250 8 Belief Networks An example of a local but not global Markov distribution is given by taking the graph a-b c-d and the same non-constant random variable at the four vertices. This is trivially local Markov, but { b} and { c} are separated by f/J, and Xb .lL Xc is false. Our main example demonstrates the inadequacy of the 'obvious' way to construct a minimal I-map, that is to include the edge {a,b} in the graph if and only if X a .f.-xb I X V\\\\{a,b} holds. This is clearly the minimal graph to satisfy the pairwise Markov property, but need not be global Markov. By the following result, if we confine attention to discrete random variables and strictly positive probability distributions this will be an I-map, and therefore the unique minimal I-map. This result has a confusing history; it is often attributed to Hammersley & Clifford in 1971, although they did not publish for nearly twenty years and gave one of a series of increasingly more general statements. Proposition 8.1 Suppose we\", 'years and gave one of a series of increasingly more general statements. Proposition 8.1 Suppose we have a collection of discrete random variables defined on the vertices of a graph. (i) Suppose the joint distribution is strictly positive. Then the pairwise Markov property implies that there are positive functions <Pc, symmetric in their arguments, such that Pr{Xv = xv} oc II </Jc(xc) c the product being over cliques of the graph. (8.2) (ii) A potential representation (8.2) implies the global Markov property for any distribution. Proof: (i) We may assume (by re-labelling if necessary) that each random variable Xs can take the value 0. The proof proceeds by induction on the size of A= {s I Xs =/= 0}, and we prove the existence of functions <Pc (with <Pc = 1 for non-complete C) such that Pr{X = x} = Pr{X = 0} II </Jc(xc). (8.3) CcA This can be reduced to a product over cliques by assigning <Pc to a clique which contains it, and multiplying the original clique function by </Jc. Define the', '<Pc to a clique which contains it, and multiplying the original clique function by </Jc. Define the functions <Pc recursively by </Jc(xc) = Pr{Xc = xc,Xcc = 0} / Pr{X = 0} II </Jv(xv) D~C The Hammersley-Clifford result relates the local Markov property to the global property and to a potential representation. See Clifford (1990) for the published version and historical comment. The rest of this section is rather technical and may be skipped at first reading. Defining </Jc = 1 for non-complete C allows us to take products over all subsets.']\n",
      "['8.1 Graphical models and networks 251 where the product is over strict subsets, and cpc(O) = 1. (Note that <Pc > 0 which avoids having 0 x p/0 in the manipulations that follow.) Clearly (8.3) holds if A is complete, and so holds if A is empty or has one element. Now suppose it holds if A has k or fewer elements. Split a non-complete A with k + 1 members as B U { s} U { t} where B has k -1 elements and s and t are not neighbours. Then Xs .lL Xt I XB,XAc, so Pr{Xv = Xv} = Pr{Xs = X8,Xt = Xt,XB = XB,XAc = 0} = Pr{XB = XB,Xs = X8,Xt = O,XAc = 0} Pr{Xt = Xt I XB = XB,Xs = X8,XAc = 0} X--~------------------------~ Pr{Xt = 0 I XB = XB,Xs = X8,XAc = 0} = Pr{XB = XB,Xs = X8,Xt = O,XAc = 0} Pr{Xt = Xt I XB = XB,Xs = O,XAc = 0} X--7---~~----------~----~ Pr{Xt = 0 I XB = XB,Xs = O,XAc = 0} = Pr{XB = XB,Xs = X8,Xt = O,XAc = 0} Pr{Xt = Xt,XB = XB,Xs = O,XAc = 0} X--~----------------------~ Pr{Xt = O,XB = XB,Xs = O,XAc = 0} = Pr{X = 0} II cpc(xc)ITccBu{t} cpc(xc) CcBU{s} ITccB cpc(xc) = Pr{X = 0} II', '= XB,Xs = O,XAc = 0} = Pr{X = 0} II cpc(xc)ITccBu{t} cpc(xc) CcBU{s} ITccB cpc(xc) = Pr{X = 0} II cpc(xc) CcBU{s}U{t} where we use conditional independence at step 3, (8.3) for sets of size at most k at step 5 and the fact that a complete subset C c A cannot contain both s and t at the last step. This establishes the result for any set A of size k + 1 and completes the inductive step of the proof. (ii) Suppose a potential representation (8.2) is given. Then Pr{Xv} II /\"\" II Pr{XA I XAc} = Pr{XAc} = cpc(Xc) ~ c/Jc(Xc) cliques C XA cliques C di!! c </>c(Xc) / t di!! c </Jc(Xc) CnA# CnA# where we cancel terms for cliques C disjoint from A. A clique with C n A=/= 0 is contained in Au 8A, so the right-hand side is a function of XAuoA and Pr{XA I XAc} = Pr{XA I XaA}· Some of the potentials ¢c may take zero values, but these calculations still hold if we take 0/0 = 0. Now suppose A and B are separated by C. Let B\\' be the set of vertices which can be reached by a path from B which does not', \"separated by C. Let B' be the set of vertices which can be reached by a path from B which does not meet C\"]\n",
      "[\"252 8 Belief Networks and let D = (B' U C)c => A; by construction D, B' and C are disjoint and C separates D and B', so no neighbour of D is in B'. Then Pr{Xv I XB',Xc} = Pr{Xv I Xvc} = Pr{Xv I Xav} does not depend on XB'· Thus D JL B' I C and hence A JL B I C as A c D,B c B'. 0 Some partial relaxation of the positivity condition is possible: see Moussouris (1974), Averintsev (1975) and Ripley & Kelly (1977). Our counter-example on page 249 also shows (with the minimal I-map shown as the right-hand graph) that a distribution can be global Markov but not have a potential representation. An alternative to imposing strict positivity on the distribution is to impose further conditions on the graph. Matus (1992) shows that all three Markov properties are equivalent for any distribution if and only if every sub graph on three vertices contains two or three edges. A graph is said to be triangulated or chordal if every cycle of length four or more has a chord (an edge joining two\", 'be triangulated or chordal if every cycle of length four or more has a chord (an edge joining two non-consecutive vertices), and we will see in Proposition 8.2 that we can construct a potential representation for a triangulated I-map. Conversely, if a graph is not triangulated, it has a chordless cycle of length four or more, and our counter-example (extended if necessary by copies of Xd along the cycle, and constant variables elsewhere) shows a distribution on the vertices of the graph that is global Markov but does not have a potential representation. Thus being global Markov and having a potential representation are equivalent for all distributions on a graph if and only if it is triangulated. We could ask if all conditional independence properties entailed by being global Markov can be read from the graph by separation . Geiger & Pearl (1993) show that this is so, by constructing a strictly positive distribution such that XA JL XB I Xc if and only if C separates A and B on the', \"a strictly positive distribution such that XA JL XB I Xc if and only if C separates A and B on the graph. (The random variables used in this construction do take a finite set of values, but not one that can be specified in advance.) Markov trees If an undirected graph is a tree, any vertex can be declared as the root, and a directed tree formed by assigning arrows to point away from the root (along the unique path from the root to a vertex). The simplest possible tree is one with no branches, that is a chain of vertices. The simplest Markov network is a Markov chain, for which the Markov property is usually stated symmetrically 'past and future are independent given the present' but the usual calculations on a Markov chain depend heavily on the time ordering of the vertices to work either forwards or backwards in If there were a potential representation, Pr{a = O,b = 1, c = O,d = 2} = </>{a,bj(O, 1)</>{a,cj(O, 0) X </>{b,dj(1, 2)</>{c,dj(O, 2) > 0.\"]\n",
      "['Here Sv is any set of values, possibly all possible values. We are excluding evidence such as x. = xb. The rest of this subsection is technical and not needed elsewhere. 8.1 Graphical models and networks 253 time. Most of these methods can be extended to trees, with calculations proceeding up towards the root or downwards away from the root. Although some of the ideas logically belong in the next section, trees are so important an idea that we treat this special case here. Markov chains are usually specified by the transition probabilities Pr{Xt = j I Xt-l = i}, and Markov trees are also commonly specified by giving Pr{Xv I Xa, a the parent of v }. This does specify the joint distribution, since we can label the vertices in increasing order away from the root of the tree so that a vertex is preceded by its parent, and then Pr{Xv}= IlPr{Xi iXj,j<i}= ITPr{XiiXa,a theparentofi} i i (8.4) where at the second step we use the separation of v from the rest of the graph by its parent. The root', \"at the second step we use the separation of v from the rest of the graph by its parent. The root has a special role in (8.4): it has no parent and Pr{Xt} appears without conditioning. In a tree the cliques each contain just one edge, so (8.4) is a potential representation of the distribution (without any positivity condition). Figure 8.2 provides a natural example of a directed tree, with the true class as the root. The calculation we would wish to perform is to condition the distribution by restrictions E = n{ Xv E Sv} on the values at some or all of the vertices (which we think of as the 'evidence'), specifically to find Pr{Xv IE} for one or more individual vertices. There are simple ways to do so making use of the tree structure (Pearl, 1982, 1988). Conditioning on the value taken by Xv has two effects. One is to break the tree at v so we can find the probability distribution of the descendants of v independently of all the rest of the tree. The other effect is to change the\", 'the descendants of v independently of all the rest of the tree. The other effect is to change the distribution of the ancestors of v and all their descendants (that are not direct descendants of v ). We can consider both effects by first propagating messages up the tree, then down from the root. In the following a vertex is considered to be a descendant of itself, but not its own ancestor. We suppose that we have calculated the marginal distributions Pv at each vertex: this can be done by X a where a is the parent of v (and we are given the marginal distribution of the root vertex). We want p~(xv) = Pr{Xv = Xv IE}. For any vertex v let E; denote the conditioning event (if any) on the descendants of v, and let E;; denote the condition on the remaining random variables.']\n",
      "['254 8 Belief Networks Then by Bayes\\' formula using the separation by vertex v. The first term is only non-trivial if v has some children ui. Partition E;; into events concerning the descendants of each child, which are conditionally independent given Xv so Pr{E;; I Xv} = l(Xv E Sv) II Pr{E;; I Xv} child u and Xu which can be computed by a pass over the tree towards the root. Now consider the second term of (8.5). Variables in E: can only influence Xv through its parent Xa, so Pr{Xv I Et} = L Pr{Xv I X a= Xa} Pr{Xa = Xa I Et}. (8.7) X a Now E: includes the restriction on Xa and E;i, plus Eb\" for any other children b of a. We have Pr{Xa I Et} = Pr{Xa I Xa E Sa,Ed\",Eb\" for other children b} oc I(Xa E Sa) Pr{Xa I Ed\"} II Pr{Eb\" I Xa}· (8.8) b The terms in the product will have been found at (8.6) in the inwards pass, and so (8.7) and (8.8) can be computed on a subsequent outwards pass. These computations may also be organized as asynchronous message-passing. Each vertex keeps a current', 'computations may also be organized as asynchronous message-passing. Each vertex keeps a current version of A.(xv) = ITu Pr{E~ I Xv} as a product over its children (empty products being one) and v(xv) = Pr{Xv = Xv IE:}. Then p~(xv) is I(xv E Sv)A.(xv)v(xv) normalized to unit sum. When information becomes available at vertex v, it passes Pr{ E;; I Xv E Sv} to its parent a, which is converted to Pr{ E;; I X a} by (8.6) and used to update A.(xa). It also passes to each child Pr{ Xv I Xv E Sv, E:, Eb\"} where that child is excluded from the b considered. This message is found from (8.8) by re-normalizing. Whenever another vertex receives a message it passes on messages to its other neighbours . It will be helpful to consider an example. The graph is shown in Figure 8.4. There are 12 binary random variables which we label']\n",
      "[\"Figure 8.4: An example of a directed tree. Table 8.1: Specifications of conditional probabilities for the directed tree of Figure 8.4. The two values of the probability refer to the values zero and one of the unspecified variable, in that order. 8.1 Graphical models and networks A ;l\\\\' 0 g 8 Pr{A = 1} = 0.3 Pr{B = 11 A= 0} = 0.9 Pr{B=1IA=1}=0.1 Pr{ C = 11 A = 0} = 0.2 Pr{ C = 11 A = 1} = 0.8 Pr{D = 11 B = 0} = 0.5 Pr{D = 11 B = 1} = 0.4 Pr{ G = 11 B = 0} = 0.7 Pr{ G = 11 B = 1} = 0.2 Pr{K = 11 B = 0} = 0.3 Pr{K = 11 B = 1} = 0.9 Pr{L = 11 C = 0} = 0.1 Pr{L=11C=1}=1.0 Pr{M = 11 C = 0} = 0.0 Pr{M = 11 C = 1} = 0.7 Pr{N = 11 G = 0} = 0.5 Pr{N = 11G= 1} =0.1 Pr{P = 11 G = 0} = 0.1 Pr{P = 11 G = 1} = 0.6 Pr{S =11M= 0} = 0.8 Pr{S =11M= 1} = 0.5 Pr{T =11M= 0} = 0.3 Pr{T =11M= 1} = 0.7 255 by capital letters. Their joint distribution can be specified via the probabilities of Table 8.1. We first calculate the marginal probabilities by a downwards pass in alphabetical order of the vertices. We\", 'calculate the marginal probabilities by a downwards pass in alphabetical order of the vertices. We then are told that C = P = S = 1 and N = T = 0, and asked for the conditional probability that B = 1 (it was 0.66 before conditioning) . The first step is to pass the information up to the root, then down to B. Knowledge of S and T is actually irrelevant, as we know C which separates B from S, T. Propagating N = O,P = 1 to g gives Pr{Eg-I G} = (0.05,0.54), and propagating this to b gives Pr{E; I B} = (0.393,0.148). Propagating C = 1,S = 1, T = 0 to a gives a term Pr{A I Et} = Pr{A I C = 1,S = 1, T = 0} = Pr{A I C = 1} = (0.14, 0.24)/0.38. This is passed down to b, giving Pr{B I Et} = (0.23,0.15)/0 .38 using (8.7). From (8.4) we have']\n",
      "[\"256 8 Belief Networks Pr{B I C = P = S = 1,N = T = 0} oc (0.393 x 0.23,0.148 x 0.15), and finally the desired probability is 0.1972. A further question we can ask is 'what is the most likely explanation of B = 1 ', that is what values of the remaining variables are most likely to have occurred with B = 1. This can be achieved by the same updating procedures, merely replacing averaging over children by a maximum operation (Pearl, 1988, Chapter 5). We can find the most probable configuration with B = 1 by finding both the most probable pattern of descendants and the most probable pattern of ancestors and their other descendants. In our example we are given B = C = 1, so Pr{A I B = C = 1} oc Pr{A}Pr{B = 11 A}Pr{C = 11 A}= (0.126,0.024), so A = 0 is the most probable ancestor. Clearly D = 0 and K = 1 are most probable. Finally Pr{ G I B = 1, N = 0, P = 1} oc Pr{ G I B = 1} Pr{N = 0 I G} Pr{P = 11 G} = (0.040,0.108) and the most plausible explanation is (A= O,C = 1,D = O,G = 1,K = 1,N = O,P\", '= 11 G} = (0.040,0.108) and the most plausible explanation is (A= O,C = 1,D = O,G = 1,K = 1,N = O,P = 1). Another method to organize these calculations is via the joint marginal distributions of each adjacent pair of vertices, which is just the marginal distribution of the parent times the transition probability to the child. When a condition is ·imposed at a vertex, this alters the joint distributions of that vertex and each neighbour. The evidence can then be propagated to each neighbour of that neighbour (and hence all over the tree) by updating the joint density of Xa,Xb as { } { } Prnew{Xa} Prnew Xa,Xb = Prold Xa,Xb X { } . Prold Xa We illustrate this for our example, considering only the vertices abcgnp for compactness. (The distributions of the remaining vertices can be found conditionally on this set quite easily.) Initially the marginal probabilities that Xv = 1 are a b c g n p 0.3 0.66 0.38 0.37 0.352 0.285 and the joint probabilities on the five edges are 00 0 1 1 0 1 1 a-b', '0.3 0.66 0.38 0.37 0.352 0.285 and the joint probabilities on the five edges are 00 0 1 1 0 1 1 a-b 0.07 0.63 0.27 0.03 a-c 0.56 0.14 0.06 0.24 b-g 0.102 0.238 0.528 0.132 g-n 0.315 0.315 0.333 0.037 g-p 0.567 0.063 0.148 0.222 This mode of reasoning is called abductive inference.']\n",
      "['8.1 Graphical models and networks 257 Now we condition on N = 0. This changes the marginal probabilities to 00 0 1 1 0 1 1 g-n 0.4861 0 0.5139 0 Vertex g then sends the message Prnew{G}/Proid{G} = (0.7716, 1.3889) to its neighbours b and p and updates Pr{ G = 1} = 0.5139. Let us consider the effect on p, which is to update the edge marginal to 00 01 10 11 g-p 0.4375 0.0486 0.2056 0.3083 If we now condition on P = 1, we find Pr{ G = 1} = 0.8638. We now update the b-g edge by rescaling by the G marginal. We have not updated it since Pr{ G} = 0.37, so the result is 00 01 10 11 b-g 0.0221 0.5556 0.1141 0.3082 which gives Pr{B = 1} = 0.4223. Updating the a-b edge we find Pr{A = 1} = 0.4780 and 00 0 1 1 0 1 1 a-b 0.1189 0.4031 0.4588 0.0192 Finally, consider conditioning on C = 1. First update the a-c edge to produce 00 01 10 11 a-c 0.4176 0.1044 0.0956 0.3824 then set the entries for C = 0 to zero and renormalize to give Pr{A = 1} = 0.7855. Now we update the a-b edge again to get 00 01 10', 'zero and renormalize to give Pr{A = 1} = 0.7855. Now we update the a-b edge again to get 00 01 10 11 a-b 0.0489 0.1656 0.7539 0.0316 This gives Pr{B = 1} = 0.1972; which is the conditional probability we require. Propagating this ends up with marginal probabilities of a b c g n p 0.7855 0.1972 1 0.916 0 1']\n",
      "['258 8 Belief Networks and the joint probabilities of the five edges are 00 0 1 1 0 1 1 a-b 0.0489 0.1656 0.7539 0.0316 a-c 0 0.2145 0 0.7855 b-g 0.0307 0.7721 0.0533 0.1439 g-n 0.084 0 0.916 0 g-p 0 0.084 0 0.916 The details of such calculations are tedious to do by hand, but lend themselves to automated recursive calculations, and have been used by Binford et al. (1989). Kim & Pearl (1983) (see also Pearl, 1988) extend the principles to polytrees, where a vertex may have more than one parent as well as more than one child. The method fails if applied to DAGs which would have loops if viewed as undirected graphs. There are a few ways to convert such DAGs into polytrees (Pearl, 1988, §4.4): 1 clustering methods, in which vertices are joined into a composite node. For example, if in Figure 8.5 we combine vertices b and c the DAG is converted to a chain (which is of course a tree). We will see a systematic version of this idea in the next section. Increased total serum calcium b', 'We will see a systematic version of this idea in the next section. Increased total serum calcium b Metastatic cancer 2 conditioning, in which the values at one or more vertices are fixed, and the results averaged over the conditioned variables. If in Figure 8.5 we condition on Xa, we have a polytree. 3 simulation methods which we discuss in the next section. Decomposable models The Markov network of a strictly positive distribution has a parametrization via the clique functions cf>c of (8.2). Computations are very much simplified for graphs that have no cycle of four or more vertices without a chord (an edge joining two non-consecutive vertices). Such graphs are called chordal or triangulated. A probability distribution is said to be decomposable with respect to a graph t§ if t§ Figure 8.5: A hypothetical medical belief network (after Cooper, 1984, and Pearl, 1988). This is equivalent to another graph property called decomposability, so these graphs are also called decomposable.']\n",
      "[\"A join tree is also called a junction tree. Blair & Peyton (1993) give an introduction to these ideas, and Lauritzen (1996, Chapter 2) gives a self-contained account of the graph-theoretical properties based on decomposability. si c cj(i) follows from the definition of a join tree:ifvEC1nC for j < i then v E CJ(i)· 8.1 Graphical models and networks 259 is an I-map and triangulated. If 'fJ is omitted, the minimal I-map is assumed (if it is unique). The undirected version of the directed graph in Figure 8.5 is not decomposable, and neither is the right-hand I-map for our counter-example on page 249. For decomposable distributions there is a natural way to specify the joint distribution by 'local' pieces, which we will call a marginal representation. These are the marginal distributions over the cliques, and are much more easily interpreted than general potential representations. Proposition 8.2 A distribution which is decomposable with respect to 'fJ can be written as a product of the\", \"8.2 A distribution which is decomposable with respect to 'fJ can be written as a product of the distributions on the cliques of 'fJ divided by the product of the distributions on their intersections. To prove this we need a few more concepts. A join tree T of cliques is a tree with the cliques as its vertices, such that if we remove all the cliques containing a vertex of V, the tree stays connected. Thus if two cliques contain a common vertex v E V, so does every clique on the path in the tree between them. Every triangulated graph has a join tree (Beeri et al., 1983), and triangulatedness can be tested and a join tree found by the following procedure (Tarjan & Yannakakis, 1984): 1 Order the vertices by maximal cardinality search. Start anywhere, and number next a vertex with the largest number of already numbered neighbours. 2 Starting with the highest numbered vertex, check for each vertex that all its lower-numbered neighbours are themselves neighbours. (By adding missing edges\", 'vertex that all its lower-numbered neighbours are themselves neighbours. (By adding missing edges here, a triangulated graph will be produced.) 3 Identify all the cliques, and order them by the highest numbered vertex in the clique. 4 Form the join tree by connecting each clique to a predecessor (in the ordering of step 3) which shares the most vertices. It can be useful to use the freedom to start anywhere in constructing the join tree, and subsequently to take any clique as its root. Incidentally, since this algorithm labels cliques by a subset of the vertices, there can be no more cliques than vertices in a triangulated graph. This construction orders the join tree so that the unique path from Ct to CJ has cliques in increasing order. For i ~ 2 let j(i) be the number of the predecessor of clique i on the unique path, and let Si = Ci n (Ct U · · · u Ci-d· Then Si c CJ(i), which is called the running intersection property (Beeri et al., 1983). Let Hi = C1 U · · · U C-t and Ri = C \\\\', 'the running intersection property (Beeri et al., 1983). Let Hi = C1 U · · · U C-t and Ri = C \\\\ Si. The clique sequence is also perfect, which means that oRin Hi is a complete subgraph for all i ~ 2.']\n",
      "['260 8 Belief Networks Proposition 8.3 The separator Si separates Ri from Hi\\\\ Si. Proof: Fix a path from Ri to Hi\\\\ Si, and suppose it has a vertex v in RJ for some j > i but none in Uk> J Rt-. Let a and b be the first vertices before and after v which are not in R1. Then a and b are both in oR1 n H1 and hence neighbours. We can construct a connected subset of the path within H1 by omitting the vertices between a and b. By repeating this process we find a connected subset of the original path wholly in Hi+l, and this path will have an edge from a E Ri to bE Hi. We will show that bE Si. Since a and b are neighbours, they both belong to some clique Ck, and since a E Ri, k ~ i. Suppose k > i. Then a, b E Ck n Hk c Cs for some s < k, and by repeating if necessary we find a,b E Cs for some s ~ i. Thus bE Ci n Hi= Si. D Proof of Proposition 8.2: The sets Ri form an ordered partition of V, with UJ<iRi = UJ<i Ci = Hi. so Pr{Xv} = IIPr{XR; IXRp···,XR;_1} = IIPr{XR; IXHJ i i The separation', 'with UJ<iRi = UJ<i Ci = Hi. so Pr{Xv} = IIPr{XR; IXRp···,XR;_1} = IIPr{XR; IXHJ i i The separation property shows that XR; _jl_ XH; I Xs;, so Pr{XR; I XH;} = Pr{XR; I Xs;} and Pr{X v} =II Pr{XR; I Xs;} =II Pr{Xc;} / Pr{Xs;} (8.9) known as the set-chain and marginal representations. If any denominator is zero, the expression is taken as zero. D Note that we have not used a positivity condition here, but (8.9) provides a potential representation. Thus for triangulated graphs we have a potential representation if and only if the global Markov property holds, by Proposition 8.1(ii). The set-chain representation provides a minimal way to specify the joint distribution. It is not the same as specifying the transition probabilities Pr{Ci I CJ(i)} = Pr{Ri I CJ(i)} of the join tree as a Markov tree, since Si = Ci n CJ(i) is strictly smaller than CJ(i)· The fill-in produced by the Tarjan-Yannakakis algorithm is fast to compute, but it may add more edges than are necessary. Algorithms to produce', 'algorithm is fast to compute, but it may add more edges than are necessary. Algorithms to produce minimal fill-ins are known (Rose et al., 1976), but are potentially very slow (this is an NP-complete problem: Yannakakis, This is corollary A.8 of Dawid & Lauritzen (1993), but their proof is skeletal. 1981; Wen, 1990). An approximate algorithm using simulated annealing See the glossary. is described by Kjcerulff (1992).']\n",
      "['We hope the sets are small. This graph bas more edges, so all separation properties on ~6 also hold on ~; thus ~6 is anI-map. 8.1 Graphical models and networks 261 The next proposition shows that we can read off conditional independence properties from the join tree. Proposition 8.4 Suppose CA and CB are sets of cliques in the join tree of a decomposable distribution separated by Cc. Then the sets of variables A = U CA and B = U CB are conditionally independent given the set C = UCc. Proof: It will suffice to show that C separates A\\\\ C and B \\\\ C on the original graph and then use the global Markov property. We prove this by contradiction. Suppose there is a path in V \\\\ C from Yo E A to Ym E B via Yt, ... , Ym-1· Associate Yo with a clique Co E CA and for s ~ 1 associate Ys with a clique Cs that contains both it and Ys-1; as the vertices are not in C, these cliques cannot belong to Cc. The cliques C5_1 and Cs need not be neighbours in the join tree, but because they have Ys in common,', 'cliques C5_1 and Cs need not be neighbours in the join tree, but because they have Ys in common, all cliques on the unique path from Cs-1 to Cs also contain Ys and so are not members of Cc. In this way we can assemble a path in the join tree from Co E CA to Cm avoiding the collection Cc. It is not necessarily the case that Cm E CB, but if it is not there is another clique in CB containing Ym, and we can adjoin the path from Cm to that clique (which again avoids Cc ). This contradicts the separation of CA and CB by Cc, so there is no such path (y). 0 These results show that for decomposable distributions we can work with the sets of variables in the join tree, and for trees the local specification of the distribution is easy. Further, we can update the marginal distributions in cliques by message-passing when conditioning information becomes available. This makes it attractive to work not with a minimal I-map C§ for a distribution, but with a triangulated I-map C§t. formed by', 'work not with a minimal I-map C§ for a distribution, but with a triangulated I-map C§t. formed by triangulating C§. The distribution is decomposable with respect to C§t., and this gives us potential and marginal representations and allows us to compute using the join tree of C§t.. Of course, some of the interpretation is lost, but this does not affect the calculations and the original graph may be used for interpretations. What may make this procedure unattractive is that C§t. may be very different from C§; it could even be complete. Conditioning on evidence Suppose we wish to introduce the evidence E = { Xv E Sv} which is a restriction on the variable at a single vertex v. We will do so by finding the marginal representation of Pr{·, E}. Then summing over any clique gives us Pr{E} and we can divide by this to give the']\n",
      "['262 8 Belief Networks marginal representation of Pr{-I E}. Select a clique Ci that contains v. Suppose we have a potential representation (c/Jc) (for example, the set-chain representation). By setting cpc(xc) = 0 whenever Xv rf. Sv we obtain a potential representation of Pr { ·, E}, and we employ the general procedure below to turn a potential representation into a marginal one. Suppose we have a potential representation (c/Jc). We can allow a little more generality by having functions lps on the separators S and asking that Pr{Xv = xv} =II c/Jc(xc) j II lps(xs) (8.10) c s where 1/0 is taken as 0 (and so if lps(xs) = 0 we can adjust cpc(xc) to be zero). For any neighbouring pair of cliques Ci. Cj with S = Ci n Cj consider the operation of replacing lps by the marginal lp~ of c/Jc; (formed by summing over the variables in Ci \\\\ S) and replacing c/Jci by c/Jci x lJ)s/lJ)s. This step maintains a potential representation. We describe lp~ /lJ)s as the message passed over the edge of the join', 'a potential representation. We describe lp~ /lJ)s as the message passed over the edge of the join tree. (When conditioning a marginal representation, this can be thought of as finding the revised marginal distribution of S, and adjusting the marginal distribution of the adjoining clique to Pr{ Cj I S}Pr{ S}.) Now suppose this step is performed for multiple conditioning events and each edge in the join tree until no further progress can be made. The steps can be organized in many ways (Jensen et al., 1990; Shenoy & Shafer, 1990; Dawid, 1992): an attractive order is to pass messages in to the root and then out to the leaves. When this has been done, we will have the marginal representation. (Formal proofs are given by Dawid, 1992.) By replacing averaging by maximizing in forming lp~ we can find the most plausible explanation just as for Markov trees (Dawid, 1992). A modified averaging (omitting evidence on S ) computes the distribution at each variable conditional on the evidence', '(omitting evidence on S ) computes the distribution at each variable conditional on the evidence everywhere else (Cowell & Dawid, 1992) which is useful in monitoring consistency of information (Spiegelhalter et al., 1993). 8.2 Causal networks We now turn to causal networks, which are defined on DAGs (directed graphs without cycles). Such graphs can always be numbered so that the parent(s) of a vertex have a lower number than the vertex itself. (This is called a topological sort of the DAG.) It is always true (for']\n",
      "['To avoid any measure-theoretic difficulties on the existence of conditional probabilities, we will formally define recursive to mean Xv JLXa<v I pa(v). Pearl (1986, p. 245) incorrectly claims uniqueness. 8.2 Causal networks 263 discrete random variables) that Pr{Xv} =II Pr{Xv I Xa,a < v} (8.11) v but to relate the distribution to the graph we ask that for a recursive model Pr{X v} =II Pr{Xv I X a, a a parent of v }. (8.12) v (We will use pa(v) as a shorthand for this condition.) By summing over Xv in reverse order of the vertices we find from (8.12) that Pr{Xv, v ~ j} =II Pr{Xv I pa(v)} v~j and so being recursive is equivalent to Pr{Xv IXa,a < v} = Pr{Xv lpa(v)}. Conversely, if we are given the conditional probabilities of each variable given its parents, (8.12) can be used to define the joint distribution. This is the main attraction of a causal representation, as the conditional distributions given the parents are often easier to supply than clique potentials. Given a distribution on', 'given the parents are often easier to supply than clique potentials. Given a distribution on a set of vertices, we can ask for what DAGs it is a recursive model. It is certainly recursive for the DAG which makes each vertex a child of all lower-numbered vertices, from (8.11), and we can form a smaller DAG by declaring as parents of vertex v only a subset of earlier vertices on which Pr{Xv I X a, a < v} actually depends. If there are zeroes in the probability distribution then this procedure may not be unique. Consider the counter-example to a potential representation on page 249 in order (a, b, c, d). We find a to be the parent of b, (a, b) to be the parents of c, but for d we have the choice of any two from (a, b, c). If the distribution is strictly positive we can select as parents those vertices b for which Xv ~ XbiXa, a =/= b, a < v (using the equivalence of pairwise and local Markov on {a: a~ v} ). Note that this construction of a DAG depends on the ordering of the vertices,', 'on {a: a~ v} ). Note that this construction of a DAG depends on the ordering of the vertices, whereas the notion of a recursive model does not: some orderings may produce much simpler DAGs than others. It is here that causality is used to select a beneficial ordering. (Todd, 1995, illustrates this for the example of Lauritzen & Spiegelhalter, 1988, by choosing a different set of causalities.)']\n",
      "['264 8 Belief Networks a 2 To make use of the machinery for Markov networks (particularly on decomposable graphs) we would like to turn a recursive model on a DAG into a Markov network. It does not in general suffice to use the graph formed by dropping directions; Figure 8.6 will help illustrate why. Suppose all the variables are binary, with Pr{Xa = 1} = Pr{Xb = 1} = 0.3 + 0.4Xt, Pr{X2 = 1} = 0.3 + 0.4/(Xa = Xb) and Pr{X3 = 1} = 0.3 + 0.4X2. Then Xa _jl_ xb I xl, but Xa .t xb I X!,X2, whereas on the undirected graph {1,2} separates a and b. Clearly two vertices that have a common child need to be joined in the undirected graph. This is reflected in the idea of the moral graph (Lauritzen & Spiegelhalter, 1988). The steps to convert the DAG to its moral (undirected) graph are 1 replace all the directed edges by undirected ones and 2 add edges joining the parents (in the DAG) of each vertex if neces-sary. The neighbours of a vertex in the moral graph are its parents and children in the', \"if neces-sary. The neighbours of a vertex in the moral graph are its parents and children in the original DAG, plus the other parents of those children. (Pearl calls the other parents of a vertex's children its mates.) Proposition 8.5 A recursive model on a DAG is global Markov and has a potential representation on the moral graph of its DAG. Proof: We will show the existence of a potential representation; by Proposition 8.1(ii) the distribution must be global Markov. Start by setting the potential for each clique to one. For each vertex v, select a clique which contains it and all its parents and multiply its potential by Pr{Xv I Xa, a a parent of v }. (There may be more than Figure 8.6: An example to illustrate conditional independence on a DAG. There must be such a clique: the graph is moral.\"]\n",
      "['This subsection is technical and not used elsewhere. Spirtes et a/. (1993) call this causal Markov. We can replace nd( v) by nd(v) \\\\ pa(v) to give disjoint sets of variables. 8.2 Causal networks 265 one choice of clique, in which case any will do.) This converts (8.12) to a potential representation by grouping terms into cliques. D Note once again that no positivity condition is required. The moral graph is then triangulated by filling in, if necessary, and converted to a join tree of its cliques, to which the methods of Markov trees can be applied. By Proposition 8.2 the joint distribution can be specified by giving the marginal probability distributions for each clique. Originally we had (8.12), which may seem to be the easiest way to specify the joint distribution. However, having the clique marginals allows us to specify the marginal distribution of each random variable. Further, the distribution will change as information becomes available. Consider Figure 8.2 on page 247.', 'the distribution will change as information becomes available. Consider Figure 8.2 on page 247. Initially the distribution can be specified conditionally on the true class. However, once some of the variables and/or the reported class are known, we will wish to find the conditional probability of the true class. In this case the moral graph comes from dropping the arrows, and any ordering that has the true class first or second may be used. The cliques are all the edges individually, and can be in any order. In this case the DAG is already a tree and can be used directly. Markov properties on DAGs We have seen that a recursive model induces a global Markov distribution on the moral graph. Because there can be some in-filling at moralization, the conditional independencies implied by the moral graph may not include all those implied by the original recursive model. Consider Figure 8.6; its moral graph is the same as that of the DAG with a link from a to b, and on the latter DAG Xa', 'its moral graph is the same as that of the DAG with a link from a to b, and on the latter DAG Xa ..fl.-Xb I X1 is allowed. Of course, the conditional independencies still hold but are encoded in the numerical values of the particular representation used. In-filling to triangulate the moral graph can produce further masking of conditional independence properties; this is tolerated for the simplification in computation that results. This has aroused interest in looking directly at Markov properties on a DAG. Several definitions of directed Markov are in use. That of Dawid & Lauritzen (1993) is Xv .JL nd(v) I pa(v) for all v E V where nd( v) is the set of variables whose vertices are not descendants of v. (This is called directed local Markov by Lauritzen et al., 1990.) The definition of Lauritzen (1989) and Lauritzen et al. (1990) (directed']\n",
      "['266 8 Belief Networks global Markov) is that XA JL XB I Xc whenever C separates A from B in the moral graph of the smallest ancestral DAG containing A, B and C. This resolves the difficulty with Figure 8.6. We know that Xa JL Xb I X1, but a and b are joined in the moral graph. They are not however joined in the moral graph of the subgraph on vertices {a,b, 1}, which is ancestral. That we have to take an ancestral subgraph is shown by noting that Xa _j._ Xb I X3; the dependence arises through the ancestor X2 of X3. Lauritzen et al. (1990) show that their two definitions of directed Markov are equivalent to each other and to being a recursive model. It is obvious that directed global Markov implies directed Markov which implies recursive. For any moralized ancestral DAG (8.12) provides a potential factorization, which by Proposition 8.l(ii) implies the distribution is global Markov on the moral graph hence XA JL XB I Xc. Another proof is given in Propositions 8.6 and 8. 7 which also', 'the moral graph hence XA JL XB I Xc. Another proof is given in Propositions 8.6 and 8. 7 which also applies to continuous random variables (without assuming a joint density). Pearl (1986, 1988) finds conditional independence properties via a more complicated notion of separation for DAGs, called d-separation. A DAG is said to be an I-map of a distribution if XA JL XB I Xc whenever C d-separates A and B. To define d-separation, consider a trail from A to B on the DAG, following edges in either direction. At each vertex inside the trail there will be two arrows. A trail is blocked by C at a vertex v if the two edges at v either (i) do not have converging arrows and v E C, or (ii) do have converging arrows and neither v nor any of its descendants are in C, and A and B are said to be d-separated by C if every trail from A to B is blocked by C at some internal vertex. Consider once again the example of Figure 8.6. Then Xa JL Xb I Xt follows from the dseparation of {a} and {b} by {1}, but', 'example of Figure 8.6. Then Xa JL Xb I Xt follows from the dseparation of {a} and {b} by {1}, but {a} and {b} are not d-separated by {1, 2} or {1, 3}, as the trail a-2-b is no longer blocked. The d-separation condition for independence (empty C) is that A and B have disjoint ancestral sets, which is as we would expect. For a DAG to be an I-map for a distribution is equivalent to the distribution being directed global Markov by the following proposition, and so is equivalent to being a recursive model. Proposition 8.6 (Lauritzen et al., 1990) For sets A, B and C of vertices, d-separation of A and B by C is equivalent to the separation of A and B by C in the moral graph of the subgraph of ancestors of A U B U C. Kiiveri et al. (1984) have other definitions of local and global Markov properties on a DAG. Spirtes et al. (1993) call a vertex with converging arrows a collider.']\n",
      "[\"8.2 Causal networks 267 Proof: Let d be the moralized ancestral subgraph . Suppose A and B are separated by C in d. We will show that A and B are d-separated by C. Consider a trail from A to B in the DAG. First we show that any trail which is not wholly in d is blocked. Follow the trail from A, and let a be the first vertex not in d. By the ancestral property, the arrow on the edge leaving d must point to a. Only descendants of a can be reached by continuing the trail without converging arrows, and these cannot be in d (or their ancestor a would also be). To reach B the trail must have a pair of converging arrows at a vertex outside d, and will be blocked at that vertex since it is not an ancestor of any vertex in C (as all such vertices are in d). Second, consider a trail from A to B on the DAG with all vertices in d. If this contains a vertex with converging arrows, the predecessor and successor will have been 'married' in the moral graph, and so by replacing the converging arrows by\", \"successor will have been 'married' in the moral graph, and so by replacing the converging arrows by the added edge we can find a path in the moral graph d which avoids vertices with converging arrows. This path must contain a vertex from C, and hence the trail on the DAG must be blocked at a vertex without converging arrows. Conversely, suppose A and B are not separated by C in d. Take a path from A to B which does not meet C, and form a trail on the DAG by replacing any edges formed at moralization by a diversion to their common child (which is in d). Suppose this trail is blocked; this can only happen at a vertex a with converging arrows and no descendants in C. Since the vertex is in d, it must have a descendant in either A or B. Thus we can follow the trail from A to a and then follow its descendants to B, or from B to a and then via descendants to A. This gives us a trail with strictly fewer blocking vertices. By repeating the process we can find an unblocked trail in the DAG\", \"fewer blocking vertices. By repeating the process we can find an unblocked trail in the DAG from A to B. 0 These results show that for a recursive model on a DAG we can read off conditional independence properties by d-separation in the DAG or separation in the ancestral moral graph. Can all conditional independencies of sets of variables be found in this way? The general answer must be 'no', as conditional independencies could be encoded in the numerical values of the distribution, but Geiger & Pearl (1990) construct a recursive model for which any sets of variables which are not d-separated are not conditionally independent. Thus no other conditional independencies can be found from the DAG alone. If we know that some relations between variables are deterministic (occur with probability one) we can extract more information by D-separation,\"]\n",
      "[\"268 8 Belief Networks which differs from d-separation by also blocking a trail at a vertex whose variable is determined by Xc (Geiger et al., 1990). Proposition 8.7 (Verma & Pearl, 1990) If a distribution is a recursive model on a DAG, that DAG is an !-map for the distribution. Proof: The proof is by induction on the size of the DAG ~ : the result is trivial if the DAG has only one element. Let w be the last vertex in the DAG (and hence has no children), and suppose the result is true for the subgraph ~' omitting w. Our distribution is still recursive on ~'. Let A, B, C be disjoint sets of vertices such that A and B are d-separated by C in ~; we will show A .JL B 1 C. Any trail with w as an internal vertex necessarily has converging arrows at w. Consider three exhaustive cases. (a) The vertex w f. AU B U C. Every trail including w is blocked, so A and B are d-separated by C in ~' and A .JL B I C. (b) Suppose that vertex w is in either A or B; we will assume w E A, and let A' = A \\\\ { w\", \"B I C. (b) Suppose that vertex w is in either A or B; we will assume w E A, and let A' = A \\\\ { w }. Note that A' is d-separated from B by C in ~'. Now w has no parents in B (or there would be a one-step trail from A to B). Let P be the set of parents of w which are not in C; P is also d-separated from B by C in ~'. (Any trail from P to B either goes through w and is blocked, or can be extended to a trail from w to B which is not blocked at the added internal vertex in P.) Thus A' U P .JL B I C and for a recursive model { w} .JL B I A' U C U P (since the conditioning set includes all the parents of w ). These imply AU P .JL B I C (see (A.4)) hence A .JL B I C. (c) We have w E C, so no trail can be blocked at w. It follows that A and B are d-separated by C' = C \\\\ { w}. Now w must be d-separated from either A or B by C' or there would be an unblocked trail from A to B via w. Suppose this is true for B, so A u { w} and B are d-separated by C'. We have AU{w} .JL B I C' by case (b), which\", \"is true for B, so A u { w} and B are d-separated by C'. We have AU{w} .JL B I C' by case (b), which implies A .JL B I C' U {w} = C (see (A.3)). D Calculations on moral graphs There are several calculations we may wish to perform on the joint distribution, but the most common are to condition on observed variables and to marginalize , especially to find the distribution over the true class. As we have seen, it will be normal to start with the conditional probability tables (8.12), which give a potential representation (see the\"]\n",
      "[\"8.2 Causal networks 269 proof of Proposition 8.5). This may be converted to a marginal representation by the standard message-passing algorithm (page 261), which also allows us to condition a marginal representation on evidence of the form E = n{xv E Sv}. In general finding a marginal distribution of a subset of random variables is difficult, but it is immediate from the marginal representation if the subset is contained in some clique. Fortunately we will mostly be interested in marginals of a single variable, and then will have at least one clique to choose from. There is a trick (Pearl, 1988, §3.5.3; Jensen, 1991) to find the probability of a specific event of the form E = n{ Xv E Sv }, as the conditioning procedure in the join tree on E without normalization finds Pr{', E}, and this can be summed over any clique to find Pr{E}. (This is done in the example which follows.) An example We give an entirely fictitious example from medical diagnosis with eight vertices presented in Figure\", 'give an entirely fictitious example from medical diagnosis with eight vertices presented in Figure 8. 7 and Table 8.2. The specification of the distribution via the conditional probability tables is p(A) p(B) p(C) p(DIA,B) p(EIA) p(FIC) p(GID,E) p(HID,F). To form the moral graph (Figure 8.8) we have to join a to b, d to e and d to f; this is already a triangulated graph. The cliques are abd, ade, cf, deg and dfh. One ordering by maximum cardinality search, starting from a, for the vertices is abdegfhc and for the cliques is C1 = abd, C2 = ade, C3 = deg, C4 = dfh, Cs = cf. The separators are then S2 = ad, S3 = de, S4 = d, Ss = f and so the marginal representation lS p(A,B,D) p(A,D,E) p(D,E, G) p(D,F,H) p(C, F) p(A, D) p(D, E) p(D) p(F) In forming the join tree we have considerable freedom, as C4 can be linked to any of its predecessors. One choice (joining C4 to C3 ) would make the join tree into a chain, but we chose the tree shown in Figure 8.9. The marginal probabilities may be', 'tree into a chain, but we chose the tree shown in Figure 8.9. The marginal probabilities may be found from p( CJ) = p(D I A, B)p(A)p(B), p(C2) = p(E I A)p(S2), p(C3) = p(G I D, E)p(S3), p(Cs) = p(F I C)p(C) and p(C4) = p(H I F,D)p(Ss)p(S4). We find, writing tables in lexicographic order (false before true, last index varies fastest).']\n",
      "['270 8 Belief Networks Smoker Unfit Figure 8.7: The DAG a for an artificial medical Breathless diagnosis problem. g Family history of heart disease b Chest pains Poor diet Indigestion ~ f Pr{A} = 0.5 Pr{B} = 0.2 Pr{C} = 0.3 Table 8.2: The Pr{ D IF, F} = 0.05 Pr{D IF, T} = 0.3 Pr{D I T,F} = 0.2 Pr{D IT, T} = 0.5 conditional probability Pr{E IF}= 0.3 Pr{E IT} = 0.5 tables for the DAG of Pr{F IF} = 0.1 Pr{F IT} = 0.4 Figure 8.7. For each Pr{ G IF, F} = 0.01 Pr{ G IF, T} = 0.5 Pr{ G I D = T} = 1 vertex the condition is Pr{H I F,F} = 0 Pr{H IF, T} = 1 Pr{H IT, F} = 0.5 Pr{H IT, T} = 1 on its parents in alphabetical order. Smoker Unfit Figure 8.8: The moral a graph associated with Breathless Figure 8.7. g Family history of heart disease b Chest pains h Poor diet Indigestion f abd ad ade de Figure 8.9: The clique Ci d c; tree associated with <:; Figure 8.8. The separators S; are d.fh marked on the edges. C4']\n",
      "['8.2 Causal networks 271 ABO 0.38 0.02 0.07 0.03 0.32 0.08 0.05 0.05 AD 0.45 0.05 0.37 0.13 ADE 0.315 0.135 0.035 0.015 0.185 0.185 0.065 0.065 DE 0.50 0.32 0.10 0.08 DEG 0.495 0.005 0.16 0.16 0 0.1 0 0.08 D 0.82 0.18 CF 0.63 0.07 0.18 0.12 F 0.81 0.19 DFH 0.6642 0 0 0.1558 0.0729 0.0729 0 0.0342 Note the treatment of clique C5 = cf; we need the marginal for f, and this demands that we process C5 before C4. It is natural to think of C4 depending on C5, but to produce a tree we have to label the edge in the opposite direction. We can also find the initial marginal probabilities via a potential representation and message-passing . Suppose we take initial potentials as p(A)p(B)p(D I A, B), p(E I A), p(G I D, E), p(H I F,D) and p(F I C)p(C). Message-passing then multiplies <Pc2 by p(A, D), <Pc3 by p(D, E) and <f>c4 by p(D) and by p(F) to form the marginal representation. Suppose a patient presents symptoms of breathlessness and chest pains, and is a smoker. What is the probability of heart', 'symptoms of breathlessness and chest pains, and is a smoker. What is the probability of heart disease? We condition on the evidence A = G = H = T. We iilustrate the messagepassing approach by sending messages to C1 = abd at the root of the tree. We enter G = H = T at cliques C3 and C4. For A = T we have a choice of cliques, and choose Ct. We start at C4 with a message over S4 = d to C1, the new-to-old ratio of the separator distributions. DFH 0 0 0 0.1558 0 0.0729 0 0.0342 D 0 .1558 0.1071 msg 0.1558/0.82 = 0.19 0.1071/0.18 = 0.595 Clique C3 sends a message over S3 = de to C2 : DEG 0 0.005 0 0.16 0 0.1 0 0.08 DE 0.005 0.16 0.1 0.08 msg 0.005/0.5 = 0.01 0.16/0.32 = 0.5 1 This is then incorporated into clique c2 and a message sent over S2 =ad to Ct: ADE 0.00315 0.0675 0.035 0.015 0.00185 0.0925 0.065 0.065 AD 0.07065 0.05 0.09435 0.13 msg 0.157 1 0.255 1']\n",
      "['272 8 Belief Networks Clique C1 then incorporates two messages and its own constraint. We need only give the results for A = T : BD 0.32 · 0.19 · 0.255 0.08 · 0.595 · 1 0.05 . 0.19 . 0.255 0.05 . 0.595. 1 0.015504 0.0476 0.002423 0.02975 D 0.017927 0.07735 so Pr{Evidence} = 0.095277 and Pr{D I Evidence}= 0.812. As C 1 is the root, we should then pass messages back down the tree, but our question has already been answered from the marginal in C1. It will be convenient at this stage to normalize, that is to divide the marginal distribution C1 by Pr{Evidence}. The messages sent to C2 and C4 are then approximately AD 0 0 0.188/0.09435 0.812/0.13 D 0.188/0.1558 0.812/0.1071 This modifies the clique marginals to (approximately) ADE 0 0 0 0 0.0037 0.1843 0.406 0.406 DFH 0 0 0 0.188 0 0.553 0 0.259 These send messages to C3 and Cs of DE 0.00037/0.005 0.1843/0.16 0.406/0.1 0.406/0.08 F 0.553/0.81 0.447/0.19 and those cliques can be updated to DEG 0 0.00037 0 0.1843 0 0.406 0 0.406 CF 0.430', \"0.447/0.19 and those cliques can be updated to DEG 0 0.00037 0 0.1843 0 0.406 0 0.406 CF 0.430 0.165 0.123 0.282 After these calculations it is often easy to answer further questions. Suppose we discover that the patient's family has a history of heart disease. This amounts to new . evidence that B = T. Rather than go through the full procedure of propagating messages, we can just examine the marginal distribution of C1 to see that the conditional probability of heart disease is now 0.02975/(0.002423 + 0.02975) ~ 0.925. Similar calculations will often allow us to evaluate the value of 'buying' various items of new evidence, and so decide which to obtain (Lauritzen & Spiegelhalter, 1988, §5.5; Jensen & Liang, 1995). Mixed models Lauritzen (1989, 1992) and Olesen (1993) consider analogues of these procedures for conditional Gaussian distributions (page 41). The distinction between discrete and continuous variables leads to consideration of marked graphs (with the marks differentiating\", 'and continuous variables leads to consideration of marked graphs (with the marks differentiating discrete and']\n",
      "['The connection to the Gibbs sampler was made by Hrycej (1990), Chavez & Cooper (1990), Shachter & Peot (1990), and, more elegantly, by York (1992). This too has been considered in genetics: Ott (1989), Ploughman & Boehnke (1990) and Kong (1991). 8.2 Causal networks 273 continuous random variables) and stronger concepts of decomposability and moralization. There is a considerable literature about multivariate normal belief nets, but for pattern recognition we need at least one discrete variable, the true class. Simulation-based calculations The calculations of the previous subsection work well for sparselyconnected graphs with discrete random variables which take a small number of values. The approach can be extended to conditional Gaussian distributions (page 41) when the evidence on continuous variables is a precise value (Gammerman et al., 1995), but most attempted extensions run into overwhelming computational complexity. Henrion (1988) (and Henrion et al., 1991) used a stochastic', 'overwhelming computational complexity. Henrion (1988) (and Henrion et al., 1991) used a stochastic simulation method he called (probabilistic) logic sampling. This uses (8.12) for unconditional simulation of the whole collection of random variables. This is easy; we move through the DAG in vertex order, at each stage sampling conditionally on the values at the parents of the current vertex (and these values must be already known). To condition, run many simulations and only keep those which are consistent with the conditions, using these to compute frequencies of any events of interest. Provided the evidence has positive probability this will work, although if the probability of the evidence is low, only a small proportion of the runs will be retained, and so the process may be slow. If we need to condition on point values of continuous variables, this approach will be impossible (or if we replace the point value with a small interval, possible but impracticably slow). Pearl (1987)', \"if we replace the point value with a small interval, possible but impracticably slow). Pearl (1987) suggested the use of iterative simulation methods such as the Gibbs sampler (see Section A.3). This approach had previously been used in image analysis and has since become popular in mainstream Bayesian statistics. These methods are now often called MCMC for 'Markov chain Monte Carlo'. We have a collection of random variables on a graph; as in the join tree each random variable might itself be a collection. For the Gibbs sampler we pick a vertex, and sample from Xv conditional on all the random variables at all the other vertices. For a local Markov distribution (to which we restrict attention) the conditional distribution depends only on the values of the random variables at neighbouring vertices. (We have already seen that the neighbours in the moral graph consist of the parents, children, and other parents of those children.) If the graph is sparsely connected, this may be a small\", 'and other parents of those children.) If the graph is sparsely connected, this may be a small enough set for the conditional simulation to be']\n",
      "['274 8 Belief Networks performed quite easily. To actually run the Gibbs sampler, vertices are picked sequentially, either at random or in some pre-assigned order. Conditioning is easy for the type of evidence we are considering; each random variable is simulated conditionally on the event(s) concerning it. This seems a very attractive method. If we want some aspect of the distribution of a subset of random variables, all we need to do is to simulate the whole set, and compute the frequency of the desired event(s). By taking a large enough sample we can compute the desired probabilities to any accuracy required. The difficulties are • we have to run the Gibbs sampler long enough to ensure that we have a close enough approximation to the asymptotic distribution, and • we need independent or close-to-independent samples from the distribution, which we can achieve by taking samples sufficiently far apart in the run of the Gibbs sampler. These can be achieved, but the convergence can be', 'far apart in the run of the Gibbs sampler. These can be achieved, but the convergence can be extremely slow; Ripley & Kirkland (1990) give a dramatic example in which equilibrium has not been approached after each random variable in the system has been sampled 10,000 times. In fact the second point is unnecessary in our application if we just count to obtain the frequency of events, since we are estimating an expectation, and expectations are additive. Thus N ~ LI[X~) E E] t=l will be an unbiased estimator of Pr{ E} provided the process is started in the equilibrium state (by a sample from the correct distribution). However, it remains true that the estimator will be very variable unless the process has been run many times longer than the interval between approximately independent samples, and that we will have to discard an initial part of the run to combat the first point. The Gibbs sampler step applied to the moral graph obtained from a recursive model needs a method to sample from', \"step applied to the moral graph obtained from a recursive model needs a method to sample from the distribution at a vertex conditional on the parents, children and 'mates' of the vertex, plus any evidence restriction on the random variable. It is easy to see that the conditional density (ignoring any evidence) is of the form p(Xv I Xa{v}) oc p(Xv I X a, a a parent of v) X IT p(Xc I Xa, a a parent of c). (8.13) children c For this problem it is known that there are better MCMC samplers than Gibbs (Peskun, 1973), but the simplicity of Gibbs sampling may prevail.\"]\n",
      "['8.3 Learning the network structure 275 (Pearl, 1988, p. 218, gives a formal proof, but this is immediate.) This can be an awkward distribution to sample by standard rejection sampling (Ripley, 1987), but it is easy to sample from the first term, then use rejection sampling on each of the child terms separately. (Gammerman et al., 1995, derive this by a very indirect route involving auxiliary variables; Besag & Green, 1993.) If evidence is involved at the node this can be incorporated by conditioning in the simulation given the parents. An attraction of the Gibbs sampler to computer scientists is its intrinsic parallelism. We can update the sample at vertices which are not neighbours simultaneously; samples can also be updated by different processors asynchronously, at least if we ensure that messages are received from neighbours before processing begins. There are messagepassing policies which prevent simultaneous updating of neighbours without needing a central controller; some are', 'which prevent simultaneous updating of neighbours without needing a central controller; some are sketched in Pearl (1988, pp. 219-222). Simulated annealing (Ripley, 1987; Aarts & Korst, 1989) can be used to find the most plausible explanation (the most probable combination of other variables given those observed) but is likely to be very slow to do so. (This method was suggested by Hrycej, 1992, Theorem 11.2.3.) There is no reason why Gibbs sampling should be applied to a single variable at a time, and blocked variants have been explored in image analysis, in which a group of variables is sampled given its neighbours; the idea being that although sampling may be more difficult, the process may traverse the sample space more rapidly and so converge to equilibrium faster. Choosing suitably sized groups of variables is a black art at present. 8.3 Learning the network structure So far we have assumed that the graph or DAG was given by the experts. Is there any hope of inducing the network', 'assumed that the graph or DAG was given by the experts. Is there any hope of inducing the network from examples? Several approaches have been pursued. From the traditional statistical viewpoint an undirected graphical model represents a restriction of a full dependence, for example (and most commonly) a log-linear model for a contingency table. The modelbuilding strategy is to move around the space of possible models, and eventually to select a small number of simplified models which are complex enough to explain the patterns in the examples, yet have']\n",
      "['276 8 Belief Networks comprehensible interpretations. (It is this last point which often leads to a consideration of only subsets of graphical models; Edwards & Havarnek, 1985; Upton, 1991.) The strategy can involve either a series of hypothesis tests or searching to minimize a penalized fit criterion such as AI C. (Lauritzen et al., 1994, give a case study of both for the CHILD network of Spiegelhalter et al., 1993.) This approach usually involves a great deal of user control, although it is beginning to be partially automated. Since interpretability is paramount, a domain-expert will always be needed to monitor the process. A variant on the testing approach is to use a series of tests for conditional independence to find the Markov boundary of each vertex (those vertices on which Pr{ Xv I X V\\\\{v}} depends), and then construct a graph with respect to which the distribution is local Markov (Fung & Crawford , 1990). There will be very many tests, some of which will give the wrong answer', '(Fung & Crawford , 1990). There will be very many tests, some of which will give the wrong answer by chance. One way to combat this problem of multiple comparisons (suggested by Fung & Crawford, 1990) is to use the significance level a of the tests as a parameter of the procedure, and select it by cross-validating a relevant measure of performance. Many fewer tests would be needed to establish a graph with respect to which the distribution is pairwise Markov, since we need only test for each pair of vertices. Each test will be a test of conditional independence in a many-way contingency table, so there will difficulties with sparse tables unless the training set is large. The Fung-Crawford approach has the advantage of allowing small sets of neighbours to be considered first. Another tradition , from the social sciences, for inferring a causal network from data (often in a regression setting) is represented by Glymour et al. (1987) and Spirtes et al. (1993). Finding a DAG makes the', 'setting) is represented by Glymour et al. (1987) and Spirtes et al. (1993). Finding a DAG makes the problem considerably harder, although it does allow the specification of smaller conditional probability tables, and the deduction of causal relationships. An edge between a and b will be absent in the DAG if there is a set S such that X a ...JL Xb I Xs, so a search over subsets is needed. Since we know that the undirected version of the DAG is a subgraph of the moral graph, finding a graph with respect to which the distribution is local Markov provides a good starting point. The main result of Spirtes et al. (1993), their Theorem 3.4, applies only to a subclass of possible distributions (those with some DAG as a perfect map) and for the example on page 249 their procedures induce a graph with no edges. (They make no attempt to check their condition in their examples, but it would be straightforward to check that the data distribution is recursive on the induced DAG.) There are', 'be straightforward to check that the data distribution is recursive on the induced DAG.) There are proposals to use these methods in insurance, where a database of millions of cases is available. In the regression setting this is called path analysis. If a follows b in the order then we can take S to be the parents of a.']\n",
      "[\"8.3 Learning the network structure 277 Markov trees Chow & Liu (1968) (see also Pearl, 1988, Chapter 8) extend the contingency-table approach by seeking the belief tree that best approximates (in Kullback-Leibler divergence) a general multi-way discrete distribution. They show that: • given the tree topology, the conditional distribution of each node given its parent in the best approximation is that in the original distribution, and • the best tree is any minimal spanning tree with distance between a and b given by the mutual information of (Xa,Xb), Since there are efficient algorithms for finding minimum spanning trees (Cormen et al., 1990, Chapter 24; Sedgewick, 1990, Chapter 31) this allows a best approximating Markov tree to be constructed. The proof of these two properties is easy. We assume for simplicity that we have discrete random variables. Given a tree, let p(i) denote the parent of vertex i (empty for the root). Any recursive model P' can be expressed as P'{X} =II\", \"the parent of vertex i (empty for the root). Any recursive model P' can be expressed as P'{X} =II P'{XiiXp(i)} i from (8.12) on specializing to a tree. Now consider the Kullback-Leibler divergence between the true distribution P and a recursive model P' : d(P,P') = I:P{xv}log [P{xv}/P'{xv}] xv = I: P{xv} logP{xv}- I:P{xv} I: logP'{xi I Xp(i)} xv xv i xv j X;, Xp(i) = I:P{xv}logP{xv} xv -I: I: P { Xp(i)} [I: P {xi I Xp(i)} log P' {Xi I Xp(i)} J. i Xp(i) X; This can be minimized by maximizing each term in square brackets; the maximum occurs at P'{xi I Xp(i)} = P{xi I Xp(i)} and the minimized\"]\n",
      "[\"278 8 Belief Networks divergence is d(P,P') = LP{xv}logP{xv}- LP{xv} L::logP{x;IXp(i)} xv xv i = Ll(X;,Xp(i)) + LP{xv}logP{xv}- LLP{xi}logP{x;}. i XV i Xi A tree can always be turned into a causal graph by choosing a root and directing edges away from the root. This can be convenient in specifying the conditional probabilities, especially if the root is chosen judiciously. However, if we are inducing structure from data, the conditional probabilities will normally be estimated from the same dataset. In that case the unknown true distribution is replaced by frequencies in the training set, and the simplest way to specify the Markov tree is via a marginal representation , with Pr{Xa, Xb} for adjacent vertices estimated by the frequency in the training set. (The consistency conditions are automatically satisfied.) This can also be seen as fitting an unrestricted Markov tree to the training set by maximum likelihood . Priors for belief networks The full Bayesian procedure is to put a prior\", 'by maximum likelihood . Priors for belief networks The full Bayesian procedure is to put a prior on the topology of belief networks, and then average the results over belief networks weighted by their posterior probabilities. This principle has been pursued by Cooper & Herskovits (1992), in tandem with learning the conditional probability tables from data. There is an enormous number of different networks; for example Cooper & Herskovits (1992, p. 319) quote approximately 4.2 x 1018 for ten vertices. Cooper & Herskovits make some very strong assumptions on the prior on topologies (such as a uniform distribution) to simplify computation. All such assumptions are unrealistic, as considering that set of vertices implies a belief in a sparsely connected network. (However, the prior may be swamped by the data and so be practically irrelevant.) My own prior might often be approximated by one on the number of edges. The full Bayesian approach is normally computationally impossible as we', 'one on the number of edges. The full Bayesian approach is normally computationally impossible as we cannot average over all topologies. Full averaging can be replaced by averaging over the few most plausible topologies , maybe even just the most plausible. This can be considered as the approach of traditional model selection and of Chow & Liu (1968), with slightly different measures of plausibility . (Cooper & Herskovits suggest how to calculate a topology with close to highest posterior probability, given an ordering on the vertices and asking that parents precede children.) The combinatoric s of DAGs are considered by Robinson (1977).']\n",
      "['8.4 Boltzmann machines 279 To make use of Bayesian methods we need prior distributions over the parameters (in the conditional probability tables for recursive models or clique marginals for decomposable models) as well as efficient means to integrate out those parameters to find the posterior probabilities of models. This is possible for Dirichlet priors for conditional probability tables and the hyper-Dirichlet priors introduced by Dawid & Lauritzen (1993) for clique marginals (which have a consistency condition). Indeed, with these priors the posterior distribution of the random variables is Markov after integrating out the current distribution of the parameters. Madigan & Raftery (1994) confine their model averaging, as we saw in Chapter 2. They employ a stepwise search procedure through the space of models, adding or deleting an edge at a time (and, for undirected graphs, staying within the class of decomposable models by removing an edge only if it is a member of just one clique,', 'the class of decomposable models by removing an edge only if it is a member of just one clique, and only adding an edge if it does not create a chordless cycle). It is possible to use simulation methods, but they will also only average over a small subset of the topologies, so the method will need to be carefully constructed to give useful results. Madigan & York (1995) illustrate the use ofMCMC methods to traverse the model space. Hidden variables A Markov or belief network can have one or more vertices representing unobserved latent variables. This device is widely used in medical applications, for example to represent the true (rather than reported) test result (as in Figure 8.2). They will cause observations to have missing values, and so complicate the learning of conditional probability tables. Unsurprisingly, the EM algorithm and variations (see Section A.2) have been used (Spiegelhalter et al., 1993, Section 7). Hidden variables can also be allowed within topologies inferred', 'et al., 1993, Section 7). Hidden variables can also be allowed within topologies inferred from data, in which case their interpretation is not specified in advance. Pearl (1988, Section 8.3) considers hidden vertices in trees, and other methods have been developed (Liu et al., 1991; Verma & Pearl, 1991). 8.4 Boltzmann machines One very specific case of our networks, the Boltzmann machine (Hinton & Sejnowski, 1983; Ackley et al., 1985; Rumelhart & McClelland, 1986, Chapter 7), has a place in the history of neural networks. A Boltzmann machine has binary random variables at a finite set of vertices V which']\n",
      "[\"280 8 Belief Networks are completely connected, and the conditional distribution at each vertex given all the other random variables is Bernoulli with 'success' parameter ev given by a logistic regression on the other vertices, so logit (e;) = w;o + 'L.N=i WijXj. As the network is completely connected, the graph properties are trivial; the expressive power comes from the restriction on the conditional distributions. The 'connection weights' W;j are restricted to be symmetric ( W;j = w ji) and without loss of generality we can take w;; = 0. The joint distribution is then where Z = L exp L W;j x;x j· xv i<j Boltzmann machines are used to 'learn' an input-output distribution, that is the joint distribution of a set of binary random variables, some of which are designated inputs I and some outputs 0. There will normally also be further units (designated 'hidden', H). Let S =I U 0, the variables which are 'visible'. Once the parameters (all the W;j) are given, we have a joint distribution\", \"which are 'visible'. Once the parameters (all the W;j) are given, we have a joint distribution over X v, which gives a joint distribution over Xs, and hence the conditional distribution Xo I XJ. Thus a Boltzmann machine models the full joint distribution of inputs and outputs (the latter indicating classes). So far the weights have been unspecified; the issue is to choose them to best approximate a given joint distribution of inputs and outputs, specifically the empirical distribution of a training set fl. This is done by maximum likelihood fitting of the parameters. However, since the joint distribution is unknown as a function of the weights, the necessary quantities are estimated by simulation, by Gibbs sampling. The precise procedure used is gradient ascent, which only entails estimating the derivative of the log-likelihood with respect to the weights. To avoid handling w;o separately, we assume a vertex 0 and implicitly condition on Xo = 1. The log-likelihood is L = LlogPrw{Xs =\", \"we assume a vertex 0 and implicitly condition on Xo = 1. The log-likelihood is L = LlogPrw{Xs = xs} = L[log I:exp LWijXiXj -logZ J. :T :T XH i<j Consider just one summand L1 of the log-likelihood. We have and aLl 'L-xH X;X j exp 'L.i<j Wij X;X j a log z awij 'L-xH exp 'L.i<j Wij x;x j aw;j\"]\n",
      "[\"8.4 Boltzmann machines 281 = E[X;Xjl(Xs = xs)] _ Pr{X; =X·= 1} Pr{Xs = xs} 1 = Pr{X; = Xj = 11Xs = xs}-Pr{X; = Xj = 1}. Thus aa~ . = L [Pr{X; = Xj = 11 Xs = xs}-Pr{X; = Xj = 1}]. (8.14) IJ xsEff Each summand in (8.14) is estimated by simulation. Two runs of the Gibbs sampler are needed, one for the unconditioned network, and one conditioned on Xs. (In the terminology of the field, the inputs and outputs are 'clamped'.) A small step is made uphill, the gradient re-estimated and so on. Once an approximation to a (local) maximum of the likelihood is found, future cases can be 'presented' by conditioning on X1 and running the Gibbs sampler to find the conditional distribution of Xo. Note that missing or partially observed values are easily accommodated by not (fully) conditioning, both during training and during prediction . Unfortunately, the convergence of the Gibbs sampler has proved to be problematic even in toy problems, since at every step of steepest ascent the Gibbs sampler has to\", 'problematic even in toy problems, since at every step of steepest ascent the Gibbs sampler has to run to convergence for each example in the training set. For example, Kohonen et al. (1988) found that Boltzmann machines out-performed feed-forward neural networks on a toy problem, but were too slow to use on their real example. The mean field approximation (Peterson & Anderson, 1987; Hay kin, 1994, §8.13) avoids the simulation in performing (8.14) by approximating the probabilities. Specifically, the random variables X; are replaced by their means, so Pr{X; = Xj = 1} is replaced by the products of the means of X; and Xj which, since they are binary, is 8;8j. (This is suggested by a saddle-point approximation given in the references.) This reduces the problem to calculating (Bv). We also replace the actual input variables by their means, so logit (8;) = w;o + ~Ni w;jBj in this approximation, and ( Bv) is the solution to this non-linear system of equations. If we want the conditional', 'and ( Bv) is the solution to this non-linear system of equations. If we want the conditional distribution, we know Xs and thus solve the mean-field equations with the constraint Bs = Xs. As all simulation is avoided, mean-field Boltzmann learning is much faster than using Gibbs sampling. It appears to work well even for small systems (Peterson & Anderson , 1987; Hinton, 1989b). Attempts have been made to improve the performance of the Boltzmann machine by abandoning its symmetry; it has every unit connected to every other, and each can influence the other. If we consider a DAG']\n",
      "['282 8 Belief Networks there is no immediate feedback of influences. Logistic units have been considered in analogues of Boltzmann machines by Apolloni & de Falco (1991) and Neal (1992a, b). Assume that the vertices of the DAG are labelled so that parents precede children. Then we assume that II II exp X; 2: .<i wijXJ Pr{Xv=xv}= Pr{X11XJ,j<i}= 1 i ... (8.15) 1 + exp J<i w11X1 and w;1 = 0 unless j is a parent of i. As we saw for Henrion\\'s logic sampler, unconditional simulation of a recursive model is very easy, but as the joint distribution is known explicitly unconditional simulation is not needed. From (8.15) we have -0-log Pr{X v = xv} = X1X1-_j_ log [1 + exp\"\"\" wijx1] ow·· ow·· ~ v v ~~ exp L:J<i WijXJ =~~-~ =~~-~~ 1 + exp LJ<i WijXJ where 81 is the \\'success\\' probability for X; conditional on its parents. Hence a a aw .. Pr{Xs = Xs} = L Pr{X v = xv} ow·. log Pr{X v = xv} I] XH I] = E[(X;-8i}XJI(Xs =Xs)] 0~iJ log Pr{Xs = xs} = E [(X;-8;)XJ I Xs = xs J so the gradient of the', 'I] = E[(X;-8i}XJI(Xs =Xs)] 0~iJ log Pr{Xs = xs} = E [(X;-8;)XJ I Xs = xs J so the gradient of the log-likelihood is :~. = L E[(X;-8;)X 1 I Xs =xs] ] XsEff (8.16) where 81 depends on the parents of X;. This is evaluated by Gibbs sampling (and we have already seen the form of the Gibbs sampler for a belief net). A special case of this belief-net Boltzmann machine was considered earlier by Yair & Gersho ( 1990a, b), under the name of a Boltzmann perceptron network. They have general inputs X1, binary hidden units X v and binary outputs X1. The hidden units depend on the output units via individual logistic regressions, and the output units depend on the inputs and hidden units via a multiple logistic regression (so the outputs are mutually exclusive). The architecture is then very similar to a single-hidden-layer neural network, except that the hidden']\n",
      "['Note that log(l + exp f3h1Jh) R; f3hiJJh when lf3hkBhl is small. 8.5 Hierarchical mixtures of experts 283 units are randomly on or off with the probability that their output value would be in the corresponding neural network. This makes the posterior probabilities encoded by a set of weights slightly different, for although the average output of a hidden unit is its probability (}h, its effect enters non-linearly into the output probabilities via the softmax output stage. (This difference disappears in the mean-field approximation; Hopfield, 1987.) However, we can average correctly by replacing f3hk(}h by log(1 + exp f3hk(}h) in the output multiple logistic regression. This gives a slightly different output layer for the neural network, but back-propagation can still be applied to find oLjowij without simulation. Other variants of Boltzmann machines have been proposed. The radial basis Boltzmann machines of Kappen (1995) have binary or continuous input and output units and continuous', \"Boltzmann machines of Kappen (1995) have binary or continuous input and output units and continuous hidden units with fixed inhibiting connections between the hidden units. A stochastic diffusion (Langevin) simulation system is used. The inhibition between hidden units essentially allows only one of them to be on at a time, and so restricts the solution space. This variant appears to be able to solve realistically sized problems. 8.5 Hierarchical mixtures of experts Hierarchical 'mixtures of experts' (HMEs; Jordan & Jacobs, 1994) are a way to specify the conditional distribution of class c given features x that has connections to several topics, and has already been touched on in Chapter 2. It is most closely related to belief networks. The mixture of experts idea was introduced by Jacobs et al. (1991), and hierarchies of experts by Jordan & Jacobs (1992). The idea is that there are a number of classifiers Ci, each of which produces for an input x a posterior distribution over\", \"are a number of classifiers Ci, each of which produces for an input x a posterior distribution over classes. Each of these can be thought of as appropriate for a particular subpopulation S of cases, and another 'gating' classifier G tells us the proportions of those subpopulations at x. Since the subpopulation is unknown, the posterior probabilities over classes is p(c I x) = LP(c I x,S = s)Pr{S =sIx}. (8.17) Note that this is a model-based approach to stacked generalization. In the hierarchical form, (8.17) is used to define a classifier, and the process repeated to combine classifiers of this form. Normally only a few levels of the hierarchy are used, but Waterhouse & Robinson (1994)\"]\n",
      "[\"284 8 Belief Networks use up to ten levels, combining a pair of networks at each stage. The terminology is perhaps a little imprecise: there is a single layer of 'experts' but the mixture is defined hierarchically. Thus (8.17) still holds for an HME, but Pr{S =sIx} is parametrized hierarchically. The classifiers used in HMEs could be quite general, but in all the examples presented in the references they are logistic discriminants. We can view a 'mixture of experts' as a belief network. The features x provide a set of vertices I which are numbered first and connected to all expert and gating vertices. Each 'expert' is represented by a vertex which has as inputs all the feature vectors XI and has as its state variable a class. The 'gating' classifier has as inputs the features, and state variable one of the experts. The output vertex 0 has state the actual class, and inputs the states of the experts and the gating classifier. This is shown in Figure 8.10. The extension to a hierarchy is\", \"experts and the gating classifier. This is shown in Figure 8.10. The extension to a hierarchy is immediate: at each stage we combine two or more subnets by adding a gating classifier with state the label of a subnet connected to the feature vectors, and an output node connected to the gate and the outputs of the subnets. Despite this representation, the marginal distribution of XI is never specified, as we always work conditionally on the feature vectors. We can then use the methods of belief nets to find the posterior probabilities Pr{ SIx, y} given an example and its true class. So far we have implicitly assumed that all the classifiers are fully specified, but in fact they are logistic discriminants with a vector of parameters. These parameter s can be chosen by maximum likelihood by a variety of algorithms. If parameters (} specify the 'experts' and <P the gating process, the log-likelihood is L((}, </>; ff) = L log L Ps(Y I x; (}s)Pr{S = s I x; </> }. (x,y)Eff s In many problems\", \"is L((}, </>; ff) = L log L Ps(Y I x; (}s)Pr{S = s I x; </> }. (x,y)Eff s In many problems this is simple enough to maximize directly. Alternatively we can use the EM algorithm (Section A.2), by viewing S, Figure 8.10: The belief network for a 'mixture of experts'.\"]\n",
      "[\"Figure 8.11 : The belief network for Bayesian inference on a 'mixture of experts'. 8.5 Hierarchical mixtures of experts 285 the true subpopulation, as a missing value. The log-likelihood for the complete data is L(8, ¢; {(x, y, s)}) = 2: logps(Y I x; Bs) +log Pr{S = s I x; 4>} (x,y,s) and at step i we have to maximize the expectation of this over S evaluated at the current parameter estimates, Q((8, ¢), (8, ¢)(i)) = l:l:Pr{S =s I y,x,¢Ul}[logps(Yix;8s)+logPr{S =sIx;¢}]. :T s The maximization over Q then splits into separate maximizations over the parameters 85 in each expert and over ¢. For Bs we have a maximum likelihood estimate with case weights Pr{ S = s 1 y, x; ¢Ul}. For 4> we have to maximize the mutual information between Pr{S = s I y, x; ¢Ul} and Pr{ S = s I x; 4> }. For a hierarchically specified gate this further divides into maximal mutual information problems at each level. Both Jordan & Jacobs (1994) and Waterhouse & Robinson (1994) use variants of this EM algorithm, for\", 'Both Jordan & Jacobs (1994) and Waterhouse & Robinson (1994) use variants of this EM algorithm, for example not maximizing fully at the M step, and finding \\'on-line\\' versions. Waterhouse & Robinson (1994) found a number of difficulties with their version, which appeared quite prone to reach a local maximum of the log-likelihood. They suggest alleviating this by having a large pool of experts, some of which are then effectively ignored. An alternative (Peng et al., 1994) is to consider Bayesian inference. As in Spiegelhalter & Lauritzen (1990), we can add a parent to each classifier which contains its parameter vector. Thus Figure 8.10 becomes Figure 8.11. Gibbs sampling can then be used to find the posterior distribution of the parameters given the training set 5\"\\', and so to integrate out the posterior distribution of the parameters to find the predictive']\n",
      "[\"286 8 Belief Networks posterior distribution. Estimating the parameters by maximum likelihood is essentially the 'plug-in' version, since the maximum likelihood parameter values will give the posterior mode if fiat priors are used for the parameters (as Peng et al., 1994, did).\"]\n",
      "['There are differences related to sex rather than colour, for example, but sex is also recorded. 9 Unsupervised Methods Unsupervised methods are used when no classes are defined a priori, or when they are but the data are to be used to confirm that these are suitable classes. Examples of the latter type are quite common in biology, where species are often defined by physical characteristics, and datasets of biochemical measurements become available. The interesting question is then whether the physical and biochemical measurements define the same classification. A variant of this occurs with our Leptograpsus crabs data. There the division into species was based on colour, and the interesting question is whether this is supported by morphological differences . Our analyses hitherto have been to find supporting morphological differences, but this begs the question of whether there might be even more striking differences unrelated to colour. Unsupervised methods are generally designed for', \"even more striking differences unrelated to colour. Unsupervised methods are generally designed for visualization, either to show views of the data which indicate groups, or to show affinities between the examples by displaying similar examples close together. Dendrograms are a one-dimensional display of similarity, with the height of the join indicating (dis)similarity. For example, Figure 9.1 shows a dendrogram of the Cushing's syndrome data. Each pair is joined in the tree, and the height at which they are joined is an indication of their dissimilarity. This plot shows clearly that one point (labelled u) is very different from the rest, and does tend to group the diseases together, imperfectly. However, this is two-dimensional data, and the data can be plotted as in Figure 1.2 on page 11. This shows that we too would have difficulty separating the groups. Groupings found by unsupervised methods are usually referred to as clusters. They are usually taken to be disjoint (we do not\", \"methods are usually referred to as clusters. They are usually taken to be disjoint (we do not allow an animal to belong to two species) but sometimes it is helpful to allow some overlap (botanical populations may contain hybrids). Finding clusters is one of the uses of the word 'classification', and the book\"]\n",
      "['288 0 N \"\\' 0 0 0 9 Unsupervised Methods by Gordon (1981) entitled Classification is entirely concerned with unsupervised methods, mainly clustering. Unsupervised methods are sometimes used to classify. For example, we could use Figure 9.1 to select the closest grouping, and take a majority vote amongst its true classes. This has been advocated (Fuzzy ARTMAP: Carpenter et al., 1992; Carpenter & Grossberg, 1994), but is dangerous as the unsupervised groupings may reflect a completely different classification of the data (colour vs sex for our Leptograpsus crabs, or, as in fact occurs, overall size). Our exposition will move from visualization towards finding structure in the data. We start with methods to show linear or smooth non-linear transformations of the dataset which will reveal interesting structure in low-dimensional plots (usually two-dimensional scatterplots). We then consider the class of methods sometimes known as multidimensional scaling which produce low-dimensional plots', \"class of methods sometimes known as multidimensional scaling which produce low-dimensional plots (again, usually in one or two dimensions) in which similar data points are plotted close together. The last two sections are concerned with clustering. The first covers methods to produce a few large clusters, and to produce taxonomic hierarchies such as Figure 9.1. The last section concerns methods which produce many clusters, but link them in a one-or two-dimensional layout wherein nearby clusters are more similar than distant clusters. 9.1 Projection methods Projection methods choose one or more linear combinations of the original features to maximize some measure of 'interestingness'. Equivalently, the space of features is rotated in IR.P, and the first few dimensions of the rotated space are retained. Figure 9.1: Dendrogram of the Cushing's syndrome data by the 'single-link' method.\"]\n",
      "['A is a p x q matrix, each column of which gives the coefficients of a linear combination. Proofs are given at the end of this section. This is the usual definition of principal components . 9.1 Projection methods 289 Principal components Principal components occur in a number of problems and by different names: they are same thing as the Karhunen-Loeve expansion of Watanabe (1969) and Devijver & Kittler (1982), for example. Jolliffe (1986) and Jackson (1991) devote whole books to this topic. Suppose the data are n ~ p vectors Xi E JRP forming the rows of an n x p data matrix X. We will assume that the column means are zero, that is that each feature has mean zero in the given sample. The idea is to take q < p linear combination XA E 1R q which in one of a number of senses best represent the original data. This is done by taking the singular value decomposition of the data matrix X (Golub & Van Loan, 1989) X = UAVT, where A is a diagonal matrix with decreasing non-negative entries', '& Van Loan, 1989) X = UAVT, where A is a diagonal matrix with decreasing non-negative entries (Jci), U is an n x p matrix with orthonormal columns, and V is a p x p orthogonal matrix. Then the principal components are the columns of XV. Since X and A must have the same rank, at most p of the singular values (the diagonal elements of A) will be non-zero. Then we have the following properties : 1 The first singular value (the first column of XV) is the linear combination aT x for a of unit length with the largest variance, the second is the combination of largest variance which is uncorrelated with the first, and so on. 2 The first q < p columns of XV are the linear projection of X into q dimensions with the largest variance. (The covariance matrix of a q-dimensional projection is a q x q matrix, and this one is largest as measured by the trace and also by the determinant.) 3 Let X = U Aq vT be the matrix formed by setting all but the first q singular values (diagonal elements of A) to', 'vT be the matrix formed by setting all but the first q singular values (diagonal elements of A) to zero. Then X is the best possible rank-q approximation to X in several senses, including the Frobenius norm, the square root of the sum of squares of the elements. 4 Another way to express this is that if we project onto the first q principal components we have the most accurate rank-q reconstruction of the original data points. 5 Yet another way to express this is to say that the points of the q-dimensional projection onto the first q principal components lie in a q-dimensional space, and this is the best-fitting q-dimensional space as measured by the sum of the squares of the distances from the data points to their projections into the space.']\n",
      "[\"290 9 Unsupervised Methods In summary, if you want a reduction to q < p dimensions by linear combinations of the features, the principal components have many optimality properties. Note that the first two properties show that the measure of 'interestingness' that is maximized is the variance. The emphasis on variance reveals the Achilles' heel of principal components: they depend on the units in which the features are measured. In a biological problem in which we might have lengths, volumes and weights, the principal components will depend critically on the units used. Even when all the measurements are lengths, do we want to regard variation in the length of a small part as equivalent to variation in the length of the whole organism? Usually not; in biological problems the first principal component will normally be a measure of overall size, and be of little interest. So unless we have good a priori reasons to regard the variances of the features to be comparable, we would normally\", 'good a priori reasons to regard the variances of the features to be comparable, we would normally make them equal by rescaling all the features to have vanance one. There is an older approach to principal components which is better known but numerically less stable. This is to form the covariance matrix S of the observations, and take its eigenvalue decomposition CDCT. As S is a covariance matrix, hence non-negative definite, the eigenvalues will be real and non-negative. Now (for centred data) (n-1)S = XTX = VAUTUAVT = VA2VT, so D = A2j(n-1) and C = V. Thus the principal components may be found from the eigendecomposition of S. It is customary to advocate using the eigendecomposition of the correlation matrix rather than the covariance matrix, which is the same procedure as rescaling the features to unit variance before calculating the covariance matrix. Viruses example We consider the Tobamovirus group of the viruses example, which has n = 38 examples with p = 18 features. Figure', \"Tobamovirus group of the viruses example, which has n = 38 examples with p = 18 features. Figure 9.2 shows plots of the first two principal components with 'raw' and scaled variables. As the data here are counts, there are arguments for both, but principally for scaling as the counts vary in range quite considerably between variables. Virus 11 (Sammon's opuntia virus) stands out on both plots: this is the one virus with a much lower total (122 rather than 157-161). Both plots suggest three subgroupings. In both cases the first two principal components have about equal variances, and together contain about 69% and 52% of the variance in the raw and scaled plots respectively.\"]\n",
      "['Figure 9.2: Principal component (top row) and Sammon mapping (bottom row) plots of the Tobamovirus group of the viruses example. The plots in the left column are of the raw data, those in the right column with variables rescaled to have unit variance. The points are labelled by the index number of the virus. Figure 9.3: Pairwise scatterplots of the first three principal components of the Leptograpsus crabs data. Males are coded as capitals, females as lower case, colours as the initial letter of blue or orange. Ill 0 u;> 0 Ill 0 u;> 0 \\'7 ~ 0 0 0 9.1 Projection methods .. C\\\\J 30 ,,]{:\\'t ,;!\\'\" \" \" 0 :Js\" \\'7 15 , .. <)I ..,., \\'it\\'\" \\'i\\' l\\'l6 -15 -10 -5 0 5 10 -4 (a) \"., .. \" <D .. \" \" -.t \" 37 35 C\\\\J \" 38\" 2~1239 \":Ia 0 ,,.,, <)I 30\" /133 \"i -20 -10 0 10 -10 (c) 010 00 005 010 015 o~oo oo ooo 0 8~ 8&\\'i~ 8 8 o boo 8 0 o 8 0 a \\'\\\\\\\\\\\\0 e 0 o\\'i! f o8 +8 b o~W~\\\\; ~ ( PC1 <lit ,.,0·~ \\'\" !l \\'6 \\'t BIIJ b B o \\'!, o~ lb <jleOB o: b 0 B 8 • 0 • b b b 0 b b b 0 0 lb B~ ~.,g!f\\'~?l,0 b \\\\• ft\" ~bo ooJo ~o', 'b B o \\'!, o~ lb <jleOB o: b 0 B 8 • 0 • b b b 0 b b b 0 0 lb B~ ~.,g!f\\'~?l,0 b \\\\• ft\" ~bo ooJo ~o ID b bB \"\\' o B o PC2 • B b ~o o B o Bo B ~ B o ecft.R\\'!ts o 8 !b llo ~ oj o eol:lg V• 0 B Boo ~IP o B B • ~. 8~~~\" B~ Bl e e ~B ~~B b u b II B B h ~Bto ~ ~ \\'1{1. 8!1\\'~ • ~ ••• ~~>-.; \"• •• • b 0 ~0 ~ o~c:S a II> \"t, 8 • o 0e~~00 ~ b 0 0 0 0 B 6!P~ocflgo oia~. o 0 0 0 0 ~ 0 0 0 o& 0 0 0 0 0 0 -1.5 -1.0 ·0.5 0.0 0.5 1.0 291 .... .. , 4f . ., 30 2~, 32 \" \" \" ..... •s 45 4342 ., -2 0 2 4 (b) a 2~0 37 ., \"\"\\'\" 35 .. 36 14!:5 \" .. \\'~\\' \" l!l\"\" 30 oi,\\', -5 0 5 (d) ~~ \\' b ~ 8 0 00 ~ 8 8 0 ~t 8 oCD of,8~1!~ oO> o~Bb goO bb b b II 0 \"\\'lb\\\\, • 0 8 • 0 0 rJ> ••• 0 ~~ o• VJ 0° \"\\'ij 0 40 \\' •• 0 ff\\' 0 0 II \\'\\\\, %0 ~B ~ o 8 B o o 0o\\'O • 11. \"tt 8 0~ ~0 \"!! B 0 ~0 8 ~ 0 ~ \"\"\" 8 cfo 0 1188 PC3 -0.10 -0.05 0.0 0.05 0.10 10 \"\\' 0 0 0 \"\\' 9 0 0 0 9']\n",
      "['292 9 Unsupervised Methods Crabs example The crabs data are displayed on their first three principal components in Figure 9.3. The data were transformed to log scale but not further scaled (although that is arguable) . Since the groupings (by colour and sex) are known, the points in the different groups are distinguished on the plots. The figure shows that the first principal component is largely unrelated to colour and sex: it is almost an average of the measurements on log scale, and so is displaying size. The second principal component tends to distinguish sex and the third colour, the most interesting grouping. Iterative methods A consequence of property 4 of the principal components is that the best we can do by taking p x q and q x p matrices A and B and forming XAB to approximate X in sum-of-squares is to take XA as the first q principal components. Now there are many other equally good solutions, for XAcc-1 B will give the same fit for any invertible q x q matrix C. We can', \"good solutions, for XAcc-1 B will give the same fit for any invertible q x q matrix C. We can express this by saying that we can only optimize over the subspace spanned by the q linear combinations. It is obvious that XAB is the outcome of a feed-forward neural network with no bias unit, all linear units and q units in the hidden layer. Thus the best possible fit by least squares of such a network trained with output equal to input (an 'auto-encoder ' or 'auto-associator ') is given by the subspace spanned by the first q principal components. This is a much-rediscovered fact (Bourlard & Kamp, 1988, and Baldi & Hornik, 1989, were amongst the first) and every 'on-line' algorithm to fit the neural network leads to an iterative algorithm to find the subspace spanned by the principal components . This has led to around 100 papers on such algorithms. Well-known versions are those of Brockett (1991), Oja (1982, 1989, 1992), Oja & Karhunen (1985) and Sanger (1989). One of the simplest methods\", 'Oja (1982, 1989, 1992), Oja & Karhunen (1985) and Sanger (1989). One of the simplest methods that extracts principal components (rather than just the subspace) is the APEX algorithm of Kung & Diamantaras (1990). (See Haykin, 1994, §9.8.) Although the idea of these algorithms is interesting, they seem unnecessary in practice. Singular value decomposition routines can handle quite large matrices X, and when they cannot cope, we can always find the covariance matrix on a single pass through the data and find its eigenvalues .']\n",
      "['9.1 Projection methods 293 Robustness The extraction of principal components is based on variances, and so is sensitive to the presence of outliers. Outliers in high-dimensional data are notoriously difficult to find, although they often emerge as a side-effect from some projection pursuit methods. It is highly desirable to use a method of extracting principal components which is less sensitive to outliers. This can be achieved by taking an eigendecomposition of a robustly estimated covariance or correlation matrix. There is a slight catch, in that it is essential to robustly estimate the means, and that this must be done by estimating the means of all features simultaneously (Rousseeuw & Leroy, 1987, p. 250). So the real task is to estimate the vector of means and the covariance matrix. There have been many attempts to do this; see for example Devlin et al. (1981). Modern approaches are discussed in Section 2.5. An alternative approach is to find a projection maximizing a robust', 'are discussed in Section 2.5. An alternative approach is to find a projection maximizing a robust measure of variance in q dimensions. This would have to be done iteratively, as for the projection pursuit methods described below. Proofs As our results depend on various properties of principal components, these are proved here for those who are interested in the details. Proposition 9.1 Consider an n x p matrix X with singular value decomposition X = U AVT. The best approximation in Frobenius norm to X by a matrix of rank k ~ min( n, p) is given by - . T X= Udmg(Al, ... ,Ak, ... ,O)V . This is also the best approximation by a projection onto a subspace of dimension at most k, the projection onto the space spanned by the first k columns of U, and maximizes the Frobenius norm of a projection of X onto a subspace of dimension at most k. Proof: We have IIX-xf = IIA-Ak112 = L~~~(n,p) AT-Now X corresponds to a projection onto the space spanned by the first k columns of U, say Uk> since that', 'corresponds to a projection onto the space spanned by the first k columns of U, say Uk> since that projection gives Consider any approximation Y of rank at most k. This can be written as Y = AB where A is n x k and B is k x p (for example, via']\n",
      "[\"294 9 Unsupervised Methods the SVD of Y ). Now consider the best approximation of the form AC for any k x p matrix C. Since the squared Frobenius norm is the sum of the squared lengths of the columns, this is solved by regressing each column of X in turn on A; the optimal choice is C = (AT A)-1 AT X and where P A = A(A T A)-1 AT is the projection matrix onto span( A). Now we choose PA to maximize IIPAXII2: min(n,p) min(n,p) IIPAXII2 = IIPAUAf = L AJIIPAujll2 = L AJPJ 1 1 and Pj ~ 1 (it is the length of a projection of a unit-length vector), ~ PJ = liP AUf = liP A 112 = k. It is then obvious that the maximum is attained if and only if the first k Pj 's are one, the rest zero, so k min(n,p) IIX-y f?: IIXII2-IIPAXII2?: IIXII2-L AT= L AT= IIX-Xll2. 1 k+l Any projection of X onto a subspace of k dimensions has rank at most k. D Proposition 9.2 Consider n observations of p features forming a matrix X. Then the projection of Proposition 9.1 (a) minimizes the sum of squared lengths from points\", 'X. Then the projection of Proposition 9.1 (a) minimizes the sum of squared lengths from points to their projections onto any subspace of dimension at most k, (b) maximizes the trace of the covariance matrix of the projected variables onto any subspace of dimension at most k, and (c) maximizes the sum of squared inter-point distances of the projections onto any subspace of dimension at most k. Proof: Without loss of generality we can centre the observations, so each variable has mean zero. Part (a) follows from the squared Frobenius norm of X -PAX being the sum of squared lengths of its rows. For (b) the squared Frobenius norm of PAX is the sum of squares of the projected variables, that is n -1 times the sum of the variances of the variables, which is the trace of the covariance matrix (and is invariant to the choice of a basis for that subspace). For (c) consider any projection PAX. Let d,5 be the distance between observations r']\n",
      "[\"9.1 Projection methods 295 and s, and drs the distance under projection (which is smaller, as it is a projection). Let Yr be the-r th projected observation. Then, using L::: Yr = 0 (since the projections are still centred), r,s r,s r,s which is maximized according to Proposition 9.1. 0 Proposition 9.3 The principal components defined by property I are given, in order, by columns of V. The first k principal components span a subspace with the properties of Proposition 9.2. Proof: Consider a linear combination y =aT x with II all = 1. Then where a' = vr a also has unit length (and this corresponds to rotating to a new basis for the feature variables). It is clear that the maximum occurs when a' is the first coordinate vector, or a the first column of V. Now consider the second principal component bT x. It must be uncorrelated with the first, so and it is obvious that the maximum variance under this constraint is given by taking b' as the second coordinate vector. An inductive argument\", \"under this constraint is given by taking b' as the second coordinate vector. An inductive argument gives the remaining principal components. Using the principal component variables we have X = U A, so it clear that the subspace spanned by the first k columns is the approximation of Propositions 9.1 and 9.2. 0 Proposition 9.4 Consider an orthogonal change X B to k new variables. Amongst such transformations the first k principal components have maximal variance, both in the sense of the trace and of the determinant of the covariance matrix.\"]\n",
      "[\"296 9 Unsupervised Methods Proof: The trace statement is Proposition 9.2(b ), but we will give an alternative proof. Consider the SVD of X B, and let its singular values be !J.l, ... ,IJ.k· We will show /.lj ~ Aj,j = 1, ... ,k, which suffices as the trace of the variance matrix is proportional to the sum of the squared singular values, and the determinant is proportional to their product. Consider a variable y = xT a which is a unit-length linear combination of the first j principal components of the B set, but is orthogonal to the first j -1 original principal components. (A dimension argument shows that such a variable exists. Since B is orthogonal y is also a unit-length combination of the original variables and of their principal components.) Thus y has variance at least llJ and at most Jc], hence /.lj ~ Aj. D Projection pursuit methods Projection pursuit methods seek a q-dimensional projection of the data that maximizes some measure of 'interestingness', usually for q = 1 or 2 so\", \"projection of the data that maximizes some measure of 'interestingness', usually for q = 1 or 2 so that it can be visualized. This measure would not be the variance, and would normally be scale-free. Indeed, most proposals are also affine invariant, so they do not depend on the correlations in the data either. The methodology was named by Friedman & Tukey (1974), who sought a measure which would reveal groupings in the data. Later reviews (Huber, 1985; Friedman, 1987; Jones & Sibson, 1987) have used the result ofDiaconis & Freedman (1984) that a randomly selected projection of a high-dimensional dataset will appear similar to a sample from a multivariate normal distribution to stress that 'interestingness' has to mean departures from multivariate normality. Another argument is that the multivariate normal distribution is elliptically symmetrical, and cannot show clustering or non-linear structure, so all elliptically symmetrical distributions should be uninteresting. The simplest way\", \"structure, so all elliptically symmetrical distributions should be uninteresting. The simplest way to achieve affine invariance is to 'sphere' the data before computing the index of 'interestingness'. Since a spherically symmetric point distribution has covariance matrix proportional to the identity, we transform the data to have identity covariance matrix. This can be done by transforming to principal components, discarding any components of zero variance (hence constant) and then rescaling each component to unit variance. As principal components are uncorrelated, the data are sphered. Of course, this process is susceptible to outliers and it may be wise to use a robust version of principal components. The idea goes back to Kruskal (1969, 1972). Kruskal (1969) needed a snappier title!\"]\n",
      "['Here <I> is the cumulative distribution function of a standard normal. 9.1 Projection methods 297 The discussion of Jones & Sibson (1987) included several powerful arguments against sphering, but as in principal component analysis something of this sort is needed unless a particular common scale for the features can be justified: Specific examples of projection pursuit indices are given below. Once an index is chosen, a projection is chosen by numerically maximizing the index over the choice of projection. A q-dimensional projection is determined by a p x q orthogonal matrix and q will be small, so this may seem like a simple optimization task. One difficulty is that the index is often very sensitive to the projection directions, and good views may occur within sharp and well-separated peaks in the optimization space. Another is that the index may be very sensitive to small changes in the data configuration and so have very many local maxima. Rather than use a method which optimizes', \"the data configuration and so have very many local maxima. Rather than use a method which optimizes locally (such as quasi-Newton methods) it will be better to use a method which is designed to search for isolated peaks and so makes large steps. In the discussion of Jones & Sibson (1987), Friedman says 'It has been my experience that finding the substantive minima of a projection index is a difficult problem, and that simple gradient-guided methods (such as steepest ascent) are generally inadequate . The power of a projection pursuit procedure depends crucially on the reliability and thoroughness of the numerical optimizer.' and our experience supports Friedman's wisdom. It will normally be necessary to try many different starting points, some of which may reveal projections with large values of the projection index. Posse (1990, 1995b) considers an almost random search which Posse (1995a) finds to be superior to his implementation of the optimization methods of Jones & Sibson and of\", 'finds to be superior to his implementation of the optimization methods of Jones & Sibson and of Friedman . Once an interesting projection is found, it is important to remove the structure it reveals to allow other interesting views to be found more easily. If clusters (or outliers) are revealed, these can be removed, and both the clusters and the remainder investigated for further structure. If non-linear structures are found, Friedman (1987) suggests non-linearly transf-orming the current view towards joint normality, but leaving the orthogonal subspace unchanged. This is easy for q = 1; any random variable with cumulative distribution function F can be transformed to a normal distribution by $-1(F(X)). For q = 2 Friedman suggests doing this for randomly selected directions until the two-dimensional projection index is small.']\n",
      "[\"298 9 Unsupervised Methods Projection indices A very wide variety of indices have been proposed, as might be expected from the many ways a distribution can look non-normal. A projection index will be called repeatedly, so needs to be fast to compute. Recent attention has shifted towards indices which are rather crude approximations to desirable ones, but very fast to compute (being based on moments). For simplicity, most of our discussion will be for one-dimensional projections; we return to two-dimensional versions at the end. Thus we seek a measure of the non-normality of a univariate random variable X. Our discussion will be in terms of the density f even though the index will have to be estimated from a finite sample. (This can be done by replacing population moments by sample moments or using some density estimate for f such as the kernel methods of Section 6.1.) The original Friedman- Tukey index had two parts, a 'spread' term and a 'local density' term. Once a scale has been\", \"Tukey index had two parts, a 'spread' term and a 'local density' term. Once a scale has been established for X (including protecting against outliers), the local density term can be seen as a kernel estimator of J f2(x) dx. The choice of bandwidth is crucial in any kernel estimation problem; as Friedman & Tukey were looking for compact non-linear features (cross-sections of 'rods'-see Tukey's contribution to the discussion of Jones & Sibson, 1987) they chose a small bandwidth. Even with efficient approximate methods to compute kernel estimates, this index remains one of the slowest to compute. Jones & Sibson (1987) introduced an entropy index J f logf (which is also very slow to compute) and indices based on moments such as [K~ + 1/4K~]/12, where the K's are cumulants, the skewness and kurtosis here. These are fast to compute but sensitive to outliers (Best & Rayner, 1988). Friedman (1987) motivated an index by first transforming normality to uniformity on [-1,1] by Y = 2<I>(X) -1 and\", 'motivated an index by first transforming normality to uniformity on [-1,1] by Y = 2<I>(X) -1 and using a moment measure of non-uniformity, specifically j(fy -1/2)2. This can be transformed back to the original scale to give the index IL = j [f(x)-¢(x))2 d 2¢(x) x. This has to be estimated from a sample, and lends itself naturally to an orthogonal series expansion, the Legendre series for the transformed density. The index I L has the unfortunate effect of giving large weight to fluctuations in the density f in its tails (where 4> is small), and so Here cf> is the standard normal density.']\n",
      "['9.1 Projection methods 299 will display sensitivity to outliers and the precise scaling used for the density. This motivated P. Hall (1989) to propose the index IH = j [f(x) -l/J(x)]2 dx and Cook et al. (1993) to propose IN= j[f(x)-l/J(x)]24J(x)dx. Both of these are naturally computed via an orthogonal series estimator of f using Hermite polynomials (Thisted, 1988, §5.3.2). Note that all three indices reduce to 2::~ wi(ai-bi)2, where ai are the coefficients in the orthogonal series estimator, and bi are constants arising from the expansion for a standard normal distribution. To make use of these indices, the series expansions have to be truncated, and possibly tapered as well (see Section 6.1). Cook et al. (1993) make the much more extreme suggestion of keeping only a very few terms, maybe the first one or two. These still give indices which are zero for the normal distribution, but which are much more attuned to large-scale departures from normality. For example, If: is formed by', \"which are much more attuned to large-scale departures from normality. For example, If: is formed by keeping the first term of the expansion of IN, (ao -1/2jii)2 where ao = J l/J(x)f(x) dx = El/J(X), and this is maximized when ao is maximal. In this case the most 'interesting' distribution has all its mass at 0. The minimal value of ao gives a local maximum, attained by giving equal weight to ±1. Now of course a point mass at the origin will not meet our scaling conditions, but this indicates that If: is likely to respond to distributions with a central clump or a central hole. To distinguish between them we can maximize ao (for a clump) or -a0 (for a central hole). These heuristics are borne out by experiment. In principle the extension of these indices to two dimensions is simple. Those indices based on density estimation just need a twodimensional density estimate and integration (and so are likely to be even slower to compute). Those based on moments use bivariate moments. For\", '(and so are likely to be even slower to compute). Those based on moments use bivariate moments. For example, the index IN becomes IN= J J [f(x,y) -ljJ(x)ljJ(y)]24J(x)ljJ(y)dxdy and bivariate Hermite polynomials are used. To maintain rotational invariance in the index, the truncation has to include all terms up to a given degree of polynomial. All the indices described so far are implemented for q = 2 in XGobi, a freely available data visualization tool from Bellcore (Swayne et al., 1991).']\n",
      "['300 9 Unsupervised Methods Eslava & Marriott (1994) defined two indices for q = 2 specifically designed to display all clusters; conventional indices have a tendency to superimpose clusters in the projection. Suppose the projected points (or those not very near the origin) have ordered polar angles ei. The polar nearest neighbour index (to be minimized) is the average of min( lei-ei-ll, i8i+l-eil), the angular separation from the remaining points. Their other criterion maximizes the mean radial distance, or equivalently for sphered data, minimizes the variance of the radial distance. Posse (1995a, b) has another two-dimensional index also based on ideas of radial symmetry, using a chi-squared index of departure from normality averaged over univariate projections. One of the very few examples of a method which is both biologically motivated and practically useful is the projection index of Intrator (1990, 1992) and Intrator & Cooper (1992) based on the BCM model of neuron selectivity', \"of Intrator (1990, 1992) and Intrator & Cooper (1992) based on the BCM model of neuron selectivity put forward by Bienenstock et al. (1982). The BCM model is based on a one-dimensional projection c = aT x of a signal x, which is chosen to maximize (9.1) where the 'threshold' e = Ec2 is adjusted according to the distribution of the population of examples (in practice the training sample). Notice that (9.1) is not scale-free, and will be negative for large c and hence a. Thus there is no need to normalize a to unit length. There is a natural on-line algorithm to minimize (9.1), namely where e will also be updated from time to time. (Intrator & Cooper, 1992, discuss the stability of the differential equation limit of this update.) We can also consider several BCM neurons with lateral inhibition, in which case c is replaced by ck -I} L_Hk Cj for neuron k. The BCM neuron is itself a projection index, but as it is based on moments it will be sensitive to outliers. Intrator replaces c = aT x\", 'index, but as it is based on moments it will be sensitive to outliers. Intrator replaces c = aT x by t(c) for the usual logistic function t; this effectively transforms to [0, 1] by the inverse of the logistic cumulative distribution function before computing the index. Not only does this give a one-dimensional projection index but the lateral inhibition BCM network may be used to project onto q > 1 dimensions. Applications are shown by Intrator (1991, 1992) and Intrator & Gold (1993). There is no unanimity over the merits of these indices (except the moment index, which seems universally poor). Some workers have reported that the Legendre index is very sensitive to outliers, and this The BCM model appears to be very well supported by experiment. The details, especially the constants , differ from paper to paper. In the original BCM paper 0 = (Ecf.']\n",
      "['9.1 Projection methods 301 is our experience. Yet Posse (1995a) found it to work well, in a study that appears to contain no ·outliers. The natural Hermite index is particularly sensitive to a central hole and hence clustering. The best advice is to try a variety of indices. Viruses example Figure 9.4 shows six views of the main group of the viruses dataset obtained by (locally) optimizing various projection indices; this is a small subset of hundreds of views obtained in interactive experimentation in XGobi. With only 38 points in 18 dimensions, there is a lot of scope for finding a view in which an arbitrarily selected point appears as an outlier, and there is no clear sense in which this dataset contains outliers (except point 11, whose total residue is very much less than the others). When viewing rotating views of multidimensional datasets (a grand tour in the terminology of Asimov, 1985) true outliers are sometimes revealed by the differences in speed and direction which they', '1985) true outliers are sometimes revealed by the differences in speed and direction which they display-certainly point 11 stands out in this dataset. Not many views showed clear groupings rather than isolated points. The Friedman- Tukey index was most successful in this example. EslavaG6mez (1989) studied all three groups (which violates the principle of removing known structure). This example illustrates a point made by Huber (1985, §21); we need a very large number of points in 18 dimensions to be sure that we are not finding quirks of the sample but real features of the generating proc~ss. Thus projection pursuit may be used for hypothesis formation, but we will need independent evidence of the validity of the structure suggested by the plots. Crabs example Various projections of the crabs data are shown in Figure 9.5. This is a rather different example, with 200 examples on only five variables, and with four groups suspected in advance. The first term of the natural Hermite', \"five variables, and with four groups suspected in advance. The first term of the natural Hermite expansion does find other local maxima, but the view shown in Figure 9.5(b) is the most commonly found. The other indices are less successful. View (b) is close to a local maximum for Friedman's index, but more often just the colour forms are separated as shown in view (d). The Friedman-Tukey index does not recognize these clusters, and instead finds views such as (c) which seem to have no interpretation.\"]\n",
      "[\"302 9 Unsupervised Methods Figure 9.4: Projections of the Tobamovirus group of the viruses data found by ,36 ,36 projection pursuit. !• •• .. :·;,}~. Views (a) and (b) were ·_.:.~;~ found using the natural )7 ,17 Hermite index, view (c) ,46 ,46 by minimizing ao, and views ( d, e, f) were ,45 found by the Friedman- Tukey index J j2 with different (a) (b) choices of bandwidth for the kernel density estimator. _.39 .,41 ~ .. Jl2 ,. ~-'11 ;·: ,14 ,45 -~h ,14 ./ (c) (d) -~· ,·II.· )0 (e) (f)\"]\n",
      "['Figure. 9.5: Projection s of the uprQgrapsus crabs data found by projection pursuit. View (a) is a random projection. View (b) was found using the natural Hennite index, view (c) by the Friedman -Tukey index and view (d) by the Friedman (1987) index. 9.1 Projection methods . .. ···~· ...... ·.·: .. ..... :.;; ...... . ... .... ... ,. \\'· .,~. .. \\' \"\\'\\'{:c.·..;. ~ .: .. ~.,.: .. _ .:. • •• • ..-. t •• • • . . . . , .. :\\' ~: ... = .. (a) \\' . . . . . . ·: .:·. ·. : .......... -:.· .. : • • I • ,• . ~ . .;.J.::.·.t: .. ,·.· . . ..... s-.......... . . .. . ., .... , ... ... ~·.. .. .... t :. • • .. : (c) . ·~· . .·:...:t,. ·<~\\'\" . .. ··· ~:-:~· . \\'\"\\'(~,,. , ·. . (b) ·-, .. , :,:\\\\ ... .. : ... \\'• : ... :.. ·.;:: -~.: .. . .. ··:-.:....-: :·. . \\' ,-l\\'tt. \\\\ .... :. .·· ~-·: .. • • .c;.,. .., • I • • ~~ ,\"\\'o • .... : .. ·· ·;:r ... ·· .. · ... .. (d) 303 It is worth noting that the successful projections have ignored the most variable direction, size, in favour of a view with more structure,', 'have ignored the most variable direction, size, in favour of a view with more structure, unlike all the other methods we illustrate on this example. Non-linear feature extraction One of the characterizations of the principal components was that if we took a linear map F:RP-+ Rq and another linear map G:Rq-+ RP, the most accurate reconstruction G(F(x)) in the sense of least squares is given by using tbe first q columns of V to form the first q principal components and for the reconstruction map G. Can we do better with non-linear mappings F and G ? The answer must be yes, for the diagonal Cantor construction can map invertibly RP into R ! (Write each of the components Xt, .•. , Xp in a binary expansion and interleave the expansions to obtain the binary expansion']\n",
      "[\"304 9 Unsupervised Methods of a number in IR. This F is continuous.) In practice we want F and G to be not too far from linear, and in particular very smooth. The most common suggestion is to use feed-forward neural networks to fit F and G (Kramer, 1991; Usui et al., 1991; Cottrell & Metcalfe, 1991; DeMers & Cottrell, 1993). If we model both F and G by networks with a single hidden layer, we end up with a five-layer network. The input and output layers have linear units, as does the middle layer (with q units). The second and fourth layers have sigmoidal units. Multi-layer networks are notoriously difficult to train, and these methods have shown limited success. None appears to have used skip-layer connections nor weight decay, and it is not clear whether the current lack of success is intrinsic or due to inefficient methods of training the network. Such networks are often referred to as 'bottlenecks', 'auto-encoders' or 'auto-associators'. Kambhatla & Leen (1994) take another\", \"to as 'bottlenecks', 'auto-encoders' or 'auto-associators'. Kambhatla & Leen (1994) take another approach, which in their examples (and that of DeMers & Cottrell) is at least competitive in fit with an auto-encoder, but very much faster to train. Whereas an auto-encoding is trained globally, Kambhatla & Leen use principal components locally within the partitioning of JRP defined by some form of vector quantization of the dataset. This can be seen as an approximation to defining a q-dimensional manifold in JRP. Clearly the vector quantization should be performed with an eye to the approximation error, and this is done by measuring the distances from a codebook vector to a vector which might be assigned to its cluster orthogonal to the local approximation, that is in the space of the omitted p -q principal components. Principal curves and surfaces Principal curves are defined by Hastie & Stuetzle (1989) as a mapping of a dataset in JRP to a one-dimensional manifold in JRP. Let f(A.) be\", 'Stuetzle (1989) as a mapping of a dataset in JRP to a one-dimensional manifold in JRP. Let f(A.) be a smooth curve in JRP parametrized by A. E IR. Then for any data point x E JRP we seek the nearest point A.(x) on the curve in Euclidean distance. The curve is called a principal curve for a distribution on JRP if E[X I A.(x) = A.] = f(A.), that is the mean of those points that project to a point on the curve is that point. There are many possible parametrizations of a one-dimensional curve; the most natural is in terms of arc length A. from a fixed point on the curve. We have defined a principal curve for a distribution, and the natural way to find such a curve is to project a distribution onto a candidate curve f(A.), and to take as the next iteration the conditional expectation There could be more than one nearest point, but this will be exceptional. For definiteness, we choose the nearest point with largest ).,']\n",
      "['These are the distances allowed for expense claims in bureaucracies. 9.2 Multidimensional scaling 305 E{X I A.(x) = A.} and re-parametrize this in terms of arc length. There are no known guarantees that this algorithm will converge. For a set of data, the points only project to a discrete set of values of A., and the conditional expectation must be replaced by a smoothing operation. We have a set of values (A.i, x;). Whereas we considered scatterplot smoothers in Chapter 4 for univariate x, these methods extend readily to multidimensional x. In most methods we smooth each coordinate of x separately. We can think of principal curves as an algorithm to map A. = F(x) by projecting to the nearest point and then projecting back by x =((A.). As such it is very similar to the method of Kambhatla & Leen (1994), but handles the projection step in a smoother way. Tibshirani (1992) proposed a variant on the original principal curves idea. In principle this technique can be extended to manifolds', \"on the original principal curves idea. In principle this technique can be extended to manifolds of q > 1 dimensions, called principal surfaces, although q-dimensional smoothing is much harder unless data are abundant, even for q = 2. As with all projection techniques, principal curves and surfaces depend critically on the scaling of the features; current algorithms also depend on choosing well the degree of smoothing. 9.2 Multidimensional scaling In multidimensional scaling we are given the distances drs between every pair of observations. These could be genuine distances in some high-dimensional space, or they could be surrogates for the Euclidean distances. For example, some favourite examples use 'official' road distances between major towns and the scheduled flight times between cities. The latter need not even be symmetric, but we will confine attention to symmetric distances. Thus we suppose we are given nonnegative symmetric numbers drs which we will call dissimilarities to\", 'we suppose we are given nonnegative symmetric numbers drs which we will call dissimilarities to indicate that they need not be genuine distances. In particular, they need not satisfy the triangle inequality satisfied if they were produced by a metric. (Gower & Legendre, 1986, explore when dissimilarities are metric.) Most of the work on multidimensional scaling has been developed in the psychological literature, but has also been discussed in ecology under the name of ordination. The recent short book by Cox & Cox (1994) has considerable detail on the various methods and their history.']\n",
      "['306 9 Unsupervised Methods Dissimilarities If we are given an n x p matrix X of data to be considered as n p-variate continuous observations, there are several ways to measure the distance between the pairs of observations. If these observations are categorical, there are even more ways. Since many of them produce measures of distance which do not satisfy the triangle inequality, the more general term dissimilarity is used. A dissimilarity is just a nonnegative symmetric function on pairs of objects; we will usually assume that the self-dissimilarities are zero. Kaufman & Rousseeuw (1990, §1.2) review many definitions of dissimilarities. Exactly the same choices of a distance measure occur when knearest neighbour methods are used in supervised classification. For continuous data, the most obvious dissimilarity is Euclidean distance computed from d2 = xxr. This does however depend critically on the scales in which the features are measured. One way out we saw for principal component', \"on the scales in which the features are measured. One way out we saw for principal component analysis is to rescale the features to unit variance, and in projection pursuit we saw the idea of 'sphering' the daJa. In this context sphering implies using Mahalanobis distances with respect to a covariance matrix I:, which could be the covariance matrix of these observations if n > p. Another idea is the Manhattan or L1 distance, that sums the absolute differences in features. For categorical data, the most commonly used dissimilarity is based on the simple matching coefficient, that is the proportion Crs of features which are common to the two observations r and s. As this is between zero and one, the dissimilarity is found by drs = 1 -Crs· For binary features, it might be thought that having a feature present in both observations should be considered a more important indication of similarity than having it absent in both. (Think of types of pottery found in neolithic graves.) The Jaccard\", 'than having it absent in both. (Think of types of pottery found in neolithic graves.) The Jaccard coefficient Crs considers the proportion of features which are present in one or other observation which are found in both. Once again, drs = 1-Crs· Coefficients between zero and one which are high for similar observations are quite common, and called similarity coefficients. For ordinal data, the most appropriate treatment seems to be to use the ranks as if they were continuous data, probably after rescaling to the range [0, 1] so that every ordinal feature is given equal weight. There then arises the question of how to handle mixtures of continuous, categorical and ordinal features. The definition of Gower (1971) has been widely adopted. For each feature f we define a dissimilarity d{s, and an indicator I fs which is one only if feature f is recorded for both observations. Further, I/s = 0 if we have a categorical feature and an']\n",
      "[\"9.2 Multidimensional scaling 307 absence-absence match. Then (9.2) The classical or metric method In the classical or metric method of multidimensional scaling, often known as principal coordinate analysis (Gower, 1966) but going back to Schoenberg (1935), Young & Householder (1938) and Torgerson (1952, 1958), we assume that the dissimilarities were derived as Euclidean distances between n points in p dimensions, for unknown p. Given the distances, we obviously cannot recover the observations themselves, since the distances are invariant to rigid motions (translations, rotation and reflections) of JRP. It transpires that this is the only freedom allowed. Proposition 9.5 For any symmetric matrix T, define the matrix T' = _! [r _ (Tl)l T _ l(Tl)T + 1 TTl] 2 n n n2 by subtracting row and column means and adding back the overall mean, or, equivalently, by removing row means then column means. (a) Given any configuration X of n points in JRP, the matrix T = (d;s = llxr-Xsf) gives a\", \"means. (a) Given any configuration X of n points in JRP, the matrix T = (d;s = llxr-Xsf) gives a non-negative definite T' = xxT. Such a set of distances is called Euclidean. (b) Given a symmetric n x n matrix T with non-negative definite T', we can find a configuration of points in JR(n-l) such that T = (d;5). (c) A necessary and sufficient condition for an n x n matrix T to be a squared distance matrix is that w T Tw ~ 0 for all w with w T 1 = 0. (d) Any two configurations of n points with the same (d;5) differ only by a shift and a rigid motion of JRP, so lie in (shifted) subs paces of the same minimal dimension, the rank of T'. Proof: (a) Without loss of generality, centre the data so every column of X has zero mean. Then T = (llxr-Xsll2) = (llxrll2+11xsll2-2x,!xs) = ElT +lET -2XXT where E = (llxrll2). Let e = ET1 so Tl = nE+el and 1 TEl= 2ne. Thus -2T' = ElT +lET-2XXT-ElT-e11T /n -lET-ell T /n + 2ne11 T jn2 = -2XXT\"]\n",
      "[\"308 9 Unsupervised Methods which is non-positive definite. (b) Let T' = CD2CT be the eigendecomposition of T', noting that the eigenvalues are non-negative, and by construction T' has zero column sums and so has rank r at most (n-1). Take X as the first r columns of CD, so T' = xxr. This configuration is centred, since IIX1112 = lTT'l = 0. Note that (llxrll2) = diag(XXT) = diag(T'), so T' determines T = (d;s) and (under zero means) this gives the same T' by result (a). (c) Note that [(/ -llT /n)w]TT[(I -llT /n)w] = -2wTT'w which is negative if T' is non-negative definite. (d) The procedure of (b) constructs a canonical configuration which is obtained by a shift (to zero mean) and a rigid motion from either configuration. D Given a Euclideap dissimilarity on n points, this proposition produces a data matrix in r ~ n-1 dimensions with distances equal to the dissimilarity, and part (d) shows that this is the minimal number of dimensions needed. If we want a lower-dimensional view,\", \"shows that this is the minimal number of dimensions needed. If we want a lower-dimensional view, Proposition 9.2 tells us to take the first q principal components of X, and this corresponds to taking only the q largest eigenvalues of T' and the first q columns of CD. This is the optimal approximation in the sense of minimizing the sum of squared dissimilarities minus squared distances over projections, and hence gives most weight to representing large dissimilarities accurately. If the set of dissimilarities is not Euclidean, we can seek an approximation by a Euclidean set in JRk for small k. We know that T' = CDCT cannot be non-negative definite, but we can set all the negative elements and the small positive elements of D to zero and use the columns of CD corresponding to the large positive eigenvalues. If the dissimilarities are close approximations to Euclidean distances in a small number of dimensions, we expect to find a small number of large positive eigenvalues, the rest being\", 'of dimensions, we expect to find a small number of large positive eigenvalues, the rest being near zero. If this is not the case, one of the other techniques may be preferable (but is likely to be much more computationally intensive). One common mistake with classical scaling is to supply squared distances: these are not likely to be simply representable by distances. Sammon mapping Sammon mapping is a multidimensional scaling technique introduced by Sammon (1969) and widely known even where other methods of']\n",
      "[\"9.2 Multidimensional scaling 309 multidimensional scaling are unheard of. Given a dissimilarity d ~n n points it constructs a k-dimensional configuration with distances d to (locally) minimize Note that this is undefined if there are pairs with zero dissimilarity. In contrast to principal coordinate analysis, this gives weight to representing small dissimilarities accurately, which may be desirable if the plot is being used to detect clusters. Sammon used a diagonal Newton method to locally optimize E; this is a Newton method in which the off-diagonal part of the Hessian is ignored, and the step length reduced by a 'magic' factor of 0.3-0.4. Details of the algorithm are given in his paper. We have found that it is quite often necessary to use a smaller step-length factor (or even to use a crude search over step length) to avoid divergence . Most implementations seem to use a random starting point, but starting from a classical solution can save much CPU time, if it is a good\", \"starting point, but starting from a classical solution can save much CPU time, if it is a good approximation. The Sammon mapping for the main group of the viruses example is shown in Figure 9.2 on page 291. This shows much less compact groupings than the principal components plots. As Sammon mapping is a more accurate representation of small distances, this should caution against over-interpretation of those groups. In the viruses example the Sammon algorithm does not converge at all unless the 'magic' factor is reduced to around 10-3. This is not uncommon behaviour when some points have to move very close to each other in the optimization run. Using a random starting configuration (as is common practice) produced very much worse local minima, with E around 0.3--0.5 rather than 0.07 for the configuration shown. Virus 22 (sunn-hemp mosaic virus) is clearly separated in the mapping of the scaled data. Ordinal methods What are known as non-metric or ordinal methods of scaling do not\", 'the scaled data. Ordinal methods What are known as non-metric or ordinal methods of scaling do not attempt to match the dissimilarity by a distance, but to choose a configuration whose distances have similar order properties, that is that points which have larger dissimilarity from a given point should be farther away. For such a method it is immaterial whether we supply (approximate) distances or squared distances, and the fit will be invariant to overall scale of the dataset, as well as to rigid motions.']\n",
      "['310 9 Unsupervised Methods A configuration X gives Euclidean distances brs between pairs of points. We choose an increasing function e so that O(drs) is close to brs· The sum of squares of the differences is used, for then e can be found by isotonic regression, for which there are simple algorithms (Barlow et al., 1972). This is then minimized over the configuration (standardized to have unit sum of squares from the origin) by a gradient descent algorithm. Equivalently we minimize over e and the configuration of points giving rise to distances (brs). This is differentiable with respect to the configuration points (Kruskal, 1971; de Leeuw, 1984). One detail in the implementation of ordinal methods is the treatment of tied dissimilarities. Clearly if dij < dk1 we want O(dij) ::::; O(dkl), but how should we consider dij ::::; dkl? If we insist that O(dij) ::::; O(dkl), we are attempting to preserve the equality of tied dissimilarities, which can be a considerable constraint on the', 'to preserve the equality of tied dissimilarities, which can be a considerable constraint on the solution. It is normal practice to allow such ties to be broken. The idea of ordinal methods is due to Shepard (1962a, b) and was developed into an objective method by Kruskal (1964a, b). \"\\' .. ., 20 .. \" \" .... ,. 0 \"\" \\':.t • C)l \"\\'\" ~g111 33 \"\\'t -5 0 5 Figure 9.6 shows a local mtmmum for ordinal multidimensional scaling for the scaled viruses data. This fit is similar to that by Sammon mapping in Figure 9.2, but the subgroups are more clearly separated, and viruses 10 (frangipani mosaic virus), 17 (cucumber green mottle mosaic virus) and 31 (pepper mild mottle virus) have been clearly Isotonic regression is the name for fitting an increasing or decreasing function by least squares. The solution is piecewise constant. Figure 9.6: Non-metric multidimensional scaling plot of the Tobamovirus group of the viruses example. The variables were scaled before Euclidean distance was used. The points', 'of the viruses example. The variables were scaled before Euclidean distance was used. The points are labelled by the index number of the virus. The fit is poor, with STRESS~ 17%, and we found several local minima differing in where the outliers were placed.']\n",
      "['Figure 9.7: Distortion plots of Sammon mapping and non-metric multidimensional scaling for the viruses data. For the right-hand plot the fitted isotonic regression is shown as a step function. 9.3 Clustering algorithms 311 separated. Figure 9.7 shows the distortions of the distances produced by the Sammon and ordinal scaling methods. Both show a tendency to increase large distances relative to short ones for this dataset, and both have considerable scatter. Sammon mapping Non-metric scaling 0 2 4 6 8 10 0 2 4 6 8 10 observed distances observed distances Figure 9.6 shows some interpretable groupings. That on the upper left is the cucumber green mottle virus, the upper right group is the ribgrass mosaic virus and two others, and a group at bottom centreright (16, 18, 19, 30, 33, 34) are the tobacco mild green mosaic and odontoglossum ringspot viruses. 9.3 Clustering algorithms Clustering algorithms are methods to divide a set of n observations into g groups so that members of the same', 'algorithms are methods to divide a set of n observations into g groups so that members of the same group are more alike than members of different groups. If this is successful, the groups are called clusters. The number of groups g may be pre-assigned, or it may be decided by the algorithm. Formally, a cluster algorithm produces a mapping c: {1, ... , n} ~ {1, ... , g} associating a group with every example. Some (but not all) clustering algorithms work by representing each group by a representative point (not necessarily an example), and these have close links with vector quantization (Section 6.3). All of these methods are just algorithms: even those which aim to optimize a criterion are not guaranteed to find the global optimum. Like all unsupervised methods they are judged by their results; a successful clustering produces groups which can be interpreted by domain experts. Of the many books on clustering, Kaufman & Rousseeuw (1990) is one of the most practically oriented and has', 'books on clustering, Kaufman & Rousseeuw (1990) is one of the most practically oriented and has example FoRTRAN programs']\n",
      "['312 9 Unsupervised Methods which can be obtained from file servers. Older and more comprehensive references are Anderberg (1973), Hartigan (1975), Spath (1985) and Jain & Dubes (1988). Partitioning methods Partitioning methods divide the examples into a pre-assigned number of groups. For data in a Euclidean space .JRP we can assign a cluster centre m to each group, and then choose the cluster centres and the groups so as to minimize the sum of squared distances from each example to its cluster centre. Formally, we minimize The minimization over the cluster centres is easy; we choose the centre of cluster j to be the mean of the examples assigned to cluster j. (Thus knowledge of the clustering c is sufficient to define the cluster centres.) The hard part is the combinatorial task of minimizing over clusterings. This method is sometimes called k-means or c-means, although those terms are also used to refer to specific algorithms. Early references are Forgy (1965), Jancey (1966) and', \"are also used to refer to specific algorithms. Early references are Forgy (1965), Jancey (1966) and MacQueen (1967), but the ISODATA algorithm of Ball (1965) and Hall & Ball (1965) (and Hall & Khanna, 1977) is closely related). All algorithms start with some division of the examples into k groups or a set of k cluster centres. In Forgy's algorithm all examples are re-assigned simultaneously to their nearest cluster centre, each cluster centre moved to the group's mean and this process repeated. A group can become empty in this algorithm, so it may choose less than k groups. MacQueen 's algorithm differs in that each example is considered in turn, and the cluster centres are updated whenever an example is assigned to a group. Both variants always reduce the sum of squared distances, and so must converge. The ISODATA algorithm is a variant of Forgy's in which groups are split or merged (so k changes dynamically); MacQueen also considered splitting and merging. A specific algorithm for\", \"k changes dynamically); MacQueen also considered splitting and merging. A specific algorithm for k-means is given (including FORTRAN code) by Hartigan & Wong (1979). This is based on transferring observations from one group to another; other algorithms also allow the exchange of observations between clusters. Koontz et al. (1975) give a branch-andbound algorithm to find the global minimum of the k-means criterion, which is feasible for small sets of examples. There are also random algorithms based on the idea of simulated annealing (Flanagan et al., 1989; Zeger et al., 1992). This algorithm goes back to Lloyd (1957), which was unpublished until 1982. Jancey's algorithm doubles the size of the moves. See the glossary.\"]\n",
      "[\"See the glossary. Figure 9.8: The clusters suggested by k-means for k = 6 for the virus data displayed on the ordinal multidimensional scaling plot. 9.3 Clustering algorithms 313 Note that k-means can assign any future example to one of the k clusters, since it defines a partition of the whole feature space by the Dirichlet tessellation of the cluster representatives. Most cluster methods do not have this predictive aspect. Figure 9.8 shows the clusters for 6-means for the virus data. The iterative process has to be started somewhere, and in this case was initialized from a hierarchical clustering discussed below. The choice of 6 clusters was by inspection of the visualization plots discussed above and the dendrograms shown in Figure 9.9 (on page 320). 0 k-medoids 3 3 3 ·5 1 1 \\\\ ~ ' 1 1 2 2 2 2 2 The k-means algorithm must choose centres in IR.P, and so can only be used when the dataset is available and consists of continuous features. We can overcome these restrictions by insisting\", 'is available and consists of continuous features. We can overcome these restrictions by insisting that the cluster centres be examples. Then we seek a clustering c and cluster centres Xmi which minimize since this squared dissimilarity is the squared Euclidean distance for (scaled) continuous measurements. This may be dominated by outliers, so it is usual to use dissimilarities without squaring. This is known as k-median or k-medoid clustering (Vinod, 1969). Once again there is a local minimization algorithm which reduces the criterion and so must converge (Kaufman & Rousseeuw, 1990, §2.4). This first selects k centres, then considers swapping a centre with an example which is not a centre and selects the most advantageous such swap. The process is']\n",
      "[\"314 9 Unsupervised Methods repeated until convergence. In general this finds a local minimum, but for k = 2 it finds the global minimum. For k > 2 Massart et al. (1983) give a branch-and-bound algorithm to find the global minimum which is only practicable for small sets of examples. Clusters of different size and shape A different extension of k-means is to allow the distance measure to vary between clusters, that is to allow the size and/or shape of clusters to vary. This can be motivated by assuming that the examples from each of the groups are drawn independently from densities fj(x;()j), but that the labels Si which determined which group was appropriate have been lost. Then both the group density parameters ()j and the labels are regarded as parameters. (This is hard to justify theoretically, and will normally give inconsistent estimates of ()j, as pointed out by Marriott, 1975.) Then the likelihood is of the form t((()j),(si);Y) = IT!s;(xi;()s;) i and the 'maximum likelihood'\", \"Then the likelihood is of the form t((()j),(si);Y) = IT!s;(xi;()s;) i and the 'maximum likelihood' assignment of labels gives the clustering of the examples. Now specialize to normal distributions for the classes. Once the clustering is known we can estimate the means as the sample means for each group. Let Wk be the sum of (xi-x)(xi-x)T within group k. Then the profile log-likelihood becomes L((si);Y) = const-L trace(Wi~::j1) + nj log l~jl j where nj is the number of observations assigned to group j. Next we make some assumptions about the covariance matrices ~j· If these are assumed equal to the identity (or to a common multiple of the identity) we recover the k-means criterion (since trace Wk is the sum of squares to the cluster centre for group j ). If we assume that the variances are equal but otherwise unknown, we find ~ = 2::: Wj/n and the clustering is chosen to minimize 1~1 or equivalently I WI for W = 2::: Wj. (Up to a scale factor, W is the within-group covariance matrix we\", \"I WI for W = 2::: Wj. (Up to a scale factor, W is the within-group covariance matrix we used for linear discriminant analysis in Chapter 3.) This criterion was proposed by H. P. Friedman & Rubin (1967). It can be thought of as applying k-means while allowing the 'sphering' of the data to be adjusted. Thus its view of clusters is as ellipsoids of the same size, shape and orientation. Any of these restrictions can be relaxed. Scott & Symons (1971) relaxed all, and Banfield & Raftery (1993)\"]\n",
      "[\"9.3 Clustering algorithms 315 discuss intermediate cases. The latter also allow a 'uniform' cluster to pick up outliers. These criteria can be optimized by algorithms of the types used for k-means and k-medoids. Adaptive resonance theory The adaptive resonance theory of Carpenter & Grossberg (1987a, b, 1990) and Carpenter et al. (1991a, b, 1992) (see also Moore, 1989; Georgiopoulos et al., 1990, 1991; Huang et al., 1995) is closely related to adaptive versions of k-means such as ISODATA and MacQueen's algorithm, but was expressed in a pseudo-biological language that clouds its simplicity. There are a variety of on-line algorithms that group the input examples in up to a pre-specified number k of clusters. The first algorithm, ART 1, works with binary inputs. Let llxll = 2:: lxd be the Lt norm, for binary vectors the number of non-zero elements. For each of k groups there is a prototype w j which is initially set to the vector of all ones (and is called 'uncommitted'). When an example x\", \"j which is initially set to the vector of all ones (and is called 'uncommitted'). When an example x is presented, it is compared in turn with each w j in order of decreasing wJ xj(€ + llwjll) until a prototype is found with wJ x > plixll. If such a Wj is found, the example is assigned to cluster j and Wj is updated by the bitwise operation Wj +--Wj AND X. This algorithm has two parameters. The tolerance € is infinitesimal, serving to break ties in favour of prototypes with more positive elements. The parameter p < 1 is called the vigilance, and controls the diffuseness of the clusters. Note that the first uncommitted prototype to be considered will be selected. Once all prototypes are committed it is possible that none will be selected and the input is then rejected. ART 1 is restricted to binary inputs and is highly sensitive to noise, since Wj can only be made smaller during updating. We can extend the process to inputs in [0, 1] by replacing the AND operation by a bitwise minimum.\", \"We can extend the process to inputs in [0, 1] by replacing the AND operation by a bitwise minimum. Adding 'momentum' (Moore, 1989) changes the update rule to Wj +--(1-,B)min(wj,X) + ,Bwj for ,8 E [0, 1 ). Complement coding includes both a feature y and its 'complement' 1 -y, so that llxll = p for all examples . With all these changes we have 'fuzzy ART'. It is often assumed that ,B = 1 if the prototype is uncommitted: this is called 'fast-commit slow-recode' learning.\"]\n",
      "['316 9 Unsupervised Methods Adaptive resonance theory provides a large family of algorithms, but only a little analysis has been performed on their properties. It is unclear if they have any advantages over the earlier adaptive kmeans algorithms. It is clear that they have a major disadvantage originally pointed out by Moore (1989), of sensitivity to noise. The update rule can only reduce the coordinates of the prototypes, so if a large number of examples are presented, each having added noise, the prototypes will shrink towards the zero vector. Prototypes which are close to the zero vector will fail the vigilance test, since wJ x = 2:::::: WjiXi ~ max[wj;] llxll-Thus for large enough training sets true clusters will be divided repeatedly into groups which depend on the order of presentation of the examples. Methods based on mixtures Suppose we believe that the examples come from a mixture of sources, and each has a parametrized density fi(x; 8i). The proportions Wi of the mixtures are', 'of sources, and each has a parametrized density fi(x; 8i). The proportions Wi of the mixtures are also unknown. The fitting of such mixture densities to data is discussed in Section 6.4. Once the mixture density has been fitted, we can ask for any future observation x what is the posterior probability that it belongs to component i; this is Now if we view this as a classification problem, we would assign the observation to the component with highest posterior probability. This can be applied to the training examples to produce a clustering method, which partitions the data into a group (possibly empty) for each component (Wolfe, 1970). This method is often confused with the likelihood-based partitioning method. Both employ models which are mixtures of components. However, the maximum likelihood method estimates the parameters in the components from classified data, then optimizes over the classifications, whereas the mixture method fits the parameters in the components and the mixing', 'classifications, whereas the mixture method fits the parameters in the components and the mixing proportions from unclassified data. Fuzzy clustering In partitioning methods, each example is definitely assigned to one cluster. Fuzzy logic allows degrees of membership of sets, so would allow us to divide the membership of example i into proportions Uiv for group v. These membership proportions must be non-negative We assume that the noise allows each coordinate to take values smaller than p.']\n",
      "[\"9.3 Clustering algorithms 317 and sum to one. From the perspective of probability theory, Uiv can be interpreted as a posterior probability of having been generated by component v of a mixture, although in the interpretation below, perhaps Ufv is closer to a posterior probability. The earliest and best-known fuzzy clustering technique is the fuzzy k-means method of Dunn (1974) and Bezdek (1974). This minimizes min L L Ufv II xi-mv 112. (U;v) . . l 1 Here the cluster centre is found as the weighted mean of the whole set of examples with weights Ufv· This method has the same disadvantages as k-means of being restricted to continuous data with X available. Kaufman & Rousseeuw (1990, §4.4) construct a fuzzy equivalent of k-medoids, to minimize It can be shown that the variant of this with squared dissimilarities reduces to fuzzy k-means. Auto Class AutoClass (Cheeseman et al., 1988a, b) is a widely-used 'Bayesian classification system' which is based on mixtures. There are J unknown\", \"is a widely-used 'Bayesian classification system' which is based on mixtures. There are J unknown classes. The major simplifying assumption made is that called idiot's Bayes in Chapter 8, that within each class the features are independent. (For a multivariate normal distribution this corresponds to assuming a diagonal covariance matrix.) A normal distribution is used for continuous features, and a general discrete distribution for discrete features. Conjugate priors are used for the parameters in the component models. Under this assumption it claims a full Bayesian solution, including a random number J of classes. In practice the integration over the parameters for each class density is too difficult, and the usual approximations (expansions about MAP estimators) are used. The value of J is set to a large quantity by trial-and-error, and classes with negligible posterior estimates of proportions are omitted. Mode separation Earlier methods of partitioning were based on the idea of\", 'proportions are omitted. Mode separation Earlier methods of partitioning were based on the idea of separating the modes of a multimodal density, implicitly assumed to be a mixture. For example, Henrichon & Fu (1968) considered projecting onto the']\n",
      "['318 9 Unsupervised Methods first principal component, forming a density estimate (by a kernel estimator or just a histogram), and splitting at each local minimum of the density estimate. Such a procedure is highly sensitive to the precise density estimate used, much more so than would have been realized in 1968. Further procedures are described by Devijver & Kittler (1982, Chapter 11), but all suffer from the need to estimate densities in a high-dimensional space. Hierarchical clustering Biologists are used to taxonomic hierarchies: species are grouped into genera which are grouped into families and so on. Thus we can think of clusters of clusters. Hierarchical methods of clustering produce a tree, usually known as a dendrogram, such as Figure 9.1. This can be read in two directions. From the bottom up, we start with n clusters and the clustering changes at each level as two existing clusters are joined. (This is the agglomerative view.) In the divisive view, we start with one cluster', 'are joined. (This is the agglomerative view.) In the divisive view, we start with one cluster and successively split clusters into two parts until this is no longer possible. These two views represent different families of algorithms. It is not necessary to split into two parts or to combine just two clusters, but this is easier to compute and so normally done. Hierarchical methods avoid specifying how many clusters are appropriate by providing the user with many different partitions by cutting the tree at some level (and normally this will achieve a partition into any specified number of clusters). Sometimes this can help to choose an appropriate number, but users should be warned that none of these partitions may be particularly good, even under the criterion used in the hierarchical algorithm. The levels on Figure 9.1 represen~ a dissimilarity between examples; we can define the tree-dissimilarity d,5 as the minimum height in the tree at which examples r and s belong to the same', 'd,5 as the minimum height in the tree at which examples r and s belong to the same cluster. Such dissimilarities obey not just the triangle inequality but the stronger ultrametric property drt ::::;; min(d, 5, dst)· Thus we can think of hierarchical clustering as approximating a given dissimilarity by an ultrametric dissimilarity. Agglomerative algorithms The essence of an agglomerative algorithm is very simple: pick the two clusters with smallest dissimilarity and merge them. Starting is easy (use each example as a cluster), but we are then faced with defining the']\n",
      "[\"9.3 Clustering algorithms 319 dissimilarity between our merged cluster and all other clusters. There are many methods to do so, and no consensus as to which is best. Two simple ideas are to define the dissimilarity between two clusters to be the minimum and the maximum dissimilarity between pairs, one from each cluster, and these give rise to clustering algorithms known as single-link and complete-link clustering respectively. In single-link clustering, two examples will be joined at level A if and only if we can find a chain of links of pairs of examples with dissimilarity less than A. Thus the tree-dissimilarity drs ~ drs, and it can be shown that single-link gives the largest ultrametric dissimilarity with this property. It will tend to produce long and loosely connected clusters, since only a single link is required. In contrast, complete-link clustering joins two clusters if and only if all members of one cluster are close to the other cluster, and so tends to produce 'compact'\", \"if all members of one cluster are close to the other cluster, and so tends to produce 'compact' clusters, and relatively similar objects can remain separated up to quite high levels in the tree. There are many other rules for combining clusters. The only other one that is widely considered is group-average clustering, in which the combined dissimilarity of two groups is the average of all dissimilarities between members of each group. Unlike single-and complete-link, this depends on the scale of the dissimilarities; the other two are equivariant to increasing transformations (such as the square) of the dissimilarities. We also note that using the increase in the k-means criterion on merging the clusters is often attributed to Ward (1963). By standard analysis of variance computations, this attributes a squared dissimilarity of to clusters A and B. Figure 9.9 shows dendrograms produced by single-link, completelink and group-average clustering for the viruses data. All identify viruses\", \"single-link, completelink and group-average clustering for the viruses data. All identify viruses 10, 11, 17, 22 and 31 as loosely connected to the rest, and single-link also highlights virus 46. (We note that 10, 11, 17, 31, 46 and 48 are called 'miscellaneous' in the original source.) Nevertheless, each graph gives the impression of three or four major groupings of viruses. Divisive algorithms Divisive algorithms are much less known (and so much less used). They do have the advantage that if most interest is on the upper levels of the dendrogram (for example to produce a partition into k clusters for small k) they are much more likely to produce rational clusterings.\"]\n",
      "[\"320 9 Unsupervised Methods single-link complete-link group average At the first step, a divisive method has to consider the 2n-l -1 partitions of n examples into two non-empty sets. This is computationally infeasible, so of course only a small proportion of those partitions are actually considered. By analogy to agglomerative methods, we might seek a division into two clusters A and B such that the dissimilarity between A and B is maximized. This is infeasible, but we can attempt to approximate it by an iterative method. We could, for example, use any of the partitioning methods into k = 2 clusters that we have discussed earlier. For example, Ward's method could be used divisively by applying 2-means recursively. (This seems not to appear in the clustering literature, but is known as an algorithm for tree-structured vector quantization; Gersho & Gray, 1992, §12.4.) Macnaughton-Smith et al. (1964) proposed a method for general dissimilarities which is discussed in detail by Kaufman &\", 'al. (1964) proposed a method for general dissimilarities which is discussed in detail by Kaufman & Rousseeuw (1990, Chapter 5). We first select a single example whose average dissimilarity to the remaining examples is greatest, and transfer that example to cluster B. For all remaining examples of cluster A we compare the average dissimilarity to B with that to the remainder of A. If any examples in A are on average nearer to B, we transfer to B that for which the difference in average dissimilarity is greatest. If there are no such examples the process stops. This process splits a single cluster. We can then split each of the clusters that are created (unless one is Figure 9.9: Dendrograms from three common hierarchical clustering techniques applied to the scaled viruses data. Such plots show the dissimilarity at which clusters are merged on the vertical scale and so show the construction process from bottom to top.']\n",
      "['9.3 Clustering algorithms 321 a singleton or all its members have zero dissimilarity from each other) and repeat the process as far as is required. The splits are not uniquely ordered; Kaufman & Rousseeuw suggest splitting first the cluster with the largest diameter (maximum dissimilarity between members), which will be biased towards clusters with many members. Divisive hierarchical clustering is reminiscent of the classification tree methods discussed in Chapter 7. As there, we can restrict the combinatorial explosion by confining attention to splits which involve just one of the features; such methods are called monothetic. In Chapter 7 the value of a split was computed from the distributions of the class variable in the two daughters; here it must be expressed by the difference in the clusters on the feature variables themselves. The obvious idea is to use the dissimilarity between the two daughters, calculated for example by group-averaging. For binary variables we can interpret', 'the two daughters, calculated for example by group-averaging. For binary variables we can interpret monothetic methods a little further. A split on a binary variable will generate clusters that differ only on the remaining variables, and we want these clusters to be as different as possible. Thus we seek one variable whose difference most accurately reflects the difference of all. This is the aim of association analysis (Williams & Lambert, 1959). Examples We will apply clustering methods to the crabs example. Since we saw in Figure 9.3 that the variation was dominated by crab size, the data were adjusted to crabs of common size, effectively by dividing each measurement by the geometric mean of all five measurements on that crab. Figure 9.10 shows some partitions into four clusters, which we know in advance to be the correct number. The k-means algorithm does rather well (but the clusters are near to spherical here). It is not surprising that the hierarchical clustering does badly; it', \"are near to spherical here). It is not surprising that the hierarchical clustering does badly; it has merged 200 examples and past groupings will tend to dominate at the last stages of agglomeration. The 'maximum likelihood' clustering with ellipsoidal clusters of the same size and shape should do well but does not, probably because the optimizer used seems less effective. Using mixtures of four normals with either a common covariance (which in this problem is close to the truth) or separate covariances did slightly better than k-means, but took considerably longer, the EM algorithm converging in about 10 iterations when started from the centres of the k-means solution but about 50-100 iterations from a random start.\"]\n",
      "['322 9 Unsupervised Methods :g 0 -0.2 -o.1 0.0 0.1 0.2 (a) ~ 9 L-~--------------~ -0.2 -o.1 0.0 0.1 0.2 (c) ~ 0 \"\\' ~ L-~----------------~ -0.2 -o.1 0.0 0.1 0.2 (e) :g 9 :g 9 -0.2 -0.2 -o.1 0.0 0.1 0.2 (b) -o.1 0.0 0.1 0.2 (d) ~ L_~--------------~ 9 -0.2 -o.1 0.0 0.1 0.2 (f) We also tried all the applicable programs of Kaufman & Rousseeuw (1990). The results for divisive clustering and fuzzy clustering are shown in Figure 9.10. In this example k-medoids does well, as well as k-means. On the other hand, fuzzy clustering shows little discrimination, allocating most objects around 40--50% to one cluster, with appreciable proportions to at least two others. The clustering shown is \\'hardened\\' by taking the cluster with the largest membership coefficient. Macnaughton-Smith et al.\\'s divisive method started with a 117/83 division, approximately by sex, then split the sexes by colour form. Group-average clustering had all the females in one group, apart from a small group of 5 outliers on the far', \"clustering had all the females in one group, apart from a small group of 5 outliers on the far left of the plot. 9.4 Self-organizing maps The self organizing map is an algorithm developed by Teuvo Kohonen (1982a, b, 1989, 1990a, 1995). This is usually described in the language of neural networks (involving 'weights') and had a biological motivation Figure 9.10: Sammon mapping plots of the Leptograpsus crabs data adjusted for overall size of the example. Plot (a) shows the true classification . The other five plots show a division into four clusters. Plot (b) shows k-means, initialized by the (c), complete-link hierarchical clustering . Plot (d) shows 'maximum likelihood' clustering with ellipsoidal clusters. Plot (e) shows the classification by Macnaughton-Smith et al.'s divisive method, and (f) shows the 'hardened' classification from fuzzy clustering. Fuzzy clustering was slow, at least 10 times slower than any other method considered.\"]\n",
      "[\"9.4 Self-organizing maps 323 discussed in these references, Kohonen (1993) and Ritter et al. (1992). It is, however, just a specific type of clustering algorithm. In the k-means method we saw that an example was assigned to the cluster whose representative mj is nearest to the example. This is precisely what happens in SOM, but the training algorithm attempts to assign some structure to the representatives mi. A large number of clusters are chosen, and arranged on a regular grid in one or two dimensions. (Both square and hexagonal grids have been used.) The idea is that the representatives (called 'weights' by Kohonen) are spatially correlated, so that representatives at nearby points on the grid are more similar that those which are widely separated. This process is conceptually similar to multidimensional scaling. That maps similar examples to nearby points in a q-dimensional space. If we were to discretize the q-dimensional space, for example by dividing it into a grid of square\", \"If we were to discretize the q-dimensional space, for example by dividing it into a grid of square bins, we would have a mapping from the space of possible examples into a discrete space that provides a clustering. Further, we could average the examples which are mapped to each bin to provide a representative for each non-empty bin, and the representatives in nearby bins would be similar. This is precisely the spirit of SOM, and it is often used to provide a crude version of multidimensional scaling. Indeed Kohonen says 'I just wanted an algorithm that would effectively map similar patterns (pattern vectors close to each other in the input signal space) onto contiguous locations in the output space.' (Kohonen, 1995, p. VI.) We have a spatial smoothness property of the cluster representatives which Kohonen refers to as topological ordering. Cherkassky & Mulier (1994) draw analogies with principal curves, but those with multidimensional scaling seem closer. Kohonen defined an 'on-line'\", \"principal curves, but those with multidimensional scaling seem closer. Kohonen defined an 'on-line' algorithm, so examples are presented in some order (possibly random) until convergence. The cluster representatives are initially assigned at random in some suitable distribution. Whenever an example x is presented, the closest representative mj is found. Then m; +--m; + a[x-m;] for all neighbours i (9.3) for all representatives i which are neighbours of j on the grid. Both the constant a and the definition of 'neighbour' are allowed to change with time. A typical specification is that a might decline linearly from 1.0 to 0.04 over 1000 examples, then linearly to zero over the second thousand, while the definition of a 'neighbour' is a grid point i within\"]\n",
      "['324 9 Unsupervised Methods distance r of j, where r declines linearly from 6 to 1 over the first 1000 examples. This defines just an algorithm , and the result will depend on the random initialization , the order of presentation of the examples and the tuning of the constants. Clearly it will be necessary to start with fairly large neighbourhoods, or no global order will emerge. Rather than update all clusters within the neighbourhood equally, it is natural to have a distance-weighted factor within the update, so for all i (9.4) where hiJ depends on the proximity of i to j, for example hiJ(t) = a1 exp -[d(i,j)/a1f. It is possible that some representatives may never get updated unless the initial neighbourhoods are very large. On the other hand, if the neighbourhoods are large, the representatives get updated in blocks, and it is wasteful to have so fine a grid. It is clearly better to refine the grid rather than shrink the neighbourhoods , an idea Haykin (1994) attributes to Luttrell', \"the grid rather than shrink the neighbourhoods , an idea Haykin (1994) attributes to Luttrell (cf Luttrell, 1989). It is helpful to note what happens if we take neighbourhoods so small that they only contain one point. Then there will never be any connection between representative points, and we might expect SOM to reduce to k-means clustering. It does. Although the algorithm appears to update only the representative for the cluster that x joins and not the one it leaves, the latter is achieved by the continual presentation of examples and the 'forgetting' property for a > 0. Thus mj is an exponentially-weighted average of all examples which have ever been assigned to cluster j, and will eventually become the average of a stable set. This suggests that we can regard SOMas a spatially smooth version of k-means, and assess the degree of fit of a particular solution by the quantization error, the sum of squared differences between examples and the corresponding cluster centres. Analysis\", \"the sum of squared differences between examples and the corresponding cluster centres. Analysis of the algorithm has been hampered by the lack of an 'energy' function that the algorithm can be considered to minimize, and Erwin et al. (1992) showed that in general no such function exists. However, if we restrict attention to randomly sampling from a training set and take fixed neighbourhoods, clearly a suitable energy function is V = ELhijiiX-md2 j where I is the cluster to which the randomly chosen input X is assigned (Ruzicka, 1993). For randomly sampled inputs from a population, few results are known except for the special case of just one feature on [0, 1]\"]\n",
      "[\"Figure 9.11: SOM mapping of the crabs data to a 6 x 6 grid. The labels of those examples mapped to each cluster are distributed randomly within the circle representing the cluster. As before the coding is upper case for males, lower case for females, 'B' for blue and '0' for orange. 9.4 Self-organizing maps 325 CD @ @Q 0~ ® ~ ® 0 8 0 Q 0 ~ b © 0 0 8 ~ b (;) b ® ~ ~ 0 0 00@ ~ C) 0 G0 Q ~ G ~ \\\\ @E!) Q and a linear grid of representations (Cottrell & Fort, 1987; Bouton & Pages, 1993, 1994; Fort & Pages, 1993). A batch version of SOM has been proposed much more recently (Kohonen, 1995, §3.14). This is a simple adaptation of Forgy's algorithm for k-means; simultaneously for all clusters the representative is updated to the (weighted) means of examples which are mapped to a neighbour of the cluster. This step is iterated, slowly decreasing the size of the neighbourhoods. The results of SOM mapping of the crabs data to a 6 x 6 grid are shown in Figure 9.11. (The grid size was chosen to allow\", \"of the crabs data to a 6 x 6 grid are shown in Figure 9.11. (The grid size was chosen to allow a reasonable number of the 200 examples to be mapped to each representative.) This figure illustrates the difficulty of displaying an SOM map. We have five-dimensional data, so cannot show the representatives directly, neither on the grid nor as points in the feature space. What we can do is map each example to its nearest representative (its cluster centre) and display the clustering, as we show in the figure. Contiguity-constrained clustering Kohonen 's SOM produces a grid of clusters. Often we wish to group those clusters into super-clusters, preserving the spatial smoothing. (This is pertinent for Figure 9.11.) One way to do so is to use segmentation methods from image analysis. Many of these reduce to agglomerative hierarchical clustering methods with contiguity constraints. Suppose we consider the group-average method of clustering, but only allow clusters to be merged if they are\", 'consider the group-average method of clustering, but only allow clusters to be merged if they are neighbours (that is']\n",
      "['326 9 Unsupervised Methods that there are members a E A and b E B which are neighbours on the grid). Then by construction clusters will always be connected subgraphs of the grid. Such algorithms are discussed by Gordon (1981), Murtagh (1985, 1995a) and Beaulieu & Goldberg (1989) and applied to a grid of SOM clusters by Murtagh (1995b). Other visualization strategies for SOMs are given by Ultsch (1993a, b), which display the similarity of the representatives mj by showing the magnitude of the gradient, viewing the representatives as a vector field (Figure 9.12). Figure 9.12: Ultsch representation of the SOM representatives for Figure 9.11. The rows and columns between the units represent the magnitude of the gradient (black being high); the greylevel at each unit represents the median of the surrounding gradients. The label is the most common class mapped to that representative.']\n",
      "['10 Finding Good Pattern Features In this chapter we consider the problem of what features should be included when designing our classifier. We should make clear at the outset that this is an impossible problem; there may be no substitute for trying them all and seeing how well the resulting classifier works. However, this may be computationally impracticable, and unless a large test set is available it may be impossible to avoid selection effects, of choosing the best of a large class of classifiers on that particular test set and not for the population. To illustrate the difficulty, consider a battery of diagnostic tests T1. ... , T m for a fairly rare disease, which perhaps around 5% of all patients tested actually have. Suppose test T1 correctly picks up 99% of the real cases and has a very low false positive rate. However, there is a rare special form of the disease that T1 cannot detect, but T2 can, yet T2 is inaccurate on the normal disease form. If we test the diagnostic tests', 'but T2 can, yet T2 is inaccurate on the normal disease form. If we test the diagnostic tests one at a time, we will never even think of including T2, yet Tt and T2 together may give a nearly perfect classifier by declaring a patient diseased if Tt is positive or Tt is negative and T2 is positive. This illustrates that considering features one at a time may not be sufficient. Our aim in this chapter is to indicate single features which are likely to have good discriminatory power (feature selection) or linear combinations of features with the same aim (feature extraction). Unfortunately the methods described can be quite effective with conventional statistical methods (linear and quadratic classifiers) but rather ineffective with modern non-linear classifiers. One reason that this is the last chapter of the book is that its methods are being supplanted by the model selection methods discussed in earlier chapters.']\n",
      "[\"328 10 Finding Good Pattern Features Throughout this chapter we will work with the conventional 0--1 loss although some of the ideas can be extended to situations with genuine costs for erroneous classifications. Thus here the Bayes risk is the error rate of the Bayes rule. We will also concentrate on K = 2 classes, as this suffices to illustrate the principles involved. 10.1 Bounds for the Bayes error The 'gold standard' for a classifier was seen in Chapter 2 to be the Bayes error, the risk of the Bayes classifier. The Bayes classifier does however depend on the information available, and the Bayes error will be higher if only some of the features are measured. Thus it is of interest to estimate the Bayes error as a function of the variables included in the classifier design. Exact calculations are impossible (except in trivial problems) but· we can obtain reasonable upper and lower bounds. Thinking in terms of the Bayes error tells us immediately which features we ideally need, p(c I\", 'Thinking in terms of the Bayes error tells us immediately which features we ideally need, p(c I x) for K -1 of the classes. Of course this is unattainable in practice, although it is one view of the derivation of linear and quadratic discriminants for normal class distributions. In a two-class problem the Bayes error is E. = j min[p( 11 x), p(2! x)] p(x) dx = j min[ntPl (x), n2P2(x)] dx and since min( a, b) ~ a5b1-s for any s E [0, 1], we have E* ~ j[ntPt(xW[n2P2(x)]1-sdx = nfnl-sexp-Jc (10.1) say, where (10.2) This is known as the Chernoff bound on the Bayes error (Chernoff, 1952, 1973). The special case of s = 1/2 was derived earlier by Bhattacharyya (1943) and is therefore known as the Bhattacharyya bound. Because of its greater simplicity it is much more widely used. We will evaluate the Bhattacharyya bound for two normal distributions. It becomes E* ~ fo1Ci. exp-JB JB = ~(Jlt- Jl2f[i(~l + ~2))-1(Jlt-Jl2) + 110 l!(~t + ~2)! 2 g J1~1\"\\'~2\\' (10.3)']\n",
      "[\"1-2El ~ 1-4£'(1-E') = (1-2£')2, and E~. E' ~ 1/2, so .J1-2El ~ 1-2E'. 10.2 Normal class distributions 329 The second term of J B disappears if the two covariance matrices are equal; in that case the Chernoff bound is tightest for s = 1/2. Devijver & Kittler (1982, p. 58) point out that we can also obtain a lower bound on the Bayes risk in terms of the Bhattacharyya coefficient. The 1-nn rule has asymptotic risk E1 = j 2p(11 x)p(21 x) p(x) dx ~ j Jp(11 x)p(21 x) p(x) dx since p(11 x)p(21 x) ~ 1/4. Thus E* ~ E1 ~ jif11f2 exp -JB. However, Proposition 6.1 gives E1 ~ 2£*(1-E*) which we can invert to give ! [1-j1-4n1n2exp-2JBJ ~! [1-)1-2£1] ~ E* (10.4) Of course, these bounds are only of any use if we know (or can estimate accurately) the Bhattacharyya coefficient. The Chernoff and Bhattacharyya coefficients are only two of a large class of separation measures which indicate how dissimilar two probability distributions are, in our case applied to the two class densities. Other measures are\", \"probability distributions are, in our case applied to the two class densities. Other measures are the divergence !{ } P1(x) Jv = 1t1P1(x)- 1t2P2(x) log P2(x) dx (10.5) and the Patrick-Fisher coefficient (Patrick & Fisher, 1969) [ ] 1/2 J p = j { n1P1 (x) -n2p(2(x)} 2 dx (10.6) The idea is to use one of the J coefficients to indicate how good a set of features is likely to be; large values of the coefficient indicate that it is likely that a classifier with low error rate can be found (although this is only guaranteed for Jc and JB ). The divergence Jv is signed, so we would look for large absolute value. 10.2 Normal class distributions In practice the class probability densities Pi(x) are unknown, but progress can be made if we assume that they are normal distributions. We have already seen at (10.3) the expression for JB. There is a similar expression for Jc, and we have Jv = !(1'1 -p2)T[:E11 + :E21HI'1 -p2) +!trace [:E!1:E2 + :E21:E1-21] (10.7) 1 1 2 )p = + - X j(2n)PI2:E11\", \"-p2)T[:E11 + :E21HI'1 -p2) +!trace [:E!1:E2 + :E21:E1-21] (10.7) 1 1 2 )p = + - X j(2n)PI2:E11 j(2n)PI2:E21 j(2n)PI:E1 + :E2I exp -!(1'1 -1'2f [:E1 + :E2r1(1'1 -1'2). (10.8)\"]\n",
      "['330 10 Finding Good Pattern Features Note that for equal covariance matrices (~1 = ~2 = ~), Jv = 8JB = (p1 -p2f~-1(p1 -p2) is the Mahalanobis distance between the class means. We saw on page 22 that the expected error rate for the linear classifier depended only on the Mahalanobis distance, so maximizing J B or J v is equivalent to minimizing the expected error rate of the linear classifier. Other class separation measures commonly used are trace(w-1 B) and IBI/ITI for the between-class B, within-class W and total T covariance matrices defined in Section 3.1. These too are obviously closely related to linear discrimination, and for K = 2 the trace measure reduces to the Mahalanobis separation of the means. Why should we use these measures rather than fit a linear or quadratic classifier and measure its performance directly? If computation permits, there is no reason not to assess performance directly, especially if the performance can be assessed on the actual distributions rather than', 'directly, especially if the performance can be assessed on the actual distributions rather than normal distributions. Even for assumed normal distributions we can compute the expected error rate, using numerical integration for K > 2 classes or for quadratic classifiers. So separation measures are best seen as a computational short cut for suboptimal feature selection. (Feature extraction in linear classifiers is simple: use the linear discriminants.) Most feature selection methods such as forwards and backwards selection and branch-and-bound (discussed in the next section) change the set of features under consideration by adding or deleting a single feature at a time. The various measures depend on the means Jli and variance matrices ~i· These can readily be found by taking the appropriate subsets of the mean vector and covariance matrix for all features, but it is worth noting that updating formulae for the inverses, determinants and traces that occur in the separation measures are', 'formulae for the inverses, determinants and traces that occur in the separation measures are available (for example in Devijver & Kittler, 1982, pp. 266-267). We have acted as if the class means and variances are known. In practice they are estimated from data, and we may bias-correct the formulae for separation measure by similar ideas to those used in Section 2.4. The correction for JB is given in Hjort (1986, §10.3). 10.3 Branch-and-bound techniques The simplest feature selection strategies are stepwise ones. Suppose we wish to choose that combination of k < p features which maximizes some measure J of class separation or classifier performance. We will assume that J is monotone, so that adding features is guaranteed not For example, using a test set or cross-validation.']\n",
      "['10.4 Feature extraction 331 to decrease J. Forwards selection adds a feature at a time, at each stage choosing the addition that most increases J. Backward elimination starts with all p features and at each step drops the feature whose presence least increases J. The backward and forward procedures are optimal at each stage, but are unable to anticipate interactions between features of the sort we considered at the beginning of the chapter. Exchange strategies would start with a subset of size k, perhaps found by forward or backward methods, then try exchanging a feature in the set with any outside it. These strategies are all heuristics to avoid considering all of the very large number of subsets of size k of p variables for even moderate p. We can often find a subset which is guaranteed to be best of size k without considering all subsets by the technique of branch-andbound which is well known in combinatorial optimization and artificial intelligence (Winston, 1992, Chapter 5), and', \"known in combinatorial optimization and artificial intelligence (Winston, 1992, Chapter 5), and was considered in this context by Narendra & Fukunaga (1977). In choosing subsets of a regression, the procedure is best known from the algorithm of Furnival & Wilson (1974). Branch-and-bound allows us to eliminate subsets A from consideration if we know that a larger subset A' has a value of J which is below that of our current best estimate a of the maximum value of J(A) over subsets of size k, for by monotonicity, necessarily J(A) < a. An initial estimate of a is found by one of the heuristic searches (or it is set to -oo ). We start by considering the set A of all the features, and search the tree of subsets of size at least k found by dropping one feature at a time. Whenever we find a subset A with J(A) < a we prune the tree at that point, to ensure that we do not consider any subsets of A (which can also occur elsewhere on the tree with the variables in a different order, and should\", \"A (which can also occur elsewhere on the tree with the variables in a different order, and should also be pruned). Whenever we find a subset of size k with J(A) >a, we remember the subset and increase a to this J(A). There are a number of strategies for the actual search of the tree. We would like to consider 'good' subsets first, and we need to reach the leaves (the subsets of size k) to be able to increase a. So the search needs to be depth-first in 'good' subsets, and is aided by having a good initial estimate of a. 10.4 Feature extraction Feature extraction is generally used to mean the construction of linear combinations aT x of continuous features which have good discrim-\"]\n",
      "['332 10 Finding Good Pattern Features inatory power between classes. It is naturally part of finding linear classifiers, and it is also often used as a data reduction technique , to reduce the number of features to be input to a non-linear classifier. The simplest (and by far the most commonly used) method of feature extraction is to take the principal components of x. This was done, for example, by Candela & Chellappa (1993) in studies of fingerprint images, and by Grother & Candela (1993) in studies of hand-written zip codes. Apparently principal components have nothing to do with discriminatory power (they are an unsupervised technique) and it is easy to envisage (and find) examples where they have little discriminatory power. In problems where the features have been carefully scaled and are highly correlated (like images), large variance of a linear combination may imply that it varies across classes. It is possible, at least in principle, to maximize a measure of class separation', \"across classes. It is possible, at least in principle, to maximize a measure of class separation over one or a few linear combinations of the features. This can be seen as a supervised version of projection pursuit (Section 9.1), in which the measure of 'interestingness' of the projection is related to how well it separates the known classes. Of course, we have to know the class-conditional densities on the projection, but they can be estimated by the methods of Chapter 6, especially by kernel methods. Devijver & Kittler (1982, §8.2.2) suggest that the Patrick-Fisher measure is most suitable for feature extraction with Gaussian kernel estimation since its derivative with respect to the projection direction a can be found analytically. Blue et al. (1994) is the journal version of these reports.\"]\n",
      "['A Statistical Side I i nes This appendix explains more of the background of some statistical ideas which are used at several points in the main text, but may not be well-known even to statistical readers. A.1 Maximum likelihood and MAP estimation A prototypical statistical problem is to estimate the value of some parameter 8 from a finite set !T = (Xi) of data. (In the parlance of pattern recognition, we will refer to this as the training set.) Since 8 is described as a parameter, this implies the existence of a family of probability densities p(x; 8) for 8 E 0, and we will assume that the observations Xi are independent samples from an unknown density po, which might be p( ·; 8) for some 8, but need not be. Two technical asides. The assumption of independence is easily circumvented by taking all the observations as X1. Readers not used to measure-theoretic treatments of probability theory will associate densities with continuous distributions and probability mass functions with', \"theory will associate densities with continuous distributions and probability mass functions with discrete ones. As probability mass functions are densities in the rigorous theory (with respect to counting measure) it is permissible to call both 'densities' and we do so. The likelihood is a function of 8 defined by t(8; !T) = p(!T; 8) = IT p(xi; 8). i Although it is another expression of the joint density, the notation reflects the change in emphasis to fixed data and varying parameter. The maximum likelihood estimator (MLE) then associates with each training set a value of 8 which maximizes t( 8; !T), or 8(!T) = argmax t(8; !T). e\"]\n",
      "[\"334 A Statistical Sidelines We will almost always drop the dependence on f7 and regard (j as a random variable. For sufficiently regular problems (for which the likelihood is differentiable and the maximum occurs in the interior of 0) the maximum likelihood will occur at a stationary point of the log-likelihood, and this is the most common way to find e. Beware though that the MLE need not occur at a stationary point, even a local maximum, but could occur at the boundary of 0. In the Bayesian paradigm, the parameter vector e is random, and so itself has a distribution. The posterior density of 8 can be found by Bayes' formula as p(81f7) cx:p(f718)p(8) =t'(8;f/)p(8). A MAP (maximum a posteriori) estimator of e maximizes p(8 If/) or, equivalently, t( e; fl) p( 8). Thus the maximum likelihood estimator is a MAP estimator for the 'flat' prior over 0, the possibly improper distribution with uniform density. This highlights the problem with a MAP estimator for a continuous parameter; it\", 'uniform density. This highlights the problem with a MAP estimator for a continuous parameter; it finds the mode of a density. Densities are with respect to an underlying measure, and the MAP will depend on that measure. This implies that it will not transform in a sensible way. Suppose e is a parameter expressing a variance. Do we want the MAP of the variance. e, the standard deviation J(J, the precision K = 1/8 or the log-variance loge? The maximum likelihood estimator will transform in the way you would expect (we say it is equivariant to 1-1 transformations) but a MAP estimator will not. Only if the posterior density is highly concentrated about its mode and we allow only smooth transformations is the MAP estimator approximately equivariant. Thus MAP estimators are most useful as a simple summary of a highly concentrated posterior distribution. A.2 The EM algorithm The Expectation-Maximization (EM) algorithm is a device to help find maximum likelihood estimators in a problem with', '(EM) algorithm is a device to help find maximum likelihood estimators in a problem with unobserved data. Suppose we have data X which have been observed and data Y which have not, and a vector of parameters e. The goal is to find the maximum likelihood estimator of e given the observed data X in a situation in which the joint density p(x, y; 8) is known explicitly, but the marginal density of X, p(x; 8), can only be found by numerical summation or integration from the joint density.']\n",
      "[\"A.2 The EM algorithm 335 One application is to problems where we have a series of pairs (Xi, Yi) of which only Xi is observed (and Yi is often the label for a component of a mixture). Then the log-likelihood is L(8;(Xi)) = Llog J p(xi,y;8)dy l and the presence of the logarithm inhibits any simplification. Another application is to missing observations of a few of the features. The idea has many precursors, including the Baum-Welch algorithm in speech recognition (see Baum et al., 1970), but was developed in some generality by Dempster et al. (1977). Let Q(8,8') = E[logp(X, Y;8) I X;8'] which is also a function of the observed data X; the conditional expectation is over the values of Y, and is evaluated as if 8' were the true parameter. The EM algorithm starts at some value 8(0) and alternates two steps: E Find Q(8,8U-1l) = E[logp(X, Y;8) 1 X;8U-1l]. M Choose 8(il to maximize Q(8, 8U-1l). Each iteration increases the log-likelihood L( 8; X) = log p(X; 8). Write L(8;X) = logp(X, Y ;8)\", \"Each iteration increases the log-likelihood L( 8; X) = log p(X; 8). Write L(8;X) = logp(X, Y ;8) -logp(Y I X;8) and take expectations using the density p( Y I X; 8') to obtain L(8;X) = Q(8, 8')-E [logp(Y I X; 8) I X; 8']. Now consider expectations E' with respect to p(Y I X; 8') and let h(Y) = p(Y IX;8)/p(Y IX;8'). Then E'logh(Y) ~ E'h(Y)- 1 = 0 from log x ~ x -1. Thus E'logp(Y IX;8) ~ E'logp(Y IX;8'). Now suppose Q(8, 8') > Q(8', 8'), so L(8;X) = Q(8,8')-E'logp(Y IX;8) > Q(8', 8')-E'logp(Y I X; 8') = L(8' ;X). The increase in likelihood at an EM iteration will be positive provided Q(8, 8U-1l) > Q(8U-1l, 8U-1l), and this is so unless 8(i-l) is already the maximizer. These arguments still apply to what is often called a GEM (generalized EM) algorithm in which Q(8, 8U-ll) is not fully maximized,\"]\n",
      "['336 A Statistical Sidelines but eUl is chosen to increase its value (except at a global maximum, and perhaps at a local maximum). The convergence properties of ( G )EM algorithms are often stated rather loosely. At each step the log-likelihood is increased. In a problem which does have a finite maximum to the likelihood (and by no means all mixture problems do) the sequence L( eUl; X) is bounded above, and so has a limit. That limit need not be a local maximum, but it will be under mild regularity conditions. (Convergence properties are discussed by Boyles, 1983, and Wu, 1983; the proofs in Dempster et al., 1977, are flawed.) Under further regularity conditions we can show the less important condition that the sequence (8Ul) itself converges to a local maximizer of the likelihood. It is often useful to know the Hessian at the (local) maximum likelihood solution, for example to find asymptotic standard errors. Louis (1982) gives an algorithm to do so, based on the complete-data', \"asymptotic standard errors. Louis (1982) gives an algorithm to do so, based on the complete-data likelihood. There is also a Bayesian view of the EM algorithm as a way to find posterior modes for a subset of the parameters. Write 8 = ( cp, 1p) which in the Bayesian paradigm is a random vector, and suppose we wish to find a mode of the posterior density p( cp I X). Now take 1p as the unobserved data. Since logp(c/J I X)= logp(c/J, 1p I X) -logp(1p I c/J,X) the same arguments apply to Q( cp, cp') = E [log p( cp, 1p I X) I c/J', X], so the EM algorithm can be used to help find a MAP estimator of cp. There are 'on-line' versions of the EM algorithm, given for example by Titterington (1984), Celeux & Diebolt (1992) and Jordan & Jacobs (1994). Mixture distributions Most applications of the EM algorithm are either to missing data or mixture distributions. The latter are often particularly simple, and were discussed by Dempster et al. (1977). Suppose we have a density of the form p(x) = L\", 'and were discussed by Dempster et al. (1977). Suppose we have a density of the form p(x) = L wif;(x; cp;) where the parameters cp of the densities may have common components (for example a common covariance matrix in a Gaussian mixture), and the mixing weights (w;) are unrestricted (apart from forming a discrete distribution for the component I ). The parameter vector ()']\n",
      "['A.3 Markov chain Monte Carlo 337 encompasses the weight distribution (w;) and all qy;. We regard the component I as the missing data: Then the E step gives Q(8, 8\\') = L L n;(x) [logf;(x; qy;) +log wd xEY i where ( ) [ ( .) I 8,J wif;(x; 4J;) n; x = E I I = l X= x; = \"\\' ·f·( . A.·)· wj WJ 1 X,\\'+\\'] To maximize this over (w;) we need only consider the second term of Q(8, 8\\'), so we are maximizing 2:: n; log w; where (n;) is the average of the n;(x) over the training set. We have so 2:: n; log w; :::; 2:: n; log n; and w; = n;. For the parameters qy; we maximize the weighted log-likelihood L L n;(x)logj;(x;qy;). i xEY If there are no common parameters, each 4J can be found separately. A.3 Markov chain Monte Carlo Markov chain Monte Carlo methods are iterative methods to simulate from distributions that are not easily simulated by more direct methods. They have been used to simulate stochastic processes for many years (Metropolis et al., 1953; Ripley, 1977, 1979; Geman & Geman, 1984;', 'processes for many years (Metropolis et al., 1953; Ripley, 1977, 1979; Geman & Geman, 1984; Ripley, 1987), but have recently become popular in mainstream Bayesian statistics following their espousal by Gelfand & Smith (1990) and Gelfand et al. (1990). (See Geyer, 1992; Smith & Roberts, 1993; Besag & Green, 1993; Tierney, 1994; Besag et al., 1995; and Gelman et al., 1995, for recent reviews.) We will consider a finite collection Xv, v E V for random variables, and use the notation of Chapter 8, that XA denotes the collection X a, a E A c V. We are interested in sampling from the whole collection X v or from the conditional distribution of XA given XAc, often with the aim of finding aspects of the marginal distribution of some subset A. We can in principle sample successively from the marginal distribution of X1, then from X2l X1, X3l X1,X2 and so on, but these distributions may not be known sufficiently explicitly to sample from. Suppose we do know how to sample from the conditional', 'known sufficiently explicitly to sample from. Suppose we do know how to sample from the conditional distribution of Xv given']\n",
      "['338 A Statistical Sidelines X V\\\\{v} for each random variable (as is required at the last step of successive sampling). Then starting with some set of values for X v, we can pick a variable, and sample it conditionally on the rest. This is repeated for all the variables in some order, and is what is known as the Gibbs sampler. This term is due to Geman & Geman (1984), who showed that for discrete random variables with a finite state space and no zero-probability configurations the joint distribution of X v after n samples converges to the required joint distribution provided only that each random variable is visited infinitely often. Thus we can visit the variables in random or systematic order, and visit some more often than others. The restriction to a strictly positive joint distribution is an essential one, as without it the Gibbs sampler may fail. Consider three binary random variables A, B, C such that B and C are independent given A = 0, but A = 1 implies B = C = 1. The Gibbs', \"A, B, C such that B and C are independent given A = 0, but A = 1 implies B = C = 1. The Gibbs sampler will eventually reach the state A = B = C = 1 and be unable to escape. Assuming irreducibility (that we can move with positive probability from any configuration with positive probability to any other in a finite number of steps) saves convergence (Ripley, 1987, §4.7). Sometimes irreducibility can be retrieved by grouping vertices or by taking zero probabilities to be the limit of very small probabilities (Sheehan & Thomas, 1993), but there are practical limits to the value of these 'tricks'. It is widely assumed that this result still holds for continuous random variables, although the theory is much more complicated and further mild conditions are required (Chan, 1993; Smith & Roberts, 1993, Appendix A; Tierney, 1994). In the discrete case there are many other MCMC schemes. We confine attention to configurations with positive probability (without any loss of generality). Suppose we\", \"attention to configurations with positive probability (without any loss of generality). Suppose we have a transition kernel q(xv, x~) which gives the conditional probability of moving from configuration xv to configuration x'v· A Metropolis algorithm generates a move according to this conditional probability, and accepts it with probability min{1,p(x~)/p(xv)}. This converges to the joint distribution provided (a) the kernel q is symmetric, (b) the process is irreducible, and (c) the process is aperiodic. (This says that the feasible return times to a state have period one, and precludes only returning with positive probability at even times, for example.) A Hastings algorithm (Hastings, 1970) allows an asymmetric kernel q,\"]\n",
      "[\"A.4 Axioms for conditional independence 339 and accepts the move with probability . { 1 p(x~ )q(x~, xv)} m1n , , . p(xv )q(xv, xv) This process converges if it is irreducible and aperiodic. Suppose in a Metropolis-Hastings algorithm we consider moves which change only one random variable. If this is Xv we have p(x'v) p(x~ I X V\\\\{v} = XV\\\\{v}) ----= ------~~--~~ p(xv) p(xviXV\\\\{v}=Xv\\\\{v}) which needs only the conditional distributions as for the Gibbs sampler. Unlike the Gibbs sampler it needs only ratios of distributions, so these need not be normalized. There are a few continuous analogues of the Metropolis-Hastings method. 'Blocked' Gibbs sampler methods use the Gibbs sampler on blocks of random variables rather than on single variables. Sometimes this is a device to ensure irreducibility or to encourage faster convergence. On the other hand, it can be quite natural. Consider a general mixture distribution as at the end of Section A.2. We can treat the set of indicators (Ii) of the\", 'mixture distribution as at the end of Section A.2. We can treat the set of indicators (Ii) of the unknown components for each member of the training set as a block, and sample these simultaneously given the real parameters. Much of the theoretical work when using an MCMC method is proving irreducibility, and in practical examples this can be lengthy (Grenander et al., 1991). In practice the difficulty is knowing when the process has reached equilibrium, and how long to wait between sampling XA if (approximately) independent samples are required. There is much discussion of empirical methods of assessing convergence in the surveys cited at the beginning of this section, but none would recognize the pseudo-equilibrium behaviour reported by Ripley & Kirkland (1990), who give an example in which equilibrium has not been approached after each random variable in the system has been sampled 10,000 times, although the process has appeared stable for 9,500 passes. So great care is needed! A.4', 'times, although the process has appeared stable for 9,500 passes. So great care is needed! A.4 Axioms for conditional independence We will use the notation X .JL Y I Z of Dawid (1979, 1980) to say that random variables X and Y are independent given Z. For discrete random variables it is clear what this means: Pr{X=x,Y =yiZ =z} = Pr{X=xiZ =z}Pr{Y =yiZ =z} whenever Pr{Z = z} > 0.']\n",
      "['340 A Statistical Sidelines If just Z is discrete we can ask that the joint conditional density factorizes. In general we may ensure that the conditional densities exist and require p( X, y I z) = p( X I z) p(y I z) except for z belonging to a set visited by Z with probability zero. Alternatively, we will have p(x I y,z) = p(x I z), and conditional independence will hold whenever there is a version of p(x I y, z) which is a function of x and z alone (since then p(x I z) = E[p(x I Y, z) I Z = z] = p(x I y, z) for any y ). Alternatively (and only for those thoroughly familiar with probability theory) we can use regular conditional probabilities (Freedman, 1971, §10.10) which define a conditional probability for each value of Z, and use the usual definition of independence, the factorization of Pr{X E A, Y E BIZ = z }. We will regard a group of random variables as still a random variable, so in the following X, Y , Z and W may be collections of random variables. The following properties', 'so in the following X, Y , Z and W may be collections of random variables. The following properties are easily derived from the definitions. XJ.LY IZ <==> YJ.LXIZ (A.1) XJ.LY,WIZ = XJ.LY IZ (A.2) XJ.LY,WIZ = XJ.LY IZ,W (A.3) X J.L W I Z, Y and X J.L Y I Z = X J.L Y, w I z. (A.4) Note that on interchanging the roles of W and Y in (A.3) we may replace (A.2-A.4) by X J.L W I Z, Y and X J.L Y I Z <==> X J.L Y, W I Z. (A.5) For strictly positive densities (only) we have also X J.L WI Z, Y and X J.L Y I Z, W =X J.L Y, WI Z. (A.6) Properties (A.4) and (A.6) are different, since in general neither of X J.L Y I Z and X J.L Y I Z, W implies the other. The graphical interpretations of conditional independence discussed in Chapter 8 are all deducible from these axioms. This has led Pearl and his co-workers to term concepts which respect (A.1-A.4) graphoids These exist on Borel spaces, which covers any practicable example. and graphoids which also obey (A.6) are called positive graphoids. This The', 'practicable example. and graphoids which also obey (A.6) are called positive graphoids. This The precise definitions is not pure axiomatization; there are other concepts which obey these vary across his papers. axioms (such as embedded multi-valued dependencies of attributes in databases; Fagin, 1977). If we allow non-disjoint collections of variables we also need to know X..lLY IY,W. (A.7)']\n",
      "['A.4 Axioms for conditional independence 341 J. Q. Smith (1989) uses (A.7), (A.1) and (A.5) as his axiom system for a discussion of conditional independence on DAGs (less powerful than that described in Chapter 8). From these axioms we deduce X,Z .JL y ,Z I z <==> X .JL y I z so we can confine attention to disjoint collections of random variables. The graphical representations of conditional independence discussed in Chapter 8 have stronger properties. Write A .l B I C if set C separates A from B. Separation on an undirected graph satisfies X.lYIZ X.lYUWIZ X.lYIZ X .l W I Z u Y and X .l Y I Z u W Y .l X I Z (A.8) X .l Y I Z (A.9) X .l Y I Z u W (A.lO) X .l Y u W I Z. (A.ll) Note that (A.10) is stronger than (A.3) (and we have already said is not true for conditional independence), and that we do have the intersection condition (A.ll). The first three conditions are immediate. Condition (A.ll) is easily proved by contradiction. (Suppose there is a path from X to Y U W avoiding Z. Then', '(A.ll) is easily proved by contradiction. (Suppose there is a path from X to Y U W avoiding Z. Then this path can be truncated if necessary to have no interior vertices in Y U W. If it ends in Y, it avoids Z U W, and if it ends in W it avoids Z U Y .) Graph separation is also transitive: X .l Y I Z ~ X .l { v} I Z or Y .l { v} I Z if v tt X u Y u Z. ( A.12) On the other hand, d-separation on a DAG satisfies (Pearl, 1988, p. 128) X.lYIZ <==> Y.lXIZ (A.13) X.lYUWIZ ~ X.lYIZ (A.14) X.lYUWIZ ~ X.lYIZUW (A.15) X .l W I Z u Y and X .l Y I Z ~ X.lYUWIZ (A.16) X .l W I Z u Y and X .l Y I Z u W ~ X.lYUWIZ (A.17) X .l Y I Z and X .l W I Z <==> X .l Y u W I Z. (A.18) The first four conditions map to properties of conditional independence, and the fifth is valid for conditional independence under strict positivity. We also have X .l Y I Z and X .l Y I Z U { v} ~ X .l { v} I z or Y .l { v} I z {a} .l { b} I { c, d} and { c} .l { d} I {a, b} ~{a} .l {b} I {d} or {a} .l {b} I {d} (A.19) (A.20)']\n",
      "['342 A Statistical Sidelines none of which are necessary for probabilistic conditional independence. Shenoy & Shafer (1990) also axiomatize marginalization, which enables them to give an abstract version of the computations of marginal probabilities on join trees; Dempster & Kong (1988) had already shown the version of these computations for Dempster-Shafer belief functions. The local computations are about taking a potential representation of a distribution (the density is taken to be a product of potential functions on cliques) and rearranging the terms of the product while keeping the product constant so that the terms are related to marginals. To pass a message from Ci to c1 we need to be able to 1 form the marginal on S = Ci n c1 and 2 combine this new marginal with the existing marginal on c1. For this to be valid, marginals have to be combined in a way that is commutative, associative, marginalization has to be consistent (marginalizing from C to S c C must be the same as first', 'marginalization has to be consistent (marginalizing from C to S c C must be the same as first marginalizing to T then S if C :::J T :::J S ), and marginalization must be distributive over combination. (This means that combining marginals on Ci and c1 and then marginalizing to c1 is the same as marginalizing from Ci to S = C U CJ and then combining marginals on S and Cj.) These axioms are true for other systems of combination and marginalization that arise in belief function theory and database theory. A.5 Optimization Optimization, while not strictly statistical, is used in many statistical procedures. In statistical applications it is only necessary to find a parameter estimate to within a small fraction of its standard error, so for our applications it is more important that the optimization algorithm is quick to reach an approximately right answer than that its convergence (to machine precision) is fast. We will concentrate on good methods for estimating many parameters. More', 'precision) is fast. We will concentrate on good methods for estimating many parameters. More detailed expositions are given by Gill et al. (1981), Dennis & Schnabel (1983) and Fletcher (1987). These methods are iterative. A generic minimization algorithm is of the form: 1 Choose an initial point x0. 2 Select a search direction p. 3 Select a step length a, set s = ap and x +---x + s. We normally ensure that f(x) is decreased. 4 Return to step 2 unless the convergence criteria are met. The convergence criteria will be problem-specific.']\n",
      "[\"The gradient is a column vector of partial derivatives. A.5 Optimization 343 Methods based on Taylor expansions Suppose we have a function f: 1R m ~ 1R which we wish to minimize. We assume that f is differentiable, and let g = V f denote the gradient. A first-order Taylor expansion about any point xo g1ves f(x) ~ f(xo) + g(xo)T (x-xo) and so the value of the function will be reduced by moving to xo-1]g(xo) for small enough 17, provided the derivative is not zero at xo. This is known as the method of steepest descent, since amongst all unit-length vectors a, aT g(xo) is smallest when a oc -g(xo). The Taylor expansion provides no idea how to choose 1], and there are two main strategies. One is to perform a line search for the minimum of f(xo + 17a). This entails that the next step will be orthogonal to this one, and the method tends to move in a series of zig-zag steps. The other strategy is to increase 11 as far as possible while f(xo)-'711g(xo)f remains an adequate approximation to\", \"is to increase 11 as far as possible while f(xo)-'711g(xo)f remains an adequate approximation to f(x-1Jg(xo)). The curvature of the surface f(x) can give us information about the step length, so we now assume that f is continuously twice differentiable and has Hessian matrix H(x). A second-order Taylor expansion gives f(x) ~ f(xo) + g(xo)T (x-xo) + !(x-xof H(xo)(x- xo).' (A.21) and the minimum of the right-hand side occurs at xo-H(xo)-1g(xo) provided that H(x) is positive-definite; otherwise the right-hand side does not have a unique minimum. Algorithms based on this expansion are called Newton methods. It is not always obvious that H(x) is positive-definite, but careful algorithms will check this, and adjust it to be so if it seems likely that it fails to be positive-definite only through rounding errors. Note that Newton methods require no line search, but do not ensure that f(x) is reduced at each step. They can diverge dramatically. Thus practical algorithms will reduce the step\", 'reduced at each step. They can diverge dramatically. Thus practical algorithms will reduce the step length to ensure that the step reduces f(x) and perhaps that (A.21) is a reasonable approximation. Eventually the full step length will always be used, and the convergence then is second-order, that is llxt+l-Xtll = O(llxt-Xt-1112). The convergence properties of Newton methods are unsurpassed, but they are not necessarily so well behaved away from the minimum. The Hessian H(x) measures the curvature at x, but this may not be useful except very close to x. The methods described next build up a quadratic approximation similar to (A.21) which is valid at about the length scale of a current step of the algorithm. The other drawback of']\n",
      "['344 A Statistical Sidelines Newton methods is that H(x) may not be known or be very expensive to compute, much more expensive than function values and derivatives. Methods based on quadratic approximations Suppose our function f is actually quadratic with Hessian G (which does not depend on x), so (A.21) is an equality everywhere. We can garner information about G from the gradients at x and x + s at the beginning and end of a step, since g(x + s) = g(x) + Gs. A quasi-Newton method uses this to build up an approximation B to G. Initially B is set to the best possible guess (the identity matrix if no further information is available). Each step is along -B-1g(x), and after each step B is updated by B ~ B + U, where the correction U is chosen so that after correction Bs = g(x + s)-g(x). If the function is quadratic, we will learn G exactly after m linearly independent steps, and for a general function we will build up a local quadratic approximation to f. There are many ways to find a', 'function we will build up a local quadratic approximation to f. There are many ways to find a correction term U. It is widely believed that the most effective update is the Broyden-Fletcher-GoldfarbShanno (BFGS) formula where y = g(x + s)-g(x). As the search is along ocp = -aB-1g(x), the BFGS update becomes B ~ B-g(x)g(x)T + yyT. g(x)Tp ayTp (A.22) For this method to be viable, we do need B to remain positive-definite. This needs yT s > 0, which can be ensured by choosing a sufficiently accurate line search over a. Quasi-Newton methods generally converge super-linearly (which means llxt+l-Xtll/llxr- Xt-III ---+ 0), but are often more effective than Newton methods away from a local minimum. As with Newton methods, the step length a = 1 is preferred, so a line search along p is only needed in the early stages. The inverse C = B-1 is all that is needed to find the search direction, and (A.22) can be converted to an update for C using the An older name for quasi-Newton methods is variable', 'can be converted to an update for C using the An older name for quasi-Newton methods is variable metric methods. This was proposed separately by all four authors in 1970. Sherman-Morrison-Woodbury formula to give See the glossary.']\n",
      "[\"Code is given by Shanno & Phua (1980). Me~ller's comparisons with BFGS are invalid, since he fails to check if rx = 1 is a sufficiently good value for the step length. A.5 Optimization 345 (A.23) Some but not all accounts suggest that this is less desirable than updating a factorization of B, for example the Cholesky factorization B = LDLT for L lower-triangular and D diagonal. When used on an exactly quadratic function, quasi-Newton methods have conjugate search directions p;, that is pf Gpj = 0 for j =!= i. There are many other methods based on conjugate gradients with this property, and all will find the minimum of a quadratic in at most m steps. We are interested in methods which do not form an m x m matrix. We can find conjugate search directions by Pl = -g(xi) and for 2 ~ i ~ m, fJ. _ Y[lg(xi) _ llg(xi)f 1-ffg(xi_l)ff2 -ffg(xi_l)ff2. The first formula for f3i is the Polak-Ribiere formula, the second the Fletcher-Reeves formula. The two are equal with exact line searches on a\", 'formula, the second the Fletcher-Reeves formula. The two are equal with exact line searches on a quadratic function, but not otherwise. The Polak-Ribiere formula is generally preferred, since if the algorithm is making little progress, f3 ~ 0 and steepest descent is used. The theory of conjugate gradient algorithms assumes that they are re-started (by f3 = 0) every m steps. However, as they should only be used when m is large, there will be at most a few times m iterations and the theory is not relevant. Fairly accurate line searches are needed for conjugate gradient methods, unlike quasi-Newton ones. M0ller (1993) has developed one particular version of conjugate gradients which seems well known in the neural network field; it uses the out-dated Hestenes-Stiefel formula for f3i with a particular line-search algorithm. An alternative way to find conjugate search directions is to use (A.22) taking C = I and exact line searches. More generally, we can retain only a small number of', '(A.22) taking C = I and exact line searches. More generally, we can retain only a small number of updates without explicitly forming C. Such methods are known as limited-memory quasi-Newton methods. Shanno (1990) refers to other promising methods intermediate between the Polak-Ribiere and quasi-Newton methods; see: Liu & Nocedal (1989); Gilbert & Lemarechal (1989); Buckley (1994); and Byrd et al. ( 1994 ). Non-linear least-squares problems Many fitting problems amount to minimizing a sum of squares of the form n E(w) = ~I: IIYj-f(xj; w)ll2 j=l']\n",
      "['346 A Statistical Sidelines for an m-dimensional vector w for parameters. Here y and f are univariate, but multivariable problems can be put in this form by making multiple entries for each example. The gradient and Hessian of E are of a special form, and minimization methods have been developed to exploit this. Let J denote the n x m matrix whose rows are the vectors of(xj;w)jow, and let ri = Yi-f(xi;w) be the residuals. Then oE(w) = J(w)T (r ·) ow 1 and ()2 E(w) = J(w)T J(w)-\"\\'\\\\:\"\" r. ()2 f(xj; w) = J(wf J(w) + Q(w), OWOWT ~ 1 OWOWT j say. The specialized methods assume that Q(w) is negligible, so either the residuals or the curvatures of f are small. This is often not the case in statistical problems. The Gauss-Newton procedure is a Newton algorithm with the Hessian replaced by J(wf J(w), that is taking Q(w) = 0. This is equivalent to the local linear approximation (A.24) It is not uncommon to find that J(wf J(w) is ill-conditioned, and ridge-regression methods may be used to fit', 'uncommon to find that J(wf J(w) is ill-conditioned, and ridge-regression methods may be used to fit (A.24). In this context they are known as Levenberg-Marquardt methods, which replace J(wf J(w) by J(wf J(w)+AI. Ifthe residuals are not small, it should be better to use general quasi-Newton methods or hybrids between Gauss-Newton and quasi-Newton methods (Gillet al., 1981). The local linearization (A.24) is also used to find standard errors for the parameters w. The variance of the least-squares solution w will be approximately (A.25) where a2 is the variance of the observational errors.']\n",
      "[\"Glossary AIC ('An Information Criterion') A method developed by Akaike (1973, 1974) to avoid over-fitting, by penalizing the deviance by twice the number of free parameters. back-fitting An iterative method of fitting additive models, by fitting each term to the residuals given the rest. It is a version of the Gauss-Seidel methods of numerical linear algebra. back-propagation is the method used to calculate the gradient vector of a fitting criterion for a feed-forward neural network with respect to the parameters (weights). Also used for a steepest-descent algorithm with the gradient vector computed in this way. Bayes formula An elementary formula of probability. If B; are disjoint events, and AcU;B; then Pr{B; I A}= Pr{A I B;}Pr{B;} . L:j Pr{A I Bj}Pr{Bj} Bayes rule is a rule which attains the Bayes risk, and so is the 'gold-standard', the best possible for that problem. bias has two meanings. (a) The bias of an estimator is the difference between its mean and the true value. (b) For\", \"(a) The bias of an estimator is the difference between its mean and the true value. (b) For a neural network, parameters which are constants (rather than multiplying signals) are often called biases. BIC has two similar meanings. Akaike (1977, 1978) introduced 'information criterion B'. Schwarz (1978) introduced something which has become known as a 'Bayesian information criterion'. Although most references mean Schwarz's BIC, to avoid confusion this is also known as SBC ('Schwarz Bayes Criterion'). Both penalize the deviance by log n times the number p of free parameters for n examples, but Akaike's has O(p) terms not depending on n. Bieoayme-chebychev inequality For a random variable X with mean J1 and variance a2 < oo we have (J2 Pr{IX- Ji.l > e} ~ 2 f' for all e > 0. This follows from Jensen's inequality applied to (X-J1)2•\"]\n",
      "[\"348 Glossary bootstrap (Efron, 1979) An idea for statistical inference, using trammg sets created by re-sampling with replacement from the original training set, so examples may occur more than once. branch-and-bound A technique in combinatorial optimization to rule out solutions without evaluating them. classification trees Classifiers which partition the examples on one feature at a time. See Chapter 7. classifier A rule to assign a class (or 'doubt' or 'outlier') to new examples. codebook vectors Representative examples of a probability distribution . The term comes from vector quantization. compact set A subset A c IR_m is compact if it is closed and bounded, that is A c [-K,K]m for some K > 0. Compact sets are also called compacta. conjugate gradients A class of methods used in optimization (and solving linear systems). See Section A.5. consistent An estimator is consistent if in large samples it converges to the true parameter value (when there is one). concave A function f is\", 'large samples it converges to the true parameter value (when there is one). concave A function f is concave if -f is convex. convex A set A c IR_m is convex if a.a + (1-a.)b E A whenever a,b EA. A function f:A-IR is convex is f(a.a + (1-a.)b) ~ af(a) + (1-a.)f(b). cross-validation A method of evaluating parameters or classifiers by dividing the training set into several parts, and in turn using one part to test the procedure fitted to the remaining parts. Sometimes used to refer to leaveone-out (or ordinary) cross-validation, where every example is dropped in turn. This term is much abused; it does not mean the use of a test set or validation set. Generalized cross-validation is a measure of the performance of a regularized classifier; see page 141. deviance A measure of fit of a statistical model. The deviance is twice (loglikelihood of the best model minus log-likelihood of the current model). The best model can be the true model or an exact fit (often called a saturated model).', 'model). The best model can be the true model or an exact fit (often called a saturated model). diagnostic paradigm In the terminology of Dawid (1976), modelling the conditional distribution of the class C given the features X. Dirichlet distribution A distribution over probability distributions (nb ... , nK) on K classes. Its density (Berger, 1985, p. 561) is, for a.; > 0 and o ~ n;,l:n; = 1, K p(n) oc IT n~;-1 i=l which has mean at (a.;/ Lt a.t), and is increasingly concentrated as (a.;) increases.']\n",
      "['Glossary 349 Dirichlet tessellation Given a set of points in RP, associate with each those points of RP to which it is nearest. This defines a tile, and the tiles partition the space. Also known as Voronoi or Thiessen polygons in R2• Preparata & Shamos (1985) give algorithmic details. dissimilarity A measure of the dissimilarity of two examples based on their features. Must be non-negative and symmetric. early stopping A method of optimization in which the objective used is not the real goal, and optimization is stopped when another measure of fit starts to rise. This may be critically dependent on the starting value chosen. editing Methods of reducing the training set for use by nearest-neighbour methods. efficiency A statistical term, measuring the performance of estimators. Unless stated otherwise, efficiency is a measure of 1/(n x variance in samples of size n) (for large n). eigendecomposition of a real symmetric matrix A. This is an orthonormal matrix C and a non-negative', \"eigendecomposition of a real symmetric matrix A. This is an orthonormal matrix C and a non-negative diagonal matrix D such that A= CDCT. EM algorithm A device to construct algorithms for maximum likelihood and MAP estimators . See Section A.2. equivariance An invariant procedure is unchanged under a transformation; an equivariant procedure transforms its answer. For example, if () is a parameter and ¢ = g(O), then '¢ = g(O). estimator A rule to assign a parameter value to a set of observations. The value assigned is called an estimate, and the distinction between estimators and estimates is not always observed. feature A measurement on an example, so the training set of examples has measured features and a class for each. feature extraction Creating useful new features by combinations (usually linear) of existing features. feed-forward network A network in which vertices can be numbered so that all connections go from a vertex to one with a higher number. In practice the vertices are\", 'so that all connections go from a vertex to one with a higher number. In practice the vertices are arranged in layers, with connections only to higher layers. generalization A measure of t~e ability of a classifier to perform well on future examples, or such a measure applied to a method to design classifiers. The term comes from psychology and refers to the ability to infer the correct structure from examples. Gibbs sampler A simulation method used in Bayesian inference. See Section A.3. Hermite polynomials Families of orthogonal polynomials. See Thisted (1988, §5.3.2).']\n",
      "[\"350 Glossary Hessian The second derivative matrix of a function f(x). hints The idea of 'hints' is to incorporate qualitative information into the classifier. HME Hierarchical mixtures of experts. A tree-structured way to select a combination of classifiers. See Section 8.5. information (matrix) The Hessian of the log-likelihood with respect to the parameters (the observed information) or its expected value (the Fisher information). Jensen's inequality Suppose f is a convex function on a convex domain, and X a random variable on that domain. Then Ef(X);::: f(EX). Kullback-Leibler divergence between distributions on the same space with densities p and q is J p(x) d(p, q) = p(x) log q(x) dx. learning Choosing the parameters of a classifier (and perhaps also the family of classifiers) from the training set. least false parameter value. If the parametric family does not contain the true distribution, this is the best possible parameter value e in the sense of minimum Kullback-Leibler\", 'distribution, this is the best possible parameter value e in the sense of minimum Kullback-Leibler divergence d(p,pe). See page 32. likelihood The probability density of the observations, viewed as a function of the parameters, not of the observations. logistic The logistic distribution has cumulative distribution function t(x) = exp(x)/[1 + exp(x)], and this function is called the logistic function. Logistic regression and discrimination are based on converting predictions to probabilities through the logistic function. Lp An Lp space is the space of random variables X such that E IXIP < oo or a space offunctions f for which J f(x)Pdx < oo. Convergence Xn --+X in Lp means E (Xn -X)P --+ 0. LVQ Learning Vector Quantization. A method of designing examples for use in nearest-neighbour procedures. See page 201. Mahalanobis distance Given a positive-definite symmetric matrix l: (a covariance matrix), the distance between examples x and y in feature space is (x-y)Tl:-l(x- y). MAP estimator', 'matrix), the distance between examples x and y in feature space is (x-y)Tl:-l(x- y). MAP estimator is the global maximum of the posterior density p(8 I:?/). See Section A.l for further discussion. MAP stands for maximum a posteriori. maximum likelihood estimate is a value which maximizes the likelihood function, or, more loosely, is a stationary point of the likelihood function. missing values are unreported values of features. These could be lost, or they could be deliberate non-measurement and so convey information. (A medical practitioner will not order tests if their value appears low.)']\n",
      "[\"Glossary 351 multilayer perceptron is another name for a feed-forward neural network. Despite the name, the 'neurons' are not usually perceptrons. multivariate analysis is the branch of statistics concerned with multiple observations on each example which are not only of interest to predict or explain just one of the observations. non-informative prior A prior distribution for a parameter vector e which is intended to express ignorance about e. This can be tricky, and often leads to the use of densities which have an infinite integral (known as improper priors), such as a uniform density on R. normal distribution In our usage, this includes both univariate and multivariate distributions. The density is NP-complete It is desirable that algorithms terminate in a time which is polynomial in the size of the problem and the accuracy required. Let P denote all problems for which there is such an algorithm. If we allow nondeterministic algorithms (which are allowed to choose the best option\", 'an algorithm. If we allow nondeterministic algorithms (which are allowed to choose the best option whenever there is a choice) the class of problems is called NP. Equivalently, NP is the class of problems for which a solution could be verified in polynomial time. It is widely believed that NP is strictly larger than P, but this remains an open research problem. A problem in NP is called NP-complete if proving it was in P would establish P = NP, which should be regarded as strong evidence that no polynomial algorithm will ever be found. (Cormen et al., 1990; Sedgewick, 1990; Garey & Johnson, 1979.) NP-bard A NP-hard problem is one that implies a solution to every problem in NP (see NP-complete) but is not known to be in NP. Thus NP-hardness is strong evidence that no polynomial algorithm for the problem will ever be found. on-line methods of parameter estimation adjust the estimate after each new example is seen. ordinal feature An ordinal measurement is one of a series of ordered', \"each new example is seen. ordinal feature An ordinal measurement is one of a series of ordered categories, for example income ('poor', 'sufficient', 'well-off', 'rich', ... ). outliers Outliers are examples which did not (or are thought not to have) come from the assumed population of examples. For example, in digit recognition, the segmentation will fail occasionally, so the data will not be from a digit at all. Parzen windows A name for kernel density estimation once common in the pattern recognition community. perceptron A simple classifier into two classes which thresholds a linear combination of the features. Much publicized by F. Rosenblatt around 1960.\"]\n",
      "[\"352 Glossary plug-in classifier A classifier constructed by assuming that estimated parameter values are in fact the true ones. posterior probability The probability of an event conditional on the observations. predictive classifier A classifier constructed by averaging over the uncertainty in the estimated parameter values. principal components are linear combinations of features with high variance. See Section 9.1. prior probability Probabilities specified before seeing the data, and so based on prior experience or belief. Commonly these are the prior probabilities nk of the classes. profile likelihood Suppose we divide the parameters () = ( ¢, tp ). The profile likelihood for ¢ is the likelihood for () maximized over tp. projection pursuit methods are based on extracting features (linear combinations of the original features). Exploratory projection pursuit (Section 9.1) looks for 'interesting' (non-normal) features, and projection pursuit regression (Section 4.1) uses the extracted\", \"(non-normal) features, and projection pursuit regression (Section 4.1) uses the extracted features in an additive model. pruning is the term used for removing parts of trees and networks with the aim of increasing generalization. See Section 7.2. quasi-Newton methods are methods of optimization which approximate the Hessian using only gradient information. See Section A.5. radial basis functions are a large class of approximating functions , computed as a linear combination of non-linear functions of the distances to a set of centres: rank (of a matrix). The number of linearly independent rows or columns. regularization A class of methods of avoiding over-fitting to the training set by penalizing the fit by a measure of 'smoothness ' of the fitted function. resistant methods are designed to be little affected by outliers. For example, the median is much more resistant than the mean. ridge regression See shrinkage methods. risk of a classifier is the expected loss from using it. The\", 'regression See shrinkage methods. risk of a classifier is the expected loss from using it. The Bayes risk is the lowest attainable risk (using these features). robust methods are designed to be resistant, and also to have high efficiency near some target distribution. For example, although the median is resistant, it is inefficient compared to a trimmed mean. sampling paradigm In the terminology of Dawid (1976), modelling the classconditional densities Pk(x) and, perhaps, the prior probabilities of the classes nk.']\n",
      "[\"Glossary 353 Sherman-Morrison-Woodbury formula Given a non-singular n x n matrix A and column vectors b and d we have provided dT A-1b =F -1. If B and D are n x m matrices for m::::; n then provided the m x m matrix I+ DT A-1B is invertible (Golub & Van Loan, 1989, p. 51). shrinkage methods of estimation 'shrink' an estimator by moving it towards some fixed value (or an overall mean). Ridge regression shrinks regression coefficients towards zero, apart from the constant. The idea is that the shrunken estimator has more bias but lower variance and hence better generalization. The James-Stein example (Cox & Hinkley, 1974, §11.8) shows that this idea works even for the mean of a normal distribution in p ;?; 3 dimensions . simulated annealing is a method of combinatorial optimization based on taking a series of random steps in the search space. See Ripley (1987) or Aarts & Korst (1989). singular value decomposition of a real matrix X = U AVT, where A is a diagonal matrix with decreasing\", 'value decomposition of a real matrix X = U AVT, where A is a diagonal matrix with decreasing non-negative entries, U is an n x p matrix with orthonormal columns, and V is a p x p orthogonal matrix (Golub & Van Loan, 1989). SOFM, SOM Self-organizing (feature) map of Kohonen. See Section 9.4. softmax Given outputs Y1, ... , YK for each of K classes, assign posterior probabilities as K p(k I x) = expyk / L::expyj. j=l The term comes from Bridle (1990a, b), but the idea is that of multiple logistic regression. splines are used in function approximation and smoothing. They are constructed by joining functions defined over a partition of the space: the simplest case is polynomials on adjoining intervals. See Section 4.1. stacked generalization A method of using cross-validation to choose a combination of classifiers. The term is from Wolpert (1992); the idea goes back at least to M. Stone (1974). steepest descent A method of minimization which takes steps along the direction to steepest', '(1974). steepest descent A method of minimization which takes steps along the direction to steepest descent, the gradient vector. For maximization the method is known as steepest ascent or hill-climbing .']\n",
      "['354 Glossary stochastic approximation aims to find the value of Oo solving f(O) = 0, but although we can measure f(O), the result will measured with error. After taking many measurements for fJ with j(fJ) near zero we will be able to find accurate estimators of 00. There are also versions which aim to find the maximizer of f(O). supervised learning Choosing a classifier from a training set of correctly classified examples. t distribution The t distribution in p dimensions with location vector J1 and scale matrix ~ is the distribution of J1 +X /S where X \"\\' Np{O, ~} and vS2 \"\\'x; (Johnson & Kotz, 1972, §37.3; Mardia et al., 1979, p. 57). For v > 2 the mean is J1 and the covariance matrix v~/(v-2). The density is test set A set of examples used only to assess the performance of a fullyspecified classifier. training set A set of examples used for learning, that is to fit the parameters of the classifier. uniform convergence A sequence of functions fn converges uniformly to f if maxx', 'of the classifier. uniform convergence A sequence of functions fn converges uniformly to f if maxx lfn(x)-f(x)l ---+ 0 as n ---+ oo. We have uniform convergence on compacta if this holds whenever the maximum is taken over any compact set K. unsupervised learning Discovering groupings in the training set when none are pre-specified. updating Changing the classifier when new examples become available, possibly lacking their true classifications. validation set A set of examples used to tune the parameters of a classifier, for example to choose the number of hidden units in a neural network. vector quantization A method of encoding data for signal transmission, in which a vector is replaced by one of a finite number of representatives. See page 201. weights The parameters in a neural network model. Also weights given to individual examples, for example to indicate multiple copies.']\n",
      "['References The following volumes are abbreviated: NIPSl (1989) Advances in Neural Information Processing Systems. Proceedings of the 1988 Conference, ed. D. S. Touretzky. San Mateo, CA: Morgan Kaufmann. NIPS2 (1990) Advances in Neural Information Processing Systems 2, ed. D. S. Touretzky. San Mateo, CA: Morgan Kaufmann . NIPS3 (1991) Advances in Neural Information Processing Systems 3, eds R. P. Lippmann , J. E. Moody & D. S. Touretzky. San Mateo, CA: Morgan Kaufmann. NIPS4 (1992) Advances in Neural Information Processing Systems 4, eds J. E. Moody, S. J. Hanson & R. P. Lippmann. San Mateo, CA: Morgan Kaufmann. NIPS5 (1993) Advances in Neural Information Processing Systems 5, eds S. J. Hanson, J. D. Cowan & C. L. Giles. San Mateo, CA: Morgan Kaufmann. NIPS6 (1994) Advances in Neural Information Processing Systems 6, eds J. D. Cowan, G. Tesauro & J. Alspector. San Francisco , CA: Morgan Kaufmann. Many of the papers are reprinted in one or more of the following volumes of reprints:', 'Kaufmann. Many of the papers are reprinted in one or more of the following volumes of reprints: Anderson , J. A. & Rosenfeld, E. (eds) (1988) Neurocomputing : Foundations of Research. Cambridge, MA: The MIT Press. Anderson, J. A., Pellionisz , A. & Rosenfeld, E. (eds) (1990) Neurocomputing 2: Directions for Research. Cambridge, MA: The MIT Press. Dasarathy, B. V. (ed.) (1991) Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques. Los Alamitos , CA: IEEE Computer Society Press. Lau, C. (ed.) (1992) Neural Networks: Theoretical Foundations and Analysis. New York: IEEE Press. Shafer, G. & Pearl, J. (eds) (1990) Readings in Uncertainty Reasoning. San Mateo, CA: Morgan Kaufmann. Shavlik, J. W. & Dietterich, T. G. (eds) (1990) Readings in Machine Learning. San Mateo, CA: Morgan Kaufmann. Aarts, E. & Korst, J. (1989) Simulated Annealing and Boltzmann Machines. New York: Wiley. Abramowitz , M. & Stegun, I. A. (1965) Handbook of Mathematical Functions with Formulas , Graphs and', ', M. & Stegun, I. A. (1965) Handbook of Mathematical Functions with Formulas , Graphs and Mathematical Tables. New York: Dover. Abu-Mo stafa, Y. S. (1989) The Vapnik-Chervonenkis dimension : information versus complexity in learning. Neural Computation 1, 312-317. Abu-Mostaf a, Y. S. (1990) Learning from hints in neural networks. Journal of Complexit y 6, 192-198. Abu-Mostafa, Y. S. (1993) A method for learning from hints. In NIPS5, pp. 73-80. Abu-Mostafa , Y. S. (1995a) Financial market applications of learning from hints. In Neural Networks in the Capital Markets, ed. A.-P. Refenes, pp. 221-232. Chichester : Wiley. Abu-Mostaf a, Y. S. (1995b) Machines that learn from hints. Scientific American 272(4), 64--69. Abu-Mostaf a, Y. S. (1995c) Hints. Neural Computation 7, 639-671. Ackley, D. H., Hinton, G. E. & Sejnowski, T. J. (1985) A learning algorithm for Boltzmann machines. Cognitive Science 9, 147-169. Reprinted in Anderson & Rosenfeld (1988). Agosta, J. M. (1990) The structure of', 'Science 9, 147-169. Reprinted in Anderson & Rosenfeld (1988). Agosta, J. M. (1990) The structure of Bayes networks for visual recognition. In Uncertainty in Artificial Intelligence 4, eds R. D. Shachter, T. S. Levitt, L. N. Kanal & J. F. Lemmer, pp. 397-405. Amsterdam: North Holland.']\n",
      "['356 References Agrawala, A. K. (ed.) (1977) Machine Recognition of Patterns. New York: IEEE Press. Aitchison , J. & Aitken, C. G. G. (1976) Multivariate binary discrimination by the kernel method. Biometrika 63, 413-420. Aitchison, J. & Dunsmore, I. R. (1975) Statistical Prediction Analysis. Cambridge: Cambridge University Press. Aitchison, J., Habbema, J. D. F. & Kay, J. W. (1977) A critical comparison of two methods of statistical discrimination. Applied Statistics 26, 15-25. Aitken, C. G. G. (1978) Methods of discrimination in multivariate binary data. In Proceedings of COM PSTAT 1978, eds L. C. A. Corsten & J. Hermans. pp. 155-161. Vienna: Physica-Verlag . Aitken, C. G. G. (1983) Kernel methods for the estimation of discrete distributions. Journal of Statistical Computation and Simulation 16, 189-200. Aizerman, M. A., Braverman, E. M. & Rozonoer, L. I. (1964a) Theoretical foundations of the potential function method in pattern recognition learning. Automation and Remote Control 25,', 'of the potential function method in pattern recognition learning. Automation and Remote Control 25, 821-837. Aizerman , M. A., Braverman , E. M. & Rozonoer, L. I. (1964b) The probability problem of pattern recognition learning and the method of potential functions. Automation and Remote Control 25, 1175-1190. Aizerman, M. A., Braverman, E. M. & Rozonoer, L. I. (1965) The Robbins-Munro process and the method of potential functions. Automation and Remote Control 26, 1882-1885. Akaike, H. (1973) Information theory and an extension of the maximum likelihood principle. In Second International Symposium on Information Theory, eds B. N. Petrov & F. Caski, pp. 267-281. Budapest : Akademiai Kaid6. Reprinted in Breakthroughs in Statistics, eds Kotz, S. & Johnson, N. L. (1992), volume I, pp. 599-624. New York: Springer. Akaike, H. (1974) A new look at statistical model identification. IEEE Transactions on Automatic Control 19, 716-723. Akaike, H . (1977) On entropy maximization principle. In', 'on Automatic Control 19, 716-723. Akaike, H . (1977) On entropy maximization principle. In Applications of Statistics, ed. P. R. Krishnaiah, pp. 27-42. Amsterdam: North-Holland . Akaike, H. (1978) A Bayesian analysis of the minimum AIC procedure. Annals of the Institute of Statistical Mathematics 30A, 9-14. Akaike, H. (1985) Prediction and entropy. In A Celebration of Statistics. The lSI Centenary Volume, eds A. C. Atkinson & S. E. Fienberg, pp. 1-24. New York: Springer. Albert, A. & Anderson, J. A. (1984) On the existence of maximum likelihood estimates in logistic regression models. Biometrika 71, 1-10. Albert, A. & Lesaffre, E. (1986) Multiple group logistic discrimination. Computers and Mathematics with Applications 12A, 209-224. Albertini, F., Sontag, E. D. & Maillot, V. (1993) Uniqueness of weights for neural networks. In Mammone (1993), pp. 115-125. Aleksander, I. & Morton, H. (1990) An Introduction to Neural Computing. London: Chapman & Hall. Alexander, K. S. (1984)', 'H. (1990) An Introduction to Neural Computing. London: Chapman & Hall. Alexander, K. S. (1984) Probability inequalities for empirical processes and a law of the iterated logarithm. Annals of Probability 12, 1041-1067 . Almond, R. G. (1995) Graphical Belief Modeling. London: Chapman & Hall. Amari, S.-1. (1967) A theory of adaptive pattern classifiers. IEEE Transactions on Electronic Computers 16, 299-307. Amari, S.-I. (1993) Mathematical methods of neurocomputing . In Networks and Chaos-Statistical and Probabilistic Aspects, eds 0. E. Barndorff-Nielsen, J. L. Jensen & W. S. Kendall, pp. 1-39. London: Chapman & Hall. Amit, D. J. (1989) Modeling Brain Function. The World of Attractor Neural Networks. Cambridge: Cambridge University Press. Anderberg, M. R. (1973) Cluster Analysis for Applications. New York: Academic Press. Andersen, S. K., Olesen, K. G., Jensen, F. V. & Jensen, F. (1989) HUGIN -a shell for building Bayesian belief universes for expert systems. In Proceedings of the 11th', '-a shell for building Bayesian belief universes for expert systems. In Proceedings of the 11th International Joint Conference on Artificial Intelligence, pp. 1080-1085. San Mateo, CA: Morgan Kaufmann. Reprinted in Shafer & Pearl (1990). Anderson, J. A. (1972) Separate sample logistic discrimination. Biometrika 59, 19-35. Anderson, J. A. (1982) Logistic discrimination. In Handbook of Statistics 2: Classification, Pattern Recognition and Reduction of Dimensionality, eds P. R. Krishnaiah & L. N. Kana!, Amsterdam : North Holland, pp. 169-191. Anderson , J. A. & Phillips, P. R. (1981) Regression, discrimination and measurement models for ordered categorical variables. Applied Statistics 30, 22-31. Anderson, J. A. & Rosenfeld, E. (eds) (1988) Neurocomputing : Foundations of Research. Cambridge, MA: The MIT Press.']\n",
      "['References 357 Anderson , J. A., Pellionisz, A. & Rosenfeld, E. (eds) (1990) Neurocomputing 2: Directions for Research . Cambridge, MA: The MIT Press. Anderson, T. W. (1984) An Introduction to Multivariate Statistical Analysis. Second edition. New York: Wiley. Anderson , T. W. & Bahadur, R. R. (1962) Classification into two multivariate normal distributions with different covariance matrices. Annals of Mathematical Statistics 33, 420--431. Andreassen, S., Jensen, F. V. & Olesen, K. G. (1991) Medical expert systems based on causal probabilistic networks. International Journal of Biomedical Computing 28, 1-30. Angluin, D. (1987) Learning regular sets from queries and counterexamples. Information and Computation 75, 87-106. Angluin, D. (1988) Queries and concept learning. Machine Learning 2, 319-342. Angluin, D. (1993) Learning with queries. In Computational Learning and Cognition, ed. E. B. Baum, pp. 1-28. Philadelphia: SIAM. Angluin, D. & Valiant, L. G. (1979) Fast probabilistic', 'E. B. Baum, pp. 1-28. Philadelphia: SIAM. Angluin, D. & Valiant, L. G. (1979) Fast probabilistic algorithms for Hamiltonian circuits and matchings. Journal of Computer and System Sciences 18, 155-193. Anthony, M. & Biggs, N. (1992) Computational Learning Theory: An Introduction . Cambridge: Cambridge University Press. Anthony, M. & Shawe-Taylor, J. (1993) A result of Vapnik with applications. Discrete Applied Mathematics 47, 207-217. Erratum (1994) 52, 211 (the proof of theorem 2.1 is corrected) . Apolloni, B. & de Falco, D. (1991) Learning by asymmetric parallel Boltzmann Machines . Neural Computation 3, 402-408. Argentiero, P., Chin, R. & Beaudet, P. (1982) An automated approach to the design of decision tree classifiers . IEEE Transactions on Pattern Analysis and Machine Intelligence 4, 51-57. Arbib, M.A. (ed.) (1995) The Handbook of Brain Theory and Neural Networks. Cambridge , MA: MIT Press. Arkedev, A. G. & Braverman , E. M. (1966) Computers and Pattern Recognition . Washington,', \"Press. Arkedev, A. G. & Braverman , E. M. (1966) Computers and Pattern Recognition . Washington, DC: Thompson. Ash, T. (1989) Dynamic mode creation in backpropagation neural networks. Connection Science: Journal of Neural Computing, Artificial Intelligence and Cognitive Research 1, 365-375. Asimov, D. (1985) The grand tour: a tool for viewing multidimensional data. SIAM Journal on Scientific and Statistical Computing 6, 128-143. Assouad , P. (1983) Densite et dimension. Annates de l'Institut Fourier Grenoble 33, 233-282. Averintsev , M. V. (1975) Gibbs description of random fields whose conditional probabilities may vanish. Problemy Peredaci Informatsii 11, 86-96. Baba, N., Mogami, Y., Kohzaki, M., Shiraishi , Y. & Yoshida, Y. (1994) A hybrid algorithm for finding the global minimum of error function of neural networks and its applications. Neural Networks 7, 1253-1265. Bahadur , R. R. (1961a) A representation of the joint distribution of responses to n dichotomous items. In Studies\", '(1961a) A representation of the joint distribution of responses to n dichotomous items. In Studies in Item Analysis and Prediction, ed. H. Solomon , pp. 158-167. Palo Alto, CA: Stanford University Press. Bahadur, R. R. (1961b) On classification based on responses to n dichotomous items. In Studies in Item Analysis and Prediction , ed. H. Solomon , pp. 169-176. Palo Alto, CA: Stanford University Press. Bah!, L. R., Brown, P. F., de Souza, P. V. & Mercer, R. L. (1989) A tree-based statistical language model for natural language speech recognition . IEEE Transactions on Acoustics, Speech and Signal Processing 37, 1001-1008. Bailey, T. & Jain, A. K. (1978) A note on distanceweighted k-nearest neighbor rules. IEEE Transactions on Systems, Man and Cybernetics 8, 311-313. Baird, H. S. (1993) Recognition technology frontiers. Pattern Recognition Letters 14, 327-334. Baldi, P. & Hornik, K. (1989) Neural networks and principal components analysis: learning from examples without local minima.', 'Neural networks and principal components analysis: learning from examples without local minima. Neural Networks 2, 53-58. Reprinted in Anderson et al. (1990). Ball, G. B. (1965) Data analysis in the social sciences: what about the details? In Proceedings of the Fall Joint Computing Conference , pp. 533-559. Washington , DC: Spartan Books. Banfield, J. D. & Raftery, A. E. (1993) Model-based Gaussian and non-Gaussian clustering . Biometric s 49, 803-821. Barlow, R. E., Bartholomew, D., Bremner, J. E. & Brunk, H. M. (1972) Statistical Inference under Order Restrictions. The Theory and Application of Isotonic Regression. London: Wiley.']\n",
      "['358 References Barron, A. R. (1990) Complexity regularization with application to artificial neural networks . In Nonparametric Functional Estimation and Related Topics, ed. G. Roussas, pp. 561-576. Dordrecht: Kluwer Academic Publishers. Barron, A. R. (1993) Universal approximation bounds for superpositions of a sigmoid function. IEEE Transactions on Information Theory 39, 93(}-945. Barron, A. R. (1994) Approximation and estimation bounds for artificial neural networks. Machine Learning 14, 115-133. Barron, A. R. & Cover, T. M. (1991) Minimum complexity density estimation. IEEE Transactions on Information Theory 37, 1034-1054 . Barry, D. (1986) Nonparametric Bayesian regression. Annals of Statistics 14, 934-953. Bartlett, P. L. (1993) Vapnik-Chervonenkis dimension bounds for two-and three-layer networks. Neural Computation 5, 371-373. Bartlett, P. L. & Williamson, R. C. (1996) The VC dimension and pseudodimension of two-layer neural networks with discrete inputs. Neural Computation 8,', \"and pseudodimension of two-layer neural networks with discrete inputs. Neural Computation 8, 625-628. Basford, K. E. & McLachlan , G. J. (1985) Estimation of a!Jocation rates in a cluster analysis context. Journal of the American Statistical Association 80, 286-293. Bashkirov , 0. A., Braverman, E. M. & Muchnik, I. B. (1964) Potential function algorithms for pattern recognition learning machines. Automation and Remote Control 25, 629-631. Bates, D. M. & Watts, D. G. (1988) Nonlinear Regression Analysis and its Applications. New York: Wiley. Bather, J. (1996) A conversation with Herman Chernoff. Statistical Science 11, 335-350. Battiti, R. (1989) Accelerated backpropagation learning: two optimization methods. Complex Systems 3, 331-342. Battiti, R. (1992) First-and second-order methods for learning: between steepest descent and Newton's method. Neural Computation 4, 141-166. Battiti, R. & Massuli, F. (1990) BFGS optimization for faster and automated supervised learning. In Proceedings\", '& Massuli, F. (1990) BFGS optimization for faster and automated supervised learning. In Proceedings of the International Neural Network Conference (Paris, 1990) 2, 757-760. Baum, E. B. (1988) On the capabilities of multilayer perceptrons. Journal of Complexity 4, 193-215. Baum, E. B. & Haussler, D. (1989) What size net gives valid generalization? Neural Computation I, 151-160. Reprinted in Shavlik & Dietterich (1990). Baum, L. E., Petrie, T., Soules, G. & Weiss, N. (1970) A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. Annals of Mathematical Statistics 41, 164-171. Baxt, W. G. (1992) Improving the accuracy of an artificial neural network using multiple differently trained networks. Neural Computation 4, 772-780. Beaulieu , J.-M. & Goldberg, M. (1989) Hierarchy in picture segmentation: a stepwise optimization approach. IEEE Transactions on Pattern Analysis and Machine Intelligence 11, 15(}-163. Beeri, C., Fagin, R., Maier, D.', 'on Pattern Analysis and Machine Intelligence 11, 15(}-163. Beeri, C., Fagin, R., Maier, D. & Yannakakis , M. (1983) On the desirability of acyclic database schemes. Journal of the Association for Computing Machinery 30, 479-513. Begg, C. B. & Gray, R. (1984) Calculation of polychotomous logistic regression parameters using individuali zed regressions. Biometrika 71, 11-18. Beigi, H. S.M. & Li, C. J. (1990) Learning algorithms for neural networks based on quasi-Newton with self-scaling. Intelligent Control Systems 23, 23-28. Beigi, H. S.M. & Li, C. J. (1993) Learning algorithms for neural networks based on quasi-Newton with self-scaling. Journal of Dynamical Systems. Measurement , and Control- Transactions of the ASME 115, 38-43. Benediktsson, J. A. & Swain, P. H. (1992) Consensus theoretic classification methods. IEEE Transactions on Systems, Man and Cybernetics 22, 688-704. Berge, C. (1973) Graphs and Hypergraph s. Amsterdam: North-Holland. Berger, J. 0. (1985) Statistical Decision', '(1973) Graphs and Hypergraph s. Amsterdam: North-Holland. Berger, J. 0. (1985) Statistical Decision Theory and Bayesian Analysis. New York: Springer. Berger, J. 0. & Delampady, M. (1987) Testing precise hypotheses (with discussion). Statistical Science 2, 317-352. Bernardo , J. M. & Smith, A. F. M. (1994) Bayesian Theory. Chichester: Wiley. Besag, J., & Green, P. J. (1993) Spatial statistics and Bayesian computation (with discussion) . Journal of the Royal Statistical Society series B 55, 25-37. Besag, J., Green, P., Higdon, D. & Mengersen, K. (1995) Bayesian computation and stochastic systems (with discussion) . Statistical Science 10, 3-66. Best, D. J. & Rayner, J. C. W. (1988) A test for bivariate normality. Statistics and Probability Letters 6, 407-412. Bezdek, J. C. ( 1974) Cluster validity with fuzzy sets. Journal of Cybernetics 3, 58-72.']\n",
      "['References 359 Bhattacharyya, A. ( 1943) On a measure of divergence between two statistical population s defined by their probability distribution s. Bulletin of the Calcutta Mathematics Society 35, 99-110. Bichsel, M. & Seitz, P. (1989) Minimum class entropy: a maximum information approach to layered networks. Neural Networks 2, 133-141. Bienenstock , E., Cooper, L. N. & Munro, W. (1982) Theory for the development of neuron selectivity: orientation specificity and binocular interaction in the visual cortex. Journal of Neuroscience 2, 32-48. Reprinted in Anderson & Rosenfeld (1988). Binford, T. 0., Levitt, T. S. & Mann, W. B. (1989) Bayesian inference in model-bas ed machine vision. In Uncertainty in Artificial Intelligence 3, eds L. N. Kanal, T. S. Levitt & J. F. Lemmer. Amsterdam: Elsevier. Bishop, C. (1991) Improving the generalization properties of radial basis function neural networks . Neural Computation 3, 579-588. Bishop, C. (1992) Exact calculation of the Hessian matrix for', '. Neural Computation 3, 579-588. Bishop, C. (1992) Exact calculation of the Hessian matrix for the multilayer perceptron. Neural Computation 4, 494-501. Bishop, C. M. (1993) Curvature-driven smoothing: a learning algorithm for feed forward networks. IEEE Transactions on Neural Networks 4, 882-884. Bishop, C. M. (1995a) Neural Networks .for Pattern Recognition. Oxford: Clarendon Press. Bishop, C. M. (1995b) Training with noise is equivalent to Tikohonov regularization. Neural Computation 7, 108-116. Blair, J. R. S. & Peyton, B. (1993) An introduction to chordal graphs and clique trees. In Graph Theory and Sparse Matrix Computations, eds A. George, J. R. Gilbert & J. H. U. Liu, pp. 1-29. New York: Springer. Block, H. D. ( 1962) The percept ron: a model for brain functioning I. Reviews of Modern Physics 34, 123-135. Reprinted in Anderson & Rosenfeld (1988). Block, H. D. & Levin, S. A. (1970) On the boundedness of an iterative procedure for solving a system of linear inequalities.', 'A. (1970) On the boundedness of an iterative procedure for solving a system of linear inequalities. Proceedings of the American Mathematical Society 26, 229-235. Block, H. D., Knight, B. W. Jr & Rosenblatt , F. (1962) Analysis of a four-layer series-coupled perceptron II. Reviews of Modern Physics 34, 135-142. Blue, J. L., Candela, G. T., Grother, P. J. and Wilson, C. L. (1994) Evaluation of pattern classifiers for fingerprint and OCR applications. Pattern Recognition 27, 485-501. Blumer, A., Ehrenfeucht, A., Haussler, D. & Warmuth, M. K. (1987) Occam Razor. Information Processing Letters 24, 377-280. Reprinted in Shavlik & Dietterich (1990). Blumer, A., Ehrenfeucht , A., Haussler, D. & Warmuth, M. K. (1989) Learnability and the VapnikChervonenkis dimension . Journal of the Association .for Computing Machinery 36, 926-965. de Boor, C. (1978) A Practical Guide to Splines. New York: Springer. Bourlard, H. & Kamp, Y. (1988) Auto-association by multilayer perceptrons and singular value', \"Bourlard, H. & Kamp, Y. (1988) Auto-association by multilayer perceptrons and singular value decomposition. Biological Cybernetics 59, 291-294. Bouton, C. & Pages, G. (1993) Self-organization of the one-dimensional Kohonen algorithm with nonuniformly distributed stimuli. Stochastic Processes and their Applications 47, 249-274. Bouton, C. & Pages, G. (1994) Convergence in distribution of the one-dimensional Kohonen algorithms when the stimuli are not uniform. Advances in Applied Probabilit y 26, 80-103. Box, G. E. P. & Tiao, G. C. (1962) A further look at robustness via Bayes's theorem. Biometrika 49, 419-432. Box, G. E. P. & Tiao, G. C. (1973) Bayesian Inference in Statistical Analysis. New York: Wiley. (Formerly Reading, MA: Addison-Wesley.) Box, G. E. P., Hunter, W. G. & Hunter, J. S. (1978) Statistics .for Experimenters: An Introduction to Design, Data Analysis and Model Building. New York: Wiley. Boyles, R. A. (1983) On the convergence of the EM algorithm. Journal of the Royal\", 'New York: Wiley. Boyles, R. A. (1983) On the convergence of the EM algorithm. Journal of the Royal Statistical Society series B 45, 47-50. Bratko, I. & Kononenko , I. (1987) Learning diagnostic rules from incomplete and noisy data. In Interactions in Artificial Intelligence and Statistical Methods, ed. B. Phelps, pp. 142-153. Aldershot: Gower Technical Press. Bratko, I. & Muggleton , S. (1995) Applications of inductive logic programming. Communications of the Association .for Computing Machinery 38, 65-70. Braverman , E. M. (1965) On the method of potential functions. Automation and Remote Control 26, 2130-2138. Breiman, L. (1991) The TI-method for estimating multivariate functions from noisy data (with discussion). Technometri cs 33, 125-160. Breiman, L. (1992) Stacked regressions. Technical Report 367, Dept of Statistics , University of California, Berkeley.']\n",
      "['360 References Breiman, L. (1993) Hinging hyperplanes for regression, classification and function approximation . IEEE Transactions on Information Theory 3, 999-1013. Breiman, L. & Ihaka, R. (1984) Nonlinear discriminant analysis via ACE and scaling. Technical Report 40, Dept of Statistics, University of California, Berkeley. Breiman, L., Friedman, J. H., Olshen, R. A. & Stone, C. J. (1984) Classification and Regression Trees. Monterey, CA: Wadsworth and Brooks/Cole. Brent, R. P. (1991) Fast training algorithms for multilayer neural nets. IEEE Transactions on Neural Networks 2, 346--354. Bridle, J. S. (1990a) Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. In Neuro-computing: Algorithms, Architectures and Applications, eds F. Fogelman Soulie & J. Herault, pp. 227-236. Berlin: Springer. (Although the volume was published in 1990, the article gives a 1989 copyright date.) Bridle, J. S. (1990b) Training', 'was published in 1990, the article gives a 1989 copyright date.) Bridle, J. S. (1990b) Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. In NIPS2, pp. 211-217. Bridle, J. S. & Cox, S. J. (1991) RecNorm: simultaneous normalisation and classification applied to speech recognition. In NIPS3, pp. 234-240. Brier, G. W. (1950) Verification of forecasts expressed in terms of probabilities . Monthly Weather Review 78, 1-3. Brockett, R. W. (1991) Dynamical systems that sort lists, diagonalize matrices and solve linear programming problems. Linear Algebra and its Applications 146, 79-91. Broffit, B., Clarke, W. R. & Lachenbruch, P. A. (1980) The effect of Huberizing and trimming on the quadratic discriminant function. Communications in Statistics-Theory and Methods A9, 13-25. Bronowski, J. & Long, W. M. (1951) Statistical methods in anthropology . Nature 1168, 794. Broomhead, D. S. & Lowe, D. (1988) Multivariable', 'methods in anthropology . Nature 1168, 794. Broomhead, D. S. & Lowe, D. (1988) Multivariable functional interpolation and adaptive networks. Complex Systems 2, 321-355. Brown, P. J. & Rundell, P. W. K. (1985) Kernel estimates for categorical data. Technometrics 28, 293-299. Brown, T. A. & Kaplowitz , J. (1979) The weighted nearest neighbor rule for class dependent sample sizes. IEEE Transactions on Information Theory 25, 617-619. Bryan, J. G. (1951) The generalized discriminant function: mathematical foundations and computational routine. Harvard Educational Review 21, 90--95. Bryant, J. (1989) A fast classifier for image data. Pattern Recognition 22, 45-48. Bryson, A. E. & Ho, Y.-C. (1969) Applied Optimal Control. New York: Blaisdell. (Revised printing New York: Hemisphere, 1975.) Buckland, S. T. (1992a) Fitting density functions with polynomials. Applied Statistics 41, 63-76. Buckland, S. T. (1992b) Algorithm AS270. Maximum likelihood fitting of Hermite and simple polynomial', 'S. T. (1992b) Algorithm AS270. Maximum likelihood fitting of Hermite and simple polynomial densities. Applied Statistics 41, 241-266. Buckley, A. G. (1994) A Fortran-90 code for unconstrained nonlinear minimization. ACM Transactions on Mathematical Software 20, 354-372. Buntine, W. L. (1992) Learning classification trees. Statistics and Computing 2, 63-73. Buntine, W. L. & Weigend, A. S. (1991) Bayesian back-propagation . Complex Systems 5, 603-643. Buntine, W. L. & Weigend, A. S. (1994) Calculating second derivatives on feed-forward networks : a review. IEEE Transactions on Neural Networks 5, 480-488. Burrascano , P. (1991) Learning vector quantization for the probabilistic neural network. IEEE Transactions on Neural Networks 2, 458-461. Byrd, R. H., Nocedal, J. & Schnabel, R. B. (1994) Representations of quasi-Newton matrices and their use in limited memory methods. Mathematical Programming 63, 129-156. Byth, K. & McLachlan , G. J. (1978) The biases associated with maximum', 'Programming 63, 129-156. Byth, K. & McLachlan , G. J. (1978) The biases associated with maximum likelihood methods of estimation of the multivariate logistic risk function. Communications in Statistics-Theory and Methods A7, 877-890. Cacoullos, T. (1966) Estimation of a multivariate density. Annals of the Institute of Statistical Mathematics 18, 179-189. Campbell, N. A. (1980a) Shrunken estimators in discriminant and canonical variate analysis. Applied Statistics 29, 5-14. Campbell, N. A. (1980b) Robust procedures in multivariate analysis I. Robust covariance estimation . Applied Statistics 29, 231-237. Campbell, N. A. (1982) Robust procedures in multivariate analysis II. Robust canonical variate analysis. Applied Statistics 31, 1-8.']\n",
      "['References 361 Campbell, N. A. & Mahon, R. J. (1974) A multivariate study of variation in two species of rock crab of genus Leptograpsus. Australian Journal of Zoology 22, 417-425. Candela, G. T. & Chellappa , R. (1993) Comparative performance of classification methods for fingerprints. US National Institute of Standards and Technology report NISTIR 5163. Cannings, C. & Thompson , E. A. (1981) Genealogical and Genetic Structure . Cambridge: Cambridge University Press. Cannings , C., Thompson , E. A. & Skolnick, M. H. (1978) Probability functions on complex pedigrees . Advances in Applied Probability 10, 26-61. Carbonell , J. G. (ed.) ( 1990) Machine Learning. Paradigms and Methods. Cambridge, MA: The MIT Press. Carpenter, G. A. & Grossberg, S. (1987a) A massively parallel architecture for a self-organizing neural pattern recognition machine. Computer Vision, Graphics, and Image Processing 37, 54-115. Carpenter , G. A. & Grossberg , S. (1987b) ART 2: stable self-organization of stable', '37, 54-115. Carpenter , G. A. & Grossberg , S. (1987b) ART 2: stable self-organization of stable category recognition codes for analog input patterns. Applied Optics 26, 4919-4930. Reprinted in Anderson eta/. (1990). Carpenter, G. A. & Grossberg, S. (1990) ART 3: hierarchical search using chemical transmitters in self-organizing pattern recognition architectures . Neural Networks 3, 129-152. Carpenter , G. A. & Grossberg , S. (1994) Selforganizing neural networks for supervised and unsupervised learning and prediction . In Cherkassky eta/. (1994), pp. 319-348. Carpenter, G. A., Grossberg, S. & Reynolds, J. H. (1991a) ARTMAP: supervised real-time learning and classification of nonstationary data by a selforganizing neural network. Neural Networks 4, 565-588. Carpenter , G. A., Grossberg, S. & Rosen, D. B. (1991b) Fuzzy ART: fast stable learning and categoriz ation of analog patterns by an adaptive resonance system. Neural Networks 4, 759-771. Carpenter, G. A., Grossberg, S., Markuzon,', 'adaptive resonance system. Neural Networks 4, 759-771. Carpenter, G. A., Grossberg, S., Markuzon, N., Reynolds , J. H. & Rosen, D. B. (1992) Fuzzy AR TMAP: a neural network architecture for incremental supervised learning of analog multidimensional maps. IEEE Transactions on Neural Networks 3, 698-713. Carroll, S.M. & Dickinson, B. W. (1989) Construction of neural nets using the Radon transform. In Proceedings of the International Joint Conference on Neural Networks I, 607-611. New York: IEEE Press. Carter, C. & Catlett, J. (1987) Assessing credit card applications using machine learning. IEEE Expert 2(3), 71-79. Casey, R. G & Nagy, G. (1984) Decision tree design using a probabilistic model. IEEE Transaction s on Information Theory 30, 93-99. Celeux, G. & Diebolt, J. (1992) A stochastic approximation type EM algorithm for the mixture problem. Stochastics and Stochastics Reports 41, 119-134. Cestnik, B., Kononenko, I. & Bratko, I. (1987) ASSISTANT 86: a knowledge-elicitation tool for', 'Cestnik, B., Kononenko, I. & Bratko, I. (1987) ASSISTANT 86: a knowledge-elicitation tool for sophisticated users. In Progress in Machine Learning, eds I. Bratko & N. Lavrac, pp. 31-45. Wilmslow: Sigma Press. Chambers, J. M. & Hastie, T. J. (eds) (1992) Statistical Models in S. Pacific Grove, CA: Wadsworth and Brooks/Cole. Chan, C. & Bao, J. (1991) On the design of a tree classifier and its application to speech recognition. International Journal of Pattern Recognition and Artificial Intelligence 5, 677-692. Chan, K. S. (1993) Asymptotic behaviour of the Gibbs sampler. Journal of the American Statistical Association 88, 320-326. Chandran, P. S. (1994) Comments on \"Comparative analysis of backpropagation and the extended Kalman filter for training multilayer perceptrons\" . IEEE Transactions on Pattern Analysis and Machine Intelligence 16, 862-863. Chang, C. L. (1974) Finding prototypes for nearest neighbor classifiers. IEEE Transactions on Computers 23, 1179-1184. Reprinted in', 'for nearest neighbor classifiers. IEEE Transactions on Computers 23, 1179-1184. Reprinted in Dasarathy (1991). Charniak, E. (1991) Bayesian networks without tears. AI Magazine 12(4), 50-63. Chauvin, Y. (1989) A back-propagation algorithm with optimal use of hidden units. In NIPSJ, pp. 519-526. Chavez, R. M. & Cooper, G. F. (1990) A randomized approximation algorithm for probabilistic inference on Bayesian belief networks. Networks 20, 661-685. Cheeseman, P. (1995) On Bayesian model selection. In Wolpert (1995), pp. 315-330.']\n",
      "['362 References Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W. & Freeman, D. (1988a) AutoClass: a Bayesian classification system. In Proceedings of the Fifth International Workshop on Machine Learning , Ann Arbor, pp. 54-64. San Mateo, CA: Morgan Kaufmann. Cheeseman, P., Self, M., Kelly, J., Taylor, W., Freeman, D. & Stutz, J. (1988b) Bayesian classification. In Proceedings of the Seventh AAAI National Conference on Artificial Intelligence , St Paul, MN, pp. 607-611. San Mateo, CA: Morgan Kaufmann. Chen, D. S. & Jain, R. C. (1994) A robust back propagation learning algorithm for function approximation. IEEE Transactions on Neural Networks 5, 467-479. Chen, S., Cowan, C. F. N. & Grant, P. M. (1991) Orthogonal least squares learning algorithm for radial basis function networks. IEEE Transactions on Neural Networks 2, 302-309. Cheng, Y.-Q., Zhuang, Y.-M. & Yang, J.-Y. (1992) Optimal Fisher discriminant analysis using the rank decomposition. Pattern Recognition 25, 101-111.', 'Optimal Fisher discriminant analysis using the rank decomposition. Pattern Recognition 25, 101-111. Cherkassky, V. & Mulier, F. (1994) Self-organizing networks for nonparametric regression . In Cherkassky et a!. (1994), pp. 188-212. Cherkassky, V., Friedman, J. H. & Wechsler, H. (eds) (1994) From Statistics to Neural Networks. Theory and Pattern Recognition Applications. Berlin: Springer. Chernick, M. R., Murthy, V. K. & Nealy, C. D. (1985) Application of bootstrap and other resampling techniques: evaluation of classifier performance. Pattern Recognition Letters 3, 167-178. Chernoff , H. (1952) A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations. Annals of Mathematical Statistics 23, 493-507. Chernoff, H. (1973) Some measures for discriminating between normal multivariate distributions with unequal covariance matrices. In Multivariate Analysis III, ed. P. R. Krishnaiah, pp. 337-344. New York: Academic Press. Chidananda Gowda, K. & Krishna, G.', 'ed. P. R. Krishnaiah, pp. 337-344. New York: Academic Press. Chidananda Gowda, K. & Krishna, G. (1979) The condensed nearest neighbor rule using the concept of mutual nearest neighborhood. IEEE Transactions on Information Theory 25, 488-490. Reprinted in Dasarathy (1991). Chou, P. A. (1989) Recognition of equations using a two-dimensional stochastic context-free grammar . In Visual Communications and Image Processing IV, ed. W. A. Pearlman . SPIE Proceedings Series 1199, 852-863. Chou, P. A. (1991) Optimal partitioning for classification and regression trees. IEEE Transactions on Pattern Analysis and Machine Intelligence 13, 340-354. Chou, W.-S. & Chen, Y.-C. (1992) A new fast algorithm for the effective training of neural classifiers . Pattern Recognition 25, 423-429. Chow, C. K. (1970) On optimum recognition error and reject tradeoff. IEEE Transactions on Information Theory 16, 41-46. Chow, C. K. & Liu, C. N. (1968) Approximating discrete probability distributions with dependence', 'Chow, C. K. & Liu, C. N. (1968) Approximating discrete probability distributions with dependence trees. IEEE Transactions on Information Theory 14, 462-467. Ciampi, A., Chang, C.-H., Hogg, S. & McKinney , S. (1987) Recursive partition : a versatile method for exploratory data analysis in biostatistics . In Biostatistics, eds I. B. MacNeil & G. J. Umphrey, pp. 23-50. Dordrecht: Reidel. Clark, L. A. & Pregibon, D. (1992) Tree-based models. Chapter 9 of Chambers & Hastie (1992). Clark, P. & Niblett, T. (1989) The CN2 induction algorithm. Machine Learning 3, 261-283. Cleveland, W. S., Grosse, E. & Shyu, W. M. (1992) Local regression models. Chapter 8 of Chambers & Hastie (1992). Clifford, P. (1990) Markov random fields in statistics. In Disorder in Physical Systems. A Volume in Honour of John M. Hammersley, eds G. R. Grimmett & D. J. A. Welsh, pp. 19-32. Oxford: Clarendon Press. Clunies-Ross, C. W. & Rilfenburgh , R. H. (1960) Geometry and linear discrimination. Biometrika 47, 185-189.', 'C. W. & Rilfenburgh , R. H. (1960) Geometry and linear discrimination. Biometrika 47, 185-189. Cohen, E., Hull, J. J. & Srihari, S. N. (1991) Understanding handwritten text in a structured environment : determining ZIP codes from addresses. International Journal of Pattern Recognition and Artificial Intelligence 5, 221-264. Cohn, D. & Tesauro, G. (1992) How tight are the Vapnik-Chervonenkis bounds? Neural Computation 4, 249-269. Cook, D., Buja, A. & Cabrera, J. (1993) Projection pursuit indices based on orthonormal function expansions. Journal of Computational and Graphical Statistics 2, 225-250.']\n",
      "['References 363 Coomans, D. & Broeckaert, I. (1986) Potential Pattern Recogniti on in Chemical and Medical Decision Making. Letchworth: Research Studies Press. Cooper, G. F. (1984) NESTOR: a computer-based medical diagnostic aid that integrates causal and probabilistic knowledge, Ph.D. thesis, Dept of Computer Science, Stanford University. Cooper, G. F. (1989) Current research directions in the development of expert systems based on belief networks . Applied Stochastic Models and Data Analysis 5, 39-52. Cooper, G. F. (1990) The computational complexity of probabilistic inference using Bayesian belief networks. Artificial Intelligen ce 42, 393-405. Cooper, G. F. & Herskovits, E. (1992) A Bayesian method for the induction of probabilistic networks from data. Machine Learning 9, 309-347. Cormen, T. H., Leiserson , C. E. & Rivest, R. L. (1990) Introduction to Algorithms. Cambridge MA: The MIT Press and New York: McGraw-Hill. Cortes, C. & Vapnik, V. (1995) Support-vector networks. Machine', \"Press and New York: McGraw-Hill. Cortes, C. & Vapnik, V. (1995) Support-vector networks. Machine Learning 20, 273-297. Cosslett, S. R. (1981) Maximum likelihood estimators for choice-based samples. Econometrica 49, 1289-1316. Cottrell, G. W. & Metcalfe, J. (1991) EMPATH: face, emotion and gender recognition using holons. In NIPS3, pp. 564-571. Cottrell, M. & Fort, J. C. ( 1987) Etude d'un algorithme d'auto-organisation. Annales de l'Institut Henri Poincare 23, 1-20. Cover, T. M. ( 1965) Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition . IEEE Transactions on Electroni c Computers 14, 326-334. Cover, T. M. (1968) Rates of convergence of nearest neighbor procedures. In Proceedings of the First Annual Hawaii Conference on Systems Theory, Honolulu, pp. 413-418. Cover, T. M. (1969) Learning in pattern recognition. In Methodol ogies of Pattern Recognition, ed. S. Watanabe, pp. 111-132. New York: Academic Press. Cover, T. M. &\", 'of Pattern Recognition, ed. S. Watanabe, pp. 111-132. New York: Academic Press. Cover, T. M. & Hart, P. E. (1967) Nearest neighbor pattern classification. IEEE Transactions on Information Theory 13, 21-27. Reprinted in Anderson eta/. (1990), Dasarathy (1991) and Lau (1992). Cowell, R. G. (1992) BAlES-a probabilistic expert reasoning shell with qualitative and quantitative learning. In Bayesian Statistics 4, eds J. M. Bernardo , J. 0. Berger, A. P. Dawid & A. F. M. Smith, pp. 595-600. Oxford: Clarendon Press. Cowell, R. G. (1995) A C++ class library for building Bayesian belief networks. In Gammerman (1995), pp. 159-165. Cowell, R. G. & Dawid, A. P. (1992) Fast retraction of evidence in a probabilistic expert system. Statistics and Computing 2, 37-40. Cox, D. R. ( 1958) Two further applications of a model for binary regression . Biometrika 45, 562-565. Cox, D. R. & Hinkley, D. V. (1974) Theoretical Statistics. London: Chapman & Hall. Cox, D. R. & Snell, E. J. (1989) Analysis of Binary', 'Theoretical Statistics. London: Chapman & Hall. Cox, D. R. & Snell, E. J. (1989) Analysis of Binary Data. Second edition. London: Chapman & Hall. Cox, T. F. & Cox, M. A. A. (1994) Multidimensional Scaling. London: Chapman & Hall. Craven, P. & Wahba, G. (1979) Smoothing noisy data with spline functions : estimating the correct degree of smoothing by the method of generalized crossvalidation. Numerische Matematik 31, 377-403. Crawford, S. L. (1989) Extensions to the CART algorithm . International Journal of Man-Machine Studies 31, 197-217. Crevier, D. (1993) AI. The Tumultuous History of the Search for Artificial Intelligence. New York: Basic Books. Cybenko , G. (1988) Continuous valued neural networks with two hidden layers are sufficient. Technical Report, Dept of Computer Science, Tufts University. Cybenko, G. (1989) Approximation by superpositions of a sigmoidal function. Mathematics of Control Signals, and Systems 2, 303-314. Darken, C. & Moody, J. (1991) Note on learning rate', 'of Control Signals, and Systems 2, 303-314. Darken, C. & Moody, J. (1991) Note on learning rate schedules for stochastic optimization. In NIPS3, pp. 832-838. Dasarathy , B. V. (ed.) (1991) Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques. Los Alamitos, CA: IEEE Computer Society Press. Dattatreya, G. R. & Sarma, V. V. S. (1981) Bayesian and decision tree approaches for pattern recognition including feature measurement costs. IEEE Transactions on Pattern Analysis and Machine Intelligence 3, 293-298. Dattatreya, G. R. & Sarma, V. V. S. (1985) Decision trees in pattern recognition. In Progress in Pattern Recognition 2, eds L. N. Kanal & A. Rosenfeld. Amsterdam: Elsevier.']\n",
      "['364 References Dawid, A. P. (1976) Properties of diagnostic data distributions. Biometrics 32, 647-658. Dawid, A. P. (1979) Conditional independence in statistical theory (with discussion). Journal of the Royal Statistical Society series B 41, 1-31. Dawid, A. P. (1980) Conditional independence for statistical operations. Annals of Statistics 8, 598-617. Dawid, A. P. (1982) The well-calibrated Bayesian (with discussion). Journal of the American Statistical Association 77, 605-613. Dawid, A. P. (1986) Probability forecasting. In Encyclopedia of Statistical Sciences, eds S. Kotz, N. L. Johnson & C. B. Read, pp. 210-218. New York: Wiley. Dawid, A. P. (1992) Applications of a general propagation algorithm for probabilistic expert systems. Statistics and Computing 2, 25-36. Dawid, A. P. & Lauritzen, S. L. (1993) Hyper Markov laws in the statistical analysis of decomposable graphical models. Annals of Statistics 21, 1272-1317. Deely, J. J. & Lindley, D. V. (1981) Bayes empirical Bayes.', 'Annals of Statistics 21, 1272-1317. Deely, J. J. & Lindley, D. V. (1981) Bayes empirical Bayes. Journal of the American Statistical Association 76, 833-841. DeMers, D. & Cottrell, G. (1993) Non-linear dimensionality reduction. In NIPS5, pp. 580-587. Dempster, A. P. & Kong, A. (1988) Uncertain evidence and artificial analysis. Journal of Statistical Planning and Inference 20, 355-368. Reprinted in Shafer & Pearl (1990). Dempster, A. P., Laird, N. M. & Rubin, D. B. (1977) Maximum likelihood from incomplete data via the EM algorithm (with discussion). Journal of the Royal Statistical Society series B 39, 1-38. Dennis, J. E. & Schnabel, R. B. (1983) Numerical Methods for Unconstrained Optimization and Nonlinear Equations. Englewood Cliffs, NJ: PrenticeHall. Devijver, P. A. & Kittler, J. V. (1982) Pattern Recognition. A Statistical Approach. Englewood Cliffs, NJ: Prentice- Hall. Devijver, P. A. & Kittler, J. V. (eds) (1987) Pattern Recognition Theory and Applications. Berlin: Springer.', 'P. A. & Kittler, J. V. (eds) (1987) Pattern Recognition Theory and Applications. Berlin: Springer. Devlin, S. J., Gnanadesikan, R. & Kettenring, J. R. (1981) Robust estimation of dispersion matrices and principal components. Journal of the American Statistical Association 76, 354-362. DeVore, R. A., Howard, R. & Micchelli, C. A. (1989) Optimal nonlinear approximation. Manuscripta Mathematica 63, 469-478. Devroye, L. (1981a) On the inequality of Cover and Hart in nearest neighbor discrimination. IEEE Transactions on Pattern Analysis and Machine Intelligence 3, 7 5-78. Devroye, L. ( 1981 b) On the almost everywhere convergence of nonparametric regression function estimates. Annals of Statistics 9, 1310-1319. Devroye, L. (1982) Bounds for the uniform deviation of empirical measures. Journal of Multivariate Analysis 12, 72-79. Devroye, L. (1988) Automatic pattern recognition: a study of the probability of error. IEEE Transactions on Pattern Analysis and Machine Intelligence 10, 530-543.', 'probability of error. IEEE Transactions on Pattern Analysis and Machine Intelligence 10, 530-543. Diaconis, P. & Freedman, D. (1984) Asymptotics of graphical projection pursuit. Annals of Statistics 12, 793-815. Diaconis, P. & Shahshahani, M. (1984) On non-linear functions of linear combinations. SIAM Journal on Scientific and Statistical Computing 5, 17 5-191. Die bolt, J. & Robert, C. P. (1994) Estimation of finite mixture distributions through Bayesian sampling. Journal of the Royal Statistical Society series B 56, 363-375. Dietterich, T. G. (1990) Machine learning. Annual Review of Computer Science 4, 255-306. Dietterich, T. G. & Bakiri, G. (1991) Error-correcting output codes: a general method for improving multiclass inductive learning programs. In Proceedings, Ninth AAAI National Conference on Artificial Intelligence, Menlo Park, CA: AAAI Press, pp. 572-577. (An identical paper appears as pp. 395-407 of Wolpert (1995).) Dietterich, T. G. & Bakiri, G. (1995) Solving multiclass', 'appears as pp. 395-407 of Wolpert (1995).) Dietterich, T. G. & Bakiri, G. (1995) Solving multiclass learning problems via error-correcting output codes. Journal of Artificial Intelligence Research 2, 263-286. Diggle, P. J. & Hall, P. (1986) The selection of terms in a orthogonal series density estimator. Journal of the American Statistical Association 81, 230-233. Donoho, D. L. & Johnstone, I. M. (1989) Projectionbased approximation and a duality with kernel methods. Annals of Statistics 17, 58-106. Doyle, P. (1973) The use of automatic interaction detector and similar search procedures. Operational Research Quarterly 24, 465-467. Doyle, P. & Fenwick, I. (1975) The pitfalls of AID analysis. Journal of Marketing Research 12, 408-413.']\n",
      "['References 365 Draper, D. (1995) Assessment and propagation of model uncertainty (with discussion). Journal of the Royal Statistical Society series B 51, 45-97. Duchon, J. (1977) Spline minimizing rotationinvariant semi-norms in Sobolev spaces. In Constructive Theory of Functions of Several Variables, eds W. Schempp & K. Zeller. Lecture Notes in Mathematics 571, 85-100. Duda, R. 0. & Hart, P. E. (1973) Pattern Classification and Scene Analysis. New York: Wiley. Dudani, S. A. (1976) The distance-weighted knearest-neighbor rule. IEEE Transactions on Systems, Man and Cybernetics 6, 325-327. Reprinted in Dasarathy (1991). Dunn, J. C. (1974) A fuzzy relative of the ISO DATA process and its use in detecting compact wellseparated clusters. Journal of Cybernetics 3, 32-57. Dyn, N. (1987) Interpolation of scattered data by radial functions. In Topics in Multivariate Approximation, eds C. K. Chui, L. L. Schumaker & F. I. Utreras. New York: Academic Press. Eaton, H. A. C. & Oliver, T. L. (1992)', 'L. L. Schumaker & F. I. Utreras. New York: Academic Press. Eaton, H. A. C. & Oliver, T. L. (1992) Learning coefficient dependence on training set size. Neural Networks 5, 283-288. Edwards, D. (1995) Introduction to Graphical Modelling. New York: Springer. Edwards, D. & Havarnek, T. (1985) A fast procedure for model-search in multidimensional contingency tables. Biometrika 12, 339-351. Efron, B. (1975) The efficiency of logistic regression compared to normal discriminant analysis. Journal of the American Statistical Association 70, 892-898. Efron, B. (1979) Bootstrap methods: another look at the jackknife. Annals of Statistics 1, 1-26. Efron, B. (1982) The Jackknife , the Bootstrap and Other Resampling Plans. Philadelphia: SIAM. Efron, B. (1983) Estimating the error rate of a prediction rule. Improvements on cross-validation. Journal of the American Statistical Association 78, 316-331. Efron, B. (1986) How biased is the apparent error rate of a prediction rule? Journal of the American', 'B. (1986) How biased is the apparent error rate of a prediction rule? Journal of the American Statistical Association 81, 461-470. Efron, B. & Gong, G. (1983) A leisurely look at the bootstrap, the jackknife, and cross-validation. American Statistician 37, 36-48. Efron, B. & Tibshirani, R. J. (1993) An Introduction to the Bootstrap. New York: Chapman & Hall. Ehrenfeucht, A., Haussler, D., Kearns, M. & Valiant, L. (1989) A general lower bound on the number of examples needed for learning. Information and Computation 82, 247-261. Eisenberger , I. (1964) Genesis of bimodal distributions. Technometrics 6, 357-363. Eriksen, P. S. (1987) Proportionality of covariances. Annals of Statistics 15, 732-748. Erwin, E., Obermayer, K. & Schulten, K. (1992) Selforganizing maps: ordering, convergence properties and energy functions. Biological Cybernetics 67, 47-55. Eslava-G6mez, G. (1989) Projection Pursuit and Other Graphical Methods for Multivariate Data. Unpublished D. Phil thesis, University of', 'and Other Graphical Methods for Multivariate Data. Unpublished D. Phil thesis, University of Oxford. Eslava, G. & Marriott, F. H. C. (1994) Some criteria for projection pursuit. Statistics and Computing 4, 13-20. Evans, M. & Swartz, T. (1995) Methods for approximating integrals in statistics with special emphasis on Bayesian integration problems. Statistical Science 10, 254-272. Fagin, R. (1977) Multivalued dependencies and a new normal form for relational databases . ACM Transactions on Database Systems 2, 262-278. Fahlman, S. E. (1989) Faster-learning variations on back-propagation: an empirical study. In Proceedings of the 1988 Connectionist Models Summer School, Pittsburg, eds D. Touretzky, G. Hinton & T. Sejnowski , pp. 38-51. San Mateo, CA: Morgan Kaufmann. Fahlman , S. E. & Lebiere, C. (1990) The cascadecorrelation learning architecture. In NIPS2, pp. 524-532. Fauquet, C., Desbois, D., Fargette, D. & Vidal, G. (1988) Classification of furoviruses based on the amino acid', 'Desbois, D., Fargette, D. & Vidal, G. (1988) Classification of furoviruses based on the amino acid composition of their coat proteins. In Viruses with Fungal Vectors, eds J. I. Cooper & M. J. C. Asher, pp. 19-38. Edinburgh: Association of Applied Biologists. Fayyad, U. M. & Irani, K. B. (1992) On the handling of continuous-valued attributes in decision tree generation. Machine Learning 8, 87-102. Fefferman, C. & Markel, S. (1994) Recovering a feed-forward network from its output. In NIPS6, pp. 335-342. Feldman, J. A. (1985) Connectionist models and their applications: introduction. Cognitive Science 9, 1-2. Fienberg, S. E. & Holland, P. W. (1973) Simultaneous estimation of multinomial cell probabilities. Journal of the American Statistical Association 68, 683-691.']\n",
      "['366 References Finnoff, W., Hergert, F. & Zimmerman, H. G. (1993) Improving model selection by nonconvergent methods. Neural Networks 6, 771-783. Fisher, R. A. ( 1936) The use of multiple measurements in taxonomic problems. Annals of Eugenics 7, 179-188. Fix, E. & Hodges, J. L. (1951) Discriminatory analysis-non parametric discrimination: consistency properties. Report no. 4, US Air Force School of Aviation Medicine , Random Field, Texas. [Published in Agrawala (1977), Silverman and Jones (1989) and Dasarathy (1991).] Flanagan, J. K., Morrell, D. R., Frost, R. L., Read, C. J. & Nelson, B. E. (1989) Vector quantization codebook generation using simulated annealing . In Proceedings of the International Conference on Acoustics , Speech and Signal Processing (Glasgow, May 1989), pp. 1759-1762 . Fleiss, J. L. (1981) Statistical Methods for Rates and Proportions . Second edition. New York: Wiley. Fletcher, R. (1987) Practical Methods of Optimization. Chichester: Wiley. Flocchini, P., Gardin,', \"Fletcher, R. (1987) Practical Methods of Optimization. Chichester: Wiley. Flocchini, P., Gardin, F., Mauri, G., Pensini, M. P., & Stofella, P. (1992) Combining image processing operators and neural networks in a face recognition system. International Journal of Pattern Recognition and Artificial Intelligence 6, 447-467. Flury, B. (1986) Proportionality of k covariance matrices. Statistics and Probability Letters 4, 29-33. Flury, B., Schmid, M. J. & Natayanan, A. (1994) Error rates in quadratic discrimination with constraints on the covariance matrices. Journal of Classification 11, 101-120. Forgy, E. W. (1965) Cluster analysis of multivariate data: efficiency vs interpretability of classifications. Biometrics 21, 768-769. Fort, M. & Pages, G. (1993) Sur Ia convergence p.s. de I'algorithme de Kohonen generalise. Note aux Compte Rendus de l'Academie des Sciences de Paris 317, Serie I, 389-394. Frank, I. E. & Friedman, J. H. (1993) A statistical view of some chemometrics regression tools\", \"Frank, I. E. & Friedman, J. H. (1993) A statistical view of some chemometrics regression tools (with discussion) . Technometrics 35, 109-148. Fraser, D. A. S. (1968) The Structure of Inference. New York: Wiley. Frean, M. (1990) The upstart algorithm: A method for constructing and training feedforward neural networks. Neural Computation 2, 198-209. Freedman, D. (1971) Markov Chains. San Francisco: Holden-Day. Friedman, H. P. & Rubin, J. (1967) Some invariant criteria for grouping data. Journal of the American Statistical Association 62, 1159-1178 . Friedman , J. H. (1977) A recursive partitioning decision rule for nonparametric classification . IEEE Transactions on Computers 26, 404-408. Friedman, J. H. (1984) SMART users' guide. Laboratory for Computational Statistics Technical Report No. 1, Dept of Statistics, Stanford University. Friedman , J. H. (1987) Exploratory projection pursuit. Journal of the American Statistical Associati on 82, 249-266. Friedman, J. H. (1989) Regularized\", 'Journal of the American Statistical Associati on 82, 249-266. Friedman, J. H. (1989) Regularized discriminant analysis. Journal of the American Statistical Association 84, 165-175. Friedman, J. H. (1991) Multivariate adaptive regression splines (with discussion). Annals of Statistics 19, 1-141. Friedman, J. H. & Silverman, B. W. (1989) Flexible parsimonious smoothing and additive modeling (with discussion) . Technometri cs 31, 3-39. Friedman, J. H. & Stuetzle, W. (1981) Projection pursuit regression. Journal of the American Statistical Association 76, 817-823. Friedman , J. H. & Tukey, J. W. (1974) A projection pursuit algorithm for exploratory data analysis. IEEE Transactions on Computers 23, 881-890. Friedman, J. H., Baskett, F. & Shustek, L. J. (1975) An algorithm for finding nearest neighbors. IEEE Transactions on Computers 24, 1000-1006. Friedman , J. H., Bentley, J. L. & Finkel, R. A. (1977) An algorithm for finding best matches in logarithmic expected time. ACM Transactions on', 'A. (1977) An algorithm for finding best matches in logarithmic expected time. ACM Transactions on Mathematical Software 3, 209-226. Friedman, J. H., Stuetzle, W. & Schroeder, A. (1984) Projection /pursuit density estimation. Journal of the American Statistical Association 79, 599-608. Fu, K.-S. (1982) Syntactic Pattern Recognition and Applications. Engel wood Cliffs, NJ: Prentice Hall. Fukunaga, K. (1990) Introduction to Statistical Pattern Recognition . Second edition. Boston: Academic Press. (First edition, 1972). Fukunaga, K. & Flick, T. E. (1984) An optimal global nearest neighbor metric. IEEE Transactions on Pattern Analysis and Machine Intelligence 6, 314--318. Reprinted in Dasarathy (1991). Fukunaga, K. & Flick, T. E. (1985) The 2-NN rule for more accurate NN risk estimation. IEEE Transactions on Pattern Analysis and Machine Intelligenc e 7, 107-112.']\n",
      "['References 367 Fukunaga , K. & Hummels , D. M. (1987a) Bias of nearest neighbor error estimates . IEEE Transactions on Pattern Analysis and Machine Intelligence 9, 103-112. Reprinted in Dasarathy (1991). Fukunaga, K. & Hummels, D. M. (1987b) Bayes error estimation using Parzen and k-NN procedures. IEEE Transactions on Pattern Analysis and Machine Intelligence 9, 634-643. Reprinted in Dasarathy (1991). Fukunaga , K. & Kessell, D. L. (1971) Estimation of classification error. IEEE Transactions on Computers 20, 1521-1527. Fukunaga, K. & Mantock , J. M. (1984) Nonparametric data reduction. IEEE Transactions on Pattern Analysis and Machine Intelligence 6, 115-118. Fukunaga , K. & Narendra, P. M. (1975) A branch and bound algorithm for computing k-nearest neighbors . IEEE Transactions on Computers 24, 750-753. Funahashi, K. (1989) On the approximat e realization of continuous mappings by neural networks. Neural Networks 2, 183-192. Fung, R. M. & Crawford , S. L. (1990) Constructor: a system', 'networks. Neural Networks 2, 183-192. Fung, R. M. & Crawford , S. L. (1990) Constructor: a system for induction of probabilistic models. In Proceedings, Eighth AAAI National Conference on Artificial Intelligence, Boston, pp. 762-769. Menlo Park, CA: AAAI Press. Furman, W. & Lindsay, B. (1994) Testing for the number of components in a mixture of normal distributions using moment estimators. Computational Statistics and Data Analysis 17, 473-492. Furnival, G. M. & Wilson, R. W. Jr (1974) Regressions by leaps and bounds. Technometrics 16, 499-511. Gader, P., Forester, B., Ganzberger , M., Gillies, A., Mitchell, B., Whalen, M. & Yocum, T. (1991) Recognition of handwritten digits using template and model matching. Pattern Recognition 24, 421-431. Gallant, S. I. (1990) Perceptron-based learning algorithms. IEEE Transactions on Neural Networks 1, 179-191. Gallant, S. I. (1993) Neural Network Learning and Expert Systems. Cambridge , MA: The MIT Press. Gammerman, A. (ed.) (1995) Probabilistic', 'and Expert Systems. Cambridge , MA: The MIT Press. Gammerman, A. (ed.) (1995) Probabilistic Reasoning and Bayesian Belief Networks . Henley-on- Thames: Alfred Waller. Gammerman, A., Luo, Z., Aitken, C. G. G. & Brewer, M. J. (1995) Exact and approximate algorithms and their implementations in mixed graphical models. In Gammerman (1995), pp. 33-53. Garey, M. R. & Johnson, D. S. (1979) Computers and Intractability: A Guide to the Theory of NPcompletene ss. New York: Freeman. Gates, G. W. (1972) The reduced nearest neighbor rule. IEEE Transactions on Information Theory 18, 431-433. Geiger, D. & Pearl, J. (1990) On the logic of causal models. In Uncertainty in Artificial Intelligence 4, eds R. D. Shachter, T. S. Levitt, L. N. Kanal & J. F. Lemmer, pp. 3-14. Amsterdam: NorthHolland. Geiger, D. & Pearl, J. (1993) Logical and algorithmic properties of conditional independence and graphical models. Annals of Statistics 21, 2001-2021. Geiger, D., Verma, T. & Pearl, J. (1990) Recognizing', 'models. Annals of Statistics 21, 2001-2021. Geiger, D., Verma, T. & Pearl, J. (1990) Recognizing independence in Bayesian networks . Networks 20, 507-534. Geisser, S. (1964) Posterior odds for multivariate normal classifications. Journal of the Royal Statistical Society series B 26, 69-76. Geisser, S. (1966) Predictive discrimination. In Multivariate Analysis, ed. P. R. Krishnaiah , pp. 149-163. New York: Academic Press. Geisser, S. (1975) The predictive sample reuse method with applications. Journal of the American Statistical Association 70, 320-328. Geisser, S. (1984) On prior distributions for binary trials (with discussion). American Statistician 38, 244-251. Geisser, S. (1987) Comment on Hodges (1987). Statistical Science 2, 277-279. Geisser, S. (1993) Predictive Inference: An Introduction. New York: Chapman & Hall. Geisser, S. & Cornfield, J. (1963) Posterior distributions for multivariate normal parameters. Journal of the Royal Statistical Society series B 25, 368-376.', 'for multivariate normal parameters. Journal of the Royal Statistical Society series B 25, 368-376. Gelfand, A. E. & Dey, D. K. (1994) Bayes model choice: asymptotics and exact calculations. Journal of the Royal Statistical Society series B 56, 501-514. Gelfand, A. E. & Smith, A. F. M. (1990) Samplingbased approaches to calculating marginal densities. Journal of the American Statistical Association 85, 398-409. Gelfand, A. E., Hills, S. E., Racine-Poon , A. & Smith, A. F. M. (1990) Illustration of Bayesian inference in normal data models using Gibbs sampling. Journal of the American Statistical Association 85, 972-985. Gelfand, S. B. & Delp, E. J. (1991) On tree structured classifiers. In Sethi & Jain (1991), pp. 51-70.']\n",
      "[\"368 References Gelfand, S. B. & Mitter, S. K. (1991) Recursive stochastic algorithms for global optimization in IR.d. SIAM Journal on Control and Optimization 29, 999-1018. Gelfand, S. B., Ravishankar, C. S. & Delp, E. J. (1991) An iterative growing and pruning algorithm for classification tree design. IEEE Transactions on Pattern Analysis and Machine Intelligence 13, 163-174. Gelman, A., Carlin, J. B., Stern, H. S. & Rubin, D. B. (1995) Bayesian Data Analysis. New York: Chapman & Hall. Geman, D. (1990) Random fields and inverse problems in imaging. In Ecole d'Ete de Probabilites de Saint-Flour XVIII -1988, ed. P. L. Hennequin. Lecture Notes in Mathematics 1427, 113-193. Geman, S. & Geman, D. (1984) Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence 6, 721-741. Reprinted in Shafer & Pearl (1990). Geman, S. & Hwang, C.-R. (1982) Nonparametric maximum likelihood estimation by the method of\", 'Geman, S. & Hwang, C.-R. (1982) Nonparametric maximum likelihood estimation by the method of sieves. Annals of Statistics 10, 401-414. Geman, S., Bienenstock, E. & Doursat, R. (1992) Neural networks and the bias/variance dilemma. Neural Computation 4, 1-58. George, E. I. & McCulloch, R. E. (1993) Variable selection via Gibbs sampling. Journal of the American Statistical Association 88, 881-889. Georgiopoulos, M., Heileman, G. L. & Huang, J. (1990) Convergence properties of learning in ARTl. Neural Computation 2, 502-509. Georgiopoulos, M., Heileman, G. L. & Huang, J. (1991) Properties of learning related to pattern diversity in ARTl. Neural Networks 4, 751-757. Gersho, A. & Gray, R. M. (1992) Vector Quantization and Signal Compression. Boston: Kluwer Academic Publishers. Geyer, C. (1992) Practical Markov chain Monte Carlo (with discussion). Statistical Science 7, 473-511. Ghurye, S. G. & Olkin, I. (1969) Unbiased estimation of some multivariate probability densities and related', 'S. G. & Olkin, I. (1969) Unbiased estimation of some multivariate probability densities and related functions. Annals of Mathematical Statistics 40, 1261-1271. Gilbert, J. C. & Lemarechal, C. (1989) Some numerical experiments with variable storage quasiNewton methods. Mathematical Programming 45, 407-436. Gill, P. E., Murray, W. & Wright, M. H. (1981) Practical 0 ptimization. London: Academic Press. Girosi, F. & Anzellotti, G. (1993) Rates of convergence for radial basis functions and neural networks. In Mammone (1993), pp. 97-114. Girosi, F. & Poggio, T. (1990) Networks and the best approximation property. Biological Cybernetics 63, 169-176. Girosi, F., Jones, M. & Poggio, T. (1995) Regularization theory and neural networks architectures. Neural Computation 7, 219-269. Glick, N. (1972) Sample-based classification procedures derived from density estimators. Journal of the American Statistical Association 67, 116-122. Glick, N. (1976) Sample-based classification procedures related to', 'Association 67, 116-122. Glick, N. (1976) Sample-based classification procedures related to empiric distributions. IEEE Transactions on Information Theory 22, 454-461. Glymour, C., Scheines, R., Spirtes, P. & Kelly, K. (1987) Discovering Causal Structure: Artificial Intelligence, Philosophy of Science, and Statistical Modeling. San Diego: Academic Press. Goldstein, M. & Dillon, W. R. (1978) Discrete Discriminant Analysis. New York: Wiley. Golomb, B. A., Lawrence, D. T. & Sejnowski, T. J. (1991) SEXNET: A neural network identifies sex from human faces. In NIPS3, pp. 572-577. Golombic, M. C. (1980) Algorithmic Graph Theory and Perfect Graphs. New York: Academic Press. Golub, G. H. & Van Loan, C. F. (1989) Matrix Computations. Second edition. Baltimore: Johns Hopkins University Press. Gonzalez, R. C. & Thomason, M.G. (1978) Syntactic Pattern Recognition: An Introduction. Reading, MA: Addison-Wesley. Good, I. J. (1965) The Estimation of Probabilities. Cambridge, MA: The MIT Press. Good,', 'Good, I. J. (1965) The Estimation of Probabilities. Cambridge, MA: The MIT Press. Good, I. J. ( 1983) Good Thinking: The Foundations of Probability and its Applications. Minneapolis: University of Minnesota Press. Goodman, R. M. & Smyth, P. (1988) Decision tree design from a communication theory standpoint. IEEE Transactions on Information Theory 34, 979-994. Gordon, A. D. (1981) Classification. Methods for Exploratory Analysis of Multivariate Data. London: Chapman & Hall. Gori, M. & Tesi, A. (1992) On the problem of local minima in backpropagation. IEEE Transactions on Pattern Analysis and Machine Intelligence 14, 76-86. Gower, J. C. (1966) Some distance properties of latent root and vector methods used in multivariate analysis. Biometrika 53, 325-328.']\n",
      "[\"References 369 Gower, J. C. (1971) A general coefficient of similarity and some of its properties . Biometrics 27, 857-871. Gower, J. C. & Legendre , P. (1986) Metric and Euclidean properties of dissimilarity coefficients. Journal of Classification 3, 5-48. Gray, R. M. (1984) Vector quantization . IEEE ASSP Magazine 1(2), 4-29. Green, P. J. & Silverman, B. W. (1994) Nonparametri c Regression and Generalized Linear Models. A Roughness Penalty Approach . London: Chapman & Hall. Grenander , U. (1981) Abstract Inference. New York: Wiley. Grenander, U., Chow, Y. & Keenan, D. M. (1991) Hands. A Pattern Theoretic Study of Biological Shapes. New York: Springer. Grinold, R. C. (1969) Comment on 'Pattern classification design by linear programming'. IEEE Transactions on Computers 18, 378-379. Grother, P. J. & Candela, G. T. (1993) Comparison of handprinted digit classifiers. US National Institute of Standards and Technology report N ISTIR 5209. Gu, C. (1990) Adaptive spline smoothing in\", 'of Standards and Technology report N ISTIR 5209. Gu, C. (1990) Adaptive spline smoothing in nonGaussian regression models. Journal of the American Statistical Association 85, 801-807. Gu, C. & Wahba, G. (1991) Minimizing GCV /GML scores with multiple smoothing parameters via the Newton method. SIAM Journal on Scientific and Statistical Computing 12, 383-398. Gu, C., Bates, D. M., Chen, Z. & Wahba, G. (1989) The computation of generalized cross-validation functions through Householder tridiagonalization with applications to the fitting of interaction spline models. SIAM Journal on Matrix Analysis and Applications 10, 459-480. Guo, H. & Gelfand, S. B. (1992) Classification trees with neural network feature extraction. IEEE Transactions on Neural Networks 3, 923-933. Guyon, I., Vapnik, V., Boser, B., Bottou, L. & Solla, S. A. (1992) Structural risk minimization for character recognition. In NIPS4, pp. 471-479. Hall, D. J. & Ball, G. B. (1965) ISODATA: a novel method of data analysis and', 'NIPS4, pp. 471-479. Hall, D. J. & Ball, G. B. (1965) ISODATA: a novel method of data analysis and pattern classification. Technical report, Stanford Research Institute, Menlo Park CA. Hall, D. J. & Khanna, D. (1977) The ISODATA method of computation for relative perception of similarities and differences in complex and real computers. In Statistical Methods for Digital Computers 3, eds K. Enslein, A. Ralston & H. S. Wilf, pp. 340-373. New York: Wiley. Hall, P. (1981) On nonparametric multivariate binary discrimination . Biometrika 68, 287-294. Hall, P. (1989) On polynomial-based projection indices for exploratory projection pursuit. Annals of Statistics 17, 589-605. Hall, P. & Wand, M. P. (1988) On nonparametric discrimin ation using density differences. Biometrika 75, 541-547. Hampel, F. R., Ronchetti, E. M., Rousseeuw , P. J. & Stahel, W. A. (1986) Robust Statistics: The Approach Based on Influence Function s. New York: Wiley. Hampson, S. E. & Volper, D. J. (1986) Linear function', 'on Influence Function s. New York: Wiley. Hampson, S. E. & Volper, D. J. (1986) Linear function neurons: structure and training. Biological Cybernetics 53, 203-217. Hand, D. J. (1981) Discrimination and Classification. Chichester: Wiley. Hand, D. J. (1982) Kernel Discriminant Analysis. Chichester: Research Studies Press. Hand, D. J. & Batchelor, B. G. (1978) An edited condensed nearest neighbor rule. Information Sciences 14, 171-180. Hannan, E. J. & Quinn, B. G. (1979) The determination of the order of an autoregression . Journal of the Royal Statistical Society series B 41, 190--195. Hansen, L. K. & Salamon, P. (1990) Neural network ensembles. IEEE Transactions on Pattern Analysis and Machine Intelligence 12, 993-1001. Hanson, S. J. & Pratt, L. Y. (1989) Comparing biases for minimal network construction with backpropagation . In NIPSJ, pp. 177-185. Hardie, W. (1990) Applied Nonparametri c Regression. Cambridge: Cambridge University Press. Hardie, W. (1991) Smoothing Techniques with', 'c Regression. Cambridge: Cambridge University Press. Hardie, W. (1991) Smoothing Techniques with Implementation in S. New York: Springer. Hardy, R. L. (1971) Multiquadric equations of topography and other irregular surfaces. Journal of Geophysi cal Research 76, 1906--1915 . Hardy, R. L. (1990) Theory and applications of the multiquadric-biharmonic method: 20 years of discovery 1968-1988. Computers and Mathematics with Applications 19, 163-208. Hart, P. E. (1968) The condensed nearest neighbor rule. IEEE Transactions on Information Theory 14, 515-516. Hartigan, J. A. (1975) Clustering Algorithm s. New York: Wiley. Hartigan, J. A. & Wong, M. A. (1979) Algorithm AS136. A K-means clustering algorithm. Applied Statistics 28, 100--108.']\n",
      "['370 References Hartman, E. J., Keeler, J. D. & Kowalski, J. M. (1990) Layered neural networks with Gaussian hidden units as universal approximations. Neural Computation 2, 210-215. Hassibi, B. & Stork, D. G. (1993) Second derivatives for network pruning: Optimal Brain Surgeon. In NIPS5, pp. 164-171. Hassibi, B., Stork, D. G., Wolff, G. & Watanabe, T. (1994) Optimal Brain Surgeon: extensions and performance comparisons. In NIPS6, pp. 263-270. Hastie, T. & Mallows, C. (1993) Discussion of Frank & Friedman (1993). Technometrics 35, 140-143. Hastie, T. & Stuetzle, W. (1989) Principal curves. Journal of the American Statistical Association 84, 502-516. Hastie, T. J. & Tibshirani, R. J. (1990) Generalized Additive Models. London: Chapman & Hall. Hastie, T. & Tibshirani, R. (1996) Discriminant analysis by Gaussian mixtures. Journal of the Royal Statistical Society series B 58, 155-176. Hastie, T., Buja, A. & Tibshirani, R. (1995) Penalized discriminant analysis. Annals of Statistics 23,', \"T., Buja, A. & Tibshirani, R. (1995) Penalized discriminant analysis. Annals of Statistics 23, 73-102. Hastie, T., Tibshirani, R. & Buja, A. (1994) Flexible discriminant analysis by optimal scoring. Journal of the American Statistical Association 89, 1255-1270. Hastings, W. K. (1970) Monte Carlo sampling methods using Markov chains and their applications. Biometrika 57, 97-109. Hathaway, R. J. (1985) A constrained formulation of maximum-likelihood estimation for normal mixture distributions. Annals of Statistics 13, 795-800. Hauck, W. W. Jr & Donner, A. (1977) Wald's test as applied to hypotheses in logit analysis. Journal of the American Statistical Association 72, 851-853. Haussler, D. (1992) Decision theoretic generalizations of the PAC model for neural net and other learning applications. Information and Computation 100, 78-150. Reprinted as pp. 37-116 of Wolpert (1995). Haykin, S. (1994) Neural Networks. A Comprehensive Foundation. New York: Macmillan College Publishing. Hebb, D.\", 'Neural Networks. A Comprehensive Foundation. New York: Macmillan College Publishing. Hebb, D. 0. (1949) The Organization of Behavior. New York: Wiley. Hellman, M. E. (1970) The nearest neighbor classification rule with a reject option. IEEE Transactions on Systems Science and Cybernetics 6, 179-185. Reprinted in Dasarathy (1991). Henrichon, E. G. Jr & Fu, K.-S. (1968) On mode estimation in pattern recognition. In Proceedings of the Seventh Symposium on Adaptive Processes, UCLA, p. 3-a-1. Henrichon, E. G. Jr & Fu, K.-S. (1969) A nonparametric partitioning procedure for pattern classification. IEEE Transactions on Computers 18, 614-624. Henrion, M. (1988) Propagating uncertainty in Bayesian networks by probabilistic logic sampling. In Uncertainty in Artificial Intelligence 2, eds J. Lemmer & L. N. Kana!, pp. 149-163. Amsterdam: North-Holland. Henrion, M., Breese, J. S. & Horvitz, E. J. (1991) Decision analysis and expert systems. AI Magazine 12(4), 64-91. Hermans, J., Habbema, J. D. F.', 'Decision analysis and expert systems. AI Magazine 12(4), 64-91. Hermans, J., Habbema, J. D. F. & Schaefer, J. R. (1982) The ALLOC80 package for discriminant analysis. Statistical Software Newsletter 8, 15-20. Hertz, J., Krogh, A. & Palmer, R. G. (1991) Introduction to the Theory of Neural Computation. Redwood City, CA: Addison-Wesley. Heskes, T. M. & Kappen, B. (1991) Learning processes in neural networks. Physical Reviews A 44, 2718-2726. Highleyman, W. H. (1962a) The design and analysis of pattern recognition experiments. Bell Systems Technical Journal 41, 723-744. Highleyman, W. H. (1962b) Linear decision functions, with application to pattern recognition. Proceedings of the IRE 50, 1501-1514. Hills, M. (1966) Allocation rules and their error rates (with discussion). Journal of the Royal Statistical Society series B 28, 1-31. Hinton, G. E. (1986) Learning distributed representations of concepts. In Proceedings of the Eighth Annual Conference of the Cognitive Science Society', 'of concepts. In Proceedings of the Eighth Annual Conference of the Cognitive Science Society (Amherst, 1986), pp. 1-12. Hillsdale: Erlbaum. Hinton, G. E. (1989a) Connectionist learning procedures. Artificial Intelligence 40, 185-234. (Reprinted in Carbonell, 1990.) Hinton, G. E. (1989b) Deterministic Boltzmann machine learning performs steepest descent in weight space. Neural Computation 1, 143-150. Hinton, G. E. & Sejnowski, T. J. (1983) Optimal perceptual inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (Washington. 1983 ), pp. 448--453. New York: IEEE Press. Hjort, N. L. (1986) Notes on the Theory of Statistical Symbol Recognition. Norwegian Computing Center Report 778.']\n",
      "['References 371 Hjort, N. L. & Glad, I. K. (1995) Nonparametric density estimation with a parametric start. Annals of Statistics 23, 882-904. Hjort, N. L. & Jones, M. C. (1996) Locally parametric nonparametric d ensity estimation . Annals of Statistics 24, 1619-1647 . Ho, Y.-C. & Kashyap, R. L. (1965) An algorithm for linear inequalities and its applications. IEEE Transacti ons on Electroni c Computers 14, 683-688. Hodges, J. S. (1987) Uncertainty, policy analysis and statistics (with discussion). Statistical Science 2, 259-291. Hoeffding , W. ( 1963) Probability inequalitie s for sums of bounded random variables. Journal of the American Statistical Association 58, 13-30. Hoerl, A. E. & Kennard , R. W. (1970a) Ridge regression : biased estimation for nonorthogonal problems. Technometrics 12, 55-67. Hoerl, A. E. & Kennard , R. W. (1970b) Ridge regression : applications to nonorthogonal problems. Technometrics 12, 69-82. Hoffgen, K.-U., Simon, H.-U. & Van Horn, K. S. (1995) Robust', 'problems. Technometrics 12, 69-82. Hoffgen, K.-U., Simon, H.-U. & Van Horn, K. S. (1995) Robust trainability of single neurons. Journal of Computer and System Sciences 50, 114-125. Holt, M. J. J. & Semnani, S. (1990) Convergence of back-propagation in neural networks using a log-likelihood cost function. Electronics Letters 26, 1964-1965. Hopfield, J. J. (1982) Neural networks and physical systems with emergent collective computational facilities. Proceedings of the National Academy of Sciences of the USA 79, 2554-2558. Reprinted in Anderson & Rosenfeld (1988) and Lau (1992). Hopfield, J. J. (1987) Learning algorithms and probability distributions in feed-forward and feed-back networks. Proceedings of the National Academy of Sciences of the USA 84, 8429-8433. Hornik, K., Stinchcombe, M. & White, H. (1989) Multilayer feedforward networks are universal approximators . Neural Networks 2, 359-366. Reprinted in White ( 1992). Hornik, K., Stinchcombe, M. & White, H. (1990) Universal', '2, 359-366. Reprinted in White ( 1992). Hornik, K., Stinchcombe, M. & White, H. (1990) Universal approximation of an unknown mapping and its derivatives using feedforward networks. Neural Networks 3, 551-560. Reprinted in White ( 1992). Hrycej, T. (1990) Gibbs sampling in Bayesian networks. Artificial Intelligen ce 46, 351-363. Hrycej, T. (1992) Modular Learning in Neural Networks. A Modulari zed Appproach to Neural Network Classification . New York: Wiley. Huang, J., Georgiopoulos, M. & Heileman, G. L. (1995) Fuzzy ART properties . Neural Networks 8, 203-213. Huber, P. J. (1967) The behavior of maximum likelihood estimates under nonstandard conditions. In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, eds L. M. Le Cam & J. Neyman, 1, pp. 221-233. Berkeley : University of California Press. Huber, P. J. (1981) Robust Statistics. New York: Wiley. Huber, P. J. (1985) Projection pursuit (with discussion). Annals of Statistics 13, 435-525. Hunt, E.', 'P. J. (1985) Projection pursuit (with discussion). Annals of Statistics 13, 435-525. Hunt, E. B., Marin, J. & Stone, P. J. (1966) Experiments in Induction. New York: Academic Press. Hwang, J.-N., Lay, S.-R., Maechler , M., Martin, D. & Schimert, J. (1994a) Regression modeling in back-propagation and projection pursuit learning. IEEE Transactions on Neural Networks 5, 342-353. Hwang, J.-N., Li, H., Maechler, M., Martin, D. & Schimert, J. (1992a) A comparison of projection pursuit and neural network regression modeling . In NIPS4, pp. 1159-1166. Hwang, J.-N., Li, H., Maechler , M., Martin, D. & Schimert, J. (1992b) Projection pursuit learning networks for regression. Engineering Applications of Artificial Intelligence 5, 193-204. Hwang, J.-N., Li, H., Martin, D. & Schimert , J. (1991) The learning parsimony of projection pursuit and back-propagation networks. In 25th Asilomar Conference on Signals, Systems and Computers, Pacific Grove, CA, pp. 491-495. Los Alamitos, CA: IEEE Computer', \"on Signals, Systems and Computers, Pacific Grove, CA, pp. 491-495. Los Alamitos, CA: IEEE Computer Society Press. Hwang, J.-N., You, S.-S., Lay, S.-R. & Jou, I.-C. (1994b) What's wrong with a cascaded correlation learning network: a projection pursuit learning perspective. Technical Report, Dept of Electrical Engineering, University of Washington. Hwang, J.-N., You, S.-S., Lay, S.-R. & Jou, I.-C. (1996) The cascaded correlation learning: A projection pursuit perspective . IEEE Transactions on Neural Networks 7, 278-289. Hyafil, R. & Rivest, R. L. (1976) Constructing optimal binary trees is NP-complete . Information Processing Letters 5, 15-17. Impedovo, S., Ottaviano, L. & Occhinegro, S. (1991) Optical character recognition -a survey. International Journal of Pattern Recogniti on and Artificial Intelligence 5, 1-24.\"]\n",
      "[\"372 References Ingrassia, S. (1992) A comparison between the simulated annealing and the EM algorithms in normal mixture decompositions. Statistics and Computing 2, 203-211. Intrator, N. (1990) A neural network for feature extraction. In NIPS2, pp. 719-726. Intrator, N. (1991) Exploratory feature extraction in speech signals. In NIPS3, pp. 241-247. Intrator, N. (1992) Feature extraction using an unsupervised neural network. Neural Computation 4, 98-107. Intrator, N. & Cooper, L. N. (1992) Objective function formulation of the BCM theory of visual cortical plasticity: statistical connections , stability conditions. Neural Networks 5, 3-17. Intrator, N. & Gold, J. I. (1993) Three-dimensional object recognition using an unsupervised BCM network: the usefulness of distinguishing features. Neural Computation 5, 61-74. Isham, V. (1981) An introduction to spatial point processes and Markov random fields. International Statistical Review 49, 21-43. Jackson, J. E. (1991) A User's Guide to\", \"random fields. International Statistical Review 49, 21-43. Jackson, J. E. (1991) A User's Guide to Principal Components. New York: Wiley. Jacobs, R. A (1988) Increased rates of convergence through learning rate adaptation. Neural Networks 1, 295-307. Jacobs, R. A, Jordan, M. I., Nowlan, S. J. & Hinton, G. E. (1991) Adaptive mixtures of local experts. Neural Computation 3, 79-87. Jain, A K. & Dubes, R. C. (1988) Algorithms for Clustering Data. Englewood Cliffs, NJ: PrenticeHall. Jain, A K., Dubes, R. C. & Chen, C.-C. (1987) Bootstrap techniques for error estimation . IEEE Transactions on Pattern Analysis and Machine Intelligence 9, 628-633. James, M. (1988) Pattern Recognition. New York: Wiley. Jancey, R. C. (1966) Multidimensional group analysis. Australian Journal of Botany 14, 127-130. Jeffreys, H. (1961) Theory of Probability . Third edition. Oxford: Clarendon Press. Jensen, F. V. (1991) Calculation in HUGIN of probabilities for specific configurations -a trick with many\", 'F. V. (1991) Calculation in HUGIN of probabilities for specific configurations -a trick with many applications . In Proceedings of the Scandinavian Conference on Artificial Intelligence , pp. 176-186. Amsterdam : lOS Press. Jensen, F. V. ( 1996) An introduction to Belief Networks. London: UCL Press (Taylor & Francis Ltd) and New York: Springer. Jensen, F. V. & Liang, J. (1995) drHugin. A system for hypothesis driven data request. In Gammerman (1995), pp. 109-124. Jensen, F. V., Lauritzen , S. L. & Olesen, K. G. ( 1990) Bayesian updating in causal probabilistic networks by local computations. Computational Statistics Quarterly 5, 269-282. Jiang, Q. & Zhang, W. (1993) An improved method for finding nearest neighbors . Pattern Recognition Letters 14, 531-535. Johansson, E. M., Dowla, F. U. & Goodman, D. M. (1991) Back-propagation learning for multi-layer feed-forward neural networks using the conjugate gradient method. International Journal of Neural Systems 2, 291-302. Johnson, N. L. &', 'the conjugate gradient method. International Journal of Neural Systems 2, 291-302. Johnson, N. L. & Kotz, S. (1972) Distributions in Statistics: Continuous Multivariate Distributions . New York: Wiley. Jolliffe, I. T. (1986) Principal Component Analysis. New York: Springer. Jones, L. K. (1987) On a conjecture of Huber concerning the convergence of projection pursuit regression. Annals of Statistics 15, 880-882. Jones, L. K. (1992) A simple lemma on greedy approximation in Hilbert space and convergence rates for projection pursuit regression and neural network training. Annals of Statistics 20, 608-613. Jones, M. C. & Sibson, R. (1987) What is projection pursuit (with discussion)? Journal of the Royal Statistical Society series A 150, 1-36. Jordan, M. I. & Jacobs, R. A (1992) Hierarchies of adaptive experts. In NIPS4, pp. 985-992. Jordan, M. I. & Jacobs, R. A (1994) Hierarchical mixtures of experts and the EM algorithm. Neural Computation 6, 181-214. Kalantari, I. & McDonald, G. (1983)', 'of experts and the EM algorithm. Neural Computation 6, 181-214. Kalantari, I. & McDonald, G. (1983) A data structure and an algorithm for the nearest point problem. IEEE Transactions on Software Engineering 9, 631-634. Kambhatla, N. & Leen, T. K. (1994) Fast non-linear dimension reduction. In NIPS6, pp. 152-159. Kamgar-Parsi, B. & Kana!, L. N. (1985) An improved branch and bound algorithm for computing knearest neighbours. Pattern Recognition Letters 3, 7-12. Kansa, E. J. (1990) Multiquadrics-a scattered data approximation scheme with applications to computational fluid dynamics. 1. Computers and Mathematics with Applications 19, 127-145.']\n",
      "['References 373 Kappen, H. J. (1995) Deterministic learning rules for Boltzmann machines. Neural Networks 8, 537-548. Karpinski , M. & Macintyre , A. (1995a) Bounding VC-dimension of neural networks : Progress and prospects . In Proceedings of the Second European Conference on Computational Learning Theory (Barcelona , Spain), ed. P. Vitanyi, number 904 in Lecture Notes in Artificial Intelligence , pp. 337-341. Berlin: Springer. Karpinski, M. & Macintyre , A. (1995b) Polynomial bounds for VC dimension of sigmoidal neural networks. In Proceedings of the Twenty-Seventh Annual ACM Symposium on Theory of Computing (Las Vegas), pp. 200-208. ACM Press. Kashyap, R. L. & Blaydon, C. C. (1968) Estimation of probability density and distribution functions. IEEE Transactions on Information Theory 14, 549-556. Kass, G. V. (1980) An exploratory technique for investigating large quantitie s of categorical data. Applied Statistics 29, 119-127. Kass, R. E. & Raftery, A. E. (1995) Bayes factors. Journal', 'data. Applied Statistics 29, 119-127. Kass, R. E. & Raftery, A. E. (1995) Bayes factors. Journal of the American Statistical Association 90, 733-795. Kass, R. E. & Vaidyanathan, S. K. (1992) Approximate Bayes factors and orthogonal parameters , with application to testing equality of two binomial proportions. Journal of the Royal Statistical Society series B 54, 129-144. Kaufman, L. & Rousseeuw, P. J. (1990) Finding Groups in Data. An Introduction to Cluster Analysis. New York: Wiley. Kendall, M. G. & Stuart, A. (1966) The Advanced Theory of Statistics, volume III. London: Griffin. Kent, J. T., Tyler, D. E. & Vardi, Y. (1994) A curious likelihood identity for the multivariate t-distribution. Communications in Statistics -Simulation and Computation 23, 441-453. Kibler, D. & Aha, D. W. (1987) Learning representative exemplars of concepts: an initial case study. In Proceedings of the Fourth International Workshop on Machine Learning (Irvine, 1987 ), ed. P. Langley, pp. 24-30. Palo Alto,', 'International Workshop on Machine Learning (Irvine, 1987 ), ed. P. Langley, pp. 24-30. Palo Alto, CA: Morgan Kaufmann. Reprinted in Shavlik & Dietterich (1990). Kiiveri, H., Speed, T. P. & Carlin, J. B. (1984) Recursive causal models. Journal of the Australian Mathemati cal Society (series A) 36, 30-52. Kim, B. S. & Park, S. B. (1986) A fast k nearest neighbor finding algorithm based on the ordered partition. IEEE Transactions on Pattern Analysis and Machine Intelligence 8, 761-766. Reprinted in Dasarathy (1991). Kim, J. H. & Pearl, J. (1983) A computational model for combined causal and diagnostic reasoning in inference systems. In Proceedings of the Eighth International Joint Conference on Artificial Intelligence (Karlsruhe , 1983), pp. 190-193. Menlo Park, CA:AAAI. King, R. D., Muggleton, S., Lewis, R. A. & Sternberg, M. J. E. (1992) Drug design by machine learning: The use of inductive logic programming to model the structure-activity relationships of trimethoprim analogues', 'inductive logic programming to model the structure-activity relationships of trimethoprim analogues binding to dihydrofolate reductase. Proceedings of the National Academy of Sciences of the USA 89, 11322-11326. Kjrerulff, U. (1992) Optimal decomposition of probabilistic networks by simulated annealing. Statistics and Computing 2, 7-17. Kleijnen, J.P. C. (1987) Statistical Tools for Simulation Practitioners. New York: Marcel Dekker. Kleijnen, J. P. C. & van Groenendaal , W. (1992) Simulation : A Statistical Perspective . Chichester: Wiley. Knerr, S., Personnaz , L. & Dreyfus, G. (1992) Handwritten digit recognition by neural networks with single-layer training. IEEE Transactions on Neural Networks 3, 962-968. Knuth, D. E. (1968) The Art of Computer Programming, Volume 1: Fundamental Algorithms. Reading, MA: Addison-Wesley. (Second edition, 1973.) Kohonen, T. (1982a) Self-organized formation of topologically correct feature maps. Biological Cybernetics 43, 59-69. Reprinted in Anderson', 'of topologically correct feature maps. Biological Cybernetics 43, 59-69. Reprinted in Anderson & Rosenfeld (1988). Kohonen, T. (1982b) Analysis of a simple selforganizing process. Biological Cybernetics 43, 135-140. Kohonen, T. (1988a) An introduction to neural computing. Neural Networks l, 3-16. Kohonen, T. (1988b) Learning vector quantization. Neural Networks 1 (suppl. 1), 303. Kohonen, T. (1989) Self-Organization and Associative Memory. Third edition. Berlin: Springer. [First edition, 1984] Kohonen, T. (1990a) The self-organizing map. Proceedings of the IEEE 78, 1464-1480. Reprinted in Lau (1992). Kohonen, T. (1990b) Improved versions of learning vector quantization. In Proceedings of the IEEE International Conference on Neural Networks , San Diego I, 545-550. New York: IEEE Press.']\n",
      "['374 References Kohonen, T. ( 1993) Physiological interpretation of the self-organizing map algorithm. Neural Networks 6, 895-905. Kohonen, T. (1995) Self-Organizing Maps. Berlin: Springer. Kohonen, T., Barna, G. & Chrisley, R. (1988) Statistical pattern recognition with neural networks: benchmarking studies. In Proceedings of the IEEE International Conference on Neural Networks , San Diego, I, 61-68. Long Beach, CA: IEEE Press. Reprinted in Anderson et al. (1990). Kohonen, T., Kangas, T., Laaksonen, J. & Torkkola, K. (1992) LVQ_PAK. The learning vector quantization program package version 2.1. Laboratory of Computer and Information Science, Helsinki University of Technology . [Version 3.1 became available in 1995.] Koiran, P. & Sontag, E. D. (1996) Neural networks with quadratic VC dimension . In Advances in Neural Information Processing Systems 8 eds D. S. Touretzky , M. C. Moser & M. E. Hasselmo, pp. 197-203. Cambridge, MA: MIT Press. Kong, A (1991) Efficient methods for computing', 'E. Hasselmo, pp. 197-203. Cambridge, MA: MIT Press. Kong, A (1991) Efficient methods for computing linkage likelihoods of recessive diseases in inbred pedigrees. Genetic Epidemiology 8, 81-103. Kononenko, 1., Bratko, I. & Roskar, E. (1984) Experiments in the automatic learning of medical diagnosis rules. Technical Report, Josef Stefan Institute, Ljubljana. Koontz, W. L. G., Narendra , P. M. & Fukunaga , K. (1975) A branch and bound clustering algorithm. IEEE Transactions on Computers 24, 908-915. Kramer, A H. & Sangiovanni-Vincentelli, A (1989) Efficient parallel learning algorithms for neural networks. In NIPSI, pp. 40-48. Kramer, M. A .. (1991) Nonlinear principal component analysis using autoassociative neural networks. AICHE Journal 37, 233-243. Krishnaiah, P. R. & Kana!, L. N. (eds) (1982) Handbook of Statistics 2: Classification, Pattern Recognition and Reduction of Dimensionality. Amsterdam: North Holland. Kruskal, J. B. (1964a) Multidimensional scaling by optimizing', \"Amsterdam: North Holland. Kruskal, J. B. (1964a) Multidimensional scaling by optimizing goodness-of-fit to a nonmetric hypothesis. Psychometrika 29, 1-29. Kruskal, J. B. (1964b) Non-metric multidimensional scaling: a numerical method. Psychometrika 29, 115-129. Kruskal, J. B. (1969) Toward a practical method which helps uncover the structure of a set of multivariate observations by finding the linear transformation which optimizes a new 'index of condensation'. In Statistical Computation, eds R. C. Milton & J. A Neider, pp. 427-440. New York: Academic Press. Kruskal, J. B. (1971) Monotone regression: continuity and differentiability properties . Psychometrika 36, 57-62. Kruskal, J. B. ( 1972) Linear transformation of multivariate data to reveal clustering. In Multidimensional Scaling: Theory and Application in the Behavioural Sciences, eds R. N. Shephard , A K. Romney & S. K. Nerlove, pp. 179-191. New York: Seminar Press. Krzanowski, W. J. (1975) Discrimination and classification\", \"pp. 179-191. New York: Seminar Press. Krzanowski, W. J. (1975) Discrimination and classification using both binary and continuous variables. Journal of the American Statistical Association 70, 782-790. Kung, S. Y. & Diamantaras, K. I. (1990) A neural network learning algorithm for Adaptive Principal component EXtraction (APEX). In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (Albuquerque, NM, 1990) 2, pp. 861-864. Long Beach, CA: IEEE Press. Kurkova, V. (1991) Kolmogorov's theorem is relevant. Neural Computation 3, 617-622. Kurkova, V. (1992) Kolmogorov's theorem and multilayer neural networks. Neural Networks 5, 501-506. Kurzynski , M. W. (1983a) Decision rules for a hierarchical classifier. Pattern Recognition Letters 1, 305--310. Kurzynski, M. W. (1983b) The optimal strategy of a tree classifier. Pattern Recognition 16, 81-87. (Correction page 361). Kushner, H. (1987) Asymptotic global behavior for stochastic approximation and\", 'page 361). Kushner, H. (1987) Asymptotic global behavior for stochastic approximation and diffusions with slowly decreasing noise effects: global minimization via Monte Carlo. SIAM Journal on Applied Mathematics 47, 169-185. Kwok, S. W. & Carter, C. (1990) Multiple decision trees. In Uncertainty in Artificial Intelligence 4, eds R. D. Shachter, T. S. Levitt, L. N. Kana! & J. F. Lemmer, pp. 327-335. Amsterdam: North Holland. Lachenbruch, P. A (1975) Discriminant Analysis. New York: Hafner Press. Lachenbruch, P. A & Mickey, M. R. (1968) Estimation of error rates in discriminant analysis. Technometrics 10, 1-11.']\n",
      "['References 375 Lange, K. L., Little, R. J. A. & Taylor, J. M. G. (1989) Robust statistical modeling using the t distribution. Journal of the American Statistical Association 84, 881-896. Langley, P. (1996) Elements ofMachine Learning. San Francisco: Morgan Kaufmann. Langley, P. & Simon, H. A. (1995) Applications of machine learning and rule induction. Communications of the Association for Computing Machinery 38, 54--64. Lau, C. (ed.) (1992) Neural Networks: Theoretical Foundations and Analysis. New York: IEEE Press. Lauritzen, S. (1989) Mixed graphical association models (with discussion). Scandinavian Journal of Statistics 16, 273-306. Lauritzen, S. (1992) Propagation of probabilities, means and variances in mixed graphical association models. Journal of the American Statistical Association 87, 1089-1108. Lauritzen, S. L. (1996) Graphical Models. Oxford: Clarendon Press. Lauritzen, S. & Spiegelhalter, D. J. (1988) Local computations with probabilities on graphical structures and their', 'Spiegelhalter, D. J. (1988) Local computations with probabilities on graphical structures and their application to expert systems (with discussion). Journal of the Royal Statistical Society series B 50, 157-224. Reprinted in Shafer & Pearl (1990). Lauritzen, S. L., Dawid, A. P., Larsen, B. N. & Leimer, H.-G. (1990) Independence properties of directed Markov fields. Networks 20, 491-505. Lauritzen, S. L., Thiesson, B. & Spiegelhalter, D. J. (1994) Diagnostic systems created by model selection methods-a case study. In Selecting Models from Data: AI and Statistics IV, eds P. Cheeseman & R. W. Oldford, pp. 143-152. Lecture Notes in Statistics 89. New York: Springer. Lazarsfeld, P. F. (1961) The algebra of dichotomous systems. In Studies in Item Analysis and Prediction, ed. H. Solomon, pp. 111-157. Palo Alto, CA: Stanford University Press. LeBlanc, M. & Tibshirani, R. J. (1993) Combining estimates in regression and classification. Preprint, Depts of Preventive Medicine and Biostatistics', 'in regression and classification. Preprint, Depts of Preventive Medicine and Biostatistics and of Statistics, University of Toronto. Le Cun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W. & Jackel, L. D. (1989) Backpropagation applied to handwritten Zip code recognition. Neural Computation 1, 541-551. Le Cun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W. & Jackel, L. D. (1990a) Handwritten digit recognition with a backpropagation network. In NIPS2, pp. 396-404. Le Cun, Y., Denker, J. S. & Solla, S. A. (1990b) Optimal brain damage. In NIPS2, pp. 598-605. Lee, S. & Kil, R. M. (1988) Multi-layer feedforward potential function network. In Proceedings of the IEEE International Conference on Neural Networks, San Diego, I, pp. 161-171. Long Beach, CA: IEEE Press. Lee, T.-C., Peterson, A. M. & Tsai, J. C. (1990) A multi-layer feed-forward neural network with dynamically adjustable structures. In Proceedings of the IEEE International', \"neural network with dynamically adjustable structures. In Proceedings of the IEEE International Conference on Systems, Man and Cybernetics, Los Angeles, pp. 367-369. Long Beach, CA: IEEE Press. Lee, Y. (1991) Handwritten digit recognition using K nearest-neighbor, radial-basis function, and backpropagation neural networks. Neural Computation 3, 440-449. de Leeuw, J. (1984) Differentiability of Kruskal's stress at a local minimum. Psychometrika 49, 111-113. Lehmann, E. L. (1983) Theory of Point Estimation. New York: Wiley. Lehmann, E. L. (1986) Testing Statistical Hypotheses. Second edition. Pacific Grove, CA: Wadsworth & Brooks/Cole. (Formerly New York: Wiley.) Leonard, J. A., Kramer, M. A. & Ungar, J. H. (1992) Using radial basis functions to approximate a function and its error bounds. IEEE Transactions on Neural Networks 3, 624--627. Lesaffre, E. & Albert, A. (1989) Partial separation in logistic discrimination. Journal of the Royal Statistical Society series B 51, 109-116. Levin,\", 'in logistic discrimination. Journal of the Royal Statistical Society series B 51, 109-116. Levin, A. U., Leen, T. K. & Moody, J. E. (1994) Fast pruning using principal components. In NIPS6, pp. 35-42. Levitt, T. S., Binford, T. 0. & Ettinger, G. L. (1990) Utility-based control for computer vision. In Uncertainty in Artificial Intelligence 4, eds R. D. Shachter, T. S. Levitt, L. N. Kana! & J. F. Lemmer, pp. 407-422. North Holland, Amsterdam. Li, X. B. & Dubes, R. C. (1986) Tree classifier design with a permutation statistic. Pattern Recognition 19, 229-235. Lincoln, W. P. & Skrzypek, J. (1990) Synergy of clustering multiple backpropagation networks. In NIPS2, pp. 650-657.']\n",
      "['376 References Lindley, D. V. (1980) Approximate Bayesian methods. In Bayesian Statistics , eds J. M. Bernardo , M. H. DeGroot, D. V. Lindley & A. F. M. Smith, pp. 223-237. Valencia: Valencia University Press. Little, R. J. A. & Rubin, D. B. (1987) Statistical Analysis with Missing Data. New York: Wiley. Liu, D. C. & Nocedal, J. (1989) On the limited memory BFGS method for large-scale optimization. Mathematical Programming 45, 503-528. Liu, L., Wilkins, D. C., Ying, X. & Bain, Z. (1991) Minimum error tree decomposition . In Proceedings of the Conference on Uncertainty in AI (Cambridge , MA), pp. 180-185. Liu, Y. (1993) Neural network model selection using asymptotic jackknife estimator and crossvalidation method. In NIPS5, pp. 599-606. Liu, Y. (1994) Robust parameter estimation and model selection for neural network regression. In N IPS6, pp. 192-199. Liu, Y. (1995) Unbiased estimate of generalization error and model selection in neural network. Neural Networks 8, 215-219. Lloyd, S. P.', 'error and model selection in neural network. Neural Networks 8, 215-219. Lloyd, S. P. (1957, 1982) Least squares quantization in PCM. Technical Note, Bell Laboratories. Published in 1982 in IEEE Transactions on Iriformation Theory 28, 128-137. Loizou, G. & Maybank, S. J. (1987) The nearest neighbor and the Bayes error rates. IEEE Transactions on Pattern Analysis and Machine Intelligence 9, 254-262. Louis, T. A. (1982) Finding the observed information matrix when using the EM algorithm . Journal of the Royal Statistical Society series B 44, 226-233. Lunts, A. L. & Brailovsky , V. L. ( 1967) Evaluation of attributes obtained in statistical decision rules. Engineering Cybernetics 3, 98-109. Luttrell, S. P. (1989) Hierarchical vector quantization. lEE Proceedings I 136, 405-413. Maass, W. G. (1994a) Neural networks with superlinear VC dimension. Neural Computation 6, 877-884. Maass, W. G. (1994b) Perspectives of current research about the complexity of learning on neural nets. Chapter 5', '(1994b) Perspectives of current research about the complexity of learning on neural nets. Chapter 5 of Theoretical Advances in Neural Computation and Learning, eds V. Roychowdhury, K.-Y. Siu & A. Orlitsky, pp. 153-172. Boston: Kluwer Academic Publishers. Maass, W. & Turan, G. (1994) How fast can a threshold gate learn? In Computational Learning Theory and Natural Learning Systems: Constraints and Prospects, eds S. J. Hanson, G. A. Drastal & R. L. Rivest, volume I, pp. 381-414. MIT Press. Macintyre , A. & Sontag, E. D. (1993) Finiteness results for sigmoidal \"neural\" networks . Proceedings of the 25th Annual ACM Symposium Theory of Computing, San Diego, 1993, pp. 325-334. New York: ACM Press. MacKay, D. J. C. (1992a) Bayesian interpolation. Neural Computation 4, 415-447. MacKay, D. J. C. (1992b) A practical Bayesian framework for backprop networks. Neural Computation 4, 448-472. MacKay, D. J. C. (1992c) Information-based objective functions for active data selection. Neural Computation', 'J. C. (1992c) Information-based objective functions for active data selection. Neural Computation 4, 590-604. MacKay , D. J. C. (1992d) The evidence framework applied to classification networks. Neural Computation 4, 720-736. MacKay, D. J. C. (1992e) Bayesian model comparison and backprop nets. In NIPS4, pp. 839-846. MacKay, D. M. & McCulloch, W. S. (1952) The limiting information capacity of a neuronal link. Bulletin of Mathematical Biophysics 14, 127-135. MacLeod, J. E. S., Luk, A. & Titterington, D. M. (1987) A re-examination of the distance-weighted k-nearest neighbor classification rule. IEEE Transactions on Systems, Man and Cybernetic s 17, 689-696. Reprinted in Dasarathy (1991). Macnaughton-Smith, P., Williams, W. T., Dale, M. B. & Mockett, L. G. (1964) Dissimilarity analysis: a new technique of hierarchical sub-division. Nature 202, 1034-1035. MacQueen , J. ( 1967) Some methods for classification and analysis of multivariate observations . In Proceedings of the Fifth Berkeley', \"for classification and analysis of multivariate observations . In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability , eds L. M. Le Cam & J. Neyman, 1, pp. 281-297. Berkeley, CA: University of California Press. Madigan, D. & Raftery, A. E. (1994) Model selection and accounting for model uncertainty in graphical models using Occam's window. Journal of the American Statistical Association 89, 1535-1546. Madigan, D. & York, J. (1995) Bayesian graphical models for discrete data. International Statistical Review 63, 215-232. Madych, W. R. & Nelson, S. A. (1990) Multivariate interpolation and conditionally positive definite functions II. Mathematics of Computation 54, 211-230. Mahalanobis , P. C. (1936) On generalized distance in statistics. Proceedings of the National lnst. Sci. (India) 12, 49-55.\"]\n",
      "['References 377 Maier, D. (1983) The Theory of Relational Databases. Rockville, Md: Computer Science Press. Makram-Ebeid, S., Sirat, J.-A. & Viala, J.-R. (1989) A rationalized back-propagation learning algorithm. In International Joint Conference on Neural Networks (Washington, 1989) II, 373-380. New York: IEEE Press. Mammone, R. J. (ed.) (1993) Artificial Neural Networks for Speech and Vision. London: Chapman & Hall. Mangarasian, 0. L. (1968) Multisurface methods of pattern separation. IEEE Transactions on Information Theory 14, 801-807. Mangarasian, 0. L., Setiono, R. & Wolberg, W. H. (1990) Pattern-recognition via linear-programming: theory and application to medical diagnosis. In Large-Scale Numerical Optimization, 1990, eds T. F. Coleman & Y. Li, pp. 22-31. Philadelphia: SIAM. Manly, B. F. J. & Rayner, J. C. W. (1987) The comparison of sample covariance matrices using likelihood ratio tests. Biometrika 74, 841-847. Mansfield, A. J. (1991) Comparison of perceptron training by', 'ratio tests. Biometrika 74, 841-847. Mansfield, A. J. (1991) Comparison of perceptron training by linear-programming and by the perceptron convergence procedure. Proceedings of the International Joint Conference on Neural Networks (Seattle 1991) II, 25-30. Long Beach, CA: IEEE Press. Mardia, K. V., Kent, J. T. & Bibby, J. M. (1979) Multivariate Analysis. London: Academic Press. Maritz, J. S. & Lwin, T. (1989) Empirical Bayes Methods. Second edition. London: Chapman & Hall. Marks, S. & Dunn, 0. J. (1974) Discriminant functions when covariance matrices are unequal. Journal of the American Statistical Association 69, 555-559. Maronna, R. A. (1976) Robust M-estimators of multivariate location and scatter. Annals of Statistics 4, 51-67. Marriott, F. H. C. (1975) Separating mixtures of normal distributions. Biometrics 31, 767-769. Martin, G. L. & Pitman, J. A. (1990) Recognizing hand-printed letters and digits. In NIPS2, pp. 405-414. Martin, G. L. & Pitman, J. A. (1991) Recognizing', 'letters and digits. In NIPS2, pp. 405-414. Martin, G. L. & Pitman, J. A. (1991) Recognizing hand-printed letters and digits using backpropagation learning. Neural Computation 3, 258-267. Massart, D. L., Plastria, F. & Kaufman, L. (1983) Non-hierarchical clustering with MASLOC. Pattern Recognition 16, 507-516. Mathieson, M. J. (1996) Ordinal models for neural networks. In Neural Networks in Financial Engineering. eds A.-P. Refenes, Y. Abu-Mostafa & J. Moody. Singapore: World Scientific, 523-536. Matus, F. (1992) On equivalence of Markov properties over undirected graphs. Journal of Applied Probability 29, 745-749. Max, J. (1960) Quantizing for minimum distortion. IRE Transactions on Information Theory 6, 7-12. McCullagh, P. & Neider, J. A. (1989) Generalized Linear Models. Second edition. London: Chapman & Hall. McCulloch, W. S. & Pitts, W. (1943) A logical calculus of ideas immanent in nervous activity. Bulletin of Mathematical Biophysics 5, 115-133. Reprinted in Anderson & Rosenfeld', 'nervous activity. Bulletin of Mathematical Biophysics 5, 115-133. Reprinted in Anderson & Rosenfeld (1988). McKay, R. J. & Campbell, N. A. (1982a) Variable selection techniques in discriminant analysis. I: Description. British Journal of Mathematical and Statistical Psychology 35, 1-29. McKay, R. J. & Campbell, N. A. (1982b) Variable selection techniques in discriminant analysis. II: Allocation. British Journal of Mathematical and Statistical Psychology 35, 30-41. McLachlan, G. J. (1992) Discriminant Analysis and Statistical Pattern Recognition. New York: Wiley. McLachlan, G. J. & Basford, K. E. (1988) Mixture Models: Inference and Applications to Clustering. New York: Marcel Dekker. Meinguet, J. (1979) Multivariate interpolation at arbitrary points made simple. Journal of Applied Mathematics and Physics (ZAMP) 30, 292-304. Meisel, W. S. (1972) Computer-Oriented Approaches to Pattern Recognition. New York: Academic Press. Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, A. &', 'Recognition. New York: Academic Press. Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, A. & Teller, E. (1953) Equations of state calculations by fast computing machines. Journal of Chemical Physics 21, 1087-1091. Mhaskar, H. N. & Micchelli, C. A. (1992) Approximation by superposition of sigmoidal function and radial basis functions. Advances in Applied Mathematics 13, 350-373. Michalski, R. S. (1980) Pattern recognition as ruleguided inductive inference. IEEE Transactions on Pattern Analysis and Machine Intelligence 2, 349-361. Michie, D. (1989) Problems of computer-aided concept formation. In Applications of Expert Systems volume 2, ed. J. R. Quinlan, pp. 310-333. Glasgow: Turing Institute Press/ Addison- Wesley.']\n",
      "['378 References Michie, D., Spiegelhalter, D. J. & Taylor, C. C. (eds) (1994) Machine Learning, Neural and Statistical Classification. New York: Ellis Horwood. Mingers, J. (1987) Expert systems-rule induction with statistical data. Journal of the Operational Research Society 38, 39-47. Minnick, R. C. (1961) Linear-input logic. IRE Transactions on Electronic Computers 10, 6-16. Minsky, M. (1961) Steps towards artificial intelligence. Proceedings of the IRE 49, 8-30. Minsky, M. L. & Papert, S. A. (1988) Perceptrons. An Introduction to Computational Geometry . Expanded edition. Cambridge, MA: The MIT Press. Moller, M (1993) A scaled conjugate gradient algorithm for fast supervised learning. Neural Networks 6, 525-533. Moody, J. E. (1989) Fast learning in multi-resolution hierarchies. In NIPSJ, pp. 29-39. Moody, J. E. (1991) Note on generalization, regularization and architecture selection in nonlinear learning systems. In First IEEE-SP Workshop on Neural Networks in Signal Processing, pp.', 'nonlinear learning systems. In First IEEE-SP Workshop on Neural Networks in Signal Processing, pp. 1-10. Los Alamitos , CA: IEEE Computer Society Press. Moody, J. E. (1992) The effective number of parameters: an analysis of generalization and regularization in nonlinear learning systems. In N I PS4, pp. 847-854. Moody, J. & Darken, C. J. (1989) Fast learning in networks of locally-tuned processing units. Neural Computation 1, 281-294. Moody, J. & Utans, J. (1992) Principled architecture selection for neural networks: application to corporate bond rating prediction. In NIPS4, pp. 683-690. Moody, J. & Utans, J. (1995) Architecture selection strategies for neural networks: application to corporate bond rating prediction. In Neural Networks in the Capital Markets, ed. A.-P. Refenes, pp. 277-300. Chichester: Wiley. Moore, B. (1989) ART 1 and pattern clustering. In Proceedings of the 1988 Connectionist Models Summer School eds D. Touretzky , G. Hinton & T. Sejnowski, pp. 174-185. San Mateo,', 'Models Summer School eds D. Touretzky , G. Hinton & T. Sejnowski, pp. 174-185. San Mateo, CA: Morgan Kaufmann. Moran, M. A. & Murphy, B. J. (1979) A closer look at two alternative methods of statistical discrimination. Applied Statistics 28, 223-232. Morgan, J. N. & Messenger, R. C. (1973) THAID: a Sequential Search Program for the Analysis of Nominal Scale Dependent Variables. Survey Research Center, Institute for Social Research, University of Michigan. Morgan, J. N. & Sonquist, J. A. (1963) Problems in the analysis of survey data, and a proposal. Journal of the American Statistical Association 58,415-434. Morin, R. L. & Raeside, D. E. (1981) A reappraisal of distance-weighted k-nearest neighbor classification for pattern recognition with missing data. IEEE Transactions on Systems, Man and Cybernetics 11, 241-243. Mosteller, F. & Wallace, D. L. (1963) Inference in an authorship problem. Journal of the American Statistical Association 58, 275-309. Moulton, B. R. (1991) A', 'problem. Journal of the American Statistical Association 58, 275-309. Moulton, B. R. (1991) A Bayesian-approach to regression selection and estimation with application to a price-index for radio services. Journal of Econometrics 49, 169-193. Moussouris , J. (1974) Gibbs and Markov random systems with constraints. Journal of Statistical Physics 10, 11-33. Murata, N., Yoshizawa , S. & Amari, S. (1991) A criterion for determining the number of parameters in an artificial neural network model. In Artificial Neural Networks. Proceedings of ICANN-91, eds T. Kohonen , K. Miikisara, 0. Simula & J. Kangas, volume I, pp. 9-14. Amsterdam: North Holland. Murata, N., Yoshizawa , S. & Amari, S. (1993) Learning curves, model selection and complexity of neural networks . In NIPS5, pp. 607-614. Murata, N., Yoshizawa , S. & Amari, S. (1994) Network information criterion-determining the number of hidden units for artificial neural network models. IEEE Transactions on Neural Networks 5, 865-872. Muroga,', 'for artificial neural network models. IEEE Transactions on Neural Networks 5, 865-872. Muroga, S. (1965) Lower bounds of the number of threshold functions and a maximum weight. IEEE Transactions on Electronic Computers 14, 136-148. Muroga, S. (1971) Threshold Logic and its Applications. New York: Wiley. Muroga, S., Toda, I. & Takasu, S. (1961) Theory of majority decision elements. Journal of the Franklin Institute 271, 376--418. Murphy, P. M. & Aha, D. W. (1995) UCJ Repository of Machine Learning Databases [Machine-readable data repository] . Irvine, CA: University of California, Dept of Information and Computer Science. Available by anonymous ftp from ics. uci. edu in directory pub/machine-learning-databases. Murtagh, F. (1985) A survey of algorithms for contiguity-constrained clustering and related problems. Computer Journal 28, 82-88.']\n",
      "['References 379 Murtagh, F. (1995a) Contiguity-constrained hierarchical clustering. In Partitioning Data Sets, eds I. J. Cox, P. Hansen & B. Julesz. DIMACS. pp. 143-152. Providence , RI: American Mathematical Society. Murtagh, F. (1995b) Interpreting the Kohonen self-organizing feature map using contiguityconstrained clustering. Pattern Recognition Letters 16, 399-408. Murthy, V. K. (1966) Nonparametric estimation of multivariate densities with applications. In Multivariate Analysis, ed. P. R. Krishnaiah, pp. 43-56. New York: Academic Press. Musavi, M. T., Ahmed, W., Chan, K. H., Faris, K. B. & Hummels , D. M. (1992) On the training of radial basis function classifiers. Neural Networks 5, 595-603. Myles, J. P. & Hand, D. J. (1990) The multiclass metric problem in nearest neighbour discrimination rules. Pattern Recognition 23, 1291-1297. Narendra, P. M. & Fukunaga , K. (1977) A branch and bound algorithm for feature subset selection. IEEE Transactions on Computers 26, 917-922. Nash, J.', 'bound algorithm for feature subset selection. IEEE Transactions on Computers 26, 917-922. Nash, J. C. (1990) Compact Numerical Methods for Computer s. Linear Algebra and Function Minimization. Second edition. Bristol: Adam Hilger. Neal, R. (1992a) Connectionist learning of belief networks. Artificial Intelligence 56, 7l-ll3. Neal, R. M. (1992b) Asymmetric parallel Boltzmann machines are belief networks . Neural Computation 4, 832-834. Neal, R. (1993) Bayesian learning via stochastic dynamics . In NIPS5, pp. 475-482. Neal, R. M. (1996) Bayesian Learning for Neural Networks . Lecture Notes in Statistics 118. New York: Springer. Neapolitan, E. (1990) Probabili stic Reasoning in Expert Systems. Theory and Algorithms. New York: Wiley. Niblett, T. (1987) Constructing decision trees in noisy domains. In Progress in Machine Learning , eds I. Bratko & N. Lavrac, pp. 67-78. Wilmslow: Sigma Press. Niblett, T. & Bratko, I. (1987) Learning decision rules in noisy domains. In Research and', \"Press. Niblett, T. & Bratko, I. (1987) Learning decision rules in noisy domains. In Research and Development in Expert Systems Ill. Proceedings of Expert Systems '86, Brighton 1986, ed. M. A. Bramer, pp. 25-34. Cambridge: Cambridge University Press. Niemann, H. & Goppert, G. (1988) An efficient branch-and-bound nearest neighbour classifier. Pattern Recognitio n Letters 7, 67-72. Reprinted in Dasarathy (1991). Nowlan, S. J. & Hinton, G. E. (1992a) Adaptive soft weight tying using Gaussian mixtures. In NIPS4, pp. 993-1000. Nowlan, S. J. & Hinton, G. E. (1992b) Simplifying neural networks by soft weight-sharing. Neural Computation 4, 473-493. Reprinted with an introduction as pp. 369-394 of Wolpert (1995). Oja, E. (1982) A simplified neuron model as a principal component analyzer. Journal of Mathematical Biology 16, 267-273. Oja, E. (1989) Neural networks, principal components and subspaces. International Journal of Neural Systems 1, 61-68. Oja, E. (1992) Principal components, minor\", 'International Journal of Neural Systems 1, 61-68. Oja, E. (1992) Principal components, minor components and linear neural networks . Neural Networks 5, 927-935. Oja, E. & Karhunen, J. (1985) On stochasticapproximation of the eigenvectors and eigenvalues of the expectation of a random matrix. Journal of Mathematical Analysis and its Applications 106, 69-84. Olesen, K. G. (1993) Causal probabilistic networks with both discrete and continuous variables. IEEE Transactions on Pattern Analysis and Machine Intelligence 15, 275-279. Oliver, L. H., Poulsen, R. S., Toussaint, G. T. & Louis, C. (1979) Classification of atypical cells in the automated cytoscreening for cervical cancer. Pattern Recognition 11, 205-212. Oliver, R. M. & Smith, J. Q. (eds) (1990) Influence Diagrams. Belief Nets and Decision Analysis. Chichester: Wiley. Olkin, I. & Tate, R. F. (1961) Multivariate correlation models with mixed discrete and continuous variates. Annals of Mathematical Statistics 32, 445-465. van Ooyen,', 'mixed discrete and continuous variates. Annals of Mathematical Statistics 32, 445-465. van Ooyen, A. & Nienhuis, B. (1992) Improving the convergence of the back-propagation algorithm . Neural Networks 5, 465-471. (See also letter to the editor and response, 6, 6ll-612.) Ott, J. (1989) Computer-simulation methods in human linkage analysis. Proceedings of the National Academy of Sciences of the USA 86,4175-4178. Owen, A. (1984) A neighbourhood-based LANDSAT classifier. Canadian Journal of Statistics 12, 191-200.']\n",
      "['380 References Owens, A. J. & Filkin, D. L. (1989) Efficient training of the back propagation network by solving a system of stiff ordinary differential equations. In Proceedings of the International Conference on Neural Networks (Washington, 1989), II, 381-386. New York: IEEE Press. Pagallo, G. (1989) Learning DNF by decision trees. In Proceedings of the Eleventh International Joint Conference on Artificial Intelligen ce (Detroit, 1989 ), pp. 639-644. Pagallo, G. & Haussler, D. (1989) Two algorithms that learn DNF by discovering relevant features. In Proceedings of the Sixth International Workshop on Machine Learning (Ithaca, 1989 ), ed. A. M. Segre, pp. 119-123. San Mateo, CA: Morgan Kaufmann. Pagallo, G. & Haussler, D. (1990) Boolean feature discovery in empirical learning. Machine Learning 5, 71-99. Parberry, I. (1994) Circuit Complexity and Neural Networks. Cambridge, MA: MIT Press. Park, J. & Sandberg , I. W. (1991) Universal approximation using radial-basis-function networks.', 'Park, J. & Sandberg , I. W. (1991) Universal approximation using radial-basis-function networks. Neural Computation 3, 246-257. Parrondo, J. M. R. & Van der Broeck, C. (1993) Vapnik-Chervonenkis bounds for generalization. J. Physics A 26, 2211-2223. Parthasarthy, G. & Chatterji, B. N. (1990) A class of new KNN methods for low sample problems. IEEE Transactions on Systems, Man and Cybernetics 20, 715-718. Parzen, E. (1962) On the estimation of a probability density function and mode. Annals of Mathematical Statistics 33, 1065-1076. Patrick, E. A. & Fisher, F. P. II (1969) Nonparametric feature selection. IEEE Transactions on Information Theory 15, 577-584. Patterson, A. & Niblett, T. (1983) ACLS User Manual. Glasgow: Intelligent Terminals Ltd. Pavlidis, T. (1993) Recognition of printed text under realistic conditions. Pattern Recognition Letters 14, 317-326. Payne, H. J. & Meisel, W. S. (1977) An algorithm for constructing optimal binary decision trees. IEEE Transactions on Computers', '(1977) An algorithm for constructing optimal binary decision trees. IEEE Transactions on Computers 26, 905-916. Pearl, J. (1979) Capacity and error estimates for Boolean classifiers with limited capacity. IEEE Transactions on Pattern Analysis and Machine Intelligence 1, 350--356. Pearl, J. (1982) Reverend Bayes on inference engines: a distributed hierarchical approach . In Proceedings of the AAAI National Conference on Artificial Intelligence (Pittsburgh), pp. 133-136. Menlo Park, CA:AAAI. Pearl, J. (1986) Fusion, propagation, and structuring in belief networks. Artificial Intelligence 29, 241-288. Reprinted in Shafer & Pearl (1990). Pearl, J. (1987) Evidential reasoning using stochastic simulation of causal models. Artificial Intelligence 32, 245-257. Pearl, J. (1988) Probabilistic Inference in Intelligent Systems. Networks of Plausible Inference. San Mateo, CA: Morgan Kaufmann . Pearl, J. ( 1993a) Belief networks revisited. Artificial Intelligence 59, 49-56. Pearl, J. (1993b)', 'Pearl, J. ( 1993a) Belief networks revisited. Artificial Intelligence 59, 49-56. Pearl, J. (1993b) Graphical models, causality and intervention. Contribution to the discussion of Spiegelhalter et al. (1993), pp. 266-269. Pearl, J. (1995) From Bayesian networks to causal networks . In Gammerman (1995), pp. 1-31. Pearlmutter , B. A. (1994) Fast exact multiplication by the Hessian. Neural Computation 6, 147-160. Pearlmutter , B. A. & Rosenfeld , R. (1991) ChaitinKolmogorov complexity and generalization in neural networks. In NIPS3, pp. 925-931. Peck, R., Fisher, L. & Van Ness J. (1989) Approximate confidence intervals for the number of clusters. Journal of the American Statistical Association 84, 184-191. Peng, F., Jacobs, R. A. & Tanner, M. A. (1994) Bayesian inference in mixtures-of-experts and hierarchical mixtures-of-experts architectures . Technical report, Dept of Biostatistics , University of Rochester, NY. Penrod, C. S. & Wagner, T. J. (1977) Another look at the edited nearest', 'of Rochester, NY. Penrod, C. S. & Wagner, T. J. (1977) Another look at the edited nearest neighbor rule. IEEE Transactions on Systems, Man and Cybernetics 7, 92-94. Peretto, P. (1992) An Introduction to the Modeling of Neural Networks. Cambridge: Cambridge University Press. Perrone, M.P. & Cooper, L. N. (1993) When networks disagree: Ensemble methods for hybrid neural networks. In Mammone (1993), pp. 126-142. Peskun, P. H. ( 1973) Optimal Monte-Carlo sampling using Markov chains. Biometrika 60, 607-612. Peterson, C. & Anderson, J. R. (1987) A mean field learning algorithm for neural networks. Complex Systems 1, 995-1019.']\n",
      "['References 381 Pitas, I. (ed.) (1993) Parallel Algorithms for Digital Image Processing, Computer Vision and Neural Networks. Chichester: Wiley. Ploughman, L. M. & Boehnke, M. (1989) Estimating the power of a proposed linkage study for a complex genetic trait. American Journal of Human Genetics 44, 543-551. Poggio, T. & Girosi, F. (1990a) Regularization algorithms for learning that are equivalent to multilayer networks. Science 247, 978-982. Poggio, T. & Girosi, F. (1990b) Networks for approximation and learning. Proceedings of the IEEE 78, 1481-1497. Reprinted in Lau (1992). Pollard, D. (1984) Convergence of Stochastic Processes. New York: Springer. Pollard, D. (1986) Rates of uniform almost-sure convergence for empirical processes indexed by unbounded classes of functions. Unpublished paper, Dept of Statistics, Yale University. Pollard, D. (1990) Empirical Processes: Theory and Applications. Hayward, CA: Institute of Mathematical Statistics and American Statistical Association. Posse,', 'Hayward, CA: Institute of Mathematical Statistics and American Statistical Association. Posse, C. (1990) An effective two-dimensional projection pursuit algorithm. Communications in Statistics-Simulation and Computation 19, 1143-1164. Posse, C. (1995a) Tools for two-dimensional exploratory projection pursuit. Journal of Computational and Graphical Statistics 4, 83-100. Posse, C. (1995b) Projection pursuit exploratory data analysis. Computational Statistics and Data Analysis. Powell, M. J. D. (1987) Radial basis functions for multivariable interpolation: a review. In Algorithms for Approximation, eds J. C. Mason & M. G. Cox, pp. 143-167. Oxford: Clarendon Press. Powell, M. J. D. (1992) The theory of radial function approximation in 1990. In Advances in Numerical Analysis volume II, ed. W. Light, pp. 105-210. Oxford: Clarendon Press. Prechelt, L. ( 1994) A study of experimental evaluation of current neural network learning algorithms: current research practice. Technical Report 19/94,', 'of current neural network learning algorithms: current research practice. Technical Report 19/94, Fakultat fiir Informatik, Universitat Kahlsruhe. Prentice, R. & Pyke, R. (1979) Logistic disease incidence models and case-control studies. Biometrika 66, 403-411. Preparata, F. P. & Shamos, M. I. (1985) Computational Geometry. An Introduction. New York: Springer. Press, W. H., Flannery, B. P., Teukolsky, S. A. & Vetterling, W. T. (1992) Numerical Recipes in C. Second edition. Cambridge: Cambridge University Press. Preston, C. J. (1974) Gibbs States on Countable Sets. London: Cambridge University Press. Preston, C. J. (1976) Random Fields. Lecture Notes in Mathematics 534. Berlin: Springer. Przytula, K. W. & Prasanna, V. K. (1993) Parallel Digital Implementation of Neural Networks. Englewood Cliffs, NJ: Prentice Hall. Quenouille, M. H. (1949) Approximate tests of correlation in time series. Journal of the Royal Statistical Society series B 11, 68-84. Quinlan, J. R. (1979) Discovering', 'Journal of the Royal Statistical Society series B 11, 68-84. Quinlan, J. R. (1979) Discovering rules by induction from large collections of examples. In Expert Systems in the Microelectronic Age, ed. D. Michie, pp. 168-201. Edinburgh: Edinburgh University Press. Quinlan, J. R. (1983) Learning efficient classification procedures and their application to chess endgames. In Machine Learning, eds R. S. Michalski, J. G. Carbonell & T. M. Mitchell, pp. 463--482. Palo Alto, CA: Tioga. Quinlan, J. R. (1986) Induction of decision trees. Machine Learning 1, 81-106. Reprinted in Shavlik & Dietterich (1990). Quinlan, J. R. (1987a) Simplifying decision trees. International Journal of Man-Machine Studies 27, 221-234. Quinlan, J. R. (1987b) Generating production rules from decision trees. In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, Milan, pp. 304-307. Quinlan, J. R. (1988) Decision trees and multivalued attributes. In Machine Intelligence 11, eds J. E.', 'J. R. (1988) Decision trees and multivalued attributes. In Machine Intelligence 11, eds J. E. Hayes, D. Michie & J. Richards, pp. 305-318. Oxford: Clarendon Press. Quinlan, J. R. (1990) Decision trees and decision making. IEEE Transactions on Systems, Man and Cybernetics 20, 339-346. Quinlan, J. R. (1993) C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann. Raftery, A. E. (1993) Approximate Bayes factors and accounting for model uncertainty in generalized linear models. Technical report 255, Dept of Statistics, University of Washington. Rao, C. R. (1948) The utilization of multiple measurements in problems of biological classification (with discussion). Journal of the Royal Statistical Society series B 10, 159-203.']\n",
      "['382 References Rao, C. R. ( 1960) Multivariate analysis: an indispensable statistical aid in applied research. Sankhyii 22, 317~338. Rayens, W. & Greene, T. (1991) Covariance pooling and stabilization for classification. Computational Statistics and Data Analysis 11, 17-42. Redner, R. A & Walker, H. F. (1984) Mixture densities, maximum likelihood and the EM algorithm. SIAM Review 26, 195~239. Reed, R. (1993) Pruning algorithms-a survey. IEEE Transactions on Neural Networks 4, 740--747. Reilly, D. L., Cooper, L. N. & Elbaum, C. (1982) A neural model for category learning. Biological Cybernetics 45, 35~41. Reprinted in Anderson et al. (1990). Richards, L. E. (1972) Refinement and extension of distribution-free discriminate analysis. Applied Statistics 21, 174-176. Riffenburgh, R. H. & Clunies-Ross, C. W. (1960) Linear discriminant analysis. Pacific Science 14, 251~256. Rimey, R. & Brown, C. (1992) Task-oriented vision with multiple Bayes nets. In Active Vision, eds A Blake & A Yuille,', 'C. (1992) Task-oriented vision with multiple Bayes nets. In Active Vision, eds A Blake & A Yuille, pp. 217~236. Cambridge, MA: The MIT Press. Ripley, B. D. (1977) Modelling spatial patterns (with discussion). Journal of the Royal Statistical Society series B 39, 172~212. Ripley, B. D. (1979) Algorithm AS137. Simulating spatial patterns: dependent samples from a multivariate density. Applied Statistics 28, 109~112. Ripley, B. D. (1987) Stochastic Simulation. New York: Wiley. Ripley, B. D. (1988) Statistical Inference for Spatial Processes. Cambridge: Cambridge University Press. Ripley, B. D. (1993) Statistical aspects of neural networks. In Networks and Chaos-Statistical and Probabilistic Aspects, eds 0. E. Barndorff-Nielsen, J. L. Jensen & W. S. Kendall, pp. 40--123. London: Chapman & Hall. Ripley, B. D. (1994a) Neural networks and related methods for classification (with discussion). Journal of the Royal Statistical Society series B 56, 409~456. Ripley, B. D. (1994b) Neural networks', 'of the Royal Statistical Society series B 56, 409~456. Ripley, B. D. (1994b) Neural networks and flexible regression and discrimination. In Statistics and Images 2, ed. K. V. Mardia. Advances in Applied Statistics 2, pp. 39~57. Abingdon: Carfax. Ripley, B. D. (1994c) Flexible non-linear approaches to classification. In Cherkassky et a/. (1994), pp. 105~126. Ripley, B. D. (1995) Statistical ideas for selecting network architectures. In Neural Networks: Artificial Intelligence and Industrial Applications, eds B. Kappen & S. Gielen. London: Springer. Ripley, B. D. & Kelly, F. P. (1977) Markov point processes. Journal of the London Mathematical Society (2) 15, 188~192. Ripley, B. D. & Kirkland, M. D. (1990) Iterative simulation methods. Journal of Computational and Applied Mathematics 31, 165~172. Rissanen, J. (1983) A universal prior for integers and estimation by minimum description length. Annals of Statistics 11, 416-431. Rissanen, J. (1987) Stochastic complexity (with discussion).', 'Annals of Statistics 11, 416-431. Rissanen, J. (1987) Stochastic complexity (with discussion). Journal of the Royal Statistical Society series B 49, 223~239. Rissanen, J. (1989) Stochastic Complexity in Statistical Inquiry. Singapore: World Scientific Publishing Co. Ritter, G. L., Woodruff, H. B., Lowry, S. R. & Isenhour, T. L. (1975) An algorithm for a selective nearest neighbor decision rule. IEEE Transactions on Information Theory 21, 665~669. Reprinted in Dasarathy (1991). Ritter, H., Martinetz, T. & Schulten, K. (1992) Neural Computation and Selj:Organizing Maps. An Introduction. Reading, MA: Addison-Wesley. Roberts, S. & Tarassenko, L. (1995) Automated sleep EEG analysis using an RBF network. In Neural Network Applications, ed. A F. Murray, pp. 305~ 322. Dordrecht: Kluwer Academic Publishers. Robinson, R. W. (1977) Counting unlabeled acyclic digraphs. In Combinatorial Mathematics V, ed. C. H. C. Little. Lecture Notes in Mathematics 622, pp. 28-43. Berlin: Springer. Roeder, K.', 'V, ed. C. H. C. Little. Lecture Notes in Mathematics 622, pp. 28-43. Berlin: Springer. Roeder, K. (1990) Density estimation with confidence sets exemplified by superclusters and voids in galaxies. Journal of the American Statistical Association 85, 617~624. Roosen, C. B. & Hastie, T. J. (1994) Automatic smoothing spline projection pursuit. Journal of Computational and Graphical Statistics 3, 235~248. Rose, D. J., Tarjan, R. E. & Lueker, G. S. (1976) Algorithmic aspects of vertex elimination on graphs. SIAM Journal on Computing 5, 266~283. Rosenblatt, F. (1957) The perceptron-a perceiving and recognizing automaton. Report 85-460-1, Cornell Aeronautical Laboratory. Rosenblatt, F. (1958) The perceptron: a probabilistic model for information storage and organization in the brain. Psychological Review 65, 386-408. Reprinted in Shavlik & Dietterich (1990).']\n",
      "['References 383 Rosenblatt , F. (1962) Principles of Neurodynamics. Washington, DC: Spartan Books. Rosenblatt , M. (1956) Remarks on some nonparametric estimates of a density function. Annals of Mathematical Statistics 27, 832-837. Rounds, E. M. (1980) A combined nonparametric approach to feature selection and binary decision tree design. Pattern Recognition 12, 313-317. Rousseeuw, P. J. & Leroy, A. M. (1987) Robust Regression and Outlier Detection . New York: Wiley. Rousseeuw, P. J. & van Zomeren , B. C. (1990) Unmasking multivariate outliers and leverage points (with discussion). Journal of the American Statistical Association 85, 633-651. Ruck, D. W., Rogers, S. K., Kabrisky , M., Maybeck, P. S. & Oxley, M. E. (1992) Comparative analysis of back propagation and the extended Kalman filter for training multilayer perceptrons . IEEE Transactions on Pattern Analysis and Machine Intelligence 14, 686--691. Ruiz, E. V. (1986) An algorithm for finding nearest neighbours in (approximately)', '14, 686--691. Ruiz, E. V. (1986) An algorithm for finding nearest neighbours in (approximately) constant average time. Pattern Recognition Letters 4, 145-158. Rumelhart, D. E. & McClelland , J. L. (eds) (1986) Parallel Distributed Processing: Exploratio ns in the Microstructure of Cognition. Volume I. Foundations. Cambridge, MA: The MIT Press. Rumelhart, D. E., Hinton, G. E. & Williams, R. J. (1986) Learning representations by backpropagating errors. Nature 323, 533-536. Reprinted in Anderson & Rosenfeld (1988). Russell, S. J. & Norvig, P. (1995) Artificial Intelligence. A Modem Approach. Englewood Cliffs, NJ: Prentice-Hall. Ruzicka, P. (1993) On the convergence of learning algorithm for topological maps. Neural Network World 4, 413-424. de Sa, V. R. & Ballard, D. H. (1993) A note on learning vector quantization. In N JPS5, pp. 220-227. Saarinen, S., Bramley, R. & Cybenko , G. (1993) Illconditioning in neural network training problems. SIAM Journal on Scientific Computing 14, 693-714.', 'in neural network training problems. SIAM Journal on Scientific Computing 14, 693-714. Safavian, S. Rand Landgrebe, D. (1991) A survey of decision tree classifier methodology. IEEE Transactions on Systems, Man and Cybernetics 21, 660-674. Sakurai, A. (1993) Tighter bounds of the VCdimension of three-layer networks. In Proceedings of the 1993 World Congress on Neural Networks, volume 3, pp. 540-543. Hillsdale, NJ: Erlbaum. Salomon , R. (1991) Improved convergence rate of back-propagation with dynamic adaption of the learning rate. ln Parallel Problem Solving From Nature (Dortmund, 1990 ). Lecture Notes in Computer Science 496, 269-273. Sarna!, A. & Iyengar, P. A. (1992) Automatic recognition and analysis of human faces and facial expressions : a survey. Pattern Recognition 25, 65-77. Sammon , J. W. Jr (1969) A non-linear mapping for data structure analysis. IEEE Transactions on Computers 18, 401-409. Sanger, T. D. (1989) Optimal unsupervised learning in a single-layer linear', \"Computers 18, 401-409. Sanger, T. D. (1989) Optimal unsupervised learning in a single-layer linear feedforward network. Neural Networks 2, 459-473. Sankar, A. & Mammone, R. J. (1993) Growing and pruning neural tree networks. IEEE Transactions on Computers 42, 291-299. Santer, T. J. & Duffy, D. E. (1986) A note on A. Albert and J. A. Anderson 's conditions for the existence of maximum likelihood estimates in logistic regression models. Biometrika 73, 755-758. Schalkoff , R. J. (1992) Pattern Recognition : Statistical, Structural and Neural Approaches . New York: Wiley. Schlimmer, J. C. & Fisher, D. H. (1986) A case study of incremental concept induction. In Proceedings of the Fifth National Conference on Artificial Intelligence, Philadelphia, pp. 496-501. San Mateo, CA: Morgan Kaufmann. Schlimmer , J. C. & Granger, R. H. Jr (1986) Incremental learning from noisy data. Machine Learning 1, 317-354. Schmidhuber , J. (1989) Accelerated learning in backpropagation nets. In Connectionism in\", '317-354. Schmidhuber , J. (1989) Accelerated learning in backpropagation nets. In Connectionism in Perspective, pp. 439-445. Amsterdam: Elsevier. Schoenberg, I. J. (1935) Remarks to Maurice Frechet\\'s article \"Sur Ia definition axiomatique d\\'une classe d\\'espaces distancies vectoriellement applicable sur l\\'espace de Hilbert\". Annals of Mathematics 36, 724--732. Schuermann, J. & Doster, D. (1984) A decisiontheoretic approach in hierarchical classifier design. Pattern Recognition 17, 359-369. Schwarz, G. (1978) Estimating the dimension of a model. Annals of Statistics 6, 461-464. Schwemer, G. T. & Dunn, 0. J. (1980) Posterior probability estimators in classification simulations. Communi cations in Statistics-Simulation and Computation B9, 133-140.']\n",
      "['384 References Scott, A. J. & Symons, M. J. (1971) Clustering methods based on likelihood ratio criteria. Biometrics 27, 387-397. Scott, A. J. & Wild, C. J. (1986) Fitting logistic models under case-control or choice based sampling. Journal of the Royal Statistical Society series B 48, 170-182. Scott, D. W. (1992) Multivariate Density Estimation. Theory, Practice and Visualization. New York: Wiley. Seber, G. A. F. & Wild, C. J. (1989) Nonlinear Regression. New York: Wiley. Sebestyen, G. S. (1962) Pattern recognition by an adaptive process of sample set construction. IEEE Transactions on Information Theory 8, S 82-S 91. Sedgewick, R. (1990) Algorithms in C. Reading, MA: Addison-Wesley. Sen, A. & Srivastava, M. (1990) Regression Analysis. Theory, Methods and Applications. New York: Springer. Sethi, I. K. (1990) Entropy nets: from decision trees to neural networks. Proceedings of the IEEE 78, 1605-1613. Reprinted in Lau (1992). Sethi, I. K. (1991) Decision tree performance enhancement', '78, 1605-1613. Reprinted in Lau (1992). Sethi, I. K. (1991) Decision tree performance enhancement using an artificial neural network implementation. In Sethi & Jain (1991), pp. 71-88. Sethi, I. K. & Jain, A. K. (eds) (1991) Artificial Neural Networks and Statistical Pattern Recognition. Old and New Connections. Amsterdam: North Holland. Sethi, I. K. & Sarvarayudu, G. P. R. (1982) Hierarchical classifier design using mutual information. IEEE Transactions on Pattern Analysis and Machine Intelligence 4, 441-445. Shachter, R. D. & Peot, M.A. (1990) Simulation approaches to general probabilistic inference on belief networks. In Uncertainty in Artificial Intelligence 5, eds M. Henrion, R. D. Shachter, L. N. Kana! & J. F. Lemmer, pp. 221-231. Amsterdam: NorthHolland. Shafer, G. (1996) Probabilistic Expert Systems. Number 67 in CBMS-NSF Regional Conference Series in Applied Mathematics. Philadelphia, PA: SIAM. Shafer, G. & Pearl, J. (eds) (1990) Readings in Uncertainty Reasoning. San Mateo,', 'PA: SIAM. Shafer, G. & Pearl, J. (eds) (1990) Readings in Uncertainty Reasoning. San Mateo, CA: Morgan Kaufmann. Shafer, G. & Shenoy, P. P. (1986) Propagating belief functions with local computations. IEEE Expert 1(3), 43-52. Shanno, D. F. (1990) Recent advances in numerical techniques for large-scale optimization. In Neural Networks for Control, eds W. T. Miller III, R. S. Sutton & P. J. Werbos, pp. 171-178. Cambridge, MA: The MIT Press. Shanno, D. F. & Phua, K. H. (1980) Remark on algorithm 500: Minimization of unconstrained multivariable functions. ACM Transactions on Mathematical Software 6, 618-622. Shavlik, J. W. & Dietterich, T. G. (eds) (1990) Readings in Machine Learning. San Mateo, CA: Morgan Kaufmann. Shawe-Taylor, J. & Anthony, M. (1991) Sample sizes for multiple-output threshold networks. Network 2, 107-117. Sheehan, N. & Thomas, A. (1993) On the irreducibility of a Markov chain defined on a space of genotype configurations by a sampling scheme. Biometrics 49, 163-175.', 'chain defined on a space of genotype configurations by a sampling scheme. Biometrics 49, 163-175. Shenoy, P. P. (1989) A valuation-based language for expert systems. International Journal of Approximate Reasoning 3, 383-411. Shenoy, P. P. & Shafer, G. (1990) Axioms of probability and belief-function propagation. In Uncertainty in Artificial Intelligence 4, eds R. D. Shachter, T. S. Levitt, L. N. Kana! & J. F. Lemmer, pp. 169-198. Amsterdam: North-Holland. Reprinted in Shafer & Pearl (1990). Shenoy, P. P., Shafer, G. & Mellouli, K. (1988) Propagation of belief functions: a distributed approach. In Uncertainty in Artificial Intelligence 2, eds J. F. Lemmer & L. N. Kana!, pp. 325-335. Amsterdam: North-Holland. Shepanski, J. F. (1987) Fast learning in artificial neural systems: multilayer perceptron training using optimal estimation. In Proceedings of IEEE First International Conference on Neural Networks, San Diego, 1987, eds M. Caudill & C. Butler I, 465-472. Long Beach, CA: IEEE Press.', \"Networks, San Diego, 1987, eds M. Caudill & C. Butler I, 465-472. Long Beach, CA: IEEE Press. Shepard, R. N. (1962a) The analysis of proximities: multidimensional scaling with an unknown distance function I. Psychometrika 27, 125-139. Shepard, R. N. (1962b) The analysis of proximities: multidimensional scaling with an unknown distance function II. Psychometrika 27, 219-246. Shibata, R. (1976) Selection of the order of an autoregressive model by Akaike's Information Criterion. Biometrika 63, 117-126. Shibata, R. (1980) Asymptotically efficient selection of the order of the model for estimating parameters of a linear process. Annals of Statistics 8, 147-164.\"]\n",
      "['References 385 Shibata, R. (1981) An optimal selection of regression variables. Biometrika 68, 45~54. Short, R. D. & Fukunaga , K. (1980) A new nearest neighbor distance measure. In Proceedings of the Fifth IEEE International Conference on Pattern Recognition (Miami Beach, 1980), pp. 81~86. Los Alamitos , CA: IEEE Computer Society Press. Short, R. D. & Fukunaga , K. (1981) The optimal distance measure for nearest neighbor classification. IEEE Transactions on Information Theory 27, 622~627 . Reprinted in Dasarathy (1991). Sietsma, J. & Dow, R. J. F. (1991) Creating artificial neural networks that generalize . Neural Networks 4, 67~79. Silva, F. M. & Almeida, L. B. (1990) Speeding up back-propagation . In Advanced Neural Computers , ed. R. Eckmiller , pp. 151-158. Amsterdam: Elsevier. Silvapulle, M. J. & Burridge, J. (1986) Existence of maximum likelihood estimates in regression models for grouped and ungrouped data. Journal of the Royal Statistical Society series B 48, 100--106.', 'for grouped and ungrouped data. Journal of the Royal Statistical Society series B 48, 100--106. Silverman, B. W. (1985) Some aspects of the spline smoothing approach to non-parametric regression curve fitting (with discussion). Journal of the Royal Statistical Society series B 47, 1-52. Silverman , B. W. ( 1986) Density Estimation for Statistics and Data Analysis. London: Chapman & Hall. Silverman , B. W. & Jones, M. C. (1989) E. Fix and J. L. Hodges (1951): An important contribution to nonparametric discriminant analysis and density estimation. International Statistical Review 57, 233-247. Simard, P., Le Cun, Y. & Denker, J. (1993) Efficient pattern recognition using a new transformation distance. In NIPS5, pp. 50--58. Simmons, G. F. (1963) Introduction to Topology and Modern Analysis. New York: McGraw-Hill. Singer, Y. & Tishby, N. (1994) Decoding cursive scripts. In N IPS6, pp. 833-840. Singhal, S. & Wu, L. (1989) Training multilayer perceptrons with the extended Kalman filter. In', \"Singhal, S. & Wu, L. (1989) Training multilayer perceptrons with the extended Kalman filter. In NIPS1, pp. 133-140. Smith, A. F. M. (1991) Discussion of'Posterior Bayes factors'. Journal of the Royal Statistical Society series B 53, 132-133. Smith, A. F. M. & Roberts, G. 0. (1993) Bayesian computation via the Gibbs sampler and related Markov chain Monte Carlo methods (with discussion). Journal of the Royal Statistical Society series B 55, 3-23. Smith, A. F. M. & Spiegelhalter, D. J. (1980) Bayes factors and choice criteria for linear models. Journal of the Royal Statistical Society series B 42, 213~220. Smith, C. A. B. (1947) Some examples of discrimination. Annals of Eugenics 13, 272-282. Smith, E. E. & Medin, D. L. (1981) Categories and Concepts. Cambridge, MA: Harvard University Press. Smith, F. W. (1968) Pattern classifier design by linear programming. IEEE Transactions on Computers 17, 367-372. Smith, F. W. (1969) Design of multicategory pattern classifiers with two-category\", '17, 367-372. Smith, F. W. (1969) Design of multicategory pattern classifiers with two-category classifier design procedures. IEEE Transactions on Computers 18, 548~551. Smith, J. Q. (1989) Influence diagrams for statistical modelling. Annals of Statistics 17, 654--672. Smith, J. W., Everhart, J. E., Dickson, W. C., Knowler, W. C. & Johannes, R. S. (1988) Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications in Medical Care (Washington, 1988), ed. R. A. Greenes, pp. 261-265. Los Alamitos, CA: IEEE Computer Society Press. Solla, S. A., Levin, E. & Fleisher, M. (1988) Accelerated learning in layered neural networks. Complex Systems 2, 625-639. Sontag, E. D. ( 1992) Feedback stabilization using two-hidden-layer nets. IEEE Transactions on Neural Networks 3, 981-990. Spackman, K. A. (1992) Maximum likelihood training of connectionist models: comparison with leastsquares back propagation and logistic regression.', 'of connectionist models: comparison with leastsquares back propagation and logistic regression. In Proceedings of the Fifteenth Annual Symposium on Computer Applications in Medical Care, Washington 1991, ed. P. D. Clayton, pp. 285-289. New York: McGraw-Hill. Spath, H. ( 1985) Cluster Dissection and Analysis. Theory, FORTRAN programs, examples. Chichester: Ellis Horwood. Specht, D. F. (1967a) Vectorcardiographic diagnosis using the polynomial discriminant method of pattern recognition . IEEE Transactions on Bio-medical Engineering 14, 90--95. Specht, D. F. (1967b) Generation of polynomial discriminant functions for pattern recognition. IEEE Transactions on Electronic Computers 16, 308-319. Specht, D. F. (1990a) Probabilistic neural networks. Neural Networks 3, 109-118.']\n",
      "['386 References Specht, D. F. (1990b) Probabilistic neural networks and the polynomial Adaline as complementary techniques for classification. IEEE Transactions on Neural Networks 1, 111-121. Specht, D. F. (1991) A general regression neural network. IEEE Transactions on Neural Networks 2, 568-576. Speed, T. (1990) Complexity, calibration and causality in influence diagrams. In Oliver & Smith (1990), pp. 49-63. Spiegelhalter, D. J. (1990) Fast algorithms for probabilistic reasoning in influence diagrams , with applications in genetics and expert systems. In Oliver & Smith (1990), pp. 361-384. Spiegelhalter, D. J. & Lauritzen , S. L. (1990) Sequential updating of conditional probabilities on directed graphical structures. Networks 20, 579-605. Spiegelhalter, D. J. & Smith, A. F. M. (1982) Bayes factors for linear and log-linear models with vague prior information. Journal of the Royal Statistical Society series B 44, 377-387. Spiegelhalter, D. J., Dawid, A. P., Lauritzen , S. L. & Cowell,', 'Society series B 44, 377-387. Spiegelhalter, D. J., Dawid, A. P., Lauritzen , S. L. & Cowell, R. G. (1993) Bayesian analysis in expert systems (with discussion). Statistical Science 8, 219-283. Spirtes, P., Glymour, C. & Scheines, R. (1993) Causality, Prediction, and Search. Lecture Notes in Statistics 81. New York: Springer. Srihari, S. N. (1992) High-performance reading machines. Proceedings of the IEEE 80, 1120--1132. Srinvas, S. & Breese, J. (1990) IDEAL: a software package for the analysis of influence diagrams. In Uncertainty in Artificial Intelligence 6, eds L. N. Kana!, J. Lemmer & T. S. Levitt, pp. 212-219. Amsterdam: North-Holland. Stace, C. (1991) New Flora of the British Isles. Cambridge: Cambridge University Press. Stanfill, C. & Waltz, D. (1986) Toward memory-based reasoning. Communications of the Association for Computing Machinery 29, 1213-1228. Stewart, L. (1987) Hierarchical Bayesian analysis using Monte Carlo integration: computing posterior distributions when there', 'Bayesian analysis using Monte Carlo integration: computing posterior distributions when there are many possible models. The Statistician 36, 211-219. Stinchcombe, M. & White, H. (1989) Universal approximation using feedforward networks with non-sigmoid hidden layer activation functions. In Proceedings of the International Joint Conference on Neural Networks I, 613-617. Long Beach, CA: IEEE Press. Stinchcombe , M. & White, H. ( 1990) Approximating and learning unknown mappings using multilayer feedforward networks with bounded weights. In Proceedings of the International Joint Conference on Neural Networks, San Diego, III, 7-16. Long Beach, CA: IEEE Press. Stone, C. J. (1977) Consistent nonparametric regression (with discussion). Annals of Statistics 5, 595-645. Stone, C. J. (1985) Additive regression and other non parametric models. Annals of Statistics 13, 689-705. Stone, C. J. (1986) The dimensionality reduction principle for generalized additive models. Annals of Statistics 14,', \"The dimensionality reduction principle for generalized additive models. Annals of Statistics 14, 590--606. Stone, M. (1974) Cross-validatory choice and assessment of statistical predictions (with discussion). Journal of the Royal Statistical Society series 8 36, 111-147. Stone, M. (1977a) Asymptotics for and against crossvalidation. Biometrika 64, 29-35. Stone, M. (1977b) An asymptotic equivalence of choice of model by cross-validation and Akaike's criterion. Journal of the Royal Statistical Society series 8 39, 44--47. Stone, M. (1979) Comments on model selection criteria of Akaike and Schwarz. Journal of the Royal Statistical Society series B 41, 276-278. Streit, R. L. & Luginbuhl, T. E. (1994) Maximum likelihood training of probabilistic neural networks. IEEE Transactions on Neural Networks 5, 764-783. Stromberg, J. E., Zrida, J. & lsaksson, A. (1991) Neural trees-using neural nets in a tree classifier structure. In IEEE International Conferen ce on Acoustics, Speech and Signal\", 'in a tree classifier structure. In IEEE International Conferen ce on Acoustics, Speech and Signal Processing (Toronto , 1991), pp. 137-140. Long Beach, CA: IEEE Press. Styblinski , M.A. & Tang, T.-S. (1990) Experiments in nonconvex optimization: stochastic approximation and simulated annealing. Neural Networks 3, 467-483. Suen, C. Y., Legault, R., Nadal, C., Cheriet, M. & Lam, L. (1993) Building a new generation of handwriting recognition systems. Pattern Recognition Letters 14, 303-315. Suen, C. Y., Nadal, C., Legault, R., Mai, T. A. & Lam, L. (1992) Computer recognition of unconstrained handwritten numerals. Proceedings of the IEEE 80, 1162-1180. Sussmann , H. J. (1992) Uniqueness of the weights for minimal feedforward nets with a given inputoutput map. Neural Networks 5, 589-593.']\n",
      "['References 387 Swain, P. H. & Hauska, H. (1977) The decision tree classifier: design and potential. IEEE Transactions on Geoscience Electronics 15, 142-147. Swayne, D. F., Cook, D. & Buja, A. (1991) XGobi: interactive dynamic graphics in the X window system with a link to S. In Proceedings of the ASA Section on Statistical Graphics, pp. 1-8. Alexandria, VA: American Statistical Association. Swonger, C. W. (1972) Sample set condensation for a condensed nearest neighbor decision rule for pattern recognition . In Frontiers of Pattern Recognition, ed. S. Watanabe, pp. 511-519. Orlando: Academic Press. Reprinted in Dasarathy (1991). Tarassenko, L., Hayton, P., Cerneaz, N. & Brady, M. (1995) Novelty detection for the identification of masses in mammograms. In Proceedings of the Fourth International lEE Con{erence on Artificial Neural Networks (Cambridge, 1995). lEE Conference Publication 409, 442-447. lEE Press. Tarjan, R. E. & Yannakakis, M. (1984) Simple lineartime algorithms to test', '409, 442-447. lEE Press. Tarjan, R. E. & Yannakakis, M. (1984) Simple lineartime algorithms to test chordality of graphs, test acyclicity of hypergraphs , and selectively reduce acyclic hypergraphs. SIAM Journal of Computing 13, 566-579. Tarter, M. E. & Lock, M.D. (1993) Model-Free Curve Estimation. New York: Chapman & Hall. Therrien, C. W. (1989) Decision, Estimation, and Classification: An Introduction to Pattern Recognition and Related Topics. New York: Wiley. Thisted, R. A. (1988) Elements of Statistical Computing. Numerical Computation. New York: Chapman &Hall. Thompson, E. A. (1985) Pedigree Analysis in Human Genetics. Baltimore , MD: Johns Hopkins University Press. Thornton, C. J. (1992) Techniques in Computational Learning. An Introduction. London: Chapman & Hall. Tibshirani, R. (1992) Principal curves revisited. Statistics and Computing 2, 183-190. Tierney, L. (1994) Markov chains for exploring posterior distributions (with discussion). Annals of Statistics 22, 1701-1762.', 'chains for exploring posterior distributions (with discussion). Annals of Statistics 22, 1701-1762. Tierney, L. & Kadane, J. B. (1986) Accurate approximations for posterior moments and marginal densities. Journal of the American Statistical Association 81, 82-86. Titterington, D. M. (1976) Updating a diagnostic system using unconfirmed cases. Applied Statistics 25, 238-247. Titterington , D. M. (1980) A comparative study of kernel-based density estimates for categorical data. Technometrics 22, 259-268. Titterington, D. M. (1984) Recursive parameter estimation using incomplete data. Journal of the Royal Statistical Society series B 46, 257-267. Titterington , D. M., Murray, G. D., Murray, L. S., Spiegelhalter , D. J., Skene, A. M., Habbema , J. D. F. & Gelpka, G. J. (1981) Comparison of discrimination techniques applied to a complex data set of head injured patients (with discussion). Journal of the Royal Statistical Society series A 144, 145-174. Titterington, D. M., Smith, A. F. M. &', 'of the Royal Statistical Society series A 144, 145-174. Titterington, D. M., Smith, A. F. M. & Makov, U. E. ( 1985) Statistical Analysis of Finite Mixture Distributions. Chichester: Wiley. Todd, B. S. (1995) Weighted inference rules and Bayesian belief networks. In Gammerman (1995), pp. 205-225. Tollenaere, T. (1990) SuperSAB: fast adaptive back propagation with good scaling propertie s. Neural Networks 3, 561-573. Tomek, I. (1976a) A generalization of the k-NN rule. IEEE Transactions on Systems, Man and Cybernetics 6, 121-126. Reprinted in Dasarathy (1991). Tomek, I. (1976b) An experiment with the edited nearest-neighbor rule. IEEE Transactions on Systems, Man and Cybernetics 6, 448-452. Reprinted in Dasarathy (1991). Tomek, I. (1976c) Two modifications of CNN. IEEE Transactions on Systems, Man and Cybernetics 6, 769-772. Torgerson , W. S. (1952) Multidimensional scaling I. Theory and method. Psychometrika 17, 401-419. Torgerson , W. S. (1958) Theory and Methods of Scaling. New York:', 'Psychometrika 17, 401-419. Torgerson , W. S. (1958) Theory and Methods of Scaling. New York: Wiley. Traven, H. G. C. (1991) A neural network approach to statistical pattern classification by \"semiparametric\" estimation of probability density functions. IEEE Transactions on Neural Networks 2, 366-377. Tsypkin, Ya. Z. (1966) Use of the stochastic approximation method in estimating unknown distribution densities from observations. Automation and Remote Control 27, 432-434. Tutz, G. (1986) An alternative choice of smoothing for kernel-based density estimates in discrete discriminant analysis. Biometrika 73, 405-411. Tutz, G. (1988) Smoothing for discrete kernels m discrimin ation. Biometrical Journal 6, 729-739.']\n",
      "['388 References Tutz, G. (1989) On cross-validation for discrete kernel estimates in discrimination. Communications in Statistics-Theory and Methods 18, 4145-4162. Ullmann, J. R. (1974) Automatic selection of reference data for use in a nearest-neighbor method of pattern classification. IEEE Transactions on Information Theory 20, 541-543. Ultsch, A. (1993a) Knowledge extraction from selforganizing neural networks. In Information and Classification, eds 0. Opitz, B. Lausen & R. Klar, pp. 301-306. Berlin: Springer. Ultsch, A. (1993b) Self-organizing neural networks for visualization and classification. In Information and Classification, eds 0. Opitz, B. Lausen & R. Klar, pp. 307-313. Berlin: Springer. Upton, G. J. G. (1991) The exploratory analysis of survey data using log-linear models. The Statistician 40, 169-182. Usui, S., Nakauchi, S. & Nakano, M. (1991) Internal color representation acquired by a five-layer neural network. In Artificial Neural Networks. Proceedings of ICANN-91, eds', 'by a five-layer neural network. In Artificial Neural Networks. Proceedings of ICANN-91, eds T. Kohonen, K. Makisara, 0. Simula & J. Kangas, volume I, pp. 867-872. Amsterdam: North Holland. Utgoff, P. E. (1988a) ID5: an incremental ID3. In Proceedings of the Fifth International Conference on Machine Learning, ed. J. Laird pp. 107-120. San Mateo, CA: Morgan Kaufmann. Utgoff, P. E. (1988b) Perceptron trees: a case study in hybrid concept representations. In Proceedings of the Seventh AAAI National Conference on Artificial Intelligence, St Paul, pp. 601-606. San Mateo, CA: Morgan Kaufmann. Utgoff, P. E. (1989) Improved training via incremental learning. In Proceedings of the Sixth International Workshop on Machine Learning (Ithaca, 1989 ), ed. A.M. Segre, pp. 362-365. San Mateo, CA: Morgan Kaufmann. Utgoff, P. E. (1990) Incremental induction of decision trees. Machine Learning 4, 161-186. Utgoff, P. E. & Brodley, C. E. (1990) An incremental method for multivariate splits in decision', 'Utgoff, P. E. & Brodley, C. E. (1990) An incremental method for multivariate splits in decision trees. In Proceedings of the Seventh International Workshop on Machine Learning, eds B. W. Porter & R. J. Mooney, pp. 58-65. San Mateo, CA: Morgan Kaufmann. Valiant, L. G. (1984) A theory of the learnable. Communications of the Association for Computing Machinery 27, 1134--1142. Reprinted in Shavlik & Dietterich (1990). Van de Weide, W. (1989) IDL, or taming the multiplexer. In Proceedings of the Fourth European Working Session on Learning, ed. K. Morik, pp. 211-226. London: Pitman. Van de Weide, W. (1990) Incremental induction of topologically minimal trees. In Proceedings of the Seventh International Workshop on Machine Learning, eds B. W. Porter & R. J. Mooney, pp. 66-74. San Mateo, CA: Morgan Kaufmann. Van Ryzin, J. (1966) Bayes risk consistency of classification procedures using density estimation. Sankhyii A28, 261-270. Vapnik, V. N. (1982) Estimation of Dependencies based on', 'density estimation. Sankhyii A28, 261-270. Vapnik, V. N. (1982) Estimation of Dependencies based on Empirical Data. New York: Springer. Vapnik, V. (1992) Principles of risk minimization for learning theory. In NIPS4, pp. 831-838. Vapnik, V. N. (1995) The Nature of Statistical Learning Theory. New York: Springer. Vapnik, V. N. (1998) Statistical Learning Theory. New York: Wiley. Vapnik, V. N. & Chervonenkis, A. Ya. (1971) On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications 16, 264--280. Venables, W. N. & Ripley, B. D. (1994) Modern Applied Statistics with S-Plus. New York: Springer. Verma, T. & Pearl, J. (1990) Causal networks: semantics and expressiveness. In Uncertainty in Artificial Intelligence 4, eds R. D. Shachter, T. S. Levitt, L. N. Kanal & J. F. Lemmer, pp. 69-76. Amsterdam: North-Holland. Verma, T. S. & Pearl, J. (1991) Equivalence and synthesis of causal models. In Uncertainty in Artificial', 'T. S. & Pearl, J. (1991) Equivalence and synthesis of causal models. In Uncertainty in Artificial Intelligence 6, eds P. P. Bonissone, M. Henrion, L. N. Kanal & J. F. Lemmer, pp. 255-268. Amsterdam: North Holland. Villegas, C. (1969) On the a priori distribution of the covariance matrix. Annals of Mathematical Statistics 40, 1098-1099. Vinod, H. (1969) Integer programming and the theory of grouping. Journal of the American Statistical Association 64, 506-517. Vlachonikolos, I. (1990) Predictive discrimination and classification with mixed binary and continuous variables. Biometrika 77, 657-662. Wagner, T. J. (1973) Convergence of the edited nearest neighbor. IEEE Transactions on Information Theory 19, 696-697. Wahba, G. (1990) Spline Models for Observational Data. Philadelphia: SIAM.']\n",
      "['References 389 Wahba, G. ( 1995) Generalization and regularization in nonlinear learning systems. In The Handbook of Brain Theory and Neural Networks, ed. M. Arbib, pp. 426--430. Cambridge , MA: The MIT Press. Wahba, G. & Wold, S. (1975) A completely automatic French curve. Communications in Statistics 4, 1-17. Wahba, G., Gu, C., Wang, Y. & Chappell , R. (1995) Soft classification a.k.a. risk estimation via penalized log likelihood and smoothing spline analysis of variance. In Wolpert (1995), pp. 331-359. Wakahara, T. (1993) Towards robust handwritten character recognition. Pattern Recognition Letters 14, 345-354. Wallace, C. S. & Freeman, P. R. (1987) Estimation and inference by compact encoding (with discussion). Journal of the Royal Statistical Society series B 49, 240--265. Wand, M.P. & Jones, M. C. (1995) Kernel Smoothing. London: Chapman & Hall. Wang, C., Venkatesh, S. S. & Judd, J. S. (1994) Optimal stopping and effective machine complexity in learning. In NIPS6, pp. 303-310.', 'J. S. (1994) Optimal stopping and effective machine complexity in learning. In NIPS6, pp. 303-310. Wang, Q. R. & Suen, C. Y. (1984) Analysis and design of a decision tree based on entropy reduction and its application to large character set recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence 6, 406--417. Wang, Q. R. & Suen, C. Y. (1987) Large tree classifier with heuristic search and global training. IEEE Transactions on Pattern Analysis and Machine Intelligence 9, 91-102. Ward, J. H. Jr (1963) Hierarchical grouping to optimize an objective function. Journal of the American Statistical Association 58, 236-244. Warner, H. R., Toronto, A. F., Veasey, L. R. & Stephenson, R. (1961) A mathematical model for medical diagnosis-application to congenital heart disease. Journal of the American Medical Association 177, 177-184. Wasserman, P. D. (1993) Advanced Methods in Neural Computing. New York: Van Nostrand Reinhold. Watanabe, S. (1969) Knowing and Guessing. New York:', 'Computing. New York: Van Nostrand Reinhold. Watanabe, S. (1969) Knowing and Guessing. New York: Wiley. Waterhouse, S. R. & Robinson, A. J. (1994) Classification using hierarchical mixtures of experts. In Proceedings of the 1994 IEEE Workshop on Neural Networks for Signal Processing IV, pp. 177-186. Long Beach, CA: IEEE Press. Watrous, R. L. (1987) Learning algorithms for connectionist networks: applied gradient methods of nonlinear optimization. In Proceedings of the IEEE First International Conference on Neural Networks (San Diego, 1987 ), eds M. Caudill & C. Butler II, 619-627. New York: IEEE Press. Webb, A. R. (1994) Functional approximation by feed-forward networks: a least-squares approach. IEEE Transactions on Neural Networks 5, 363-371. Weigend, A. S. & Gershenfeld, N. A. (eds) (1993) Time Series Prediction: Forecasting the Future and Understanding the Past. Reading, MA: AddisonWesley. Weigend, A. S., Huberman, B. A. & Rumelhart, D. E. (1990) Predicting the future: a', 'AddisonWesley. Weigend, A. S., Huberman, B. A. & Rumelhart, D. E. (1990) Predicting the future: a connectionist approach. International Journal of Neural Systems 1, 193-209. Weigend, A. S., Huberman, B. A. & Rumelhart , D. E. (1992) Predicting sunspots and exchange rates with connectionist networks. In Nonlinear Modeling and Forecasting, eds M. Casdagli & S. Eubank, pp. 395-432. Redwood City, CA: Addison- Wesley. Weigend, A. S., Rumelhart, D. E. & Huberman, B. A. (1991) Generalization by weight-elimination with application to forecasting. InN IPS3, pp. 875-882. Weiss, S.M. (1991) Small sample error rate estimation for k-NN classifiers. IEEE Transactions on Pattern Analysis and Machine Intelligence 3, 285-289. Weiss, S. M. & Kulikowski , C. A. (1991) Computer Systems that Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning and Expert Systems. San Mateo, CA: Morgan Kaufmann. Wen, W. X. (1990) Optimal decomposition of belief functions . In', 'San Mateo, CA: Morgan Kaufmann. Wen, W. X. (1990) Optimal decomposition of belief functions . In Proceedings of the Sixth Workshop on Uncertainty in Artificial Intelligence (Cambridge , MA), pp. 245-256. Werbos, P. J. (1974) Beyond Regression: New Tools for Prediction and Analysis in the Behavioural Sciences. Ph.D. thesis, Harvard University. Reprinted in Werbos (1994). Werbos, P. J. (1988) Backpropagation: past and future. In Proceedings of the IEEE International Conference on Neural Networks, San Diego, 1988 I, 343-353. Long Beach, CA: IEEE Press. Werbos, P. J. (1994) The Roots of Backpropagation. From Ordered Derivatives to Neural Networks and Political Forecasting. New York: Wiley. West, M. & Harrison, P. J. (1989) Bayesian Forecasting and Dynamic Models. New York: Springer.']\n",
      "['390 References Wetterschereck, D. & Dietterich, T. (1992) Improving the performance of radial basis function networks by learning center locations. In N IPS4, pp. 1133-1140. White, H. (1982) Maximum-likelihood estimation of mis-specified models. Econometrica 50, 1-25. White, H. (1989a) Learning in artificial neural networks: a statistical perspective. Neural Computation 1, 425-464. Reprinted in White (1992). White, H. (1989b) Some asymptotic results for learning in single hidden-layer feedforward networks. Journal of the American Statistical Association 84, 1003-1013. Reprinted in White (1992). Correction: 87, 1252. White, H. (1990) Connectionist nonparametric regression: multilayer feedforward networks can learn arbitrary mappings. Neural Networks 3, 535-549. Reprinted in White (1992). White, H. (1992) Artificial Neural Networks: Approximation and Learning Theory. Oxford: Blackwell. White, H. & Woolridge, J. (1991) Some results on sieve estimation with dependent observations. In', 'White, H. & Woolridge, J. (1991) Some results on sieve estimation with dependent observations. In Nonparametric and Semi-Parametric Methods in Econometrics and Statistics, eds W. Barnett, J. Powell & G. Tauchen. New York: Cambridge University Press. Reprinted in White (1992). Widrow, B. & Hoff, M. E. Jr. (1960) Adaptive switching circuits. IRE WESCON Convention Record 4, 96-104. Reprinted in Anderson & Rosenfeld (1988). Williams, W. T. & Lambert, J. M. (1959) Multivariate methods in plant ecology. I. Association-analysis in plant communities. Journal of Ecology 47, 83-101. Wilson, D. L. (1972) Asymptotic properties of nearest neighbor rules using edited data. IEEE Transactions on Systems, Man and Cybernetics 2, 408-421. Reprinted in Dasarathy (1991). Winston, P. H. (1992) Artificial Intelligence. Third edition. Reading, MA: Addison- Wesley. Wolberg, W. H. & Mangarasian, 0. L. (1990) Multisurface method of pattern separation for medical diagnosis applied to breast cytology. Proceedings', 'method of pattern separation for medical diagnosis applied to breast cytology. Proceedings of the National Academy of Sciences of the USA 87, 9193-9196. Wolfe, J. H. (1970) Pattern clustering via multivariate mixture analysis. Multivariate Behavioural Research 5, 329-350. Wolpert, D. H. (1992) Stacked generalization . Neural Networks 5, 241-259. Wolpert, D. H. (1993) On the use of evidence m neural networks. In NIPS5, pp. 539-546. Wolpert, D. H. (1994a) Bayesian backpropagation over 1-0 functions rather than weights. In NIPS6, pp. 200--207. Wolpert, D. H. (1994b) Discussion of Ripley (1994a). Journal of the Royal Statistical Society series B 56, 450-451. Wolpert, D. H. (ed.) (1995) The Mathematics of Generalization . Reading, MA: Addison-Wesley. Wu, C. F. J. (1983) On the convergence properties of the EM algorithm. Annals of Statistics 11, 95-103. Xu, L., Kryzak, A. & Suen, C. Y. (1992) Methods of combining multiple classifiers and their applications to handwriting recognition. IEEE', 'Methods of combining multiple classifiers and their applications to handwriting recognition. IEEE Transactions on Systems, Man and Cyberneti cs 22, 418-435. Xu, L., Kryzak, A. & Yuille, A. (1994) On radial basis function nets and kernel regression: statistical consistency, convergence rates, and receptive field sizes. Neural Networks 7, 609-628. Yair, E. & Gersho, A. (1990a) The Boltzmann perceptron network: a soft classifier. Neural Networks 3, 203-221. Yair, E. & Gersho, A. (1990b) Maximum a posteriori decision and evaluation of class probabilities by Boltzmann perceptron classifiers. Proceeding s of the IEEE 78, 1620--1678. Reprinted in Lau (1992). Yannakakis, M. (1981) Computing the minimal fill-in is NP-complete . SIAM Journal of Algebraic and Discrete Methods 2, 77-79. York, J. (1992) Use of the Gibbs sampler in expert systems. Artificial Intelligence 56, 115-130, 397-398. Young, G. & Householder, A. S. (1938) Discussion of a set of points in terms of their mutual distances .', 'G. & Householder, A. S. (1938) Discussion of a set of points in terms of their mutual distances . Psychometrika 3, 19-22. Young, T. Y. & Calvert, T. W. (1974) Classification, Estimation and Pattern Recognition. New York: American Elsevier. Zador, P. L. (1982) Asymptotic quantization error of continuous signals and the quantization dimension . IEEE Transaction s on Itiformation Theory 28, 139-149. Zeger, K., Vaisey, J. & Gersho, A. (1992) Globally optimal vector quantization design by stochastic relaxation. IEEE Transactions on Signal Processing 40, 310-322. Zhao, Y. & Atkeson, C. G. (1992) Some approximation properties of projection pursuit learning networks. In NJPS4, pp. 936-943.']\n",
      "['Author Index Aarts, E., 275, 353 Abramowitz, M., 56, 185 Abu-Mostafa, Y. S., 7, 83 Ackley, D. H., 279 Agosta, J. M., 245 Agrawala, A.K., 191 Aha, D. W., 13, 14, 201 Ahmed, W., 131, 134 Aitchison, J., 11, 46, 50-52, 56, 190 Aitken, C. G. G., 113, 190, 245, 273, 275 Aizerman, M. A, 135, 147 Akaike, H., 34, 61, 347 Albert, A, 113 Albertini , F., 159 Aleksander, 1., 4 Alexander , K. S., 83 Almeida, L. B., 156 Almond, R. G., 245 Amari, S.-1., 16, 33, 34, 61, 140, 155, 171 Amit, D. J., 16 Anderberg, M. R., 312 Andersen, S. K., 245 Anderson , J. A., 7, 109, 111-113 Anderson, J. R., 281 Anderson , T. W., 16, 37, 101 Andreassen , S., 245 Angluin, D., 7, 80 Anthony, M., 81, 83, 87, 88, 180 Anzellotti , G., 133, 178 Apolloni , B., 282 Argentiero , P., 237 Arkedev , A. G., 135 Arbib, M. A, 16 Ash, T., 172 Asimov, D., 301 Assouad, P., 81 Atkeson, C. G., 126 Averintsev , M. V., 252 Baba, N., 160 Bahadur, R. R., 37, 190 Bah!, L. R., 225 Bailey, T., 198 Bain, Z., 279 Baird, H. S., 16 Bakiri, G., 91,', 'R. R., 37, 190 Bah!, L. R., 225 Bailey, T., 198 Bain, Z., 279 Baird, H. S., 16 Bakiri, G., 91, 121 Baldi, P., 292 Ball, G. B., 312 Ballard, D. H., 205 Banfield, J. D., 108, 314 Bao, J., 237 Barlow, R. E., 310 Barna, G., 281 Barron, A. R., 61, 176, 178 Barry, D., 139 Bartholomew, D., 310 Bartlett, P. L., 180 Basford, K. E., 75, 208, 210 Bashkirov , 0. A, 135 Baskett, F., 198 Batchelor, B. G., 199 Bates, D. M., 139, 148 Bather, J., 80 Battiti, R., 160 Baum, E. B., 179 Baum, L. E., 335 Baxt, W. G., 66 Beaudet, P., 237 Beaulieu , J.-M., 326 Beeri, C., 259 Begg, C. B., 110 Beigi, H. S. M., 160 Benediktsson, J. H., 66 Bentley, J. L., 198 Berge, C., 246 Berger, J. 0., 20, 46, 50, 54, 62, 166, 348 Bernardo, J. M., 63 Besag, J., 275, 337 Best, D. J., 298 Bezdek, J. C., 317 Bhattacharyya, A., 328 Bibby, J. M., 16, 21, 36, 39, 49, 56, 354 Bichsel, M., 149 Bienenstock, E., 140, 300 Biggs, N. L., 81 Binford, T. 0., 245, 258 Bishop, C. M., 16, 136, 138, 151, 167 Blair, J. R. S., 259 Blaydon, C. C.,', 'Binford, T. 0., 245, 258 Bishop, C. M., 16, 136, 138, 151, 167 Blair, J. R. S., 259 Blaydon, C. C., 136 Block, H. D., 116, 118 Blue, J. L., 76, 332. Blumer, A., 79, 81-83 Boehnke. M., 273 de Boor, C., 123 Boser, B., 16 Bottou, L., 16 Bourlard , H., 292 Bouton, C., 325 Box, G. E. P., 8, 50, 63, 76 Boyles, R. A., 336 Brady, M., xi, 26 Brailovsky, V. L., 70 Bramley, R., 160 Bratko, 1., 16, 235, 236 Braverman, E. M., 135, 147 Breese, J. S., 245, 273 Breiman , L., 65, 101-103, 121, 127, 128, 173,213,217,218,221, 222, 224, 232, 237, 238, 240 Bremner, J. E., 310 Brent, R. P., 241 Brewer, M. J., 245, 273, 275 Bridle, J. S., 66, 109, 149, 353 Brier, G. W., 69']\n",
      "['392 Brockett, R. W., 292 Brodley, C. E., 239 Broeckaert, I., 16, 183 Broffit, B., 106 Bronowski, J., 26 Broomhead, D. S., 131 Brown, C., 245 Brown, P. F., 225 Brown, P. J., 190 Brown, T. A., 191 Brunk, H. M., 310 Bryan, J. G., 93 Bryant, J., 198 Bryson, A. E., 151 Buckland, S. T., 187 Buckley, A. G., 345 Buja, A., 105, 108, 299 Buntine, W. L., 153, 163-167, 169, 225, 226, 241 Burrascano P., 210 Burridge, J., 113 Byrd, R. H., 345 Byth, K., 114 Cabrera, J ., 299 Cacoullos, T., 184 Calvert, T., 136 Campbell, N. A., 13, 100, 106, 107 Candela, G. T., 76, 332 Cannings, C., 246 Carlin, J. B., 209, 266, 337 Carpenter, G. A., 288, 315 Carroll, S. M., 147 Carter, C., 240, 242 Casey, R. G., 236, 237 Catlett, J., 240 Celeux, G., 336 Cerneaz, N., xi, 26 Cestnik, B., 235 Chan, C., 237 Chan, K. H., 131, 134 Chan, K. S., 338 Chandran, P. S., 157 Chang, C.-H., 219, 222, 237 Chang, C. L., 202 Chappell, R., 15, 125 Charniak, E., 245 Chatterji, B. N., 198 Chauvin, Y., 170 Chavez, R. M., 273 Cheeseman, P.,', '15, 125 Charniak, E., 245 Chatterji, B. N., 198 Chauvin, Y., 170 Chavez, R. M., 273 Cheeseman, P., 62, 317 Chellappa, R., 76, 334 Chen, C.-C., 74 Author Index Chen, D. S., 150 Chen, S., 134 Chen, Y.-C., 208 Chen, Z., 139 Cheng, Y.-Q., 96 Cheriet, M., 16 Cherkassky, V., 16, 323 Chernick, M. R., 74 Chernoff, H., 80, 328 Chervonenkis, A. Ya., 82 Chidananda Gowda, K., 200 Chin, R., 237 Chou, P. A., 6, 220, 225, 226, 238 Chou, W.-S., 208 Chow, C. K., 20, 277 Chow, Y., 339 Chrisley, R., 281 Ciampi. A., 219, 222, 237 Clark, L. A., 219, 225, 226 Clark, P., 236 Clarke, W. R., 106 Cleveland, W. S., 123 Clifford, P., 250 Clunies-Ross, C. W., 37 Cohen, E., 16 Cohn, D., 82 Cook, D., 299 Coomans, D., 16, 183 Cooper, G. F., 243, 245, 258, 278 Cooper, L. N., 66, 131, 300 Cormen, T. H., 246, 277, 351 Cornfield, J ., 50 Cortes, C., 120 Cosslett, S., 111 Cottrell, G. W., 304 Cottrell, M., 325 Cover, T. M., 62, 71, 81, 119, 192, 195 Cowan, C. F. N., 135 Cowell, R. G., 245, 262, 276, 279 Cox, D. R., 7,', 'M., 62, 71, 81, 119, 192, 195 Cowan, C. F. N., 135 Cowell, R. G., 245, 262, 276, 279 Cox, D. R., 7, 25, 33, 69, 109, 110, 169, 353 Cox, M. A. A., 305 Cox, S. J., 66 Cox, T. F., 305 Craven, P., 141 Crawford, S. L., 74, 238, 239, 276 Crevier, D., 243 Cybenko, G., 147, 160 Dale, M. B., 320 Darken, C. J., 131, 132, 134, 156 Dasarathy, B. V., 191, 198 Dattatreya, G. R., 237 Dawid, A. P., 6, 27, 69, 245, 260, 262, 265, 266, 276, 279, 339, 348, 352 Deely, J. J., 55 Delampady, M., 62 Delp, E. J., 223, 226, 227, 230 DeMers, D., 304 Dempster, A. P., 245, 335, 336, 342 Denker, J. S., 6, 16, 169 Dennis, J. E, 342 Desbois, D., 12, 13 Devijver, P. A., 15, 16, 20, 101, 191, 195, 196, 198, 289, 318, 329, 330, 332 Devlin, S. J., 293 DeVore, R., 178 Devroye, L., 83, 84, 192, 195 Dey, D. K., 64 Diaconis, P., 126, 174, 296 Diamantaras, K. I., 292 Dickinson, B. W., 147 Dickson, W. C., 14 Diebolt, J., 209, 336 Dietterich, T. G., 91, 121, 134, 216, 239 Diggle, P. J., 187 Dillon, W. R., 16 Donner, A., 170', '336 Dietterich, T. G., 91, 121, 134, 216, 239 Diggle, P. J., 187 Dillon, W. R., 16 Donner, A., 170 Donoho, D. L., 126 Doster, D., 237 Doursat, R., 140 Dow, R. J. F., 138 Dowla, F. U., 160 Doyle, P., 213 Draper, D., 63, 64 Dreyfus, G., 16 Dubes, R. C., 74, 237, 312 Duchon, J., 139 Duda, R. 0., 15, 101, 118, 121, 182, 184 Dudani, S. A., 198 Duffy, D. E., 113 Dunn, J. C., 317 Dunn, 0. J., 75, 106 Dunsmore, I. R., 11, 46, 50-52 Dyn, N., 137 Eaton, H. A. C., 156 Edwards, D., 41, 276 Efron, B., 45, 73-75, 101, 238, 348 Ehrenfeucht, A., 79, 81-83 Eisenberger, 1., 42 Elbaum, C., 131 Eriksen, P. S., 108']\n",
      "['Erwin, E., 324 Eslava-G6mez, G., 12, 300, 301 Ettinger, G. L., 245 Evans, M., 63 Everhart, 1. E., 14 Fagin. R., 245, 259, 340 Fahlman, S. E., 156, 158, 172 de Falco, D., 282 Fargette, D., 12, 13 Faris, K. B., 131, 134 Fauquet, C., 12, 13 Fayyad, U. M., 237 Fefferman, C., 159 Feldman, 1. A., 146 Fenwick, 1., 213 Fienberg, S. E., 190 Filkin, D. L., 157 Finkel, R. A., 198 Finnoff, W., 155 Fisher, D. H., 239 Fisher, F. P. II, 329 Fisher, L., 210 Fisher, R. A., 23, 36, 93, 101 Fix, E., 184, 191 Flanagan, 1. K., 312 Flannery, B. P., 158, 159 Fleisher, M., 149 Fleiss, 1. L., 77 Fletcher, R., 158, 342 Flick, T. E., 196, 197 Flocchini, P., 16 Flury, B., 108 Forester, B., 16 Forgy, E. W., 312 Fort, 1. C., 325 Fort, M., 325 Frank, I. E., 108 Fraser, D. A. S., 50 Frean, M., 171 Freedman, D., 296, 340 Freeman, D., 317 Freeman, P. R., 62 Friedman, H. P., 314 Friedman, 1. H., 16, 107, 108, 122, 125, 126, 128, 187, 188, 198, 213, 217, 218, 221, 222, 224, 232, 237, 238, 240, 296~298, 303 Frost, R. L.,', '128, 187, 188, 198, 213, 217, 218, 221, 222, 224, 232, 237, 238, 240, 296~298, 303 Frost, R. L., 312 Fu, K.-S., 6, 237, 240, 317 Fukunaga, K., 2, 15, 20, 100, 101, 195~198, 200, 312, 331 Funahashi, K., 14 7 Author Index Fung, R., 276 Furman, W., 210 Furnival, G. M., 331 Gader, P., 16 Gallant, S. 1., 171 Gammerman, A., 245, 273, 275 Ganzberger, M., 16 Gardin, F., 16 Garey, M. R., 351 Gates, G. W., 200 Geiger, D., 252, 267, 268 Geisser, S., 46-50, 52, 54, 62, 63, 71 Gelfand, A. E., 64, 337 Gelfand, S. B., 156, 223, 226, 227, 230, 240 Gelman, A., 209, 337 Gelpka, G. 1., 18 Geman, D., 136, 248, 337, 338 Geman, S., 88, 136, 140, 248, 337, 338 George, E. 1., 63 Georgiopoulos, M., 315 Gershenfeld, N. A., 16 Gersho, A., 202, 282, 312, 320 Geyer, C., 337 Ghurye, S. G., 56 Gilbert, 1. C., 345 Gill, P. E., 118, 158, 342, 346 Gillies, A., 16 Girosi, F., 131~135, 137, 178 Glad, I. K., 185 Glick, N., 29 Glymour, C., 265, 266, 276 Gnanadesikan, R., 293 Gold, 1. I., 300 Goldberg, M., 326 Goldstein,', '29 Glymour, C., 265, 266, 276 Gnanadesikan, R., 293 Gold, 1. I., 300 Goldberg, M., 326 Goldstein, M., 16 Golomb, B. A., 16 Golombic, M. C., 246 Golub, G. H., 94, 289, 353 Gong, G., 74 Gonzalez, R. C., 6 Good, I. 1., 55, 69 Goodman, D. M., 160 Goodman, R. M., 237 Goppert, G., 198 Gordon, A. D., 288, 326 Gori, M., 159 Gower, 1. C., 305~307 Granger, R. H. 1r, 239 Grant, P. M., 135 Gray, R., 110 Gray, R. M., 202, 320 Green, P. 1., 123, 139, 275, 337 Greene, T., 107 Grenander, U., 88, 339 Grinold R. C., 117 van Groenendaal, W., 76 Grossberg, S., 288, 315 Grosse, E., 123 Grother, P. 1., 332 Gu, C., 15, 125, 139 Guo, H., 223, 240 Guyon, 1., 16 393 Habbema, 1. D. F., 18, 52, 56, 100, 183, 244 Hall, D. 1., 312 Hall, P., 182, 187, 190, 299 Hampel, F. R., 135, 150 Hampson, S. E., 118 Hand, D. 1., 16, 101, 183, 197, 199 Hannan, E. 1., 61 Hansen, L. K., 66 Hanson, S. 1., 170 Hardie, W., 182, 198 Hardy, R. L., 131 Harrison, P. 1 ., 62 Hart, P. E., 15, 101, 118, 121, 182, 184, 192, 195, 200', '198 Hardy, R. L., 131 Harrison, P. 1 ., 62 Hart, P. E., 15, 101, 118, 121, 182, 184, 192, 195, 200 Hartigan, 1. A., 312 Hartman, E. 1., 133 Hassibi, B., 169 Hastie, T. 1., 105, 108, 122, 124, 125, 127, 141, 142, 210, 304 Hastings, W. K., 338 Hathaway, R. 1., 42 Hauck, W. W. 1 r, 170 Hauska, H., 237 Haussler, D., 79~83, 88, 179, 180, 239 Havarnek, T., 276 Haykin, S., 4, 16, 281, 292, 324 Hayton, P., xi, 26 Hebb, D. 0., 146 Heileman, G. L., 315 Hellman, M. E., 191 Henderson, D., 16 Henrichon, E. G. 1r, 237, 240, 317 Henrion, M., 273 Hergert, F., 155 Hermans, 1., 100, 183 Herskovits, E., 278 Hertz, 1., 16 Heskes, T. M., 155 Higdon, D., 337 Highleyman, W. H., 67, 101']\n",
      "['394 Hills, M., 70 Hills, S. E., 337 Hinkley, D. V., 25, 33, 169, 353 Hinton, G. E., 2, 66, 149, 151, 157, 167, 168, 279, 281, 283 Hjort, N. L., x, 32, 43, 50, 52, 75, 100, 185, 330 Ho, Y.-C., 118, 151 Hodges, 1. L., 184, 191 Hoeffding, W., 79, 87 Hoed, A. E., 108 Hoff, M. E., 117, 143, 147 Hoffgen, K.-U., 116 Hogg, S., 219, 222, 237 Holland, P. W., 190 Holt, M. 1. 1., 149 Hopfield, J. 1., 2, 283 Hornik, K., 147, 176,292 Horvitz, E. 1., 273 Householder, A. S., 307 Howard, R., 178 Howard, R. E., 16 Hrycej, T., 157, 273, 275 Huang, J., 315 Hubbard, W., 16 Huber, P. 1., 32, 35, 39, 40, 57, 135, 150, 296, 301 Huberman, B. A., 170 Hull, J. 1., 16 Hummels, D. M., 131, 134, 195 Hunt, E. B., 235 Hunter, J. S., 8, 76 Hunter, W. G., 8, 76 Hwang, C.-R., 88 Hwang, J.-N., 127, 173 Hyafil, R., 216 Ihaka, R., 101-103, 121 Impedovo, S., 16 Ingrassia, S., 209 Intrator, N., 300 Irani, K. B., 237 Isaksson, A., 240 Isenhour, T. L., 200 Isham, V., 248 Iyengar, P. A., 16 Jackel, L. D., 16 Jackson, J. E., 289', '240 Isenhour, T. L., 200 Isham, V., 248 Iyengar, P. A., 16 Jackel, L. D., 16 Jackson, J. E., 289 Jacobs, R. A., 66, 156, 283, 285, 336 Jain, A. K., 74, 198, 312 Jain, R. C., 150 James, M., 15 Author Index Jancey, R. C., 312 Jeffreys, H., 50, 64 Jensen, F., 245 Jensen, F. V., 245, 262, 269, 272 Jiang, Q., 198 Johannes, R. S., 14 Johansson, E. M., 160 Johnson, D. S., 351 Johnson, N. L., 39, 354 Johnstone, I. M., 126 Jolliffe, I. T., 289 Jones, L. K., 126, 176, 177 Jones, M., 132, 137 Jones, M. C., 182, 185, 191, 296-298 Jordan, M. I., 66, 283, 285, 336 lou, I.-C., 173 Judd, 1. S., 155 Kabrisky, M., 157 Kadane, J. B., 63 Kalantari, I., 198 Kambhatla, N., 304, 305 Kamgar-Parsi, B., 198 Kamp, Y., 292 Kana!, L. N., 16, 198 Kangas, T., 203, 204, 206 Kansa, E. 1., 131 Kappen, B., 155 Kappen, H. 1., 283 Karhunen, J., 292 Karpinski, M., 180 Kashyap, R. L., 118, 136 Kass, G. V., 233, 236 Kass, R. E., 63, 64 Kaufman, L., 306, 311, 313, 314, 317, 320, 321, 322 Kay, 1. W., 52, 56 Kearns, M., 82', 'R. E., 63, 64 Kaufman, L., 306, 311, 313, 314, 317, 320, 321, 322 Kay, 1. W., 52, 56 Kearns, M., 82 Keeler, 1. D., 133 Keenan, D. M., 339 Kelly, F. P., 252 Kelly, 1., 317 Kelly, K., 276 Kendall, M.G., 236 Kennard, R. W., 108 Kent, J. T., 16, 21, 36, 39, 49, 56, 354 Kessell, D. L., 100 Kettenring, J. R., 293 Khanna, D., 312 Kibler, D., 201 Kiiveri, H., 266 Kil, R. M., 131 Kim, B. S., 198 Kim, 1. H., 258 King, R. D., 16 Kirkland, M. D., 274, 339 Kittler, 1. V., 15, 16, 20, 101, 191, 195, 196, 198, 289, 318, 329, 330, 332 Kj.erulff, U., 260 Kleijnen, J. P. C., 76 Knerr, S., 16 Knight, B. W. Jr, 115 Knowler, W. C., 14 Knuth, D. E., 246 Kohonen, T., 202-204, 206, 281, 323, 325 Kohzaki, M., 160 Koiran, P., 180 Kong, A., 245, 273, 342 Kononenko, I., 235, 236 Koontz, W. L. G., 312 Kaplowitz, 1., 191 Korst, 1., 275, 353 Kotz, S., 39, 354 Kowalski, J. M., 133 Kramer, A. H., 160 Kramer, M. A., 134, 304 Krishna, G., 200 Krishnaiah, P. R., 16 Krogh, A., 16 Kruskal, J. B., 296, 310 Kryzak, A., 66,', '304 Krishna, G., 200 Krishnaiah, P. R., 16 Krogh, A., 16 Kruskal, J. B., 296, 310 Kryzak, A., 66, 132 Krzanowski, W. 1., 41 Kulikowski, C. A., 157 Kung, S. Y., 292 Kurkova, v., 176 Kurzynski, M. W., 237 Kushner, H., 156 Kwok, S. W., 242 Laaksonen, J., 203, 204, 206 Lachenbruch, P. A., 16, 70, 106 Laird, N. M., 335, 336 Lam, L., 16 Lambert, 1. M., 321 Landgrebe, D., 216, 237 Lange, K., 39, 40 Langley, P., 16 Larsen, B. N., 265, 266 Lauritzen, S. L., 41, 243-245, 259, 260, 261-266, 272, 276, 279, 285 Lawrence, D. T., 16 Lay, S.-R., 127, 173 Lazarsfeld, P. F., 190 Le Cun, Y., 6, 16, 169 Lebiere, C., 172']\n",
      "['LeBlanc, M., 66 Lee, S., 131 Lee, T.-C., 173 Lee, Y., 16, 133 Leen, T. K., 170, 304, 305 de Leeuw, J., 310 Legault, R., 16 Legendre, P., 305 Lehmann, E. L., 20, 25, 33, 60, 169 Leimer, H.-G., 265, 266 Leiserson, C. E., 246, 277, 351 Lemarechal, C., 345 Leonard, J. A., 134 Leroy, A. M., 135, 293 Lesaffre, E., 113 Levin, A. U., 170 Levin, E., 149 Levin, S. A., 118 Levitt, T. S., 245, 258 Lewis, R. A., 16 Li, C. J., 160 Li, H., 127 Li, X., 237 Liang, J., 272 Lincoln, W. P., 66 Lindley, D. V., 55, 63 Lindsay, B., 210 Little, R. J. A., 24, 39, 40 Liu, C. N., 277 Liu, D. C., 345 Liu, L., 279 Liu, Y., 71, 140, 150 Lloyd, S. P., 312 Lock, M. D., 185, 186 Loizou, G., 191, 195 Long, W. M., 26 Louis, C., 41 Louis, T. A., 336 Lowe, D., 131 Lowry, S. R., 200 Lueker, G. S., 260 Luginbuhl, T. E., 208 Luk, A., 198 Lunts, A. L., 70 Luo, Z., 245, 273, 275 Luttrell, S. P., 324 Lwin, T., 54 Maass, W. G., 118, 180 Macintyre, A., 180 MacKay, D. J. C., 164, 166, 168 MacKay, D. M., 146 MacLeod, J. E. S., 198', '180 Macintyre, A., 180 MacKay, D. J. C., 164, 166, 168 MacKay, D. M., 146 MacLeod, J. E. S., 198 Author Index Macnaughton-Smith, P., 320 MacQueen, J. B., 312 Madigan, D., 62, 63, 279 Madych, W. R., 137 Maechler, M., 127 Mahalanobis, P. C., 21 Mahon, R. J., 13 Mai, T. A., 16 Maier, D., 246, 259 Maillot, V., 159 Makov, U. E., 43, 208 Makram-Ebeid, S., 160 Mallows, C., 108 Mammone, R. J., 240 Mangarasian, 0. L., 119 Manly, B. F. J., 108 Mann, W. B., 245, 258 Mansfield, A. J., 118 Mantock, J. M., 200 Mardia, K. V., 16, 21, 36, 39, 49, 56, 354 Marin, J., 234 Maritz, J. S., 54 Markel, S., 159 Marks, S., 106 Markuzon, N., 288, 315 Maronna, R. A., 39 Marriott, F. H. C., 300, 314 Martin, D., 127 Martin, G. L., 16 Martinetz, T., 323 Massar!, D. L., 314 Massuli, F., 160 Mathieson, M. J., 112 Matus, F., 252 Mauri, G., 16 Max, J., 202 Maybank, S. J., 191, 195 Maybeck, P. S., 157 McClelland, J. L., 2, 143, 151, 153, 279 McCullagh, P., 110, 112, 125, 189 McCulloch, R. E., 63 McCulloch, W. S., 145,', '2, 143, 151, 153, 279 McCullagh, P., 110, 112, 125, 189 McCulloch, R. E., 63 McCulloch, W. S., 145, 146 McDonald, G., 198 McKay, R. J., 100 McKinney, S., 219, 222, 237 McLachlan, G. J., 16, 46, 57, 75, 100, 114, 208, 210 Medin, D. L., 201 Meinguet, J., 139 Meisel, W. S., 136, 216 Mellouli, K., 245 Mengersen, K., 337 Mercer, R. L., 225 Messenger, R. C., 213, 236 Metcalfe, J., 304 Metropolis, N., 337 Mhaskar, H. N., 178 Micchelli, C. A., 178 Michalski, R. S., 214 Michie, D., 3, 5, 9, 16, 215 Mickey, M. R., 70 Mingers, J., 237 Minnick, R. C., 117 Minsky, M. L., 109, 118 Mitchell, B., 16 Mitter, S. K., 156 Mockett, L. G., 320 Mogami, Y., 160 Moller, M., 345 395 Moody, J. E., 34, 61, 131, 132, 134, 140, 141, 156, 170--172 Moore, B., 315, 316 Moran, M. A., 52, 56 Morgan, J. N., 213, 236 Morin, R. L., 198 Morrell, D. R., 312 Morton, H., 4 Mosteller, F., 70 Moulton, B. R., 63 Moussouris, J., 252 Muchnik, I. B., 135 Muggleton, S., 16 Mulier, F. P., 323 Munro, W., 300 Murata, N., 33, 34, 61,', '252 Muchnik, I. B., 135 Muggleton, S., 16 Mulier, F. P., 323 Munro, W., 300 Murata, N., 33, 34, 61, 140, 171 Muroga, S., 81, 117, 118 Murphy, B. 1., 52, 56 Murphy, P.M., 13, 15 Murray, G. D., 18, 244 Murray, L. S., 18, 244 Murray, W., 118, 158, 342, 346 Murtagh, F., 326 Murthy, V. K., 74, 184 Musavi, M. T., 131, 134 Myles, J. P., 197 Nadal, C., 16 Nagy, G., 236, 237 Nakano, M., 304 Nakauchi, S., 304 Narendra, P.M., 198, 312, 331 Nash, J. C., 158, 159 Natayanan, A., 108 Neal, R. M., 165, 166, 282']\n",
      "['396 Nealy, C. D., 74 Neapolitan, E., 245 Neider, J. A., 110, 112, 125, 189 Nelson, B. E., 312 Nelson, S. A., 137 Niblett, T., 235, 236 Niemann, H., 198 Nienhuis, B., 149, 156 Nocedal, J., 345 Norvig, P., 16 Nowlan, S. J., 66, 167, 168, 283 Obermayer, K., 324 Occhinegro, S., 16 Oja, E., 292 Olesen, K. G., 245, 262, 272 Oliver, L. H., 41 Oliver, R. M., 245 Oliver, T. L., 156 Olkin, I., 41, 56 Olshen, R. A., 213,217,218,221,222, 224, 232, 237, 238, 240 van Ooyen, A., 149, 156 Ott, J., 273 Ottaviano, L., 16 Owen, A., 108 Owens, A. J., 157 Oxley, M. E., 157 Pagallo, G., 239 Pages, G., 325 Palmer, R. G., 16 Papert, S. A., 118 Parberry, I., 118 Park, J., 132 Park, S. B., 198 Parrondo, J. M. R., 82 Parthasarthy , G., 198 Parzen, E., 184 Patrick, E. A., 329 Patterson, A., 235 Pavlidis, T., 16 Payne, H. J., 216 Pearl, J., 83, 244-246, 248, 252, 253, 256, 258, 263, 266--269, 273, 275, 279, 341 Pearlmutter , B. A., 66, 153 Peck, R., 210 Peng, F., 285, 286 Penrod, C. S., 199 Pensini, M. P., 16', 'Pearlmutter , B. A., 66, 153 Peck, R., 210 Peng, F., 285, 286 Penrod, C. S., 199 Pensini, M. P., 16 Peot, M. A., 273 Peretto, P., 16 Author Index Perrone, M. P., 66 Personnaz , L., 16 Peskun, P. H., 274 Peterson, A. M., 173 Peterson, C., 281 Petrie, T., 335 Peyton, B., 259 Phillips, P. R., 112 Phua, K. H., 345 Pitas, I., 4 Pitman, J. A., 16 Pitts, W., 145 Plastria, F., 314 Ploughman , L., 273 Poggio, T., 131-135, 137 Pollard, D., 32, 80, 81, 85, 88 Posse, C., 297, 300, 301 Poulsen, R. S., 41 Powell, M. J. D., 131 Pratt, L., 170 Prechelt, L., 9 Pregibon , D., 219, 225, 226 Prentice, R., 111 Preparata, F. P., 198, 349 Press, W. H., 158, 159 Preston, C. J., 248 Pyke, R., 111 Quenouille, M., 72 Quinlan, J. R., 214, 216, 217, 223, 227, 232, 233, 235, 236, 239, 240 Quinn, B. G., 61 Racine-Poon, A., 337 Raeside, D. E., 198 Raftery, A. E., 62-64, 108, 279, 314 Rao, C.-R., 26, 36, 93 Ravishankar, C. S., 223, 226, 227, 230 Rayens, W., 107 Rayner, J. C. W., 108, 298 Read, C. J., 312 Redner, R.', 'C. S., 223, 226, 227, 230 Rayens, W., 107 Rayner, J. C. W., 108, 298 Read, C. J., 312 Redner, R. A., 208 Reed, R., 169 Reilly, D. L., 131 Reynolds , J. H., 288, 315 Richards, L. E., 236 Riffenburgh , R. H., 37 Rimey, R., 245 Ripley, B. D., x, xi, 11, 16, 42, 48, 77, 113, 129, 151, 163, 165, 171, 173, 182, 252, 274, 275, 337-339, 353 Rissanen , J ., 61 Ritter, G. L., 200 Ritter, H., 323 Rivest, R. L., 216, 246, 277, 351 Robert, C. P., 209 Roberts, G. 0., 135, 337, 338 Roberts, S., 135 Robinson, A. J., 283, 285 Robinson , R. W., 278 Roeder, K., 207 Rogers, S. K., 1 57 Ronchetti , E. M., 135, 150 Roosen, C. B., 127 Rose, D. J., 260 Rosen, D. B., 288, 315 Rosenblatt , F., 116, 143, 147 Rosenblatt, M., 184 Rosenbluth, A., 337 Rosenbluth, M., 337 Rosenfeld, R., 66 Roskar, E., 235 Rounds, E. M., 237 Rousseeuw, P. J., 58, 135, 150, 293, 306, 311, 313, 317,320,321,322 Rozonoer, L. I., 13 5, 14 7 Rubin, D. B., 24, 335, 336 Rubin, J., 314 Ruck, D. W., 157 Ruiz, E. V., 198 Rumelhart, D. E., 2,', '7 Rubin, D. B., 24, 335, 336 Rubin, J., 314 Ruck, D. W., 157 Ruiz, E. V., 198 Rumelhart, D. E., 2, 143, 151, 153, 170, 279 Rundell, P. W. K., 190 Russell, S. J., 16 Ruzicka, P., 324 de Sa, V. R., 205 Saarinen , S., 160 Safavian , S. R, 216, 237 Sakurai, A., 180 Salamon, P., 66 Salomon, R., 156 Sarna!, A., 16 Sammon, J. W., 308 Sandberg, I. W., 132 Sanger, T. D., 292 Sangiovanni- Vincentelli, A., 160 Sankar, A., 240 Santer, T. J., 113 Sarma, V. V. S., 237 Sarvarayudu, G. P. R., 213, 217, 237 Schaefer , J. R., 100, 183 Schalkoff, R. J., 15 Scheines, R., 265, 266, 276 Schimert, J., 127']\n",
      "['Schlimmer , J. C., 239 Schmid, M. J., 108 Schmidhuber, J., 156 Schnabel, R. B., 342, 345 Schoen berg, I. J ., 307 Schroeder, A., 187, 188 Schuerm ann, J., 237 Schulten, K., 323, 324 Schwarz, G., 61, 64, 347 Schwemer , G. T., 75 Scott, A. J., 111, 314 Scott, D. W., 182, 198 Seber, G. A. F., 148 Sebestyen, G., 208 Sedgewick, R., 246, 277, 351 Seitz, P., 149 Sejnowski , T. J., 16, 279 Self, M., 317 Semnani, S., 149 Sen, A., 108 Sethi, I. K., 213, 217, 237, 241 Setiono, R., 119 Shachter , R. D., 273 Shafer, G., 245, 262, 342 Shahshahani , M, 126, 174 Shamos, M. I., 198, 349 Shanno, D. F., 345 Shawe-Taylor, J., 83, 87, 88, 180 Sheehan, N., 338 Shenoy, P. P., 245, 262, 342 Shepanski , J. F., 157 Shepard, R. N., 310 Shibata, R., 61 Shiraishi, Y., 160 Short, R. D., 197 Shustek, L. J., 198 Shyu, W. M., 123 Sibson, R., 296-298 Sietsma, J., 138 Silva, F. M., 156 Silvapulle, M. J., 113 Silverman , B. W., 122, 123, 139, 141, 182, 191 Simard, P., 6, 16 Simmons , G. F., 175 Simon, H. A., 16 Simon,', 'B. W., 122, 123, 139, 141, 182, 191 Simard, P., 6, 16 Simmons , G. F., 175 Simon, H. A., 16 Simon, H.-U., 116 Singer, Y., 16 Singhal, S., 157 Sirat, J.-A., 160 Skene, A., 18, 244 Skolnick , M. H., 246 Skrzypek, J., 66 Author Index Smith, A. F. M., 43, 62-64, 208, 337, 338 Smith, C. A. B., 37 Smith, E. E., 201 Smith, F. W., 117, 118 Smith, J. Q., 245, 341 Smith, J. W., 14 Smyth, P., 237 Snell, E. J., 109, 110 Solla, S. A., 16, 149, 169 Sonquist , J. A., 213 Sontag, E. D., 159, 179, 180 Soules, G., 335 de Souza, P. V., 225 Spackman, K. A., 149 Spath, H., 312 Specht D. F., 184, 210 Speed, T. P., 246, 266 Spiegelhalter, D. J., 3, 5, 9, 16, 18, 62, 64, 243-246, 262-264, 272, 276, 279, 285 Spirtes, P., 265, 266, 276 Srihari, S. N., 16, 66 Srinvas, S., 245 Srivastava, M., 108 Stace, C., 214 Stahel, W. A., 135, 150 Stanfill, C., 200 Stegun, I. A., 56, 185 Stephenson, R., 244 Stern, H. S., 209, 337 Sternberg, M. J. E., 16 Stewart, L., 62 Stinchcombe, M., 147, 176 Stofella, P., 16 Stone, C. J.,', 'Sternberg, M. J. E., 16 Stewart, L., 62 Stinchcombe, M., 147, 176 Stofella, P., 16 Stone, C. J., 125, 192, 195, 213, 217, 218, 221, 222, 224, 232, 237, 238, 240 Stone, M., 33, 34, 61, 65, 71, 353 Stone, P. J., 235 Stork, D. G., 169 Streit, R. L., 208 Stromberg , J. E., 240 Stuart, A., 236 Stuetzle, W., 125, 187, 188, 304 Stutz, J., 317 Styblinski, M. A., 156 Suen, C. Y., 16, 66, 237 Sussmann, H. J., 159 Swain, P. H., 66, 237 Swartz, T., 63 Swayne, D. F., 299 Swonger , C. W., 200 Symons, M. J., 314 Takasu, S., 117, 118 Tang, T.-S., 156 Tanner, M. A., 285, 286 Tarassenko, L., xi, 26, 135 Tarjan, R. E., 259, 260 Tarter, M. E., 185, 186 Tate, R. F., 41 Taylor, C. C., 3, 5, 9, 16 Taylor, J. M. G., 39, 40 Taylor, W., 317 Teller, A., 337 Teller, E., 337 Tesauro, G., 82 Tesi, A., 159 Teukolsky, S. A., 158, 159 Therrien, C. W., 15 Thiesson, B., 276 Thisted, R. A., 185, 299, 349 Thomas, A., 338 Thomason , M. G., 6 Thompson , E. A., 246 Thornton, C. J., 155 Tiao, G. C., 50, 63 397 Tibshirani, R.', ', M. G., 6 Thompson , E. A., 246 Thornton, C. J., 155 Tiao, G. C., 50, 63 397 Tibshirani, R. J., 66, 74, 105, 108, 122, 124, 125, 141, 142, 210, 305 Tierney, L., 63, 337, 338 Tishby, N., 16 Titterington, D. M., 11, 18, 43, 190, 198, 208, 244, 336 Toda, I., 117, 118 Todd, B. S., 263 Tollenaere , T., 156 Tomek, I., 195, 199, 200 Torgerson, W. S., 307 Torkkola, K., 203, 204, 206 Toronto, A. F., 244 Toussaint, G. T., 41 Triiven, H. G. C., 208, 209 Tsai, J. C., 173 Tsypkin, Ya. Z., 136 Tukey, J. W., 72, 296 Turan, G., 118 Tutz, G. E., 190 Tyler, D. E., 39 Ullmann, J. R., 200 Ultsch, A., 326 Ungar, J. H., 134 Upton, G. J. G., 276 Usui, S., 304 Utans, J., 171, 172']\n",
      "['398 Utgoff, P. E., 239 Vaidyanathan, S. K., 63 Vaisey, J., 312 Valiant, L. G., 77, 80, 82 Van der Broeck, C., 82 Van Horn, K. S., 116 Van Loan, C. F., 94, 289, 353 Van Ness, J., 210 Van Ryzin, J., 29 Van de Weide, W., 237 Vapnik, V. N., 16, 62, 82-84, 120 Vardi, Y., 39 Veasey, L. R., 244 Venables, W. N., xi, 42, 182 Venkatesh, S. S., 155 Verma, T. S., 268, 279 Vetterling, W. T., 158, 159 Viala, J.-R., 160 Vidal, G., 12, 13 Villegas, C., 50 Vinod, H., 313 Vlachonikolos, I., 52 Volper, D. J., 118 Wagner, T. J., 199 Wahba, G., 15, 123, 125, 137, 139, 141, 142 Wakahara, T., 16 Walker, H. F., 208 Wallace, C. S., 62 Wallace, D. L., 70 Waltz, D., 200 Wand, M.P., 182 Wang, C., 155 Author Index Wang, Q. R., 237 Wang, Y., 15, 125 Ward, J. H. Jr, 319 Warmuth, M. K., 79, 81-83 Warner, H. R., 244 Wasserman, P. D., 184 Watanabe, S., 289 Watanabe, T., 169 Waterhouse, S. R., 283, 285 Watrous, R.L., 160 Watts, D. G., 148 Webb, A. R., 138 Wechsler, H., 16 Weigend, A. S., 16, 153, 163-167, 169, 170', '160 Watts, D. G., 148 Webb, A. R., 138 Wechsler, H., 16 Weigend, A. S., 16, 153, 163-167, 169, 170 Weiss, N., 335 Weiss, S. M., 74, 157 Wen, W. X., 260 Werbos, P. J., 150, 151, 153 West, M., 62 Wetterschereck, D., 134 Whalen, M., 16 White, H., 32, 140, 147, 155, 156, 176 Widrow, B., 117, 143, 147 Wild, C. J., 111, 148 Wilkins, D. C., 279 Williams, R. J., 151 Williams, W. T., 320, 321 Williamson, R. C., 180 Wilson, C. L., 76, 332 Wilson, D. L., 199 Wilson, R. W. Jr, 331 Winston, P. H., 16, 331 Wolberg, W. H., 119 Wold, S., 123 Wolfe, J. H., 316 Wolff, G., 169 Wolpert, D. H., 65, 79, 164, 168, 353 Wong, M. A., 312 Woodruff, H. B., 200 Woolridge, J., 140 Wright, M. H., 118, 158, 342, 346 Wu, C. F. J., 336 Wu, L., 157 Xu, L., 66, 132 Yair, E., 282 Yang, J.-Y., 96 Yannakakis, M., 259, 260 Ying, X., 279 Yocum, T., 16 York, J., 63, 273, 279 Yoshida, Y., 160 Yoshizawa, S., 33, 34, 61, 140, 171 You, S.-S., 173 Young, G., 307 Young, T. Y., 136 Yuille, A., 132 Zador, P. L., 202 Zeger, K., 312', 'You, S.-S., 173 Young, G., 307 Young, T. Y., 136 Yuille, A., 132 Zador, P. L., 202 Zeger, K., 312 Zhang, W., 198 Zhao, Y., 126 Zhuang, Y.-M., 96 Zimmerman, H. G., 155 van Zomeren, B. C., 58 Zrida, J., 240']\n",
      "['Subject Index Page numbers in bold refer to entries in the glossary. Lp. 91, 132, 133, 176, 178, 350 n-method, 128, 129 abductive inference, 256 adaptive resonance theory, 315, 316 additive model, 122-125 AIC, 34, 61, 65, 169, 211, 222, 347 ancestral subgraph, 247 ART, 315, 316 ARTMAP, 288 ASSISTANT, 236 ASSISTANT86, 236 association analysis, 321 attributes, 217 AutoCiass, 317 B-splines, 123 back-fitting, 124, 127, 128, 172, 347 back-propagation, 150-155, 347 Bahadur-Lazarsfeld expansion, 190 batch methods, 4 Baum-Welch algorithm, 335 Bayes error, 20 factor, 62, 141 formula, 347 networks, 244 risk, 20 estimating, 196 rule, 20, 36, 347 Bayesian expert systems, 244 BCM model, 300 best linear rule, 36, 43 best quadratic rule, 36, 43, 100 BFGS formula, 344 Bhattacharyya bound, 328 bias, 35, 45, 67, 68, 347, see also de biasing correction, 36, 66, 72-76, 330 BIC, 61, 64, 347 Bienayme-Chebychev inequality, 79, 85, 176, 347 Boltzmann machines, 279-283 radial basis, 28 3 Boltzmann perceptron', 'inequality, 79, 85, 176, 347 Boltzmann machines, 279-283 radial basis, 28 3 Boltzmann perceptron network, 282 bootstrap, 66, 73-75, 238, 348 boundary, 249 bounds Bhattacharyya, 328 Chernoff, 80, 328 branch-and-bound, 314, 331, 348 Brier scores, 69 BRUTO, 124, 125, 129, 130, 142 C4.5, 228, 232, 233, 235 canonical correlation analysis, 96 canonical var:i,ate, 95, 96, 210 Cantor construction, 303 cascade correlation, 172 causal Markov, 265 causal networks, 244 causality, 246 CG distribution, 41 CHAID, 236 character recognition, 16 Chernoff bound, 80, 328 chordal graph, 252 class separation measures, 330 classification trees, 213-242, 348 classifier, 5, 18, 348 design issues, 6 plug-in, 28 predictive, 45-55, 113-114, 122, 164-167, 173 tree-structured, 213-242 clique, 247 clustering, 287, 311-322 complete-link, 319 contiguity-constrained, 325, 326 fuzzy, 316 group-average, 319 hierarchical, 318-322 monothetic, 321 partitioning, 312-318 single-link, 319 CN2, 236 codebook vectors, 202, 348', '318-322 monothetic, 321 partitioning, 312-318 single-link, 319 CN2, 236 codebook vectors, 202, 348 coding class targets, 91 compact set, 348 compacta, 126, 133, 147, 173, 348 complete subgraph, 247 complete-link clustering, 319 concave, 110, 218, 221, 348 conditional Gaussian distribution, 41 conditional independence, 243, 244, 248, 339-342 definitions of, 339, 340 confusion matrix, 75 conjugate gradients, 158, 160, 345, 348 connected subgraph, 247 connectionist school, 2 consistent, 29, 30, 32, 42, 45, 71, 314, 348 convex, 348 combination, 37, 190, 225 cost-complexity pruning, 221-226 crabs example, 13, 77, 97, 105, 116, 121, 129, 287, 288, 291, 292, 301, 303, 321, 322, 325 cross-validation, 62, 65, 66, 69-72, 141,200,225,226,230,348 generalized, 141, 142 in LDA, 100']\n",
      "[\"400 Cushing's syndrome , 11, 25, 36, 37, 40, 44, 53, 56, 160, 161, 165, 166, 182, 287, 288 D-separation, 267 d-separation , 266, 267, 341 DAG, 247 data editing, 198-200 debiasing , 52, 55, 56, 330 decision trees, see classification trees decomposable distributions, 258 delta rule, 117 generalized , 149 dendrogram, 287, 288, 318, 320 density estimation kernel, 181-185 orthogonal series, 185-187 design of experiments, 76 deviance , 60, 62, 65, 66, 71, 110, 138, 142, 163, 219, 221, 222, 225, 238, 241, 348 expected , 33, 61 diabetes, 14, 23, 99, 105, 114, 115, 125, 131, 161, 201, 207, 211, 228, 229, 233, 234 diagnostic paradigm , 7, 27, 28, 181, 348 digamma function, 56 directed Markov, 265 Dirichlet distribution, 53, 209, 242, 279, 348 kernel, 186 tessellation, 191, 210, 313, 349 discrete distributions estimating , 188-190 discriminants flexible, 121-142 linear, 92-100 dissimilarity, 306, 307, 349 divergence, 329 double-blind trials, 9 doubt probabilities, 18 doubt reports, xii, 5, 17,\", \"307, 349 divergence, 329 double-blind trials, 9 doubt probabilities, 18 doubt reports, xii, 5, 17, 23, 191 early stopping , 155, 349 editing, 198-200, 349 efficiency, 45, 72, 101, 110, 349 eigendecomposition , 349 EM algorithm , 43, 208, 209, 284, 334-337, 349 generalized , 335 empirical Bayes, 54, 55, 166 empirical risk minimization, 84 entropy index, 217 Subject Index entropy nets, 241 equivariance , 106, 137, 319, 334, 349 error rate, 66-69 apparent , 67 comparisons of, 76, 77 error-based pruning, 227 estimator, 349 exact test, 77 examples crabs, 13, 77, 97, 105, 116, 121, 129, 287, 288, 291, 292, 301, 303, 321, 322, 325 Cushing's syndrome, 11, 25, 36, 37, 40, 44, 53, 56, 160, 161, 165, 166, 182, 287, 288 diabetes, 14, 23, 99, 105, 114, 115, 125, 131, 161, 201, 207, 211, 228, 229, 233, 234 forensic glass, 13, 38, 97, 98, 105, 106, 113, 115, 129-131, 161, 162, 201, 207, 211, 230, 231 synthetic data, 11, 12, 199, 200, 204, 206, 208, 210, 211 viruses, 12, 13, 290, 291, 301, 302,\", \"231 synthetic data, 11, 12, 199, 200, 204, 206, 208, 210, 211 viruses, 12, 13, 290, 291, 301, 302, 309-311, 313, 319, 320 experiments comparative, 9 design of, 76 expert systems, 243 face recognition, 1, 16 feature, 5, 349 extraction , 331, 332, 349 non-linear, 303-305 ordinal, 351 selection, 327-331 feed-forward network, 143-180, 349 Fejer kernel, 186 fiducial distribution, 50 Fisher's linear discriminant, 93-96, 102 flexible discriminants, 121-142 forensic glass, 13, 38, 97, 98, 105, 106, 113, 115, 129-131, 161, 162, 201, 207, 211, 230, 231 Fourier series, 186 fRINGE, 239 fuzzy ART, 315 ARTMAP, 288 k-means, 317 gating network, 283 Gauss-Newton procedure, 126, 346 GCV, see cross-validation, generalized GEM algorithm, 335 general regression neural network, 184 generalization, 17, 77-89, 349 stacked, 65 generalized delta rule, 149 generalized linear discrimination , 121 Gibbs sampler, 23, 273, 274, 280, 285, 338, 339, 349 Gini index, 217, 219, 221, 222, 225, 226, 228 glass fragments ,\", \"273, 274, 280, 285, 338, 339, 349 Gini index, 217, 219, 221, 222, 225, 226, 228 glass fragments , 13, 38, 97, 98, 105, 106, 113, 115, 129-131, 161, 162, 201, 207, 211, 230, 231 gooseberries, 3 grammars stochastic, 6 grand tour, 301 graph, 247 chordal, 252, 258 decomposable , 258 directed, 24 7 moral, 264 triangulated, 252, 258 undirected, 24 7 graphoids, 340 Green's function, 137 group-average clustering, 319 Hammersley-Clifford theorem, 250 Hastings algorithm, 338, 339 Hermite polynomials , 127, 186, 299, 349 Hessian, 33, 35, 63, 64, 110, 113, 151, 153, 157-160, 165, 169, 350 via EM, 336 hierarchical Bayes, 54 hierarchical mixtures of experts, 283-286 hinging hyperplanes, 127 hints, 7, 10, 350 HME, 283-286, 350 Hoeffding's inequality, 79, 87 hold-out method, 67 hyperparameters, 54, 55, 164 1-map, 248 perfect, 248 ID3, 235, 236, 239 idiot's Bayes rule, 244, 317 importance sampling, 76, 113, 165, 173\"]\n",
      "[\"impurity measure, 217 incremental learning, 238 information, 350 Fisher, 33 gain, 235 observed, 35 Iris, 214 irreducibility, 338 ISODATA algorithm, 312 isotonic regression, 310 Jaccard coefficient, 306 jackknife, 72, 73 Jeffreys' information principle, 50 Jensen's inequality, 218, 350 join tree, 259 junction tree, 259 k-means, 134, 202, 210, 312 fuzzy, 317 k-medoid clustering, 313 k-nearest neighbour rule, 191 Kalman filter, 157 kernel density estimation, 99, 181-185, 207 discrete distributions, 190 kernel regression, 184 knots, 123 Kohonen mapping, 322 Kolmogorov-Smirnov distance, 237 Kullback-Leibler divergence, 32, 188, 277, 350 Laplace's method, 63 Laplacian error estimate, 236 leaf, 213 learning, 3, 350 by queries, 7 from hints, 7 learning vector quantization, 201-207 initialization, 205-207 least false parameter, 32, 350 Legendre series, 298 Leptograpsus crabs, 13, 77, 97, 105, 116, 121, 129, 287, 288, 291, 292, 301, 303, 321, 322, 325 Levenberg-Marquardt methods, 346 likelihood,\", \"121, 129, 287, 288, 291, 292, 301, 303, 321, 322, 325 Levenberg-Marquardt methods, 346 likelihood, 30, 350 weighted, 59 linear discriminant, 95 linear discriminant analysis, 21, 36, 92-100 linear rule best, 36 linear separation, 116 Subject Index local minima, 154, 156, 159, 161, 165, 169-172, 285, 297, 301 location model, 41, 52 location vector, 39 loess, 123 log-linear model, 43, 189, 275 logarithmic scoring, 69 logic sampling, 273, 282 logistic, 350 discrimination, 7, 43-45, 109-115 predictive approach, 113, 114 regression, 69 ordinal, 112 loss function, 18 LVQ, 134, 201-207, 211, 350 LVQ1, 203 LVQ2.1, 204 LVQ3, 205 machine learning, 3, 16, 213 Mahalanobis distance, 21, 92, 350 MAP estimator, 167, 168, 334, 350 marginal representation, 256, 259-262, 269, 278 Markov chain Monte Carlo, 63, 273, 337-339 Markov network, 248 Markov properties, 248 on a DAG, 265 Markov trees, 252-258 learning the structure of, 277, 278 MARS, 128-131, 139 Maurey's lemma, 177 maximal cardinality search,\", \"the structure of, 277, 278 MARS, 128-131, 139 Maurey's lemma, 177 maximal cardinality search, 259 maximum a posteriori, 334 maximum likelihood, 333, 334 maximum likelihood estimate, 350 MCMC, see Markov chain Monte Carlo McNemar's test, 77 MDL, 61 mean field approximation, 281 message passing, 254, 256, 262 metric, 305 Metropolis algorithm, 338 Metropolis-Hastings method, 338, 339 minimum description length, 61 minimum message length, 61 misclassification probabilities, 18 missing values, 15, 23, 24, 28, 279, 281, 285, 335, 336, 350 in classification trees, 222, 231-233, 236 401 mixture distributions, 41, 42, 165, 207-211 EM for, 336 in clustering, 316 two components, 41, 42 mixtures of experts, 283 ML-11, 55, 166 MML, 61 model combination, 65, 66 model selection, 8, 59-65 in LDA, 99, 100 in neural networks, 168-173 predictive, 62-65 momentum, 154 moral graph, 264 multidimensional scaling, 288, 305-311 classical, 307, 308 ordinal, 309-311 Sammon, 308, 309 multilayer perceptron, 351\", '288, 305-311 classical, 307, 308 ordinal, 309-311 Sammon, 308, 309 multilayer perceptron, 351 multiple logistic model, 44, 91, 109, 122, 149 multiquadric, 131 multivariate analysis, 351 naive Bayes rule, 244 nearest neighbour methods, 191-201 choice of metric, 197 data editing, 198-200 neural networks, 2, see also Boltzmann machines, ART, LVQ, SOM artificial, 2 definitions, 2, 4 feed-forward, 143-180 general regression, 184 probabilistic, 184 neural trees, 240 Newton methods, 343 NIC, 34, 61, 65, 71, 140, 171 non-informative prior, 351 normal distribution, 21, 35, 48, 351 NP-complete, 216, 260, 351 NP-hard, 245, 351 OLVQ1, 203, 205 on-line methods, 4, 351 one SE rule, 225 Optimal Brain Damage, 169 Optimal Brain Surgeon, 169 option trees, 242 ordered derivatives, 150']\n",
      "['402 ordinal feature, 351 ordinal logistic models, 112 ordination, 305 orthant, 88 orthogonal expansion estimators, 185-187 outliers, 5, 17, 24-26, 293, 296-298, 300, 301, 310, 313, 315, 322, 351 PAC-learning, 77 paradigm diagnostic, 7, 27, 28, 181, 348 sampling, 6, 27, 28, 55, 181, 352 parallel distributed processing, 2 parallelization, 4 Parzen windows, see kernel density estimation, 351 path, 247 Patrick-Fisher coefficient, 329 pattern recognition, 1, 2 statistical, 6 supervised, 3 syntactic, 6 unsupervised, 3, 287-326 penalization, 60, 136 perceptron, 81, 116,136, 351 capacity of, 119 multilayer, see feed-forward neural network • · · perceptron trees, 240 perfect clique sequence, 259 perfect map, 248 performance assessment, 8, 9, 66-77 pessimistic pruning, 227 Pima Indians, 14, 23, 99, 105, 114, 115, 125, 131, 161, 201, 207, 211, 228, 229, 233, 234 PIMPLE, 128, 129 plug-in classifier, 28, 352 Poisson distribution, 22, 48 polytree, 247 posterior probability, 352 potential functions,', '28, 352 Poisson distribution, 22, 48 polytree, 247 posterior probability, 352 potential functions, 135 potential representation, 250, 252, 253, 259-264, 266, 268, 271 PPR, see projection pursuit regression precise hypotheses, 62 predictive classifier, 45-55, 113, 114, 122, 164-167, 173, 352 model selection, 62-65 principal components, 170, 289-296, 352 Subject Index principal coordinate analysis, 307 principal curves, 304, 305 prior improper, 49, 54, 64, 141, 166, 170, 334, 351 non-informative, 351 prior probability, 352 probabilistic expert systems, 244 probabilistic neural network, 184 profile likelihood, 31, 111, 112, 314, 352 projection pursuit, 296-303, 332, 352 density estimation, 187, 188 indices, 298-303 regression, 125-128 pruning, 221-231, 352 cost-complexity, 221-226 error-based, 227 neural networks, 169, 170 pessimistic, 227 pseudo-dimension, 88, 180 QR decomposition, 94 quadratic rule, 36, 106 quasi-Newton, 158-160, 344, 352 limited-memory, 345 queries, 7 Quickprop, 156,', \"rule, 36, 106 quasi-Newton, 158-160, 344, 352 limited-memory, 345 queries, 7 Quickprop, 156, 158 radial basis functions, 131-136, 352 rank, 352 RBF, see radial basis functions recursive model, 263 regression spline, 123 regularization, 136-142, 352 regularized discriminant analysis, 107 reject option, see doubt reports rejects, 5 resistant methods, 39, 352, see also robust methods ridge regression, 108, 157, 346, 352 risk, 19, 352 consistency, 29, 30, 140 overall, 46 total, 19 risk averaging, 68, 69, 75, 196, 197 robust methods, 57, 58, 123, 150, 352 for principal components, 293 for RBFs, 135 in LOA, 105, 106 running intersection property, 259 saddle-point approximation, 63 Sammon mapping, 308, 309 sampling paradigm, 6, 27, 28, 55, 181, 352 Sauer's lemma, 81 scale matrix, 39 scores canonical, 97 selection stepwise, 60, 169, 170, 330 self-organizing map, 134, 322-325 separation linear, 116 on a DAG, 266-268, 341 on a graph, 248, 341 set-chain representation, 260\", 'separation linear, 116 on a DAG, 266-268, 341 on a graph, 248, 341 set-chain representation, 260 Sherman-Morrison-Woodbury formula, 169, 344, 353 shrinkage methods, 106-109, 225, 226, 353 similarity, see dissimilarity simple matching coefficient, 306 simulated annealing, 156, 209, 260, 275, 312, 353 single-link clustering, 319 singular value decomposition, 94, 289, 353 SMART, 126, 172 smoothing splines, 123, 137 SOFM, see self-organizing map soft splits, 239, 240 softmax, 149, 353 SOM, see self-organizing map sphering, 296, 297, 306, 314 splines, 123, 126, 353 additive, 139 interaction, 139 multidimensional, 138, 139 regression, 123 smoothing, 123 thin-plate, 131, 138 stabilizer, 136 stacked generalization, 65, 353 STAGGER, 239 statistical pattern recognition, 6 steepest descent, 343, 353 stepwise selection, 60, 169, 170, 330 stochastic approximation, 156, 354 Stone-Weierstrass theorem, 133, 175 stratified sampling, 76 structural risk minimization, 62 subgraph, 247 subtree, 213', '133, 175 stratified sampling, 76 structural risk minimization, 62 subgraph, 247 subtree, 213 rooted, 213 super-smoother, 126 supervised learning, 3, 354']\n",
      "['t distribution, 39, 50, 166, 354 test set, 7, 354 biased, 67 THAID, 236 thin-plate splines, 131, 138 time series, 16 topological ordering, 323 topological sort, 262 total risk, 19 training set, 3, 28, 44, 354 tree, 247 join, 259 junction, 259 pruning, 221-231 shrinking, 225 triangulated graph, 252 Subject Index ultrametric, 318 unclassified observations, 31 uniform approximation, 176 uniform convergence, 126, 133, 147, 173, 354 universal approximation, 126, 127, 132, 173, 174, 178 unsupervised learning, 354 updating, 42, 43, 354 validation set, 7, 154, 225, 354 Vapnik-Chervonenkis dimension, see VC dimension variable metric, 344 variance between-group, 93 within-group, 93 403 VC dimension, 81, 179 vector quantization, 202, 207, 354 tree-structured, 320 viruses example, 12, 13, 290, 291, 301, 302, 309-311, 313, 319, 320 visualization, 287, 296, 299, see also multidimensional scaling grand tour, 301 of SOMs, 326 weight decay, 157, 163, 164 weights, 220, 280, 354 Widrow-Holf learning, 117', 'tour, 301 of SOMs, 326 weight decay, 157, 163, 164 weights, 220, 280, 354 Widrow-Holf learning, 117 XGobi, 299 Zip codes, 1, 3, 5, 6, 25']\n",
      "Created 1205 chunks of text\n"
     ]
    }
   ],
   "source": [
    "# 2. Split text into chunks\n",
    "print(\"Splitting text into chunks...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "pages2 = [\"qwer\",\"tyui\",\"op[]\",\"adf\",\"ghjk\",\"l;'\"]\n",
    "texts = []\n",
    "for page in pages:\n",
    "    cont = page.page_content.replace(\"\\xad\\n\",\"\").replace(\"\\n\",\"\")\n",
    "    chunks = text_splitter.split_text(cont)\n",
    "    print(chunks)\n",
    "    texts.extend(chunks)\n",
    "\n",
    "print(f\"Created {len(texts)} chunks of text\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings...\n"
     ]
    }
   ],
   "source": [
    "# 3. Create embeddings\n",
    "print(\"Creating embeddings...\")\n",
    "embedding_function = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating and persisting vector store...\n"
     ]
    }
   ],
   "source": [
    "# 4. Create and persist the vector store\n",
    "print(\"Creating and persisting vector store...\")\n",
    "vector_store = Chroma.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=persist_dir,\n",
    "    collection_name=\"pdf_collection\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 0\n",
      "bias. (It is defined and discussed below.) The widespread use of the\n",
      "\n",
      "\n",
      "\n",
      " 1\n",
      "estimate that bias, and correct it using our estimate. For concreteness\n",
      "\n",
      "\n",
      "\n",
      " 2\n",
      "the bias. Efron (1982, Chapter 7) gives a suggestive argument why the\n",
      "\n",
      "\n",
      "\n",
      " 3\n",
      "less biased, if they are not unbiased in the first place. For example,\n",
      "\n",
      "\n",
      "\n",
      " 4\n",
      ". L:j Pr{A I Bj}Pr{Bj} Bayes rule is a rule which attains the Bayes risk, and so is the 'gold-standard', the best possible for that problem. bias has two meanings. (a) The bias of an estimator is the\n",
      "\n",
      "\n",
      "\n",
      " 5\n",
      "References 367 \n",
      "Fukunaga , K. & Hummels , D. M. (1987a) Bias of\n",
      "\n",
      "\n",
      "\n",
      " 6\n",
      "two meanings. (a) The bias of an estimator is the difference between its mean and the true value. (b) For a neural network, parameters which are constants (rather than multiplying signals) are often\n",
      "\n",
      "\n",
      "\n",
      " 7\n",
      "bias has two meanings. (a) The bias of an estimator is the difference between\n",
      "\n",
      "\n",
      "\n",
      " 8\n",
      "biases. \n",
      "When the biased training set is created by subsampling a larger\n",
      "\n",
      "\n",
      "\n",
      " 9\n",
      "bias. (It is defined and discussed below.) The widespread use of the plug-in rule with the ML estimator has been caused by the good general reputation the ML method enjoys and the fact that several\n",
      "\n",
      "\n",
      "\n",
      " 10\n",
      "appropriate bias. \n",
      "How do we estimate the biases? The method of Quenouille (1949),\n",
      "\n",
      "\n",
      "\n",
      " 11\n",
      "Section 2.5 (bias correction). \n",
      "Sections 2.6 and 2.7 discuss how we assess the adequacy of a\n",
      "\n",
      "\n",
      "\n",
      " 12\n",
      "underestimate the bias. This property is not shared by Efron's (1983)\n",
      "\n",
      "\n",
      "\n",
      " 13\n",
      "be to introduce bias, even asymptotically, but to decrease the variance.\n",
      "\n",
      "\n",
      "\n",
      " 14\n",
      "one. This degree of bias in the training set puts the two groups on an equal footing in the parameter estimation, thereby reducing the estimation biases. When the biased training set is created by\n",
      "\n",
      "\n",
      "\n",
      " 15\n",
      "p(d I x). Under some circumstances this can lead to serious bias in\n",
      "\n",
      "\n",
      "\n",
      " 16\n",
      "This degree of bias in the training set puts the two groups on an equal\n",
      "\n",
      "\n",
      "\n",
      " 17\n",
      "its estimate of the bias. Efron (1982, Chapter 7) gives a suggestive argument why the relative difference between the two bias estimates might be Op(1/n), and hence there would be little practical\n",
      "\n",
      "\n",
      "\n",
      " 18\n",
      "There are two ideas to alleviate these biases. A simple idea is to\n",
      "\n",
      "\n",
      "\n",
      " 19\n",
      "rather variable; taking a smaller V can give a larger bias but smaller\n",
      "\n",
      "\n",
      "\n",
      " 20\n",
      "use a biased sample in the training set; this normally means randomly\n",
      "\n",
      "\n",
      "\n",
      " 21\n",
      "less likely to be biased. (We will see in Chapter 3 that (2.30) can arise\n",
      "\n",
      "\n",
      "\n",
      " 22\n",
      "Moran & Murphy (1979) give explicitly the effect of this bias\n",
      "\n",
      "\n",
      "\n",
      " 23\n",
      "biased. The two biases are E[pmc- pmc] and E pmc-pmc0, and\n",
      "\n",
      "\n",
      "\n",
      " 24\n",
      "be biased. The two biases are E[pmc- pmc] and E pmc-pmc0, and in each case we will correct pmc by subtracting an estimate of the appropriate bias. How do we estimate the biases? The method of\n",
      "\n",
      "\n",
      "\n",
      " 25\n",
      "if the training set is used there will be some bias, the size of which\n",
      "\n",
      "\n",
      "\n",
      " 26\n",
      "bias results from the disproportion of the two classes in the training\n",
      "\n",
      "\n",
      "\n",
      " 27\n",
      "estimate of the bias is the average (over bootstrap samples) of the error\n",
      "\n",
      "\n",
      "\n",
      " 28\n",
      "It is less obvious how jackknifing can be used to estimate the bias\n",
      "\n",
      "\n",
      "\n",
      " 29\n",
      "are used exactly as for the apparent error in Section 2.7. The bias­\n",
      "\n",
      "\n",
      "\n",
      " 30\n",
      "bias unit) which is permanently at + 1 and connected to all other units.\n",
      "\n",
      "\n",
      "\n",
      " 31\n",
      "bias results from the disproportion of the two classes in the training set, resulting in underestimation of p(d I x) (since there are many more 'normal' cases to be fitted).\n",
      "\n",
      "\n",
      "\n",
      " 32\n",
      "(itself a size-biased selection criterion).\n",
      "\n",
      "\n",
      "\n",
      " 33\n",
      "the bias is (n-1) --n m~an[pmc(i)- ei] where the scale factor is a sample-size adjustment. Under mild regularity conditions we would expect E pmc = pmc0 + bjn + O(n-2), and so Eei =\n",
      "\n",
      "\n",
      "\n",
      " 34\n",
      "bias has two meanings. (a) The bias of an estimator is the difference between \n",
      "its mean and the true value. (b) For a neural network, parameters which \n",
      "are constants (rather than multiplying signals) are often called biases. \n",
      "BIC has two similar meanings. Akaike (1977, 1978) introduced 'informa­\n",
      "tion criterion B'. Schwarz (1978) introduced something which has become \n",
      "known as a 'Bayesian information criterion'. Although most references \n",
      "mean Schwarz's BIC, to avoid confusion this is also known as SBC \n",
      "('Schwarz Bayes Criterion'). Both penalize the deviance by log n times \n",
      "the number p of free parameters for n examples, but Akaike's has O(p) \n",
      "terms not depending on n. \n",
      "Bieoayme-chebychev inequality For a random variable X with mean J1 and \n",
      "variance a2 < oo we have \n",
      "(J2 \n",
      "Pr{IX- Ji.l > e} ~ 2 f' \n",
      "for all e > 0. This follows from Jensen's inequality applied to (X-J1)2•\n",
      "\n",
      "\n",
      "\n",
      " 35\n",
      "bias has two meanings. (a) The bias of an estimator is the difference between \n",
      "its mean and the true value. (b) For a neural network, parameters which \n",
      "are constants (rather than multiplying signals) are often called biases. \n",
      "BIC has two similar meanings. Akaike (1977, 1978) introduced 'informa­\n",
      "tion criterion B'. Schwarz (1978) introduced something which has become \n",
      "known as a 'Bayesian information criterion'. Although most references \n",
      "mean Schwarz's BIC, to avoid confusion this is also known as SBC \n",
      "('Schwarz Bayes Criterion'). Both penalize the deviance by log n times \n",
      "the number p of free parameters for n examples, but Akaike's has O(p) \n",
      "terms not depending on n. \n",
      "Bieoayme-chebychev inequality For a random variable X with mean J1 and \n",
      "variance a2 < oo we have \n",
      "(J2 \n",
      "Pr{IX- Ji.l > e} ~ 2 f' \n",
      "for all e > 0. This follows from Jensen's inequality applied to (X-J1)2•\n",
      "\n",
      "\n",
      "\n",
      " 36\n",
      "samples used to estimate the bias of R(T(a)). That is, for each of B\n",
      "\n",
      "\n",
      "\n",
      " 37\n",
      "This debiasing is usually unimportant, but can make a difference if\n",
      "\n",
      "\n",
      "\n",
      " 38\n",
      "(Tarter & Lock, 1993, §4.2) and with a bias-correction argument this\n",
      "\n",
      "\n",
      "\n",
      " 39\n",
      "biased. In practice the estimates of R(T(rx)) are highly variable over the choice of parts of the training set, and the estimated function may have no minimum within the range of rx considered, or a\n",
      "\n",
      "\n",
      "\n",
      " 40\n",
      "Glossary \n",
      "AIC ('An Information Criterion') A method developed by Akaike (1973, 1974) \n",
      "to avoid over-fitting, by penalizing the deviance by twice the number of \n",
      "free parameters. \n",
      "back-fitting An iterative method of fitting additive models, by fitting each term \n",
      "to the residuals given the rest. It is a version of the Gauss-Seidel methods \n",
      "of numerical linear algebra. \n",
      "back-propagation is the method used to calculate the gradient vector of a \n",
      "fitting criterion for a feed-forward neural network with respect to the \n",
      "parameters (weights). Also used for a steepest-descent algorithm with the \n",
      "gradient vector computed in this way. \n",
      "Bayes formula An elementary formula of probability. If B; are disjoint events, \n",
      "and AcU;B; then \n",
      "Pr{B; I A}= Pr{A I B;}Pr{B;} . L:j Pr{A I Bj}Pr{Bj} \n",
      "Bayes rule is a rule which attains the Bayes risk, and so is the 'gold-standard', \n",
      "the best possible for that problem. \n",
      "bias has two meanings. (a) The bias of an estimator is the difference between\n",
      "\n",
      "\n",
      "\n",
      " 41\n",
      "Glossary \n",
      "AIC ('An Information Criterion') A method developed by Akaike (1973, 1974) \n",
      "to avoid over-fitting, by penalizing the deviance by twice the number of \n",
      "free parameters. \n",
      "back-fitting An iterative method of fitting additive models, by fitting each term \n",
      "to the residuals given the rest. It is a version of the Gauss-Seidel methods \n",
      "of numerical linear algebra. \n",
      "back-propagation is the method used to calculate the gradient vector of a \n",
      "fitting criterion for a feed-forward neural network with respect to the \n",
      "parameters (weights). Also used for a steepest-descent algorithm with the \n",
      "gradient vector computed in this way. \n",
      "Bayes formula An elementary formula of probability. If B; are disjoint events, \n",
      "and AcU;B; then \n",
      "Pr{B; I A}= Pr{A I B;}Pr{B;} . L:j Pr{A I Bj}Pr{Bj} \n",
      "Bayes rule is a rule which attains the Bayes risk, and so is the 'gold-standard', \n",
      "the best possible for that problem. \n",
      "bias has two meanings. (a) The bias of an estimator is the difference between\n",
      "\n",
      "\n",
      "\n",
      " 42\n",
      "Glossary \n",
      "AIC ('An Information Criterion') A method developed by Akaike (1973, 1974) \n",
      "to avoid over-fitting, by penalizing the deviance by twice the number of \n",
      "free parameters. \n",
      "back-fitting An iterative method of fitting additive models, by fitting each term \n",
      "to the residuals given the rest. It is a version of the Gauss-Seidel methods \n",
      "of numerical linear algebra. \n",
      "back-propagation is the method used to calculate the gradient vector of a \n",
      "fitting criterion for a feed-forward neural network with respect to the \n",
      "parameters (weights). Also used for a steepest-descent algorithm with the \n",
      "gradient vector computed in this way. \n",
      "Bayes formula An elementary formula of probability. If B; are disjoint events, \n",
      "and AcU;B; then \n",
      "Pr{B; I A}= Pr{A I B;}Pr{B;} . L:j Pr{A I Bj}Pr{Bj} \n",
      "Bayes rule is a rule which attains the Bayes risk, and so is the 'gold-standard', \n",
      "the best possible for that problem. \n",
      "bias has two meanings. (a) The bias of an estimator is the difference between\n",
      "\n",
      "\n",
      "\n",
      " 43\n",
      "Glossary \n",
      "AIC ('An Information Criterion') A method developed by Akaike (1973, 1974) \n",
      "to avoid over-fitting, by penalizing the deviance by twice the number of \n",
      "free parameters. \n",
      "back-fitting An iterative method of fitting additive models, by fitting each term \n",
      "to the residuals given the rest. It is a version of the Gauss-Seidel methods \n",
      "of numerical linear algebra. \n",
      "back-propagation is the method used to calculate the gradient vector of a \n",
      "fitting criterion for a feed-forward neural network with respect to the \n",
      "parameters (weights). Also used for a steepest-descent algorithm with the \n",
      "gradient vector computed in this way. \n",
      "Bayes formula An elementary formula of probability. If B; are disjoint events, \n",
      "and AcU;B; then \n",
      "Pr{B; I A}= Pr{A I B;}Pr{B;} . L:j Pr{A I Bj}Pr{Bj} \n",
      "Bayes rule is a rule which attains the Bayes risk, and so is the 'gold-standard', \n",
      "the best possible for that problem. \n",
      "bias has two meanings. (a) The bias of an estimator is the difference between\n",
      "\n",
      "\n",
      "\n",
      " 44\n",
      "Glossary \n",
      "AIC ('An Information Criterion') A method developed by Akaike (1973, 1974) \n",
      "to avoid over-fitting, by penalizing the deviance by twice the number of \n",
      "free parameters. \n",
      "back-fitting An iterative method of fitting additive models, by fitting each term \n",
      "to the residuals given the rest. It is a version of the Gauss-Seidel methods \n",
      "of numerical linear algebra. \n",
      "back-propagation is the method used to calculate the gradient vector of a \n",
      "fitting criterion for a feed-forward neural network with respect to the \n",
      "parameters (weights). Also used for a steepest-descent algorithm with the \n",
      "gradient vector computed in this way. \n",
      "Bayes formula An elementary formula of probability. If B; are disjoint events, \n",
      "and AcU;B; then \n",
      "Pr{B; I A}= Pr{A I B;}Pr{B;} . L:j Pr{A I Bj}Pr{Bj} \n",
      "Bayes rule is a rule which attains the Bayes risk, and so is the 'gold-standard', \n",
      "the best possible for that problem. \n",
      "bias has two meanings. (a) The bias of an estimator is the difference between\n",
      "\n",
      "\n",
      "\n",
      " 45\n",
      "Glossary AIC ('An Information Criterion') A method developed by Akaike (1973, 1974) to avoid over-fitting, by penalizing the deviance by twice the number of free parameters. back-fitting An iterative method of fitting additive models, by fitting each term to the residuals given the rest. It is a version of the Gauss-Seidel methods of numerical linear algebra. back-propagation is the method used to calculate the gradient vector of a fitting criterion for a feed-forward neural network with respect to the parameters (weights). Also used for a steepest-descent algorithm with the gradient vector computed in this way. Bayes formula An elementary formula of probability. If B; are disjoint events, and AcU;B; then Pr{B; I A}= Pr{A I B;}Pr{B;} . L:j Pr{A I Bj}Pr{Bj} Bayes rule is a rule which attains the Bayes risk, and so is the 'gold-standard', the best possible for that problem. bias has two meanings. (a) The bias of an estimator is the difference between its mean and the true value. (b) For\n",
      "\n",
      "\n",
      "\n",
      " 46\n",
      "the bias gives a lower bound on the mean square error of the bias-corrected estimator. Introductions to the bootstrap are given by Efron (1982), Efron & Gong (1983) and Efron & Tibshirani (1993);\n",
      "\n",
      "\n",
      "\n",
      " 47\n",
      "biased sample, which can be a good idea when almost all the errors\n",
      "\n",
      "\n",
      "\n",
      " 48\n",
      "biased sample, which can be a good idea when almost all the errors\n",
      "\n",
      "\n",
      "\n",
      " 49\n",
      "set. We now take a different viewpoint , of accepting that the performance measure on the training set is biased, but trying to estimate that bias, and correct it using our estimate. For concreteness\n",
      "\n",
      "\n",
      "\n",
      " 50\n",
      "Section 2.5 (bias correction). \n",
      "Sections 2.6 and 2.7 discuss how we assess the adequacy of a \n",
      "parametric model and estimate the performance of a classification rule. \n",
      "The final section considers 'generalization' , a more abstract way to find\n",
      "\n",
      "\n",
      "\n",
      " 51\n",
      "Section 2.5 (bias correction). \n",
      "Sections 2.6 and 2.7 discuss how we assess the adequacy of a \n",
      "parametric model and estimate the performance of a classification rule. \n",
      "The final section considers 'generalization' , a more abstract way to find\n",
      "\n",
      "\n",
      "\n",
      " 52\n",
      "(a) The bias of an estimator is the difference between its mean and the true value. (b) For a neural network, parameters which are constants (rather than multiplying signals) are often called biases. BIC has two similar meanings. Akaike (1977, 1978) introduced 'information criterion B'. Schwarz (1978) introduced something which has become known as a 'Bayesian information criterion'. Although most references mean Schwarz's BIC, to avoid confusion this is also known as SBC ('Schwarz Bayes Criterion'). Both penalize the deviance by log n times the number p of free parameters for n examples, but Akaike's has O(p) terms not depending on n. Bieoayme-chebychev inequality For a random variable X with mean J1 and variance a2 < oo we have (J2 Pr{IX- Ji.l > e} ~ 2 f' for all e > 0. This follows from Jensen's inequality applied to (X-J1)2•\n",
      "\n",
      "\n",
      "\n",
      " 53\n",
      "do we need? 59 There are two ideas to alleviate these biases. A simple idea is to use a biased sample in the training set; this normally means randomly subsampling the 'normal' group. Let nk denote\n",
      "\n",
      "\n",
      "\n",
      " 54\n",
      "estimate that bias, and correct it using our estimate. For concreteness \n",
      "we will work with error rates (although the principles apply much \n",
      "more widely). We need to distinguish between the pmc, the true error \n",
      "rate for our classifier trained on this training set, E pmc, its average \n",
      "over training sets, and pmc0, the true error rate with the 'least false' \n",
      "parameter eo plugged in. We can then aim to correct the bias of pmc \n",
      "as an estimator of either pmc or pmc0. The first is most relevant for\n",
      "\n",
      "\n",
      "\n",
      " 55\n",
      "the best possible for that problem. \n",
      "bias has two meanings. (a) The bias of an estimator is the difference between \n",
      "its mean and the true value. (b) For a neural network, parameters which \n",
      "are constants (rather than multiplying signals) are often called biases. \n",
      "BIC has two similar meanings. Akaike (1977, 1978) introduced 'informa­\n",
      "tion criterion B'. Schwarz (1978) introduced something which has become \n",
      "known as a 'Bayesian information criterion'. Although most references\n",
      "\n",
      "\n",
      "\n",
      " 56\n",
      "which suggest that the bootstrap estimator of the bias R(T) will systematically underestimate the bias. This property is not shared by Efron's (1983) .632 bootstrap (Section 2.7). Crawford (1989)\n",
      "\n",
      "\n",
      "\n",
      " 57\n",
      "2.5 (bias correction). Sections 2.6 and 2.7 discuss how we assess the adequacy of a parametric model and estimate the performance of a classification rule. The final section considers\n",
      "\n",
      "\n",
      "\n",
      " 58\n",
      "be the indicator of the error of predicting the class of Xi from §{i), and pmc(i) the apparent error rate on fitting to §{i)· Then the estimator of the bias is (n-1) --n m~an[pmc(i)- ei] where the\n",
      "\n",
      "\n",
      "\n",
      " 59\n",
      "that the bias of the plug-in estimates of the posterior probabilities\n",
      "\n",
      "\n",
      "\n",
      " 60\n",
      "on prior experience or belief. Commonly these are the prior probabilities \n",
      "nk of the classes.\n",
      "\n",
      "\n",
      "\n",
      " 61\n",
      "and hence there would be little practical difference. Our arguments show that if we drop the sample-size correction, thereby making a relative error of 0(1/n), the difference between the two bias\n",
      "\n",
      "\n",
      "\n",
      " 62\n",
      "non-linear operation. We would expect the bias to be small for a test\n",
      "\n",
      "\n",
      "\n",
      " 63\n",
      "the bias of e. This can be used by actually resampling B times and\n",
      "\n",
      "\n",
      "\n",
      " 64\n",
      "How do we estimate the biases? The method of Quenouille (1949), later termed the jackknife by Tukey, is sufficiently similar to leaveone-out cross-validation to have caused considerable confusion in\n",
      "\n",
      "\n",
      "\n",
      " 65\n",
      "estimated in the usual unbiased way (from other data).\n",
      "\n",
      "\n",
      "\n",
      " 66\n",
      "on the mean square error of the bias-corrected estimator.\n",
      "\n",
      "\n",
      "\n",
      " 67\n",
      "discriminant, the effect of the debiasing is to increase the effective variance by a factor nkf(nk-p-1) over the usual nk/(nk- 1), and to add a constant which depends on nk. This debiasing is usually\n",
      "\n",
      "\n",
      "\n",
      " 68\n",
      "2.5 (bias correction). Sections 2.6 and 2.7 discuss how we assess the adequacy of a parametric model and estimate the performance of a classification rule. The final section considers 'generalization' , a more abstract way to find\n",
      "\n",
      "\n",
      "\n",
      " 69\n",
      "variance of predictions at the expense of bias. This idea is applied to\n",
      "\n",
      "\n",
      "\n",
      " 70\n",
      "are too extreme, and this will result in a bias when c is small. A further\n",
      "\n",
      "\n",
      "\n",
      " 71\n",
      "Bayes rule is a rule which attains the Bayes risk, and so is the 'gold-standard', \n",
      "the best possible for that problem. \n",
      "bias has two meanings. (a) The bias of an estimator is the difference between \n",
      "its mean and the true value. (b) For a neural network, parameters which \n",
      "are constants (rather than multiplying signals) are often called biases. \n",
      "BIC has two similar meanings. Akaike (1977, 1978) introduced 'informa­\n",
      "tion criterion B'. Schwarz (1978) introduced something which has become \n",
      "known as a 'Bayesian information criterion'. Although most references \n",
      "mean Schwarz's BIC, to avoid confusion this is also known as SBC \n",
      "('Schwarz Bayes Criterion'). Both penalize the deviance by log n times \n",
      "the number p of free parameters for n examples, but Akaike's has O(p) \n",
      "terms not depending on n. \n",
      "Bieoayme-chebychev inequality For a random variable X with mean J1 and \n",
      "variance a2 < oo we have \n",
      "(J2 \n",
      "Pr{IX- Ji.l > e} ~ 2 f' \n",
      "for all e > 0. This follows from Jensen's inequality applied to (X-J1)2•\n",
      "\n",
      "\n",
      "\n",
      " 72\n",
      "bootstrap sample, and the bias in the error rates for the bootstrap \n",
      "samples used to estimate the bias of R(T(a)). That is, for each of B \n",
      "bootstrap samples we grow and prune a tree to find Tb(a) and evaluate \n",
      "the difference between the error rate for the real training set and the \n",
      "bootstrap sample. The average of this quantity over the B samples, \n",
      "@(a), is the bootstrap estimate of the bias of R(T(a)), so finally a IS \n",
      "chosen to minimize \n",
      "R(T(a)) +@(a).\n",
      "\n",
      "\n",
      "\n",
      " 73\n",
      "bias on the posterior probabilities is much more pronounced when the\n",
      "\n",
      "\n",
      "\n",
      " 74\n",
      "in each case we will correct pmc by subtracting an estimate of the \n",
      "appropriate bias.\n",
      "\n",
      "\n",
      "\n",
      " 75\n",
      "unbiasedness of an average. This justifies all the approaches except\n",
      "\n",
      "\n",
      "\n",
      " 76\n",
      "the bias decreases as n --+ oo but the variance remains under control,\n",
      "\n",
      "\n",
      "\n",
      " 77\n",
      "first a population viewpoint. That is, there is a known probability\n",
      "\n",
      "\n",
      "\n",
      " 78\n",
      "relative difference between the two bias estimates might be Op(1/n), and\n",
      "\n",
      "\n",
      "\n",
      " 79\n",
      "biased. (We will see in Chapter 3 that (2.30) can arise from other models of the class densities.) Logistic discrimination is a very important template for many of the generalizations we will\n",
      "\n",
      "\n",
      "\n",
      " 80\n",
      "to be used in linear discrimination. This is an example of our consider­\n",
      "\n",
      "\n",
      "\n",
      " 81\n",
      "biased estimates if the covariance matrix in the 'diseased' group is somewhat different from that in the 'normal' group. The effect of this bias on the posterior probabilities is much more pronounced\n",
      "\n",
      "\n",
      "\n",
      " 82\n",
      "be determined inaccurately from the training set.\n",
      "\n",
      "\n",
      "\n",
      " 83\n",
      "strap pmc -pmc to estimate the bias correction. This bias is the\n",
      "\n",
      "\n",
      "\n",
      " 84\n",
      "knowledge about the subject domain into the structure of the probability distributions. Such knowledge is often about causal relationships, or perhaps the lack of causality as expressed by\n",
      "\n",
      "\n",
      "\n",
      " 85\n",
      "The biases in the diagnostic paradigm are often more serious. The\n",
      "\n",
      "\n",
      "\n",
      " 86\n",
      "The most obvious application of the jackknife is to reduce the bias\n",
      "\n",
      "\n",
      "\n",
      " 87\n",
      "= IJg-1 Zi where Zi are independent X~-i random variables (Mardia et al., 1979, pages 85 and 73 respectively). Moran & Murphy (1979) give explicitly the effect of this bias correction on the linear\n",
      "\n",
      "\n",
      "\n",
      " 88\n",
      "be used and motivated outside the Bayesian paradigm. Suppose that\n",
      "\n",
      "\n",
      "\n",
      " 89\n",
      "footing in the parameter estimation, thereby reducing the estimation \n",
      "biases.\n",
      "\n",
      "\n",
      "\n",
      " 90\n",
      "calls it the Bayes error.\n",
      "\n",
      "\n",
      "\n",
      " 91\n",
      "term comes from psychology and refers to the ability to infer the correct\n",
      "\n",
      "\n",
      "\n",
      " 92\n",
      "n I X; e)nn Inn t or p(d I x; e) > 11(1 + tndnnlnnnd). If we under-sample the 'normal' group by a factor of t, this declares 'diseased' if the odds exceed one. This degree of bias in the training set\n",
      "\n",
      "\n",
      "\n",
      " 93\n",
      "Hand, D. J. (1981) Discrimination and Classification. \n",
      "Chichester: Wiley.\n",
      "\n",
      "\n",
      "\n",
      " 94\n",
      "nothing to do with discriminatory power (they are an unsupervised\n",
      "\n",
      "\n",
      "\n",
      " 95\n",
      "apparent error rate on fitting to §{i)· Then the estimator of the bias is \n",
      "(n-1) --\n",
      "\n",
      "\n",
      "\n",
      " 96\n",
      "of the bootstrap samples used to estimate the bias gives a lower bound\n",
      "\n",
      "\n",
      "\n",
      " 97\n",
      "of the error of predicting the class of Xi from §{i), and pmc(i) the \n",
      "apparent error rate on fitting to §{i)· Then the estimator of the bias is \n",
      "(n-1) --\n",
      "n m~an[pmc(i)- ei] \n",
      "where the scale factor is a sample-size adjustment. Under mild regularity \n",
      "conditions we would expect E pmc = pmc0 + bjn + O(n-2), and so \n",
      "Eei = pmc0+b/(n-1)+0(n-2) and E pmc-pmc = (al-b)/n+O(n-2). \n",
      "Then our bias estimator has mean \n",
      "n -1 [ a1 2 b 2] --pmc0 + --+ O(n-) -pmc0----O(n-) n n-1 n-1 \n",
      "= al -b + O(n-2) \n",
      "n\n",
      "\n",
      "\n",
      "\n",
      " 98\n",
      "biased is the apparent error rate of a prediction rule? Journal of the American Statistical Association 81, 461-470. Efron, B. & Gong, G. (1983) A leisurely look at the bootstrap, the jackknife, and\n",
      "\n",
      "\n",
      "\n",
      " 99\n",
      "nition. The training set is regarded as a sample from a population of\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load the saved vector store\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    persist_directory=\"pdf_store\",\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"pdf_collection\"\n",
    ")\n",
    "\n",
    "# Example query\n",
    "query = \"what is bias?\"\n",
    "docs = vector_store.similarity_search(query, k=100)  # Get top 3 most relevant chunks\n",
    "for i,doc in enumerate(docs):\n",
    "    print(\"\\n\\n\\n\",i)\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='nition. The training set is regarded as a sample from a population of')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
