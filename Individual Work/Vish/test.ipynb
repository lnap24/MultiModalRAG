{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF from ..\\..\\data\\pdf_books\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf\n",
      "Splitting text into chunks...\n",
      "Created 1329 chunks of text\n",
      "Creating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishg\\AppData\\Local\\Temp\\ipykernel_14188\\4197005375.py:49: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_function = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vishg\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishg\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating and persisting vector store...\n",
      "Vector store created and saved to pdf_store\n",
      "\n",
      "Vector store creation completed!\n",
      "You can now load this vector store from pdf_store for querying\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishg\\AppData\\Local\\Temp\\ipykernel_14188\\4197005375.py:64: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vector_store.persist()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(pdf_path: str) -> List:\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = loader.load()\n",
    "    return pages\n",
    "\n",
    "def create_vector_store(pdf_path: str, persist_directory: str = \"pdf_store\"):\n",
    "    # 1. Load PDF\n",
    "    print(f\"Loading PDF from {pdf_path}\")\n",
    "    pages = load_pdf(pdf_path)\n",
    "    \n",
    "    # 2. Split text into chunks\n",
    "    print(\"Splitting text into chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    texts = []\n",
    "    for page in pages:\n",
    "        chunks = text_splitter.split_text(page.page_content)\n",
    "        texts.extend(chunks)\n",
    "    \n",
    "    print(f\"Created {len(texts)} chunks of text\")\n",
    "    \n",
    "    # 3. Create embeddings\n",
    "    print(\"Creating embeddings...\")\n",
    "    embedding_function = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "        model_kwargs={'device': 'cpu'}\n",
    "    )\n",
    "    \n",
    "    # 4. Create and persist the vector store\n",
    "    print(\"Creating and persisting vector store...\")\n",
    "    vector_store = Chroma.from_texts(\n",
    "        texts=texts,\n",
    "        embedding=embedding_function,\n",
    "        persist_directory=persist_directory,\n",
    "        collection_name=\"pdf_collection\"\n",
    "    )\n",
    "    \n",
    "    # 5. Persist the vector store\n",
    "    vector_store.persist()\n",
    "    print(f\"Vector store created and saved to {persist_directory}\")\n",
    "    \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = r\"..\\..\\data\\pdf_books\\BrianDRipley-PatternRecognitionandNeuralNetworks(1996).pdf\"  \n",
    "persist_dir = \"pdf_store\"  # Directory where the vector store will be saved\n",
    "\n",
    "vector_store = create_vector_store(pdf_path, persist_dir)\n",
    "\n",
    "print(\"\\nVector store creation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 0\n",
      "test set A set of examples used only to assess the performance of a fully­\n",
      "specified classifier. \n",
      "training set A set of examples used for learning, that is to fit the parameters \n",
      "of the classifier. \n",
      "uniform convergence A sequence of functions fn converges uniformly to f if \n",
      "maxx lfn(x)-f(x)l ---+ 0 as n ---+ oo. We have uniform convergence on \n",
      "compacta if this holds whenever the maximum is taken over any compact \n",
      "set K. \n",
      "unsupervised learning Discovering groupings in the training set when none are \n",
      "pre-specified. \n",
      "updating Changing the classifier when new examples become available, possi­\n",
      "bly lacking their true classifications. \n",
      "validation set A set of examples used to tune the parameters of a classifier, for \n",
      "example to choose the number of hidden units in a neural network. \n",
      "vector quantization A method of encoding data for signal transmission, in \n",
      "which a vector is replaced by one of a finite number of representatives. \n",
      "See page 201.\n",
      "\n",
      "\n",
      "\n",
      " 1\n",
      "latter have been experienced) . But we will need a teacher to tell us that \n",
      "the common factor is that they were made (in part) from the sauvignon \n",
      "blanc grape. The discovery of new groupings is called unsupervised \n",
      "pattern recognition. A more common mode of learning both for us and \n",
      "for machines is to be given a collection of labelled examples, known \n",
      "as the training set, and from these to distil the essence of the grouping. \n",
      "This is supervised pattern recognition and is used to classify future \n",
      "examples into one of the same set of classes (or say it is none of these). \n",
      "There is a subject known as machine learning which has emerged \n",
      "from the artificial intelligence and computer science communities. It too \n",
      "is concerned with distilling structure from labelled examples , although \n",
      "the labels are usually 'true' and 'false'. \n",
      "'Machine Learning is generally taken to encompass automatic learning \n",
      "procedures based on logical or binary operations, that learn a task\n",
      "\n",
      "\n",
      "\n",
      " 2\n",
      "354 Glossary \n",
      "stochastic approximation aims to find the value of Oo solving f(O) = 0, but \n",
      "although we can measure f(O), the result will measured with error. After \n",
      "taking many measurements for fJ with j(fJ) near zero we will be able to \n",
      "find accurate estimators of 00. There are also versions which aim to find \n",
      "the maximizer of f(O). \n",
      "supervised learning Choosing a classifier from a training set of correctly clas­\n",
      "sified examples. \n",
      "t distribution The t distribution in p dimensions with location vector J1 and \n",
      "scale matrix ~ is the distribution of J1 +X /S where X \"' Np{O, ~} and \n",
      "vS2 \"'x; (Johnson & Kotz, 1972, §37.3; Mardia et al., 1979, p. 57). For \n",
      "v > 2 the mean is J1 and the covariance matrix v~/(v-2). The density is \n",
      "test set A set of examples used only to assess the performance of a fully­\n",
      "specified classifier. \n",
      "training set A set of examples used for learning, that is to fit the parameters \n",
      "of the classifier.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load the saved vector store\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    persist_directory=\"pdf_store\",\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"pdf_collection\"\n",
    ")\n",
    "\n",
    "# Example query\n",
    "query = \"what is supervised learning?\"\n",
    "docs = vector_store.similarity_search(query, k=3)  # Get top 3 most relevant chunks\n",
    "for i,doc in enumerate(docs):\n",
    "    print(\"\\n\\n\\n\",i)\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
