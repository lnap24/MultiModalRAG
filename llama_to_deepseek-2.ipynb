{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Initiation",
   "id": "83c947a007cad28d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:34:02.793396Z",
     "start_time": "2025-02-05T23:33:57.604962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
   ],
   "id": "ddfc3040225af5c0",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Embedding Just images",
   "id": "c2268de2901cb20e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:34:17.324735Z",
     "start_time": "2025-02-05T23:34:17.321017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "def image_embedding(path):\n",
    "    result_dict = {}\n",
    "    images_list = os.listdir(path)\n",
    "    sorted_img_filenames = sorted(images_list, key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "    for i in sorted_img_filenames:\n",
    "        image = Image.open(os.path.join(path, i))\n",
    "        inputs = clip_processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "        #outputs = model(**inputs)\n",
    "        outputs = clip_model.get_image_features(**inputs)\n",
    "        image_embeds = outputs\n",
    "        result_dict[i] = image_embeds\n",
    "        #print(image_embeds.shape)\n",
    "        #break\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "    "
   ],
   "id": "88b0b2dc759edb36",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T03:41:24.318726Z",
     "start_time": "2025-02-04T03:41:11.604752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result_image_embeddings = {}\n",
    "for i in os.listdir(\"data_processed\"):\n",
    "    \n",
    "    image_embedding_dictionary = image_embedding(os.path.join(\"data_processed\", i, \"Images\"))\n",
    "    result_image_embeddings[i] = image_embedding_dictionary\n",
    "result_image_embeddings"
   ],
   "id": "c06150b405d36c02",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m result_image_embeddings \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39mlistdir(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata_processed\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m----> 4\u001B[0m     image_embedding_dictionary \u001B[38;5;241m=\u001B[39m \u001B[43mimage_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata_processed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mImages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m     result_image_embeddings[i] \u001B[38;5;241m=\u001B[39m image_embedding_dictionary\n\u001B[1;32m      6\u001B[0m result_image_embeddings\n",
      "Cell \u001B[0;32mIn[2], line 10\u001B[0m, in \u001B[0;36mimage_embedding\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m      8\u001B[0m inputs \u001B[38;5;241m=\u001B[39m clip_processor(images\u001B[38;5;241m=\u001B[39mimage, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m#outputs = model(**inputs)\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mclip_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_image_features\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m image_embeds \u001B[38;5;241m=\u001B[39m outputs\n\u001B[1;32m     12\u001B[0m result_dict[i] \u001B[38;5;241m=\u001B[39m image_embeds\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject4/.venv/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:1304\u001B[0m, in \u001B[0;36mCLIPModel.get_image_features\u001B[0;34m(self, pixel_values, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001B[0m\n\u001B[1;32m   1299\u001B[0m output_hidden_states \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1300\u001B[0m     output_hidden_states \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39moutput_hidden_states\n\u001B[1;32m   1301\u001B[0m )\n\u001B[1;32m   1302\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m-> 1304\u001B[0m vision_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvision_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1305\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1306\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1307\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1308\u001B[0m \u001B[43m    \u001B[49m\u001B[43minterpolate_pos_encoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minterpolate_pos_encoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1309\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1310\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1312\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m vision_outputs[\u001B[38;5;241m1\u001B[39m]  \u001B[38;5;66;03m# pooled_output\u001B[39;00m\n\u001B[1;32m   1313\u001B[0m image_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvisual_projection(pooled_output)\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject4/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject4/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject4/.venv/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:1096\u001B[0m, in \u001B[0;36mCLIPVisionTransformer.forward\u001B[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict, interpolate_pos_encoding)\u001B[0m\n\u001B[1;32m   1093\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(pixel_values, interpolate_pos_encoding\u001B[38;5;241m=\u001B[39minterpolate_pos_encoding)\n\u001B[1;32m   1094\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpre_layrnorm(hidden_states)\n\u001B[0;32m-> 1096\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1097\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1098\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1099\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1100\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1101\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m last_hidden_state \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1104\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m last_hidden_state[:, \u001B[38;5;241m0\u001B[39m, :]\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject4/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject4/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject4/.venv/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:876\u001B[0m, in \u001B[0;36mCLIPEncoder.forward\u001B[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    868\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m    869\u001B[0m         encoder_layer\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m    870\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    873\u001B[0m         output_attentions,\n\u001B[1;32m    874\u001B[0m     )\n\u001B[1;32m    875\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 876\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mencoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    877\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    878\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    879\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcausal_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    880\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    881\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    883\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    885\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject4/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject4/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject4/.venv/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:617\u001B[0m, in \u001B[0;36mCLIPEncoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001B[0m\n\u001B[1;32m    615\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[1;32m    616\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_norm2(hidden_states)\n\u001B[0;32m--> 617\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    618\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[1;32m    620\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (hidden_states,)\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject4/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject4/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject4/.venv/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:574\u001B[0m, in \u001B[0;36mCLIPMLP.forward\u001B[0;34m(self, hidden_states)\u001B[0m\n\u001B[1;32m    572\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc1(hidden_states)\n\u001B[1;32m    573\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation_fn(hidden_states)\n\u001B[0;32m--> 574\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    575\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject4/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject4/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject4/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load the Necessary API Keys",
   "id": "850f3e0ff70af3f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:34:19.099086Z",
     "start_time": "2025-02-05T23:34:19.091988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "huggingface_api_key = os.getenv(\"hugging_face_key\")\n",
    "pinecone_key = os.getenv(\"pinecone_api_key\")\n",
    "mongo_uri = os.getenv(\"mongo_db_key\")\n",
    "open_ai_key = os.getenv(\"open_ai_api_key\")\n"
   ],
   "id": "445467849134e21d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Set-up/Connect to Pinecone",
   "id": "716b6d3055e0e9d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:34:24.343497Z",
     "start_time": "2025-02-05T23:34:20.973914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#create index\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_key)\n",
    "index_name = \"rag-app-images\"\n",
    "\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name = index_name,\n",
    "        dimension=512,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud = \"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "#vector_store = PineconeVectorStore(embedding=embeddings, index=index)"
   ],
   "id": "2d87d8c5dce624bb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Delete all the content",
   "id": "2e1eab9f24b85223"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T23:48:18.244832Z",
     "start_time": "2025-01-30T23:48:18.116019Z"
    }
   },
   "cell_type": "code",
   "source": "# index.delete(delete_all=True)",
   "id": "558d32f7248cf08a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Convert Image Embeddings to Vectors",
   "id": "3f6b333828123d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T16:11:06.128560Z",
     "start_time": "2025-02-04T16:11:05.930556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "embedding_vectors = {}\n",
    "for file_name, image_embedding_dictionary in result_image_embeddings.items():\n",
    "    image_embeddings = {k: v.squeeze().tolist() for k, v in image_embedding_dictionary.items()}\n",
    "    vectors = [(k, v) for k, v in image_embeddings.items()]\n",
    "    embedding_vectors[file_name] = vectors"
   ],
   "id": "c030ed9a3754fd89",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_image_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      2\u001B[0m embedding_vectors \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m file_name, image_embedding_dictionary \u001B[38;5;129;01min\u001B[39;00m \u001B[43mresult_image_embeddings\u001B[49m\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m      4\u001B[0m     image_embeddings \u001B[38;5;241m=\u001B[39m {k: v\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39mtolist() \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m image_embedding_dictionary\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[1;32m      5\u001B[0m     vectors \u001B[38;5;241m=\u001B[39m [(k, v) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m image_embeddings\u001B[38;5;241m.\u001B[39mitems()]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'result_image_embeddings' is not defined"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T17:59:47.055467Z",
     "start_time": "2025-01-29T17:59:47.049954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import torch\n",
    "# \n",
    "# image_embeddings = {k: v.squeeze().tolist() for k, v in image_embedding_dictionary.items()}\n",
    "# vectors = [(k, v) for k, v in image_embeddings.items()]\n"
   ],
   "id": "20db836d6df4ca3d",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Push them to Pinecone",
   "id": "a1eb3d7a0c780fd7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T23:48:26.952082Z",
     "start_time": "2025-01-30T23:48:26.949480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "upsert_data = []\n",
    "for file_name, vectors in embedding_vectors.items():\n",
    "    for i, (key, vector) in enumerate(vectors):\n",
    "        unique_id = f\"{file_name}/{key}\"  # Create a unique ID\n",
    "        metadata = {\"file_name\": file_name, \"image_key\": key}  # Metadata including filename and key\n",
    "        upsert_data.append((unique_id, vector, metadata))\n"
   ],
   "id": "3b0d6ee5d4205e8d",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T23:06:38.504437Z",
     "start_time": "2025-01-30T23:06:38.499855Z"
    }
   },
   "cell_type": "code",
   "source": "len(upsert_data)",
   "id": "66bab13ec1192c6e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "643"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T23:48:30.346468Z",
     "start_time": "2025-01-30T23:48:29.079655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index.upsert(vectors=upsert_data)\n",
    "\n",
    "#index.upsert(vectors)"
   ],
   "id": "5a2a85cf41e77c09",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 643}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Embed the text",
   "id": "767b87024972009"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T04:04:06.230726Z",
     "start_time": "2025-02-04T04:04:06.226156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def text_embedding(path):\n",
    "#     text_embeddings = {}\n",
    "#     text_list = os.listdir(path)\n",
    "#     sorted_text_filenames = sorted(text_list, key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "#     for i in sorted_text_filenames:\n",
    "#         file = os.path.join(path, i)\n",
    "#         with open(file, \"r\") as f:\n",
    "#             contents = f.read()\n",
    "#         #print(type(contents))\n",
    "#         inputs = clip_processor(text=[contents], return_tensors=\"pt\", padding=True)\n",
    "#         with torch.no_grad():\n",
    "#             outputs = clip_model.get_text_features(**inputs)\n",
    "#         embedding = outputs.squeeze().tolist()\n",
    "#         text_embeddings[i] = embedding\n",
    "#     return text_embeddings\n",
    "#     \n"
   ],
   "id": "3fc120e82b9062d7",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T04:11:05.191341Z",
     "start_time": "2025-02-04T04:11:05.186013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data_processed_list = os.listdir(\"data_processed\")\n",
    "# result_text_embeddings = {}\n",
    "# for i in data_processed_list:\n",
    "#     print(\"name\", i)\n",
    "#     text_embedding_dictionary = text_embedding(os.path.join(\"data_processed\", i, \"Texts\"))\n",
    "#     result_text_embeddings[i] = text_embedding_dictionary"
   ],
   "id": "e208c786c5ed6461",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:34:38.848384Z",
     "start_time": "2025-02-05T23:34:38.792510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "from huggingface_hub import whoami\n",
    "\n",
    "login(huggingface_api_key)\n",
    "whoami()"
   ],
   "id": "10e2863a8ad23085",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'user',\n",
       " 'id': '66d5147ab005ad82ca47182f',\n",
       " 'name': 'dorukozar',\n",
       " 'fullname': 'Doruk Ozar',\n",
       " 'email': 'dorukozar@gmail.com',\n",
       " 'emailVerified': True,\n",
       " 'canPay': False,\n",
       " 'periodEnd': None,\n",
       " 'isPro': False,\n",
       " 'avatarUrl': '/avatars/06335824f9a6991ec7b901b31802dd5b.svg',\n",
       " 'orgs': [],\n",
       " 'auth': {'type': 'access_token',\n",
       "  'accessToken': {'displayName': 'Presentation',\n",
       "   'role': 'read',\n",
       "   'createdAt': '2025-01-16T00:00:59.134Z'}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:34:48.214934Z",
     "start_time": "2025-02-05T23:34:46.968656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "huggingface_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ],
   "id": "fbe76ebc37081441",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:34:58.963278Z",
     "start_time": "2025-02-05T23:34:55.867101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#create index\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_key)\n",
    "index_name = \"rag-app\"\n",
    "\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name = index_name,\n",
    "        dimension=768,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud = \"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "index_text = pc.Index(index_name)\n",
    "\n",
    "vector_store_text = PineconeVectorStore(embedding=huggingface_embeddings, index=index_text)"
   ],
   "id": "2d1a513e1f6c997e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:34:59.995739Z",
     "start_time": "2025-02-05T23:34:59.991051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def concat_text(path):\n",
    "    text_embeddings = {}\n",
    "    text_list = os.listdir(path)\n",
    "    sorted_text_filenames = sorted(text_list, key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "    resulting_str = \"\"\n",
    "    for i in sorted_text_filenames:\n",
    "        file = os.path.join(path, i)\n",
    "        with open(file, \"r\") as f:\n",
    "            contents = f.read()\n",
    "            resulting_str += contents.strip()\n",
    "            #for line in file:\n",
    "            #    resulting_str += line.strip() + \"\\n\"\n",
    "        #print(type(contents))\n",
    "    #print(resulting_str)\n",
    "    return resulting_str"
   ],
   "id": "722ec21913880e96",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:35:00.871181Z",
     "start_time": "2025-02-05T23:35:00.855999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_text(combined_texts):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  # Define the maximum chunk size\n",
    "        chunk_overlap=100  # Define the overlap between chunks\n",
    "    )\n",
    "    \n",
    "    chunks_split = text_splitter.split_text(combined_texts)\n",
    "    \n",
    "    return chunks_split"
   ],
   "id": "42a402fc01fd23a6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T22:04:19.100946Z",
     "start_time": "2025-02-04T22:04:19.001969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.schema import Document\n",
    "data_processed_list = os.listdir(\"data_processed\")\n",
    "result_text_embeddings = {}\n",
    "documents = []\n",
    "for i in data_processed_list:\n",
    "    #print(\"Name:\", i)\n",
    "    concatenated_text = concat_text(os.path.join(\"data_processed\", i, \"Texts\"))\n",
    "    chunks = split_text(concatenated_text)\n",
    "    #print(len(chunks))\n",
    "    #documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "    for chunk in chunks:\n",
    "        documents.append(Document(page_content=chunk, metadata={\"source_id\":i}))\n",
    "    #print(len(documents))\n",
    "    \n",
    "    #result_text_embeddings[i] = text_embedding_dictionary\n",
    "    "
   ],
   "id": "30aeb1bde9741992",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T04:31:54.568831Z",
     "start_time": "2025-02-04T04:31:46.729816Z"
    }
   },
   "cell_type": "code",
   "source": "vector_store_text.add_documents(documents)",
   "id": "c3d53be69237ec2e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2cf20eb1-81b7-419b-a853-b4bd2cdc66b8',\n",
       " 'da4c378f-798b-46f4-9f4f-ecbb73cec7b0',\n",
       " '0731fc4a-0827-4d71-ac9e-5ffe6c2df642',\n",
       " 'bd443e81-a377-48b1-ac4f-a82c4efe8e2f',\n",
       " 'a07659c1-cfa2-4210-b85c-9bd0c463bbc4',\n",
       " '3b323f23-8480-4a6b-be53-339c4ec466a5',\n",
       " 'ea5fdf1d-2621-45dc-93e7-a32181b680d6',\n",
       " '09674ca7-5a34-4d1d-a8ef-e6e1b9f437c2',\n",
       " '7f88bf93-59bb-4219-abe0-f13b194aff06',\n",
       " 'dc07119d-1815-4e76-8307-10d570064ef3',\n",
       " '24ec12fc-3764-4e43-a2b9-7bc0f542a8b7',\n",
       " 'd53edb23-33b9-4da6-a271-e931e92ca158',\n",
       " '3896d376-515d-4fb4-9f6f-085f0d6b54f9',\n",
       " '9dfaf1ab-d5c5-4f69-af3c-c0c1d7b7e955',\n",
       " '67a07167-5617-4eb6-939e-04a1a758f6bc',\n",
       " 'b0fec8dc-a570-4ba3-b2f1-ae914161c877',\n",
       " 'b506b42f-5999-4b8c-99dd-d95490331ce5',\n",
       " '081e5f75-b0e6-4e3e-b897-02e510207f32',\n",
       " '33a50344-bec1-463e-8fad-487162537dc5',\n",
       " '4405246a-20e6-49db-b55b-dcea45a7624b',\n",
       " 'b656c779-aada-4993-b93e-4f2175872804',\n",
       " 'b89a69e8-81de-4e73-b4cd-d6cd6700b109',\n",
       " '9f6cbb8f-bdf9-4ece-8441-e418a4aedd70',\n",
       " 'a9ba9ac2-b104-4e13-935a-4dab0bd52415',\n",
       " '006299be-e795-4dfc-b4ad-e467504c1f14',\n",
       " '4950b463-b04c-4452-a442-642a7df47eca',\n",
       " '23696717-e1ab-494f-b056-9bfe6a151270',\n",
       " '8d0cadce-949e-42fe-a3c4-0625fc921e84',\n",
       " 'eccd4485-387c-4b22-b9a0-47a740bd03a7',\n",
       " 'dfbc4eca-6669-46b4-8705-93b0f2f612e9',\n",
       " '7bc110db-2b7b-40f0-905b-4cb0c08a5406',\n",
       " '792da712-f205-4a2f-a95b-862ca9cea423',\n",
       " '3e29776f-7277-4421-b0ef-ede34275d11d',\n",
       " 'c8ac81be-4702-419b-b99d-8211fd11441d',\n",
       " 'd9382321-eac3-4a7a-a320-c0a9b090eb05',\n",
       " 'a47a2e2a-2aa9-4adb-8865-430561ab218c',\n",
       " '75d5dbcf-f657-4065-9a01-d660966caf58',\n",
       " '196fb89b-39f6-496b-8387-422e0dfd483e',\n",
       " 'b47787e6-65fa-4062-80c0-5132138f7aa5',\n",
       " '51e6c39f-076d-4ff5-b956-c4675b09c00c',\n",
       " 'db73a77e-9bd0-4911-9e37-30ac8950671e',\n",
       " '2eaca6b9-f4d3-4aab-856b-1c0141c3e418',\n",
       " '016e3872-f6cf-4710-9613-143ecf8543a7',\n",
       " '24d1d2ca-30ef-4cbf-9f6c-40317a24bcf9',\n",
       " '76c7fa16-5793-43d0-a5cf-d409634ede5b',\n",
       " 'd5084324-ffeb-4faa-bda1-cb891c6aa520',\n",
       " '9c66ef6d-da14-41a6-94a3-5658ecd3e66f',\n",
       " '2255ee87-b442-4542-b52b-f905d44f1011',\n",
       " 'a055471e-b52c-416b-a06a-0877a85c1df9',\n",
       " '3870eb6d-a0d3-47c0-a6e0-45acb5cb8d46',\n",
       " '5e656d22-c7b0-48eb-8857-c18f9c302f00',\n",
       " 'fd830796-d09b-45fc-b3e1-260fbf430f3d',\n",
       " 'b9e8f2a5-1853-4925-82cc-59b3f23ee4ed',\n",
       " '0d6e87ca-8878-4df9-96c1-8e8efdf29735',\n",
       " '77bf710a-9acb-4c9a-a349-15a489e6d80a',\n",
       " '6034ba73-7684-4d4d-9918-240d23f4d363',\n",
       " 'e6fc4d79-6016-4537-99f1-951f3acb763d',\n",
       " '165d7db4-5ae3-4c43-a4cd-73a7aff02ddb',\n",
       " 'a195b506-0069-43d4-b254-894f5ab6b434',\n",
       " '94271206-2315-4cdc-aa45-a8ac72febb3d',\n",
       " '7d693b8e-3754-4c0f-8a08-432493deee9f',\n",
       " '3aa47638-31ca-4780-af9f-e388e874e717',\n",
       " '9e9e3855-b83f-4c68-a4f0-5d8f352cf9a9',\n",
       " 'c9341d00-48e9-4e21-afb3-8342619e94b3',\n",
       " '36ff44f4-b81c-4956-adb9-3394ae63fa59',\n",
       " '2c20f11c-96dd-4671-9ea3-9c466264e0f7',\n",
       " 'cadd972e-df69-45d2-9772-51b338c75864',\n",
       " '063d2d9c-a415-420a-87e2-cfe47348bdce',\n",
       " 'e60eda7b-f584-4b73-9619-050d258f10c0',\n",
       " 'caac7cee-e5b8-4a3f-ab13-d962788b3356',\n",
       " 'b1468f69-0275-4fcf-90b9-220143130e23',\n",
       " 'eb8233c5-4ca1-4959-adc5-e344df89016a',\n",
       " '5d15abb4-1fa0-4412-aff9-fed3e50284b4',\n",
       " '2d7cdfbd-e66e-4422-b478-94dfc348c7c3',\n",
       " '68da2ad4-3d40-4df9-a33d-d8a3ac189eef',\n",
       " '49c24584-234d-49dc-bd97-8d390ab8c47f',\n",
       " '7b1c3d13-afd7-4f05-afd4-89b36b2b7683',\n",
       " 'd50ecf7c-5d16-4219-9d41-6e2fea396e17',\n",
       " '8332b146-09b3-414f-8f33-1279a81d8e39',\n",
       " 'b1babca2-47bb-4924-adbe-0f28a2625ef6',\n",
       " '6eec6b9f-3b3c-49aa-a3e4-f18d939c3878',\n",
       " '2d459992-9bde-4db0-ae40-d579330d1439',\n",
       " '1d1aa84c-098a-4f83-b9de-39e532560d7f',\n",
       " '46e39b6f-28b3-40a5-8df6-bc6b37fea68b',\n",
       " '92630efc-ca59-4cfd-a682-1ac38744a57d',\n",
       " 'b8f667f9-c08a-4c72-a886-bcb2f9acd1ef',\n",
       " 'a0413d01-a063-416f-a441-f67ba2b90b66',\n",
       " '178ff100-d4cc-4a55-a42f-dc7392780f83',\n",
       " 'e5fb1210-7825-4da6-811e-58d8a80aa401',\n",
       " 'a9362376-d9f1-4411-af23-4edba75fff8e',\n",
       " '07709c0b-d751-40b7-aa49-71c7b3ecf55b',\n",
       " '0aa1d102-690f-46ee-9cd5-30ec3525a96f',\n",
       " '40013705-632d-4b78-8792-b3f1a0851b79',\n",
       " 'c44aeeee-ad35-4668-9f00-8998e50a012e',\n",
       " '4a98a1a8-4099-4e46-b82b-679a3c784596',\n",
       " 'cf3c1ce3-d861-4b6b-85c6-89d6eb48b53a',\n",
       " 'fd3da897-67e3-4fab-818f-4c75087d5de4',\n",
       " '297eb458-57b2-426d-b3a8-a2bbe1930f3c',\n",
       " '2d20bf84-ce3a-4186-8bd0-e250ceb34635',\n",
       " '39a7b35a-bf8a-4d16-bb9d-7a3088e6d8d2',\n",
       " '0e61a05c-f9f4-442e-a742-cd37738b07ae',\n",
       " '428de64f-bf61-43f5-80a4-2b1360b664b8',\n",
       " 'd9843921-f042-47bb-a05d-9310df6d273e',\n",
       " '64acbfef-d7af-4ed2-b161-1c91091bfa90',\n",
       " 'b7de1455-2bc3-4fc7-a5f0-397d1eb75ecd',\n",
       " '6064c912-f05e-4145-a299-68944aa3fecd',\n",
       " '55bf500e-dc07-4bfd-ab64-1eb25d459b24',\n",
       " 'f527fcc6-8bae-4712-87e0-19b7c6632c35',\n",
       " 'b9b00ea9-ac7f-4043-a544-a635de2e0cb3',\n",
       " 'f8554ed2-0c84-4aed-a4ad-c7b9ef6e9350',\n",
       " '8c53c7c0-82e6-4a81-b555-2ddf1e1853c4',\n",
       " 'c27bf72c-faef-4070-8a5a-12aa72960fa7',\n",
       " '3e523f62-2e9f-415a-8c58-468f43fd08fe',\n",
       " '29fdf37e-9003-43fe-86b0-24c66041f72d',\n",
       " 'a17b75f8-a3aa-492a-b5cf-0d72eac13583',\n",
       " 'a61c94c5-d644-4e56-b10e-27cf975b5835',\n",
       " 'c64485c1-732a-40bf-949f-902660bc4d88',\n",
       " 'ccc423e6-0f59-4bba-9858-46e8deb9da45',\n",
       " '6b65755f-251f-4b44-86b0-9b702ea1d3e7',\n",
       " 'f4bdd7b9-2df4-478f-a8e1-7d44d08ea289',\n",
       " '62d98efe-788d-4945-a36c-14c543efa894',\n",
       " 'cc92f245-def6-4644-a2f1-1e2f4eef1aa1',\n",
       " '4a875c07-31a2-425b-b952-f4284374b545',\n",
       " 'f98bf072-5366-4299-880a-fa22a73dd179',\n",
       " 'ab72e59c-b389-4634-b92c-dd0b731b5521',\n",
       " 'a8446eef-0d0e-4ae1-a0f1-0c2e70b04af0',\n",
       " '5d60ebc2-0277-4b9f-be96-280fe6bef999',\n",
       " '87c76034-4c8d-4b66-a81c-faa679730e85',\n",
       " '89f4d1de-04bc-4d4a-896f-8439dac07e4a',\n",
       " '23d8c619-d38c-4acb-9db2-afaee4e86d9c',\n",
       " '8252221a-8b69-4d3c-8a17-c68ca0d0f5c9',\n",
       " '4dd2b3b2-c5c8-4359-bf52-ad2d1391ea9e',\n",
       " '472d585e-cd2d-4325-b415-5650b5adbd70',\n",
       " 'f21f7671-432a-4485-995e-1ee64419205e',\n",
       " 'e90cc81f-dbac-4e39-8cfa-ba100b8bc379',\n",
       " '2fee5888-a227-46f1-beab-29b14d3d991d',\n",
       " '48a3671a-7c65-4e4b-9a86-3bc528e1ef90',\n",
       " '92a31f05-8b50-419b-9aa8-06983e85c68b',\n",
       " 'ab1fe7ac-69b3-4607-bcc0-e6d78e381ef8',\n",
       " 'ae734dc6-e1e2-45b5-8479-8d0117e1f77d',\n",
       " 'a2ad61a7-8e9a-4d9c-8c95-56d408fe1eaa',\n",
       " '22cb8c02-bbc0-4df4-a866-610d9be6d990',\n",
       " '5db15731-cdd1-4dfc-ab6d-aad1c933749c',\n",
       " '7eb61e4c-2b63-412b-868b-67be6009b1c5',\n",
       " '02c106a7-beba-4038-9017-4b741ced0dc0',\n",
       " '041facd1-ffc0-4326-81ba-fd385a1ed70e',\n",
       " 'b0d4a7a6-eccb-4c37-a67f-86ea20ecbbb1',\n",
       " '5effc3f8-34eb-4fa6-bb04-55f652ed0001',\n",
       " '6f43dc0c-d8a1-401e-9100-d1412aebaaea',\n",
       " 'd08b469c-4d38-4d94-9326-d8af402b9ee2',\n",
       " 'bcd04105-cc5f-4a15-a90c-544d2df2d839',\n",
       " 'ed37fcc9-b129-49d7-8414-1825bb989097',\n",
       " '932d943d-304a-42ea-bd2e-d26ce25a9cbe',\n",
       " '03d9c3db-a3d9-4182-ac40-b8999e180130',\n",
       " 'ae21860b-1e44-4813-b546-653332c240ad',\n",
       " 'b289aec1-26ea-4591-9df8-cb37369bafb7',\n",
       " '1b21d9c9-bee1-4c52-9c71-bf8d466a534f',\n",
       " '5dbf02ab-6741-426a-b3b2-59c838c92f2c',\n",
       " '14f601bf-2d06-48d0-9176-7b8cd6ec4574',\n",
       " '18a761a5-a0e9-49da-bf91-f9cef67611de',\n",
       " '0e6b05d3-5b24-420e-a0ef-218507b81ba9',\n",
       " 'f78c4e0a-738d-4ce5-8938-fe6e466a76f0',\n",
       " '003505a4-b5aa-4160-bc2f-9fef0d44eb86',\n",
       " 'ab38653c-9a10-40db-a4e0-64de9167d8a2',\n",
       " 'b52ce745-79f9-499b-bce0-a79a1b8a47a8',\n",
       " '937f5dcf-8d79-4c0f-bcad-d3231d7a8e40',\n",
       " 'eb909878-d42e-4469-8029-92d1f1d67008',\n",
       " 'fb35c3b5-d4cc-4122-8482-c080fbe901e3',\n",
       " '8a13669f-b43a-4529-bc32-fd2c3e392dbe',\n",
       " 'e1caecd7-b51c-4e42-acfe-49bf9deef5cb',\n",
       " '948fc5b0-296e-4d56-a2e9-fccb6297e1cc',\n",
       " '3f4f9038-1d4b-4598-9063-9bcd58d5a5af',\n",
       " '730976b0-26c7-41fb-89c8-52e8a194feb7',\n",
       " '466af6cf-61c1-4c65-b42e-d0554b2fcf41',\n",
       " '1eb8b3b7-3f35-4d66-b590-33dada32ab36',\n",
       " 'dc034d91-1705-4903-99c5-83ad7c796925',\n",
       " '141c272d-a551-4764-886b-0478ad5280f4',\n",
       " 'bd6aaf42-310c-4043-88c3-207416353877',\n",
       " 'a1a4a3bb-df6d-482f-82ed-4d922cbba8de',\n",
       " 'ee718c05-a867-48c7-8bbe-d9f620cd0868',\n",
       " 'cd6593ed-0b81-4364-bf60-b7c337c0b057',\n",
       " 'c582a804-3694-43be-ba1c-dfa95e6cf132',\n",
       " '1cffd24e-bbcf-4e8e-8f9c-e21893586926',\n",
       " 'f888fda2-c79d-410d-aa8c-02edc786887a',\n",
       " 'a040a2c0-f254-4b66-afe6-26d77b1c567b',\n",
       " '8b2507bb-12f6-4aef-bf65-09c3e3e26a13',\n",
       " '3a90e287-8232-4070-bd4e-f817f7aaa18e',\n",
       " '47625222-6584-410c-b0b8-ecf3bc5a6b5a',\n",
       " '5c5145e1-3707-4765-8af2-463181389fb3',\n",
       " '30fca71c-08a6-4e71-94ed-5377269c9cb8',\n",
       " '269d7012-367d-4b04-9763-9a9eac17e642',\n",
       " '35fb3648-54ee-454d-b1d1-fd1635853eac',\n",
       " '4b49a17c-d79f-40de-aabe-9abd3f074c18',\n",
       " '678ae113-1dfb-4d0d-8530-fa7dff097d39',\n",
       " '6513f0c2-faa2-426f-96de-b4050e70974c',\n",
       " '53f0c935-d6ef-46a0-9cbf-607a486f9d4b',\n",
       " '99690faf-e8de-4d5b-8af7-5a54b0849cac',\n",
       " 'c0d26dac-759f-4896-b2c7-76bc57176701',\n",
       " '919c681a-8e54-4bc3-865f-868b100415b0',\n",
       " '396e68ed-4d17-4626-8c14-0da8923bac59',\n",
       " '850c2273-9bcf-4f21-a7bb-9b6b9fd1211f',\n",
       " '8af25ca3-7825-415e-8aba-322c84d67175',\n",
       " '06181f70-9148-4bc4-9007-b0cfdb1054a5',\n",
       " '54480385-f3de-4f7e-8872-bd8739328678',\n",
       " 'd5ed806f-251b-40f8-bd13-62572f25bf74',\n",
       " 'ff3c1fa9-077a-496f-93c0-21c370415952',\n",
       " 'e68ddff1-f254-4cbe-b4a5-839d37f5615a',\n",
       " 'e1187efb-0031-45d9-bbda-9aca5e3b5996',\n",
       " '9315a27a-48a3-4c3e-bd98-1ce185907b18',\n",
       " '584c04b1-a64b-472c-b5e2-f7632f70e95d',\n",
       " 'b5c81cf2-a6af-41d5-8c13-e14b9eeb362b',\n",
       " 'c5d27f13-26ed-48da-9a20-4b0690573261',\n",
       " '3f32ea38-8a85-4b82-9f80-50b9a880e027',\n",
       " 'a30c5bce-807a-4f6a-b663-36fee093ff41',\n",
       " '02d6f064-e2cb-4b95-b569-865af88c36c7',\n",
       " '42fdda3e-acf6-4d3e-a61d-ed3f170ad925',\n",
       " '9c1d1d73-5232-47d8-b684-c20da5b35084',\n",
       " '3218193e-9015-45a0-a82b-680c63affd4d',\n",
       " '1b37bc86-8f42-4d2d-9808-4f6c2514ef73',\n",
       " '4bbc23d8-bb44-4290-9057-3fe6650e02fe',\n",
       " 'a0aec856-07de-4a8a-9d05-7652a65369a8',\n",
       " 'b896f554-ca94-47f2-9c5f-9cf3b64e8781',\n",
       " '97ad2123-ee9f-4894-9794-9f488b30db44',\n",
       " 'fc0cf253-3ecc-4bea-b401-a606a08cfacc',\n",
       " 'fa622d07-f746-4ae4-87b2-70322d6720e0',\n",
       " '16e5179f-5f49-4d21-beb1-483481697ee9',\n",
       " 'c2bed261-e6b8-4e56-a66d-0720643d17ec',\n",
       " 'cae857be-006b-423b-baab-601e0cd10fc9',\n",
       " '6d69ed0c-7208-4df3-9aa6-e25d247b7ecb',\n",
       " 'ff1ccec9-9379-4010-a0bf-a7e1effabed6',\n",
       " '60d991fb-9464-4ac4-bbeb-d829d093343a',\n",
       " 'bec6d211-c530-4dee-83d7-0ef3cb7140a2',\n",
       " '5e1499c9-fd27-465e-9e11-083c221d0bcf',\n",
       " '97d1241b-8d41-46be-bef8-dca94f976f5b',\n",
       " 'ac0cd831-0924-426c-b8c6-006e217b6050',\n",
       " '4902090e-a794-44e3-911c-f2c0136f48b1',\n",
       " '40498caf-dcc0-404d-8e16-e40406467bf9',\n",
       " '16771725-a09e-4b2b-b534-26877101faa8',\n",
       " 'be5ba054-7619-484c-843c-63ded5bbcd44',\n",
       " '7bec6f62-787f-435b-b9c1-89a447871c85',\n",
       " '3130809a-86b9-42e4-9947-f4ebc9817371',\n",
       " '3919e5fd-2e4a-4154-a619-965186d235b3',\n",
       " 'b2a6e227-d307-4258-a312-b5e48d7ebbd9',\n",
       " '23b98412-3ea8-4e10-963f-5ea3b80bc0e4',\n",
       " '80bd55b6-cf85-4d98-be65-5e14eca458c6',\n",
       " 'aa53b7d6-8fe4-42af-8df9-c61dcc03ac55',\n",
       " 'c9129cb6-115f-4573-b30e-e9b93fa2dc3f',\n",
       " 'd62887fa-4c3b-4bbc-b4a7-f07f0ed82026',\n",
       " '21076dee-6424-4aa1-b4f5-5238ec327d48',\n",
       " '54f6b6a4-3ecc-449e-9092-6ce71b05ee59',\n",
       " '857788f1-7320-4273-9ea5-d28fd94182e6',\n",
       " '8e9766df-1320-45df-a9d5-d3b36b38edf5',\n",
       " '76588f12-8e8e-4585-81b2-f818dac49583',\n",
       " '7a471357-8f6a-4fea-a4fa-c493d6edfca6',\n",
       " 'd4815455-b37d-4dc3-abf1-5156578106d9',\n",
       " '7120b564-a62b-434e-80ac-d5a0dd68e930',\n",
       " 'c3427f0e-186b-4e7d-b725-ae53a4e2aa81',\n",
       " 'cfc8b6fc-a631-45c7-8f1f-13494dea29b5',\n",
       " 'c0849aa5-b802-4566-8891-19342784f315',\n",
       " '422d15a1-acee-4692-b98f-fd9dc9ab07db',\n",
       " '2f377013-3dd5-47a3-80ec-4d2f7bb5f3ca',\n",
       " 'e39c9f90-b54a-4781-89cc-746580473783',\n",
       " 'eb310944-7136-42e3-8919-51393f1a380c',\n",
       " 'a74d761b-5ca4-422a-81cd-5f89d351c392',\n",
       " '9d5115fe-ce41-43a1-bbad-bcfab68e65a6',\n",
       " '2d49d5a1-9946-4eb1-a729-7c218193354e',\n",
       " '32826deb-b8be-4135-87e4-834d0426da69',\n",
       " 'f696870e-0365-4f9c-99a6-533883248f03',\n",
       " '316127f9-7625-45e4-913d-9459aab48104',\n",
       " 'ea166afb-6766-49a6-89a6-d5869c59b17b',\n",
       " '5606ebd2-0f8a-4c11-9f96-4c1f6939324e',\n",
       " 'c43bc444-0d1e-44b9-9544-2a4dccec6b5b',\n",
       " 'fa5ee4e0-b142-424a-a44b-4fb1bec7afac',\n",
       " '98098b9b-d918-47e5-a02b-e1ff505c1d90',\n",
       " '8ac8054d-b3ad-4cc4-b37b-e9307ce693fd',\n",
       " '69283cf7-c6f3-49ad-90c3-fb163d6a0ed1',\n",
       " '6e570389-3e0c-4937-8f06-aef096b93557',\n",
       " 'd1052c98-1ebe-4cc9-bd7b-de63577ac381',\n",
       " '99186edc-aad5-4c51-8e5e-98961a253bea',\n",
       " '867e3a65-7777-4524-8c3e-9cf06d67161a',\n",
       " 'bc37752b-79f8-4793-b1e6-6cbade5f7596',\n",
       " '671179a1-c9f8-48d1-9d3a-f3903fed9e90',\n",
       " 'cadd15ca-e950-4c8a-9475-1c924fa07a8e',\n",
       " 'b972085b-3b01-4b88-ae6c-6fcdcfebc161',\n",
       " '419c199e-7ea9-4987-99a3-4815bcb72838',\n",
       " '14e4c420-90e1-4c9a-8a53-d7164c7bab0e',\n",
       " '42a33f8c-3ecb-42a2-a6fb-af52c9b4fd52',\n",
       " '761f490a-da87-4a12-bcfc-7a71ebc22e1c',\n",
       " '371cfbf8-0f1a-4e6d-b9eb-fa64561caab2',\n",
       " 'bdfbf356-5062-4981-8f0f-0fd7f58146f4',\n",
       " 'bef9be4f-8a5a-421e-99a0-57875b874ece',\n",
       " '67f935cb-9a9b-4ef2-a026-0e1f8a5408ad',\n",
       " '9949459e-075d-4eec-b80e-fa558f6dec7d',\n",
       " '14d802a0-4d63-434e-84f9-3f2fcf8081d1',\n",
       " '2f0e1f93-d3ad-4d8f-8513-75440dadbd71',\n",
       " 'a1dc2bb4-2f2b-4e4b-bfcd-ed581d51d3a4',\n",
       " 'f48c0253-f0ba-4bbb-9bd2-324459b3fe91',\n",
       " 'ea6641c5-15a4-4697-8dd9-4706c958c30a',\n",
       " '4e8e6391-7dfd-4112-9593-b1a357a425c6',\n",
       " '38a46860-85ad-43a7-be90-7d9ab2782df0',\n",
       " '7b450d1d-cca1-4152-8972-458167969643',\n",
       " '57d715b4-212d-4519-a3b8-90e15b527174',\n",
       " '36401e81-958a-468d-ade3-65878282f969',\n",
       " 'd0b6dbfd-c496-419e-acc3-978f2dca914e',\n",
       " '57776c29-c076-4c6d-8218-5b13e711550c',\n",
       " '27657173-658f-46eb-bc3d-4c0ade2d442a',\n",
       " 'c01dd583-6c46-4f48-a2fc-e6caaf5726fc',\n",
       " '01436c58-c632-451a-a83e-0944c66bec4a',\n",
       " 'ca2d729d-a25b-4149-82cf-a0610da2ab81',\n",
       " 'a1e790e8-85e3-43a5-91b0-49c827364569',\n",
       " 'f7cdaabb-3b88-4520-892e-2176e2bf5943',\n",
       " '1e93184a-76e4-4534-b644-2ab80ca98a4b',\n",
       " '011f2a0a-c212-4ed3-a9c8-20ea948b329e',\n",
       " 'b18d5ae6-6748-4233-a9fd-208be2034cbd',\n",
       " '43faa8aa-5a8b-4f4a-806e-8d3d9939ad7d',\n",
       " '765f9656-97ad-44b6-b84c-0a470a59d8fe',\n",
       " '39275b06-0dff-400d-bdb0-c26b37cc7419',\n",
       " '7b060e11-e6c9-4d83-ac21-9d7f93b2fc7c',\n",
       " '92f3819c-0d0b-4a21-a6c1-0383dea79866',\n",
       " 'a42b6ffa-0215-4e4f-9046-d45e657c2b71',\n",
       " '7ad385f5-9a40-4b9f-a62a-42d3cc7ec4e7',\n",
       " '2954cc07-5665-4fe7-bd4f-4973e3f80f25',\n",
       " '349b6c57-ad97-4e3f-91aa-9eef02676493',\n",
       " '252675d1-0142-4015-9305-da529174eb3b',\n",
       " '6847b765-bb3a-4317-a1f7-4bc30fb658ea',\n",
       " 'dd8bc3e6-6974-4f2e-9c9f-3bce69d0be02',\n",
       " '155fd6f5-9dce-41f6-8873-5200d6f63621',\n",
       " '40561a0c-25b0-47ca-ad0a-2a77de012bdc',\n",
       " '8ee8e6ef-754f-420a-8f1d-2800f3210481',\n",
       " 'ae5970b8-0409-4293-9257-5ccf3f0a0753',\n",
       " '3a1df5c6-7b6a-4c16-9a03-8d553ff267a4',\n",
       " '6357f2a3-d911-4faf-b90d-fe7f7846edbe',\n",
       " '28183dc7-7b5e-4658-a700-67d85a6bbe81',\n",
       " '32ace160-bca9-45a3-b782-d59ce5c1d5e9',\n",
       " 'd65d911a-d6f7-416a-b227-3e3e7c6cf6df',\n",
       " '3763b3f8-8485-4e3e-ac88-0e8f2fc69a60',\n",
       " '378efd46-db33-4ff4-a683-402573d98938',\n",
       " '62b9270f-9490-4571-9237-8693b5a507f2',\n",
       " 'cc672188-3292-44cc-8b5f-a5155d1a30f5',\n",
       " 'a917eba0-726a-4333-b5a2-5fd5b4419663',\n",
       " 'cb19d353-b97c-4bac-b7ed-063f64f9d45a',\n",
       " 'acf2aa50-ebe6-4f78-ba5d-d87401534d1e',\n",
       " '196a8948-333a-44a0-88d5-8796b2cbe08d',\n",
       " '06fa9f5d-d76b-4f71-8fd1-9151ffa6568b',\n",
       " '96894fff-669e-4c21-b0f8-f29451c15530',\n",
       " 'dadb74e0-afa8-4411-ae60-e8d1708d8bcf',\n",
       " '107a6e99-e51e-4470-8658-a6abf7ad793e',\n",
       " '800f113e-5078-41b4-9522-2a5b84436fcc',\n",
       " '6223fa28-3516-4e1d-9947-8dc2525baad1',\n",
       " 'd433e5a1-65e3-4f5a-ab83-5d6c5ffe1d0c',\n",
       " 'da138709-4bbe-40b8-b3ab-5a2ee7e701e3',\n",
       " 'c300d8e7-bf39-4dc6-a635-777bb977d95e',\n",
       " '6aaa16d8-b064-428e-8d45-04f559c3e139',\n",
       " '6e38d604-271d-46aa-89b6-0e42e7ec35ca',\n",
       " 'd6aa0afb-73e7-453d-a5e7-c1df151e1bfa',\n",
       " '189c53e8-928b-4b5d-bc16-4173f7b4f5f5',\n",
       " '3f4f42a0-61b0-48a8-abe3-ecd0aeca1221',\n",
       " 'dd7f1d7e-0f4c-4148-87d9-6184422cc8ba',\n",
       " '136dc635-91e2-4368-afc9-151a60f4485c',\n",
       " '26df8169-ccaf-41c6-97c5-3357cfbf2383',\n",
       " 'a72257ec-27b6-4f5b-9bb8-76fea4aed4c4',\n",
       " '76f4328e-ae9c-4ec3-8144-de72656b6ade',\n",
       " '199e265b-8856-4d08-b272-29d001be9f6d',\n",
       " 'edc8626a-51b0-4494-a901-f928d87ffde9',\n",
       " '5b285e76-4894-4ffd-9d74-bc1f845b4a9d',\n",
       " '2d5a879c-6d9d-4461-a037-6f7043028f72',\n",
       " 'e0702e8b-80f3-4a12-8ea4-760c0deef182',\n",
       " '007ea8c8-1b0e-4293-b501-74742b92d26d',\n",
       " '00d33389-c03d-4e74-b428-bba7c29817e0',\n",
       " '29cba5a5-9e9a-46e9-a1b1-7424c9113453',\n",
       " 'e2664412-2b88-4308-8423-59670160b778',\n",
       " '4e317794-1299-44ee-bc70-51e71649da27',\n",
       " 'cd83aa6a-0ffc-46b8-9ade-ded996110fab',\n",
       " '30c18e59-f21e-4765-b634-deee08b52b9b',\n",
       " 'b5b38f69-2480-4055-9d6c-fe26ed51ce8c',\n",
       " 'b5e8b960-f9e2-4573-8256-0a092c8cfd6c',\n",
       " 'd4d4163c-bad8-4e98-baf2-7d6a3987c427',\n",
       " '82b4f08e-d1fe-4c1e-a204-3eb85d259855',\n",
       " '531437cb-96ce-46f7-83c1-5d94934ef618',\n",
       " '8b414fec-cdc3-4629-bc54-8502f2280c0d',\n",
       " 'aa3da233-ad94-4c54-bcbb-7d03898f4c88',\n",
       " '5b3f15e4-1468-4af3-819b-a997109ad2f2',\n",
       " '51fd040c-6f97-457f-9645-32f84104efe9',\n",
       " '6d79f550-e6b7-4427-b07d-4542d1d3bf01',\n",
       " 'c4fd575a-ca61-4a15-9e3b-490793d1521d',\n",
       " '9b447831-23cb-4240-ab8d-31641b9ddd13',\n",
       " '35f1a577-3f67-4002-a6b0-5c74c319ad45',\n",
       " '9049b1cd-a633-470c-9bbe-7f2cdecbe807',\n",
       " 'a7bca747-823b-4c5a-ad39-61cf7e691543',\n",
       " 'd1a7e43f-f948-46f8-af13-91296ebe6533',\n",
       " 'f657edbc-24b6-4eee-b5db-a6ddc07422f8']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:35:06.880843Z",
     "start_time": "2025-02-05T23:35:06.398127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_query = \"What is supervised and unsupervised machine learning?\"\n",
    "retriever = vector_store_text.as_retriever(search_kwargs={\"k\":5})\n",
    "# retriever.get_relevant_documents(query)\n",
    "retrieved_docs = retriever.invoke(user_query)\n",
    "retrieved_docs"
   ],
   "id": "73d668c8639195bc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='ea6641c5-15a4-4697-8dd9-4706c958c30a', metadata={}, page_content='Machine Learning Overview, EDA and Clustering\\nBrian Wright\\nbrianwright@virginia.edu1. What is Machine Learning ?\\n2. What is exploratory data analysis?\\n3. k-means clustering\\n– Does Congress vote in patterns?\\n4. Multi-dimensional k-means clustering\\n– Are NBA players compensated according to performance?\\nOutline: Intro to Unsupervised ML\\n2“A field of Computer Science that gives computers the ability to learn\\nwithout being explicitly programmed.”\\n-\\nArthur Samuel (Coined the term in 1959 at IBM)\\n“The ability [for systems] to acquire their own knowledge, by\\nextracting patterns from raw data.”\\n-\\nDeep Learning, Goodfellow et alMachine vs. human\\nMachine\\nHuman\\nUnderstanding context\\n✔\\nThinking through the problem\\n✔\\nAsking the right questions\\n✔\\nSelecting the right tools\\n✔\\nPerforming calculations quickly\\n✔\\nPerforming repetitive tasks\\n✔\\nFollowing pre-defined rules\\n✔\\nInterpreting results\\n✔5Pattern discovery when inputs (x) and outputs (y) are known\\nSupervised machine learning\\nInput x:\\nVoter\\nOutput y:'),\n",
       " Document(id='ccc423e6-0f59-4bba-9858-46e8deb9da45', metadata={}, page_content='without being explicitly programmed.”\\n-\\nArthur Samuel (Coined the term in 1959 at IBM)\\n“The ability [for systems] to acquire their own knowledge, by\\nextracting patterns from raw data.”\\n-\\nDeep Learning, Goodfellow et al\\n“A computer program is said to learn from experience E with respect\\nto some set of tasks T and performance measure P if its performance\\ntasks in T, as measured by P, improves with experience E.”\\n- Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs. human\\nMachine\\nHuman\\nUnderstanding context\\n✔\\nThinking through the problem\\n✔\\nAsking the right questions\\n✔\\nSelecting the right tools\\n✔\\nPerforming calculations quickly\\n✔\\nPerforming repetitive tasks\\n✔\\nFollowing pre-defined rules\\n✔\\nInterpreting results\\n✔8Pattern discovery when inputs (x) and outputs (y) are known\\nSupervised machine learning\\nInput x:\\nVoter\\nOutput y:\\nPolitical \\naffiliation\\nExamples: Classification and regression are supervised machine learning \\n9The data inputs (x) have no target outputs (y)'),\n",
       " Document(id='d5084324-ffeb-4faa-bda1-cb891c6aa520', metadata={}, page_content='without being explicitly programmed.”\\n-\\nArthur Samuel (Coined the term in 1959 at IBM)\\n“The ability [for systems] to acquire their own knowledge, by\\nextracting patterns from raw data.”\\n-\\nDeep Learning, Goodfellow et al\\n“A computer program is said to learn from experience E with respect\\nto some set of tasks T and performance measure P if its performance\\ntasks in T, as measured by P, improves with experience E.”\\n- Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs. human\\nMachine\\nHuman\\nUnderstanding context\\n✔\\nThinking through the problem\\n✔\\nAsking the right questions\\n✔\\nSelecting the right tools\\n✔\\nPerforming calculations quickly\\n✔\\nPerforming repetitive tasks\\n✔\\nFollowing pre-defined rules\\n✔\\nInterpreting results\\n✔8Pattern discovery when inputs (x) and outputs (y) are known\\nSupervised machine learning\\nInput x:\\nVoter\\nOutput y:\\nPolitical \\naffiliation\\nExamples: Classification and regression are supervised machine learning \\n9The data inputs (x) have no target outputs (y)'),\n",
       " Document(id='e90cc81f-dbac-4e39-8cfa-ba100b8bc379', metadata={}, page_content='without being explicitly programmed.”\\n-\\nArthur Samuel (Coined the term in 1959 at IBM)\\n“The ability [for systems] to acquire their own knowledge, by\\nextracting patterns from raw data.”\\n-\\nDeep Learning, Goodfellow et al\\n“A computer program is said to learn from experience E with respect\\nto some set of tasks T and performance measure P if its performance\\ntasks in T, as measured by P, improves with experience E.”\\n- Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs. human\\nMachine\\nHuman\\nUnderstanding context\\n✔\\nThinking through the problem\\n✔\\nAsking the right questions\\n✔\\nSelecting the right tools\\n✔\\nPerforming calculations quickly\\n✔\\nPerforming repetitive tasks\\n✔\\nFollowing pre-defined rules\\n✔\\nInterpreting results\\n✔8Pattern discovery when inputs (x) and outputs (y) are known\\nSupervised machine learning\\nInput x:\\nVoter\\nOutput y:\\nPolitical \\naffiliation\\nExamples: Classification and regression are supervised machine learning \\n9The data inputs (x) have no target outputs (y)'),\n",
       " Document(id='06181f70-9148-4bc4-9007-b0cfdb1054a5', metadata={}, page_content='without being explicitly programmed.”\\n-\\nArthur Samuel (Coined the term in 1959 at IBM)\\n“The ability [for systems] to acquire their own knowledge, by\\nextracting patterns from raw data.”\\n-\\nDeep Learning, Goodfellow et al\\n“A computer program is said to learn from experience E with respect\\nto some set of tasks T and performance measure P if its performance\\ntasks in T, as measured by P, improves with experience E.”\\n- Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs. human\\nMachine\\nHuman\\nUnderstanding context\\n✔\\nThinking through the problem\\n✔\\nAsking the right questions\\n✔\\nSelecting the right tools\\n✔\\nPerforming calculations quickly\\n✔\\nPerforming repetitive tasks\\n✔\\nFollowing pre-defined rules\\n✔\\nInterpreting results\\n✔8Pattern discovery when inputs (x) and outputs (y) are known\\nSupervised machine learning\\nInput x:\\nVoter\\nOutput y:\\nPolitical \\naffiliation\\nExamples: Classification and regression are supervised machine learning \\n9The data inputs (x) have no target outputs (y)')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:35:09.362670Z",
     "start_time": "2025-02-05T23:35:09.359365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "print(format_docs(retrieved_docs))"
   ],
   "id": "236fb65fb085b2e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning Overview, EDA and Clustering\n",
      "Brian Wright\n",
      "brianwright@virginia.edu1. What is Machine Learning ?\n",
      "2. What is exploratory data analysis?\n",
      "3. k-means clustering\n",
      "– Does Congress vote in patterns?\n",
      "4. Multi-dimensional k-means clustering\n",
      "– Are NBA players compensated according to performance?\n",
      "Outline: Intro to Unsupervised ML\n",
      "2“A field of Computer Science that gives computers the ability to learn\n",
      "without being explicitly programmed.”\n",
      "-\n",
      "Arthur Samuel (Coined the term in 1959 at IBM)\n",
      "“The ability [for systems] to acquire their own knowledge, by\n",
      "extracting patterns from raw data.”\n",
      "-\n",
      "Deep Learning, Goodfellow et alMachine vs. human\n",
      "Machine\n",
      "Human\n",
      "Understanding context\n",
      "✔\n",
      "Thinking through the problem\n",
      "✔\n",
      "Asking the right questions\n",
      "✔\n",
      "Selecting the right tools\n",
      "✔\n",
      "Performing calculations quickly\n",
      "✔\n",
      "Performing repetitive tasks\n",
      "✔\n",
      "Following pre-defined rules\n",
      "✔\n",
      "Interpreting results\n",
      "✔5Pattern discovery when inputs (x) and outputs (y) are known\n",
      "Supervised machine learning\n",
      "Input x:\n",
      "Voter\n",
      "Output y:\n",
      "\n",
      "without being explicitly programmed.”\n",
      "-\n",
      "Arthur Samuel (Coined the term in 1959 at IBM)\n",
      "“The ability [for systems] to acquire their own knowledge, by\n",
      "extracting patterns from raw data.”\n",
      "-\n",
      "Deep Learning, Goodfellow et al\n",
      "“A computer program is said to learn from experience E with respect\n",
      "to some set of tasks T and performance measure P if its performance\n",
      "tasks in T, as measured by P, improves with experience E.”\n",
      "- Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs. human\n",
      "Machine\n",
      "Human\n",
      "Understanding context\n",
      "✔\n",
      "Thinking through the problem\n",
      "✔\n",
      "Asking the right questions\n",
      "✔\n",
      "Selecting the right tools\n",
      "✔\n",
      "Performing calculations quickly\n",
      "✔\n",
      "Performing repetitive tasks\n",
      "✔\n",
      "Following pre-defined rules\n",
      "✔\n",
      "Interpreting results\n",
      "✔8Pattern discovery when inputs (x) and outputs (y) are known\n",
      "Supervised machine learning\n",
      "Input x:\n",
      "Voter\n",
      "Output y:\n",
      "Political \n",
      "affiliation\n",
      "Examples: Classification and regression are supervised machine learning \n",
      "9The data inputs (x) have no target outputs (y)\n",
      "\n",
      "without being explicitly programmed.”\n",
      "-\n",
      "Arthur Samuel (Coined the term in 1959 at IBM)\n",
      "“The ability [for systems] to acquire their own knowledge, by\n",
      "extracting patterns from raw data.”\n",
      "-\n",
      "Deep Learning, Goodfellow et al\n",
      "“A computer program is said to learn from experience E with respect\n",
      "to some set of tasks T and performance measure P if its performance\n",
      "tasks in T, as measured by P, improves with experience E.”\n",
      "- Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs. human\n",
      "Machine\n",
      "Human\n",
      "Understanding context\n",
      "✔\n",
      "Thinking through the problem\n",
      "✔\n",
      "Asking the right questions\n",
      "✔\n",
      "Selecting the right tools\n",
      "✔\n",
      "Performing calculations quickly\n",
      "✔\n",
      "Performing repetitive tasks\n",
      "✔\n",
      "Following pre-defined rules\n",
      "✔\n",
      "Interpreting results\n",
      "✔8Pattern discovery when inputs (x) and outputs (y) are known\n",
      "Supervised machine learning\n",
      "Input x:\n",
      "Voter\n",
      "Output y:\n",
      "Political \n",
      "affiliation\n",
      "Examples: Classification and regression are supervised machine learning \n",
      "9The data inputs (x) have no target outputs (y)\n",
      "\n",
      "without being explicitly programmed.”\n",
      "-\n",
      "Arthur Samuel (Coined the term in 1959 at IBM)\n",
      "“The ability [for systems] to acquire their own knowledge, by\n",
      "extracting patterns from raw data.”\n",
      "-\n",
      "Deep Learning, Goodfellow et al\n",
      "“A computer program is said to learn from experience E with respect\n",
      "to some set of tasks T and performance measure P if its performance\n",
      "tasks in T, as measured by P, improves with experience E.”\n",
      "- Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs. human\n",
      "Machine\n",
      "Human\n",
      "Understanding context\n",
      "✔\n",
      "Thinking through the problem\n",
      "✔\n",
      "Asking the right questions\n",
      "✔\n",
      "Selecting the right tools\n",
      "✔\n",
      "Performing calculations quickly\n",
      "✔\n",
      "Performing repetitive tasks\n",
      "✔\n",
      "Following pre-defined rules\n",
      "✔\n",
      "Interpreting results\n",
      "✔8Pattern discovery when inputs (x) and outputs (y) are known\n",
      "Supervised machine learning\n",
      "Input x:\n",
      "Voter\n",
      "Output y:\n",
      "Political \n",
      "affiliation\n",
      "Examples: Classification and regression are supervised machine learning \n",
      "9The data inputs (x) have no target outputs (y)\n",
      "\n",
      "without being explicitly programmed.”\n",
      "-\n",
      "Arthur Samuel (Coined the term in 1959 at IBM)\n",
      "“The ability [for systems] to acquire their own knowledge, by\n",
      "extracting patterns from raw data.”\n",
      "-\n",
      "Deep Learning, Goodfellow et al\n",
      "“A computer program is said to learn from experience E with respect\n",
      "to some set of tasks T and performance measure P if its performance\n",
      "tasks in T, as measured by P, improves with experience E.”\n",
      "- Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs. human\n",
      "Machine\n",
      "Human\n",
      "Understanding context\n",
      "✔\n",
      "Thinking through the problem\n",
      "✔\n",
      "Asking the right questions\n",
      "✔\n",
      "Selecting the right tools\n",
      "✔\n",
      "Performing calculations quickly\n",
      "✔\n",
      "Performing repetitive tasks\n",
      "✔\n",
      "Following pre-defined rules\n",
      "✔\n",
      "Interpreting results\n",
      "✔8Pattern discovery when inputs (x) and outputs (y) are known\n",
      "Supervised machine learning\n",
      "Input x:\n",
      "Voter\n",
      "Output y:\n",
      "Political \n",
      "affiliation\n",
      "Examples: Classification and regression are supervised machine learning \n",
      "9The data inputs (x) have no target outputs (y)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Embed the query",
   "id": "2e2b8c59dea09d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:35:11.583638Z",
     "start_time": "2025-02-05T23:35:11.457326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = clip_processor(text=[user_query], return_tensors=\"pt\", padding=True)\n",
    "text_embedding = clip_model.get_text_features(**inputs).detach().numpy().tolist()[0]"
   ],
   "id": "87a06198d92826bf",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Query the Pinecone vector db and return top 5 matches",
   "id": "4fafe291424f1c99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:35:13.721145Z",
     "start_time": "2025-02-05T23:35:13.618048Z"
    }
   },
   "cell_type": "code",
   "source": "query_results = index.query(vector=text_embedding, top_k=5, include_metadata=True)",
   "id": "ad6971ecde8670d6",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:35:14.063477Z",
     "start_time": "2025-02-05T23:35:14.059735Z"
    }
   },
   "cell_type": "code",
   "source": "query_results[\"matches\"]",
   "id": "e76cdfc92d9ad6bf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '3001_ETA/page_9_img_2.png',\n",
       "  'metadata': {'file_name': '3001_ETA', 'image_key': 'page_9_img_2.png'},\n",
       "  'score': 0.334317118,\n",
       "  'values': []},\n",
       " {'id': 'machine_learning_bootcamp_II/page_8_img_1.png',\n",
       "  'metadata': {'file_name': 'machine_learning_bootcamp_II',\n",
       "               'image_key': 'page_8_img_1.png'},\n",
       "  'score': 0.322387815,\n",
       "  'values': []},\n",
       " {'id': 'machine_learning_overview/page_8_img_1.png',\n",
       "  'metadata': {'file_name': 'machine_learning_overview',\n",
       "               'image_key': 'page_8_img_1.png'},\n",
       "  'score': 0.322387815,\n",
       "  'values': []},\n",
       " {'id': 'machine_learning_III/page_8_img_1.png',\n",
       "  'metadata': {'file_name': 'machine_learning_III',\n",
       "               'image_key': 'page_8_img_1.png'},\n",
       "  'score': 0.322387815,\n",
       "  'values': []},\n",
       " {'id': 'machine_learning_bootcamp_II copy/page_8_img_1.png',\n",
       "  'metadata': {'file_name': 'machine_learning_bootcamp_II copy',\n",
       "               'image_key': 'page_8_img_1.png'},\n",
       "  'score': 0.322387815,\n",
       "  'values': []}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Connect to Mongodb",
   "id": "8d92e0000c8d1050"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:36:36.403047Z",
     "start_time": "2025-02-05T23:36:36.252354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pymongo import MongoClient\n",
    "import gridfs\n",
    "\n",
    "client = MongoClient(mongo_uri)\n",
    "\n",
    "#client = MongoClient(MONGO_URI)\n",
    "\n",
    "db = client[\"images\"]\n",
    "\n",
    "collection = db[\"images_for_rag\"]\n",
    "\n",
    "print(\"Connected to MongoDB successfully!\")"
   ],
   "id": "8af376f705e4db3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MongoDB successfully!\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pushing all the images to mongodb",
   "id": "f4eb03346c1b6208"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:36:40.411698Z",
     "start_time": "2025-02-05T23:36:40.408399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_names_list = os.listdir(\"data_processed\")\n",
    "\n",
    "fs = gridfs.GridFS(db)\n",
    "\n"
   ],
   "id": "3733ade1dcfdc1f6",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for i in file_names_list:\n",
    "    images_list = os.listdir(os.path.join(\"data_processed\", i, \"Images\"))\n",
    "    for j in images_list:\n",
    "        with open(os.path.join(\"data_processed\", i, \"Images\", j), \"rb\") as image_file:\n",
    "            image_id = fs.put(image_file, filename=i + \"/\" + j)"
   ],
   "id": "9af02d3d0dcb6824"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T23:45:21.715690Z",
     "start_time": "2025-01-29T23:45:20.118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pushing one image to mongodb \n",
    "# fs = gridfs.GridFS(db)\n",
    "# \n",
    "# # Store an image\n",
    "# image_path = \"/Users/dorukozar/PycharmProjects/pythonProject4/clustering ss.png\"\n",
    "# \n",
    "# with open(image_path, \"rb\") as image_file:\n",
    "#     image_id = fs.put(image_file, filename=\"clustering.png\")\n",
    "# \n",
    "# print(f\"Image stored with ID: {image_id}\")"
   ],
   "id": "14abfea71089089b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image stored with ID: 679abd90fc38751164dac1ef\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Query MongDB",
   "id": "43b34bcd62d43dc0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:36:44.608674Z",
     "start_time": "2025-02-05T23:36:42.920444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# image_data = fs.find_one({\"filename\": \"clustering.png\"})\n",
    "image_data_list = []\n",
    "for i in query_results[\"matches\"]:\n",
    "    filename = i[\"id\"]\n",
    "    image_data = fs.find_one({\"filename\": filename})\n",
    "    image_data_list.append(image_data)"
   ],
   "id": "2edaa7fcdf7d18a5",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:36:44.620513Z",
     "start_time": "2025-02-05T23:36:44.613131Z"
    }
   },
   "cell_type": "code",
   "source": "image_data_list",
   "id": "52c4ada0555646ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<gridfs.synchronous.grid_file.GridOut at 0x1b94a08e0>,\n",
       " <gridfs.synchronous.grid_file.GridOut at 0x1b73360b0>,\n",
       " <gridfs.synchronous.grid_file.GridOut at 0x1b7443730>,\n",
       " <gridfs.synchronous.grid_file.GridOut at 0x1b74438b0>,\n",
       " <gridfs.synchronous.grid_file.GridOut at 0x1b952d660>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Delete Multiple Files",
   "id": "7c685ea18b632a28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T23:37:08.925326Z",
     "start_time": "2025-01-30T23:36:45.646665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# for file_data in fs.find():  # This retrieves all stored files\n",
    "#     file_id = file_data._id\n",
    "#     fs.delete(file_id)  # Deletes both metadata (fs.files) and chunks (fs.chunks)\n",
    "#     print(f\"✅ Deleted file: {file_data.filename}\")\n",
    "# \n",
    "# print(\"✅ All files deleted from GridFS!\")"
   ],
   "id": "976d36088c0d6b91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Deleted file: page_14_img_1.png\n",
      "✅ Deleted file: page_19_img_2.png\n",
      "✅ Deleted file: page_21_img_1.png\n",
      "✅ Deleted file: page_19_img_1.png\n",
      "✅ Deleted file: page_42_img_1.png\n",
      "✅ Deleted file: page_9_img_1.png\n",
      "✅ Deleted file: page_56_img_1.png\n",
      "✅ Deleted file: page_33_img_1.png\n",
      "✅ Deleted file: page_49_img_1.png\n",
      "✅ Deleted file: page_2_img_1.png\n",
      "✅ Deleted file: page_27_img_1.png\n",
      "✅ Deleted file: page_12_img_1.png\n",
      "✅ Deleted file: page_49_img_2.png\n",
      "✅ Deleted file: page_44_img_1.png\n",
      "✅ Deleted file: page_50_img_1.png\n",
      "✅ Deleted file: page_27_img_2.png\n",
      "✅ Deleted file: page_15_img_1.png\n",
      "✅ Deleted file: page_20_img_1.png\n",
      "✅ Deleted file: page_34_img_1.png\n",
      "✅ Deleted file: page_18_img_1.png\n",
      "✅ Deleted file: page_8_img_1.png\n",
      "✅ Deleted file: page_20_img_2.png\n",
      "✅ Deleted file: page_43_img_1.png\n",
      "✅ Deleted file: page_39_img_1.png\n",
      "✅ Deleted file: page_26_img_1.png\n",
      "✅ Deleted file: page_32_img_1.png\n",
      "✅ Deleted file: page_48_img_1.png\n",
      "✅ Deleted file: page_45_img_2.png\n",
      "✅ Deleted file: page_45_img_3.png\n",
      "✅ Deleted file: page_13_img_1.png\n",
      "✅ Deleted file: page_51_img_1.png\n",
      "✅ Deleted file: page_45_img_1.png\n",
      "✅ Deleted file: page_40_img_2.png\n",
      "✅ Deleted file: page_37_img_1.png\n",
      "✅ Deleted file: page_6_img_1.png\n",
      "✅ Deleted file: page_16_img_1.png\n",
      "✅ Deleted file: page_40_img_1.png\n",
      "✅ Deleted file: page_37_img_2.png\n",
      "✅ Deleted file: page_6_img_2.png\n",
      "✅ Deleted file: page_31_img_4.png\n",
      "✅ Deleted file: page_6_img_3.png\n",
      "✅ Deleted file: page_31_img_1.png\n",
      "✅ Deleted file: page_25_img_1.png\n",
      "✅ Deleted file: page_28_img_2.png\n",
      "✅ Deleted file: page_31_img_3.png\n",
      "✅ Deleted file: page_31_img_2.png\n",
      "✅ Deleted file: page_28_img_1.png\n",
      "✅ Deleted file: page_7_img_1.png\n",
      "✅ Deleted file: page_22_img_1.png\n",
      "✅ Deleted file: page_36_img_1.png\n",
      "✅ Deleted file: page_17_img_1.png\n",
      "✅ Deleted file: page_22_img_2.png\n",
      "✅ Deleted file: page_1_img_4.png\n",
      "✅ Deleted file: page_11_img_1.png\n",
      "✅ Deleted file: page_24_img_1.png\n",
      "✅ Deleted file: page_1_img_1.png\n",
      "✅ Deleted file: page_30_img_1.png\n",
      "✅ Deleted file: page_11_img_2.png\n",
      "✅ Deleted file: page_1_img_3.png\n",
      "✅ Deleted file: page_53_img_1.png\n",
      "✅ Deleted file: page_1_img_2.png\n",
      "✅ Deleted file: page_29_img_1.png\n",
      "✅ Deleted file: page_30_img_2.png\n",
      "✅ Deleted file: page_47_img_1.png\n",
      "✅ All files deleted from GridFS!\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Delete one specific file",
   "id": "a640959c50fa8e63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T00:02:54.076441Z",
     "start_time": "2025-01-30T00:02:53.722592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# fs.delete(image_data._id)\n",
    "# image_data._id"
   ],
   "id": "b2454c99fcef187a",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T05:04:42.942922Z",
     "start_time": "2025-02-04T05:04:42.910890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "if len(image_data_list)>0:\n",
    "    for j in image_data_list:\n",
    "        if j:\n",
    "            # Convert binary data to a PIL Image\n",
    "            image = Image.open(io.BytesIO(j.read()))\n",
    "            \n",
    "            # Display the image\n",
    "            image.show()\n",
    "        else:\n",
    "            print(\"❌ Image not found\")"
   ],
   "id": "e05e5d96b152baf5",
   "outputs": [
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file <_io.BytesIO object at 0x1e4698630>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUnidentifiedImageError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[86], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m image_data_list:\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m j:\n\u001B[1;32m      7\u001B[0m         \u001B[38;5;66;03m# Convert binary data to a PIL Image\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m         image \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mBytesIO\u001B[49m\u001B[43m(\u001B[49m\u001B[43mj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m         \u001B[38;5;66;03m# Display the image\u001B[39;00m\n\u001B[1;32m     11\u001B[0m         image\u001B[38;5;241m.\u001B[39mshow()\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject4/.venv/lib/python3.10/site-packages/PIL/Image.py:3532\u001B[0m, in \u001B[0;36mopen\u001B[0;34m(fp, mode, formats)\u001B[0m\n\u001B[1;32m   3530\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message)\n\u001B[1;32m   3531\u001B[0m msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcannot identify image file \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (filename \u001B[38;5;28;01mif\u001B[39;00m filename \u001B[38;5;28;01melse\u001B[39;00m fp)\n\u001B[0;32m-> 3532\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m UnidentifiedImageError(msg)\n",
      "\u001B[0;31mUnidentifiedImageError\u001B[0m: cannot identify image file <_io.BytesIO object at 0x1e4698630>"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T23:43:36.015709Z",
     "start_time": "2025-01-29T23:43:36.012340Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7ada4e576e3e9832",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedWriter name='retrieved_image.png'>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "break\n",
    "---"
   ],
   "id": "1e015c8e9a01b01e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Embedding text and image at the same time",
   "id": "a025521686269183"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T17:47:52.463911Z",
     "start_time": "2025-02-05T17:47:51.098901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "images = []\n",
    "count = 0\n",
    "if len(image_data_list)>0:\n",
    "    for j in image_data_list:\n",
    "        if j:\n",
    "            print(count)\n",
    "            # Convert binary data to a PIL Image\n",
    "            image = Image.open(io.BytesIO(j.read()))\n",
    "            images.append(image)\n",
    "            count += 1\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ Image not found\")"
   ],
   "id": "7bc4b26c3f91917e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T05:48:21.885692Z",
     "start_time": "2025-02-04T05:48:21.474404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "#image = Image.open(\"/Users/dorukozar/PycharmProjects/pythonProject4/clustering ss.png\")\n",
    "\n",
    "\n",
    "\n",
    "inputs = clip_processor(text=user_query, images=images, return_tensors=\"pt\", padding=True)\n",
    "outputs = clip_model(**inputs)\n",
    "\n",
    "image_embeds = outputs.image_embeds  # Image embeddings\n",
    "text_embeds = outputs.text_embeds  # Text embeddings"
   ],
   "id": "1829910f6227c662",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T17:48:37.019361Z",
     "start_time": "2025-02-05T17:48:01.462993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor, AutoTokenizer\n",
    "\n",
    "# Specify the model\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "# Load the model and set it to MPS (Apple Silicon GPU)\n",
    "device = torch.device(\"mps\")  # Set the device to MPS\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,  # Use float16 precision\n",
    "    device_map=None  # Explicitly manage the device\n",
    ")\n",
    "model.to(device)  # Move the model to the MPS device\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.model_max_length = 2048  # Increase from 77 to 2048\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "702c3d9ce15b47c5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8cca66bf968441029118ece43082edfa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T17:51:09.111729Z",
     "start_time": "2025-02-05T17:51:09.107362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = f\"\"\"You are an expert LLM assistant specialized in answering questions related to computer science/data science/machine learning/LLM. Use the retrieved information from RAG (Retrieved information and Image Descriptions) and your knowledge to respond accurately and clearly to each question.\n",
    "\n",
    "Guidelines:\n",
    "1. Provide concise and informative answers.\n",
    "2. If the question is beyond the scope of your knowledge or the provided information, state, \"I don't know.\"\n",
    "3. If the context section (the information that is returned by RAG pipeline) has no information about a part of the question, please express that \"The retrieved information did not contain answer to this question\" but if you can answer it based on your knowledge please do\n",
    "4. Use examples where applicable to illustrate your answers.\n",
    "5. Maintain a professional and helpful tone.\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Retrieved Information: {format_docs(retrieved_docs)}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ],
   "id": "d6a6682fcf296401",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T17:51:21.248291Z",
     "start_time": "2025-02-05T17:51:19.150652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the processor\n",
    "processor = AutoProcessor.from_pretrained(model_id, tokenizer=tokenizer)\n",
    "\n",
    "# Load and preprocess the image\n",
    "#img_path = \"/Users/dorukozar/PycharmProjects/pythonProject4/clustering ss.png\"\n",
    "#image = Image.open(img_path)\n",
    "\n",
    "# Prepare the messages\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": [{\"type\": \"image\"} for _ in images] + [{\"type\": \"text\", \"text\": \"describe what these images are trying to explain\"}]}\n",
    "# ]\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"image\"} for _ in images] + [{\"type\": \"text\", \"text\": prompt}]}\n",
    "]\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "# Preprocess the inputs\n",
    "inputs = processor(\n",
    "    images,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)  # Move inputs to MPS device"
   ],
   "id": "c10d1f7025c99c43",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T18:56:33.881082Z",
     "start_time": "2025-02-05T17:52:14.312446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "# Generate output\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=10000)\n",
    "\n",
    "# Decode and print the result\n",
    "decoded_response = processor.decode(output[0])\n",
    "print(decoded_response)"
   ],
   "id": "8afba196b4e8b3b7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dorukozar/PycharmProjects/pythonProject4/.venv/lib/python3.10/site-packages/transformers/pytorch_utils.py:332: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T22:47:51.233618Z",
     "start_time": "2025-02-04T22:47:51.230989Z"
    }
   },
   "cell_type": "code",
   "source": "print(decoded_response)",
   "id": "cd493e60e9c5df84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|><|image|><|image|><|image|><|image|>You are an expert LLM assistant specialized in answering questions related to computer science/data science/machine learning/LLM. Use the retrieved information from RAG (Retrieved information and Image Descriptions) and your knowledge to respond accurately and clearly to each question.\n",
      "\n",
      "Guidelines:\n",
      "1. Provide concise and informative answers.\n",
      "2. If the question is beyond the scope of your knowledge or the provided information, state, \"I don't know.\"\n",
      "3. If the context section (the information that is returned by RAG pipeline) has no information about a part of the question, please express that \"The retrieved information did not contain answer to this question\" but if you can answer it based on your knowledge please do\n",
      "4. Use examples where applicable to illustrate your answers.\n",
      "5. Maintain a professional and helpful tone.\n",
      "\n",
      "Question: What is supervised and unsupervised machine learning?\n",
      "\n",
      "Retrieved Information: Machine Learning Overview, EDA and Clustering\n",
      "Brian Wright\n",
      "brianwright@virginia.edu1. What is Machine Learning?\n",
      "2. What is exploratory data analysis?\n",
      "3. k-means clustering\n",
      "– Does Congress vote in patterns?\n",
      "4. Multi-dimensional k-means clustering\n",
      "– Are NBA players compensated according to performance?\n",
      "Outline: Intro to Unsupervised ML\n",
      "2“A field of Computer Science that gives computers the ability to learn\n",
      "without being explicitly programmed.”\n",
      "-\n",
      "Arthur Samuel (Coined the term in 1959 at IBM)\n",
      "“The ability [for systems] to acquire their own knowledge, by\n",
      "extracting patterns from raw data.”\n",
      "-\n",
      "Deep Learning, Goodfellow et alMachine vs. human\n",
      "Machine\n",
      "Human\n",
      "Understanding context\n",
      "✔\n",
      "Thinking through the problem\n",
      "✔\n",
      "Asking the right questions\n",
      "✔\n",
      "Selecting the right tools\n",
      "✔\n",
      "Performing calculations quickly\n",
      "✔\n",
      "Performing repetitive tasks\n",
      "✔\n",
      "Following pre-defined rules\n",
      "✔\n",
      "Interpreting results\n",
      "✔5Pattern discovery when inputs (x) and outputs (y) are known\n",
      "Supervised machine learning\n",
      "Input x:\n",
      "Voter\n",
      "Output y:\n",
      "\n",
      "without being explicitly programmed.”\n",
      "-\n",
      "Arthur Samuel (Coined the term in 1959 at IBM)\n",
      "“The ability [for systems] to acquire their own knowledge, by\n",
      "extracting patterns from raw data.”\n",
      "-\n",
      "Deep Learning, Goodfellow et al\n",
      "“A computer program is said to learn from experience E with respect\n",
      "to some set of tasks T and performance measure P if its performance\n",
      "tasks in T, as measured by P, improves with experience E.”\n",
      "- Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs. human\n",
      "Machine\n",
      "Human\n",
      "Understanding context\n",
      "✔\n",
      "Thinking through the problem\n",
      "✔\n",
      "Asking the right questions\n",
      "✔\n",
      "Selecting the right tools\n",
      "✔\n",
      "Performing calculations quickly\n",
      "✔\n",
      "Performing repetitive tasks\n",
      "✔\n",
      "Following pre-defined rules\n",
      "✔\n",
      "Interpreting results\n",
      "✔8Pattern discovery when inputs (x) and outputs (y) are known\n",
      "Supervised machine learning\n",
      "Input x:\n",
      "Voter\n",
      "Output y:\n",
      "Political \n",
      "affiliation\n",
      "Examples: Classification and regression are supervised machine learning \n",
      "9The data inputs (x) have no target outputs (y)\n",
      "\n",
      "without being explicitly programmed.”\n",
      "-\n",
      "Arthur Samuel (Coined the term in 1959 at IBM)\n",
      "“The ability [for systems] to acquire their own knowledge, by\n",
      "extracting patterns from raw data.”\n",
      "-\n",
      "Deep Learning, Goodfellow et al\n",
      "“A computer program is said to learn from experience E with respect\n",
      "to some set of tasks T and performance measure P if its performance\n",
      "tasks in T, as measured by P, improves with experience E.”\n",
      "- Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs. human\n",
      "Machine\n",
      "Human\n",
      "Understanding context\n",
      "✔\n",
      "Thinking through the problem\n",
      "✔\n",
      "Asking the right questions\n",
      "✔\n",
      "Selecting the right tools\n",
      "✔\n",
      "Performing calculations quickly\n",
      "✔\n",
      "Performing repetitive tasks\n",
      "✔\n",
      "Following pre-defined rules\n",
      "✔\n",
      "Interpreting results\n",
      "✔8Pattern discovery when inputs (x) and outputs (y) are known\n",
      "Supervised machine learning\n",
      "Input x:\n",
      "Voter\n",
      "Output y:\n",
      "Political \n",
      "affiliation\n",
      "Examples: Classification and regression are supervised machine learning \n",
      "9The data inputs (x) have no target outputs (y)\n",
      "\n",
      "without being explicitly programmed.”\n",
      "-\n",
      "Arthur Samuel (Coined the term in 1959 at IBM)\n",
      "“The ability [for systems] to acquire their own knowledge, by\n",
      "extracting patterns from raw data.”\n",
      "-\n",
      "Deep Learning, Goodfellow et al\n",
      "“A computer program is said to learn from experience E with respect\n",
      "to some set of tasks T and performance measure P if its performance\n",
      "tasks in T, as measured by P, improves with experience E.”\n",
      "- Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs. human\n",
      "Machine\n",
      "Human\n",
      "Understanding context\n",
      "✔\n",
      "Thinking through the problem\n",
      "✔\n",
      "Asking the right questions\n",
      "✔\n",
      "Selecting the right tools\n",
      "✔\n",
      "Performing calculations quickly\n",
      "✔\n",
      "Performing repetitive tasks\n",
      "✔\n",
      "Following pre-defined rules\n",
      "✔\n",
      "Interpreting results\n",
      "✔8Pattern discovery when inputs (x) and outputs (y) are known\n",
      "Supervised machine learning\n",
      "Input x:\n",
      "Voter\n",
      "Output y:\n",
      "Political \n",
      "affiliation\n",
      "Examples: Classification and regression are supervised machine learning \n",
      "9The data inputs (x) have no target outputs (y)\n",
      "\n",
      "without being explicitly programmed.”\n",
      "-\n",
      "Arthur Samuel (Coined the term in 1959 at IBM)\n",
      "“The ability [for systems] to acquire their own knowledge, by\n",
      "extracting patterns from raw data.”\n",
      "-\n",
      "Deep Learning, Goodfellow et al\n",
      "“A computer program is said to learn from experience E with respect\n",
      "to some set of tasks T and performance measure P if its performance\n",
      "tasks in T, as measured by P, improves with experience E.”\n",
      "- Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs. human\n",
      "Machine\n",
      "Human\n",
      "Understanding context\n",
      "✔\n",
      "Thinking through the problem\n",
      "✔\n",
      "Asking the right questions\n",
      "✔\n",
      "Selecting the right tools\n",
      "✔\n",
      "Performing calculations quickly\n",
      "✔\n",
      "Performing repetitive tasks\n",
      "✔\n",
      "Following pre-defined rules\n",
      "✔\n",
      "Interpreting results\n",
      "✔8Pattern discovery when inputs (x) and outputs (y) are known\n",
      "Supervised machine learning\n",
      "Input x:\n",
      "Voter\n",
      "Output y:\n",
      "Political \n",
      "affiliation\n",
      "Examples: Classification and regression are supervised machine learning \n",
      "9The data inputs (x) have no target outputs (y)\n",
      "\n",
      "Answer:\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "**Supervised Machine Learning**\n",
      "\n",
      "Supervised machine learning is a type of machine learning where the algorithm is trained on labeled data, meaning the data is already tagged with the correct output. The goal is to learn a mapping between the input data and the output labels, so that the algorithm can make predictions on new, unseen data.\n",
      "\n",
      "**Unsupervised Machine Learning**\n",
      "\n",
      "Unsupervised machine learning is a type of machine learning where the algorithm is trained on unlabeled data, meaning the data does not have any target outputs. The goal is to identify patterns, relationships, or structures in the data that can be used to make predictions or recommendations.\n",
      "\n",
      "**Classification and Regression**\n",
      "\n",
      "Classification and regression are two common types of supervised machine learning tasks. Classification involves predicting a categorical output, such as spam/not spam emails, while regression involves predicting a continuous output, such as the price of a house.\n",
      "\n",
      "**Clustering and Association**\n",
      "\n",
      "Clustering and association are two common types of unsupervised machine learning tasks. Clustering involves grouping similar data points together, while association involves identifying relationships between different variables in the data.\n",
      "\n",
      "**Dimension Reduction**\n",
      "\n",
      "Dimension reduction is a technique used in both supervised and unsupervised machine learning to reduce the number of features in a dataset while preserving the most important information. This can help improve the performance of machine learning models and reduce the risk of overfitting.\n",
      "\n",
      "**Summary**\n",
      "\n",
      "In summary, supervised machine learning involves training an algorithm on labeled data to make predictions, while unsupervised machine learning involves identifying patterns and relationships in unlabeled data. Classification and regression are common supervised tasks, while clustering and association are common unsupervised tasks. Dimension reduction is a technique used to reduce the number of features in a dataset while preserving the most important information.<|eot_id|>\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T16:27:22.656039Z",
     "start_time": "2025-02-04T16:27:22.648306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = f\"\"\"You are an expert LLM assistant specialized in answering questions related to computer science/data science/machine learning/LLM. Use the retrieved information from RAG (Retrieved information and Image Descriptions) and your knowledge to respond accurately and clearly to each question.\n",
    "\n",
    "Guidelines:\n",
    "1. Provide concise and informative answers that (mostly undergrad) students can understand.\n",
    "2. If the question is beyond the scope of your knowledge or the provided information, state, \"I don't know.\"\n",
    "3. If the context section (the information that is returned by RAG pipeline) has no information about a part of the question, please express that \"The retrieved information did not contain answer to this question\" but if you can answer it based on your knowledge please do\n",
    "4. Use examples where applicable to illustrate your answers.\n",
    "5. Maintain a professional and helpful tone.\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Retrieved Information: {format_docs(retrieved_docs)}\n",
    "\n",
    "Image Descriptions: {decoded_response}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "#prompt = template.format(question = user_query, context =  format_docs(retrieved_docs))\n",
    "print(prompt)"
   ],
   "id": "6f9bd2080c45122c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert LLM assistant specialized in answering questions related to computer science/data science/machine learning/LLM. Use the retrieved information from RAG (Retrieved information and Image Descriptions) and your knowledge to respond accurately and clearly to each question.\n",
      "\n",
      "Guidelines:\n",
      "1. Provide concise and informative answers.\n",
      "2. If the question is beyond the scope of your knowledge or the provided information, state, \"I don't know.\"\n",
      "3. If the context section (the information that is returned by RAG pipeline) has no information about a part of the question, please express that \"The retrieved information did not contain answer to this question\" but if you can answer it based on your knowledge please do\n",
      "4. Use examples where applicable to illustrate your answers.\n",
      "5. Maintain a professional and helpful tone.\n",
      "\n",
      "Question: What is supervised and unsupervised machine learning?\n",
      "\n",
      "Retrieved Information: Machine Learning Overview, EDA and Clustering\n",
      "Brian Wright\n",
      "brianwright@virginia.edu1. What is Machine Learning ?\n",
      "2. What is exploratory data analysis?\n",
      "3. k-means clustering\n",
      "– Does Congress vote in patterns?\n",
      "4. Multi-dimensional k-means clustering\n",
      "– Are NBA players compensated according to performance?\n",
      "Outline: Intro to Unsupervised ML\n",
      "2“A field of Computer Science that gives computers the ability to learn\n",
      "without being explicitly programmed.”\n",
      "-\n",
      "Arthur Samuel (Coined the term in 1959 at IBM)\n",
      "“The ability [for systems] to acquire their own knowledge, by\n",
      "extracting patterns from raw data.”\n",
      "-\n",
      "Deep Learning, Goodfellow et alMachine vs. human\n",
      "Machine\n",
      "Human\n",
      "Understanding context\n",
      "✔\n",
      "Thinking through the problem\n",
      "✔\n",
      "Asking the right questions\n",
      "✔\n",
      "Selecting the right tools\n",
      "✔\n",
      "Performing calculations quickly\n",
      "✔\n",
      "Performing repetitive tasks\n",
      "✔\n",
      "Following pre-defined rules\n",
      "✔\n",
      "Interpreting results\n",
      "✔5Pattern discovery when inputs (x) and outputs (y) are known\n",
      "Supervised machine learning\n",
      "Input x:\n",
      "Voter\n",
      "Output y:\n",
      "\n",
      "without being explicitly programmed.”\n",
      "-\n",
      "Arthur Samuel (Coined the term in 1959 at IBM)\n",
      "“The ability [for systems] to acquire their own knowledge, by\n",
      "extracting patterns from raw data.”\n",
      "-\n",
      "Deep Learning, Goodfellow et al\n",
      "“A computer program is said to learn from experience E with respect\n",
      "to some set of tasks T and performance measure P if its performance\n",
      "tasks in T, as measured by P, improves with experience E.”\n",
      "- Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs. human\n",
      "Machine\n",
      "Human\n",
      "Understanding context\n",
      "✔\n",
      "Thinking through the problem\n",
      "✔\n",
      "Asking the right questions\n",
      "✔\n",
      "Selecting the right tools\n",
      "✔\n",
      "Performing calculations quickly\n",
      "✔\n",
      "Performing repetitive tasks\n",
      "✔\n",
      "Following pre-defined rules\n",
      "✔\n",
      "Interpreting results\n",
      "✔8Pattern discovery when inputs (x) and outputs (y) are known\n",
      "Supervised machine learning\n",
      "Input x:\n",
      "Voter\n",
      "Output y:\n",
      "Political \n",
      "affiliation\n",
      "Examples: Classification and regression are supervised machine learning \n",
      "9The data inputs (x) have no target outputs (y)\n",
      "\n",
      "without being explicitly programmed.”\n",
      "-\n",
      "Arthur Samuel (Coined the term in 1959 at IBM)\n",
      "“The ability [for systems] to acquire their own knowledge, by\n",
      "extracting patterns from raw data.”\n",
      "-\n",
      "Deep Learning, Goodfellow et al\n",
      "“A computer program is said to learn from experience E with respect\n",
      "to some set of tasks T and performance measure P if its performance\n",
      "tasks in T, as measured by P, improves with experience E.”\n",
      "- Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs. human\n",
      "Machine\n",
      "Human\n",
      "Understanding context\n",
      "✔\n",
      "Thinking through the problem\n",
      "✔\n",
      "Asking the right questions\n",
      "✔\n",
      "Selecting the right tools\n",
      "✔\n",
      "Performing calculations quickly\n",
      "✔\n",
      "Performing repetitive tasks\n",
      "✔\n",
      "Following pre-defined rules\n",
      "✔\n",
      "Interpreting results\n",
      "✔8Pattern discovery when inputs (x) and outputs (y) are known\n",
      "Supervised machine learning\n",
      "Input x:\n",
      "Voter\n",
      "Output y:\n",
      "Political \n",
      "affiliation\n",
      "Examples: Classification and regression are supervised machine learning \n",
      "9The data inputs (x) have no target outputs (y)\n",
      "\n",
      "without being explicitly programmed.”\n",
      "-\n",
      "Arthur Samuel (Coined the term in 1959 at IBM)\n",
      "“The ability [for systems] to acquire their own knowledge, by\n",
      "extracting patterns from raw data.”\n",
      "-\n",
      "Deep Learning, Goodfellow et al\n",
      "“A computer program is said to learn from experience E with respect\n",
      "to some set of tasks T and performance measure P if its performance\n",
      "tasks in T, as measured by P, improves with experience E.”\n",
      "- Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs. human\n",
      "Machine\n",
      "Human\n",
      "Understanding context\n",
      "✔\n",
      "Thinking through the problem\n",
      "✔\n",
      "Asking the right questions\n",
      "✔\n",
      "Selecting the right tools\n",
      "✔\n",
      "Performing calculations quickly\n",
      "✔\n",
      "Performing repetitive tasks\n",
      "✔\n",
      "Following pre-defined rules\n",
      "✔\n",
      "Interpreting results\n",
      "✔8Pattern discovery when inputs (x) and outputs (y) are known\n",
      "Supervised machine learning\n",
      "Input x:\n",
      "Voter\n",
      "Output y:\n",
      "Political \n",
      "affiliation\n",
      "Examples: Classification and regression are supervised machine learning \n",
      "9The data inputs (x) have no target outputs (y)\n",
      "\n",
      "without being explicitly programmed.”\n",
      "-\n",
      "Arthur Samuel (Coined the term in 1959 at IBM)\n",
      "“The ability [for systems] to acquire their own knowledge, by\n",
      "extracting patterns from raw data.”\n",
      "-\n",
      "Deep Learning, Goodfellow et al\n",
      "“A computer program is said to learn from experience E with respect\n",
      "to some set of tasks T and performance measure P if its performance\n",
      "tasks in T, as measured by P, improves with experience E.”\n",
      "- Tom Mitchell (Computer Scientist & Professor at Carnegie Mellon)Machine vs. human\n",
      "Machine\n",
      "Human\n",
      "Understanding context\n",
      "✔\n",
      "Thinking through the problem\n",
      "✔\n",
      "Asking the right questions\n",
      "✔\n",
      "Selecting the right tools\n",
      "✔\n",
      "Performing calculations quickly\n",
      "✔\n",
      "Performing repetitive tasks\n",
      "✔\n",
      "Following pre-defined rules\n",
      "✔\n",
      "Interpreting results\n",
      "✔8Pattern discovery when inputs (x) and outputs (y) are known\n",
      "Supervised machine learning\n",
      "Input x:\n",
      "Voter\n",
      "Output y:\n",
      "Political \n",
      "affiliation\n",
      "Examples: Classification and regression are supervised machine learning \n",
      "9The data inputs (x) have no target outputs (y)\n",
      "\n",
      "Image Descriptions: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|><|image|><|image|><|image|><|image|>describe what these images are trying to explain<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The image presents a comprehensive overview of classical machine learning, categorizing it into two primary branches: Supervised and Unsupervised learning. The Supervised learning branch is further divided into Classification and Regression, while the Unsupervised learning branch is divided into Clustering and Association.\n",
      "\n",
      "**Supervised Learning**\n",
      "\n",
      "*   **Classification**: This involves predicting a category or label based on input data. It can be pre-categorized or numerical.\n",
      "*   **Regression**: This involves predicting a numerical value based on input data. It can be pre-categorized or numerical.\n",
      "\n",
      "**Unsupervised Learning**\n",
      "\n",
      "*   **Clustering**: This involves dividing data by similarity. It is used to find hidden dependencies and split up similar clothing into stacks.\n",
      "*   **Association**: This involves finding what clothes I often wear together. It is used to make the best outfits from the given clothes.\n",
      "\n",
      "**Key Takeaways**\n",
      "\n",
      "*   Classical machine learning encompasses both supervised and unsupervised learning approaches.\n",
      "*   Supervised learning is further divided into classification and regression tasks.\n",
      "*   Unsupervised learning is further divided into clustering and association tasks.\n",
      "*   These categories provide a framework for understanding the different types of machine learning problems and their applications.<|eot_id|>\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T16:27:53.886322Z",
     "start_time": "2025-02-04T16:27:38.458026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ollama\n",
    "desired_model = \"deepseek-r1:7b\"\n",
    "\n",
    "response = ollama.chat(model=desired_model, messages=[\n",
    "    {\n",
    "        'role':'user',\n",
    "        'content':prompt,\n",
    "    },\n",
    "])\n",
    "\n",
    "\n",
    "ollama_response = response['message']['content']\n",
    "\n",
    "print(ollama_response)"
   ],
   "id": "b9253f6a936eac26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to understand what supervised and unsupervised machine learning are. From what I gather from the information provided, supervised learning involves models that learn from labeled data, where both input features (x) and corresponding outputs (y) are known. This is used for tasks like classification and regression. The examples given were predicting a voter's political affiliation based on their voting history.\n",
      "\n",
      "Unsupervised learning, on the other hand, deals with unlabeled data. Here, the model tries to find patterns or inherent groupings in the input data without knowing the corresponding outputs. Clustering is mentioned as an example, such as grouping customers by purchasing behavior without any prior information about their preferences.\n",
      "\n",
      "Looking at the image description, it confirms this by showing Supervised Learning split into Classification and Regression, while Unsupervised Learning includes Clustering and Association. Association seems to involve finding relationships between variables, like which products are often bought together.\n",
      "\n",
      "I think the key takeaway is that supervised learning uses labeled data for prediction tasks, whereas unsupervised uses unlabeled data to find hidden structures or groupings. Both have their own specific applications depending on the type of data and the problem at hand.\n",
      "</think>\n",
      "\n",
      "**Supervised Machine Learning:**\n",
      "\n",
      "- **Definition:** Involves training models using labeled data, where both input features (x) and corresponding outputs (y) are known.\n",
      "- **Tasks:** Includes classification (predicting categories) and regression (predicting numerical values).\n",
      "- **Example:** Predicting a voter's political affiliation based on their voting history.\n",
      "\n",
      "**Unsupervised Machine Learning:**\n",
      "\n",
      "- **Definition:** Deals with unlabeled data, aiming to find patterns or groupings without knowing the outputs.\n",
      "- **Tasks:** Includes clustering (grouping similar data) and association (finding relationships between variables).\n",
      "- **Example:** Grouping customers by purchasing behavior or identifying which products are often bought together.\n",
      "\n",
      "**Key Takeaways:**\n",
      "\n",
      "- Supervised learning is used for prediction tasks with labeled datasets, while unsupervised learning discovers hidden structures in unlabeled data.\n",
      "- Both approaches have distinct applications depending on the nature of the data and the problem being addressed.\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3102a5ded49114aa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
