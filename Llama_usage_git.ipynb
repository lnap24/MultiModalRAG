{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00082e633fd44ace86d28ea5221ae9f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_81b56db5286e4c6ab2d5a9cfcdd5008e",
       "IPY_MODEL_a43df4dcd6304e30bc72fae70245867c",
       "IPY_MODEL_ef371492bb60440db10b9bfaf1dae134"
      ],
      "layout": "IPY_MODEL_14ebd79b9c024bc68d6a15bb552679d5"
     }
    },
    "81b56db5286e4c6ab2d5a9cfcdd5008e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6307ae353cdc4500868d0156ba5086de",
      "placeholder": "​",
      "style": "IPY_MODEL_900c4cd8ab774a81b93b1acbffb87746",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "a43df4dcd6304e30bc72fae70245867c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc32a295040344ab9e6fb15158a2420e",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2549b3ffbed04c5d84a736049ab7283d",
      "value": 2
     }
    },
    "ef371492bb60440db10b9bfaf1dae134": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33bbb0d99e6744d1a34b89b298401ded",
      "placeholder": "​",
      "style": "IPY_MODEL_157f60b446e04c39a734edca8437f181",
      "value": " 2/2 [00:00&lt;00:00,  3.26it/s]"
     }
    },
    "14ebd79b9c024bc68d6a15bb552679d5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6307ae353cdc4500868d0156ba5086de": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "900c4cd8ab774a81b93b1acbffb87746": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc32a295040344ab9e6fb15158a2420e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2549b3ffbed04c5d84a736049ab7283d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "33bbb0d99e6744d1a34b89b298401ded": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "157f60b446e04c39a734edca8437f181": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T17:18:44.463077Z",
     "start_time": "2025-01-29T17:18:44.453726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"hugging_face_key\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "from huggingface_hub import whoami\n",
    "\n",
    "login(api_key)\n",
    "whoami()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-UQ9RJFh31_",
    "outputId": "0eaf659c-4362-4f19-cef2-c870c8e7c06b",
    "ExecuteTime": {
     "end_time": "2025-01-29T17:18:55.498885Z",
     "start_time": "2025-01-29T17:18:55.131294Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'user',\n",
       " 'id': '66d5147ab005ad82ca47182f',\n",
       " 'name': 'dorukozar',\n",
       " 'fullname': 'Doruk Ozar',\n",
       " 'email': 'dorukozar@gmail.com',\n",
       " 'emailVerified': True,\n",
       " 'canPay': False,\n",
       " 'periodEnd': None,\n",
       " 'isPro': False,\n",
       " 'avatarUrl': '/avatars/06335824f9a6991ec7b901b31802dd5b.svg',\n",
       " 'orgs': [],\n",
       " 'auth': {'type': 'access_token',\n",
       "  'accessToken': {'displayName': 'Presentation',\n",
       "   'role': 'read',\n",
       "   'createdAt': '2025-01-16T00:00:59.134Z'}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading the file"
   ],
   "metadata": {
    "id": "W9oS7BUJEgPk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "file_path = 'Week7-lecture.txt'\n",
    "file_path2 = 'Week9-lecture.txt'\n",
    "\n",
    "def file_reader(path):\n",
    "  result = \"\"\n",
    "\n",
    "  with open(path, 'r') as file:\n",
    "      for index, line in enumerate(file):\n",
    "          if index % 2 == 0:  # Only process even-indexed lines (0-based)\n",
    "              #print(line.strip())  # Process the line (e.g., print it)\n",
    "              result += line.strip() + \"\\n\"\n",
    "\n",
    "  return result\n",
    "\n",
    "week7 = file_reader(file_path)\n",
    "week9 = file_reader(file_path2)\n",
    "# print(week9)"
   ],
   "metadata": {
    "id": "TsjLUhLkjh99",
    "ExecuteTime": {
     "end_time": "2025-01-29T17:18:57.749933Z",
     "start_time": "2025-01-29T17:18:57.743271Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Splitting the text into Chunks"
   ],
   "metadata": {
    "id": "GBCL1qcPEo0w"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_text(txt):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "      chunk_size=1000,  # Define the maximum chunk size\n",
    "      chunk_overlap=100  # Define the overlap between chunks\n",
    "  )\n",
    "\n",
    "  chunks = text_splitter.split_text(txt)\n",
    "  return chunks\n",
    "\n",
    "week7_chunks = split_text(week7)\n",
    "week9_chunks = split_text(week9)\n",
    "\n",
    "for i, chunk in enumerate(week9_chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n{'-'*80}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Eo-_p4Dmqqz",
    "outputId": "ff98cdc1-de93-4474-9c01-46ff33e94fa8",
    "ExecuteTime": {
     "end_time": "2025-01-29T17:19:00.706604Z",
     "start_time": "2025-01-29T17:19:00.443872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "okay all right so let me pop this up here\n",
      "bigger i can do that there we go all right so we've been talking about decision trees today all right we'll build a\n",
      "little bit maybe towards ensemble models towards the end but principally we're just going to talk about decision trees\n",
      "we're going to talk about how they're made or give you some theoretical background related to uh rates of how they are how constructed\n",
      "and how they're used they can be used for both regression and classification we're principally going to talk about classification\n",
      "at this point which is the primary use case for them and then um i'll talk about some of the weaknesses and strengths generally all right so\n",
      "let's just jump right into it here all right okay so this is what a little\n",
      "ahead of myself trying to shrink my there we go okay okay\n",
      "so uh i said all of that already so all right so let's talk right away\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 2:\n",
      "so uh i said all of that already so all right so let's talk right away\n",
      "about uh about the way that trees work okay so when we think about them they're kind of this you know this upside down triangle\n",
      "basically they kind of build out this way and they're constructed largely of these uh you know basically these\n",
      "four things right so we have a root node which is basically this top note up here in a decision tree they have these internal\n",
      "nodes which are basically these split nodes they come out like this all right so it has one edge\n",
      "and these edges are just basically the lines that are connecting to connecting the nodes okay and then they\n",
      "have a leaf node or terminal node kind of the same thing it has one node coming in one edge and then no outgoing edges\n",
      "basically you think about them as kind of at the bottom of the tree okay so this is our this is our terminal\n",
      "i'm sorry our root up here this is an internal node and then these are the leaf nodes and\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 3:\n",
      "i'm sorry our root up here this is an internal node and then these are the leaf nodes and\n",
      "then all of these things together have these edges that connect them all right hey it's really easy just to look at it this\n",
      "way and so here what we see this is our this is our root note up here all right this is the old classic iris\n",
      "iris data set that everybody just loves seeing over and over again so here we have the root nodes these are\n",
      "internals and then down here on the bottom we see the leaves all right so this is a\n",
      "matplotlib graphic actually coming out of python it\n",
      "does a bit better job and we'll talk about kind of this all the components of this as we move along so what's going on\n",
      "inside the boxes i don't have to totally understand but uh we'll take a look but this is\n",
      "also important note here is that right so a leaf node doesn't have to be all the way down on the bottom all right then we have one right here\n",
      "because we have no edges coming out of this basically what this is saying is that\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 4:\n",
      "because we have no edges coming out of this basically what this is saying is that\n",
      "petal width just to give you some context here pedal width does such a good job\n",
      "of classifying setosa right that's remember that's a type of flower\n",
      "that we have there does such a good job of classifying setosa there's no reason to move on as you can see here basically any any\n",
      "petal width that is less than or equal to 1.75\n",
      "all right is going to be fall into the subtosa class right essentially all right so let's\n",
      "keep moving all right so that's that's it at a very high level is what we think about when\n",
      "we we think about decision trees and again it's just like this a cyclical graph that kind of moves down all right but\n",
      "how do they make these choices so let's let's talk about that all right so the most important question so what\n",
      "is the most important question uh to move on to a second date right it seems to me\n",
      "like uh you know are you married is probably a more important criteria than uh hey\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 5:\n",
      "like uh you know are you married is probably a more important criteria than uh hey\n",
      "what's your favorite music you know if you don't know this one i guess it depends on your your\n",
      "your ethical standing but yeah in general you have to say to yourself what are these important questions that\n",
      "that you need to ask or need to be informed of in order to move on to a second date all right so\n",
      "when we think about that the question that the most amount of relevant information this is exactly how a decision tree works all\n",
      "right they're going to need they want to be able to figure out which variable gives it the most relevant information\n",
      "to be able to decide uh how to classify our target variable all right all right so\n",
      "you know depending on the condition of the first answer all right then you might select you know the next most important\n",
      "question for in terms of just information gain as maybe the question we're trying to work out here obviously is\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 6:\n",
      "you know whether we're going to move to a move to a second date all right so are you married all right if the answer\n",
      "to this question is yes hopefully most people would say whoa okay well check please\n",
      "all right so that that might be it all right if the answer here is yes is yes all right again you would stop\n",
      "all right but then you know the question would be like what is the next series of questions that you might ask\n",
      "all right you know belief in a blue colored sky you know maybe that's good because you could you know\n",
      "get a sense of whether someone is insane or not or you know maybe then you do move on to say you know do we have more things in common you\n",
      "know what are your favorite foods or music or whatever all right so if you if the answer was to\n",
      "no then maybe you move on to the next variable which talks about something that is just kind of more personal to see if\n",
      "you're you know if your likes and dislikes overlap okay\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 7:\n",
      "you're you know if your likes and dislikes overlap okay\n",
      "any question is like when should you stop asking questions all right so say you made it you passed the uh past the marriage\n",
      "hurdle and now you're on to what music do you like right and so\n",
      "here you know it might be that you know a certain person you know fifty percent will go on uh on\n",
      "a date if they believe that they like you know acoustic and then maybe this this is disco or something and so you know 50\n",
      "percent of people will go that so then you start getting very specific about likes and dislikes in terms of what type\n",
      "of music alright so you can imagine this would be just a factor level variable that could have all sorts of different types of\n",
      "classical and brock and you know the rap and\n",
      "lots of other types of music which apparently i'm having trouble naming but anyway so you think about that and\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 8:\n",
      "depending on the answer to that maybe people would have more information about whether their likes or dislikes overlapped right okay all right\n",
      "so this is basically how this how this works okay so uh it's fairly intuitive that's a big\n",
      "reason that people like decision trees because of this nature you can kind of explain it in simple terms about how they're used and\n",
      "how they make choices all right so we asked the question you know what is\n",
      "what is the variable that has the most important information in terms of the ones and zeros that we're trying to\n",
      "classify and here we're trying to classify whether someone is likely to go on a second date or not all right so we\n",
      "asked this first question about marriage all right and that results similar to what we saw in the flower\n",
      "example to maybe a perfect classification of whether they will go on to the second date or not right the majority of people would not\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 9:\n",
      "go on to a second date right knowing that someone was married but if the answer to this is no all right this would be no this would be\n",
      "yes and then up above here we basically would be you know\n",
      "second date is the variable we're trying to classify right so that would just be a list of\n",
      "zeros and ones right zero being no second date and one being yes\n",
      "okay excuse me and then we get more specific right based off the type of\n",
      "music that people are interested in there might be more of a disjointed result\n",
      "right okay so we asked the most important information and then conditioned on the first answer\n",
      "we select the next most important question and when the answer is no longer providing any additional information we\n",
      "stop growing the branch all right it looks like maybe this is a 50 50 split you know 50 50. something like that\n",
      "we kind of stopped growing the blanche all right and we just do this all right and we'll talk about stopping criteria it also has a lot to do with\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 10:\n",
      "the complexity perimeter things like that or hyper parameters that you can tune to keep it above a certain level\n",
      "all sorts of different things you just basically keep repeating these two steps over and over again until no more\n",
      "information can be gained with the variables that you have all right so you use all the variables that\n",
      "you have inside your inside your data set to try and gradually get to know better and better\n",
      "whether someone will go on a second date or not all right okay all right\n",
      "so decision trees are hierarchical all right what that means is basically um\n",
      "they're built just in the way that you know we've kind of been viewing them is that they you know they start start small and grow\n",
      "more complex all right decisions decisions are made until a predetermined metric is met and we will\n",
      "talk about what that metric might look like all right it could be you know a certain amount of\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 11:\n",
      "talk about what that metric might look like all right it could be you know a certain amount of\n",
      "terminal nodes all right predefined there's lots of different hyper parameters you could actually set a lot of people set the\n",
      "number of layers say if you have you know a huge amount of data at your disposal\n",
      "you know the more layers that are in a tree the more complex the decisions get all right all right and so\n",
      "the model is is built such that a sequence of ordered decisions concerning the value result and you know at the end\n",
      "the model inside the terminal nodes are the leaf nodes all right actually predicts the class label okay i'll give\n",
      "you a percentage likelihood of that as well so it does it has a probabilistic output similar to k n it's non-parametric\n",
      "uh it just means that you know the number of parameters is not determined all right as is the case with linear\n",
      "models you know you kind of know what the variables are going to be with this it's it's not always\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 12:\n",
      "models you know you kind of know what the variables are going to be with this it's it's not always\n",
      "predestined right you could have a couple of variables that are incredibly good at predicting these classes you know marriage might be one\n",
      "of them and so you actually wouldn't need that many to have a very high level of certainty or to hit some level of\n",
      "metric that you're targeting or it could be that there are you have a bunch of weak variables so you need to have a very complex tree to\n",
      "understand how people's decisions are are being made right and so we also don't need to worry about\n",
      "any assumptions being met about distributions or perimeters okay\n",
      "so there's no normality assumptions no standardization all right it's just a cyclical graph all\n",
      "right so it's used in model probabilities and causality you can use there are causal trees a bit more\n",
      "complicated to translate but trees consist of these nodes and edges and they're defined by this decision rule\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 13:\n",
      "and we're going to we're going to talk about what that rule is all right okay let's talk about the background a\n",
      "little bit all right so it uses recursive binary\n",
      "splitting which basically means that it it considers every possible partition space\n",
      "and then it divides the one based off uh it chooses the partition space basically chooses where to split\n",
      "a variable to locally optimize that information gain right so it does\n",
      "not consider the future of the tree doesn't consider the past it only tries to optimize that\n",
      "individual node those individual internal leaves okay all right\n",
      "and it will consider every possible partition uh available when doing that okay so it\n",
      "this is this is what we call kind of a greedy approach right it's kind of like localized optimality um optimality\n",
      "optimization inside that one internal node okay so it's going to try and make the best choice it can\n",
      "uh inside that internal node right so it's each step of the tree building\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 14:\n",
      "uh inside that internal node right so it's each step of the tree building\n",
      "process it picks the best the best split for that particular that particular split it does not consider\n",
      "the future all right all right so trees you know i talked about this they can be they can use be used for um prediction\n",
      "or classification but they use recursive binary splitting in both cases all right um the difference between\n",
      "regressive trees uh is that they're they're going to predict okay so classification trees are going\n",
      "to predict the actual class whereas regression is going to predict a particular number all right\n",
      "right okay so the probability measure um that derives this building for\n",
      "classification comes in two forms all right so we have these these two um measures which are\n",
      "generally used for are very typical it's either it's either a genie index or\n",
      "enthalpy genie index is actually a bit more common you saw that on the on the graph that we showed earlier with the iris data set it actually gave you\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 15:\n",
      "the genie index at each internal node but we're going to go through both of them they're very similar they're both\n",
      "related to basically to information gain which is the key component to kind of\n",
      "understand how decision trees work is this idea about that they're they're selecting the variable and how to split that\n",
      "up variable based off the idea that they are maximizing information gained locally at that particular node okay\n",
      "all right all right so the background here this is\n",
      "the cart algorithm all right um it's a classification and regression\n",
      "tree all right it was first uh introduced here by 1984 with these four\n",
      "researchers all right it can be used in numerical categorical data all right so the first or splits the\n",
      "training data into two subsets all right i guess we kind of know that a single feature all right it searches through all the possible features\n",
      "all right to identify the split that produces the purest subsets right so the the purest idea is that purity is\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 16:\n",
      "related to maximizing information gain okay all right so it stops once you can't find a pla a split all right that\n",
      "that reduces impurity right or by some other pre-predetermined hyper parameter\n",
      "all right so let me kind of explain a bit more as we go all right it\n",
      "all right so that's a bit of a theoretical background of those two papers it really just takes kind of this idea of information gain and puts it into\n",
      "practice so it's similar to those four steps that we talked about the card algorithm is basically those four steps\n",
      "in practice you know we're repeating steps two and three all right okay it does have uh some pretty\n",
      "some pretty weighty advantages right it is simple to understand and interpret through data visualization\n",
      "this is a single tree remember you just produce a single tree all right it doesn't require almost any data preparation at all all right the\n",
      "other techniques often require normalization or dummy variables need to be created and blank values need\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 17:\n",
      "to be removed you know trees have a lot of advantages that they do not have to do a lot of that all right so they're able to handle\n",
      "numerical categorical data all right it uses it what we call a white box bottle all right so you can actually see what's\n",
      "going on all right you can understand why it made the decisions it's made and uh it's very interpretable okay as\n",
      "compared to maybe more black box models some neural network approaches even though we're getting better at explainable ai things like that um\n",
      "and then even when you get up to ensemble models with random force they also become a little a little hard to\n",
      "understand it's pretty straight straightforward to evaluate i mean most of the models nowadays are not too bad you can use\n",
      "these the evaluation metrics that we went over um you know in you know last\n",
      "week in the week before adding you know edit at a pretty high rate uh pretty much for all of these but\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 18:\n",
      "they're also very usable here for tree based methods um okay okay it does have some\n",
      "limitations as all algorithms do so this is important to remember what they are otherwise uh\n",
      "we'll be using to use uh use these methods in the wrong circumstances all right they do have\n",
      "like a legitimate uh tendency to overfit you know if you're just growing a tree\n",
      "without any type of restrictions you're almost guaranteeing that it's going to overfit linear models are the same way\n",
      "so we have to control for those a bit all right so you have to think about you know if you have 100 you know if you hunt as an example if\n",
      "you have 100 data points right you have six levels you're gonna have 64 terminal nodes right if you only have 100 data points\n",
      "so you basically you don't have a lot of single a lot of single terminal nodes which means you're basically just memorizing\n",
      "the data they're not going to generalize well and this is again this is our bias variance trade-off we'd want to use hyperparameter two need to\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 19:\n",
      "pull that back up or maybe some other uh metrics that we'll talk about in particular this this this idea of this complexity\n",
      "parameter to set that to a certain level that minimizes the error associated with the predictions and the actual values\n",
      "and that would be the metric that we try and target so there's lots of ways to be able to control this so it has this problem\n",
      "we also can control it all right they can also be generally unstable to\n",
      "small variations in the data because of the nature of how they're how they're designed all right so\n",
      "this is why you know a single decision tree depending on how good or bad your data is can be very\n",
      "useful but the truth of the matter is that often people build towards these ensemble models which like\n",
      "range random forest which is just you know thousands hundreds or thousands of trees\n",
      "to be able to compensate for kind of this instability associated with a single tree\n",
      "all right all right so practical distribution tree algorithms are based off of you know\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 20:\n",
      "all right all right so practical distribution tree algorithms are based off of you know\n",
      "heuristic which is a greedy we talked about this they make local optimal decisions all right so they can't\n",
      "guarantee that you'll get like the global global optimum all right similar to uh some\n",
      "other methods like this is that it could be possible that you know you don't get the perfect tree which again is a reason to move\n",
      "potentially towards ensembles all right all right so the great uh bias trees if\n",
      "some classes dominate it is there for recommended that you try and balance the data set prior to\n",
      "fitting all right so if you have a heavily unbalanced data set and this is true of many of the classifiers we're going to be talking about it just doesn't perform\n",
      "quite as well all right support vector machines actually have a tendency to work a little bit better as an example for\n",
      "unbalanced data sets a single tree can can be can be can be problematic with data sets that are\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 21:\n",
      "unbalanced data sets a single tree can can be can be can be problematic with data sets that are\n",
      "quite unbalanced but again that can be cured by moving to kind of more of an ensemble approach\n",
      "but until we get there we have to understand what one tree is doing before we start building forests all right so let's let's take a look all\n",
      "right these mathematical approaches and i have some examples all right okay so decision trees uh use several\n",
      "types of node splitting criteria all right so these are these are those i've talked about these\n",
      "uh this is what it would be if we had you know kind of a continuous data problem we're trying to actually predict the value all right as\n",
      "compared to predict the class and these are the two common ones for classification they're really very\n",
      "similar all right we're going to take a look at we're actually not going to take a look at this one but we're going to take a look at each one of these\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 22:\n",
      "and they both use this idea of information gain to determine variable split criteria all\n",
      "right so let's what is this crazy information gain thing all right so this is enthropy\n",
      "all right all right so that's the that's the um\n",
      "that's the equation there all right so basically it's just this all right so it's the the probability\n",
      "times the log probability of a particular instance you're going to summarize these\n",
      "together all right so just let me let me walk you through this instead of detailing through the equation here\n",
      "all right so what we would see here okay so this is we have a total set of six all\n",
      "right so we're thinking about basically this is our equation in\n",
      "practice here just this portion of it so we're going to sum these two up and then we're going to subtract them from each other and then that's going to give us base\n",
      "the general enthalpy of these six dots\n",
      "all right and you can see here what we have is basically perfect classification all right so or maybe\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 23:\n",
      "where we have uh three and three all right so when we subtract these two things from each other\n",
      "right we just get kind of the net entropy associated with this example right we do the same thing here\n",
      "we have a little bit of more discontinuity right so we have four and two okay so we just plug and\n",
      "chug basically into our equation we're just doing the probability here which is two out of six\n",
      "all right and then probability here so four out of six we're just multiplying that by the\n",
      "log two of exactly that same ratio and then subtracting them from each other just to get an idea of the balance right\n",
      "here so what we see here right it's just a total you know kind of one class example obviously we we don't\n",
      "have to do the blue calculation because we know it's zero and so we just do the green side of it which is going to result in basically no\n",
      "information so you can so we can put these let's extend this a bit further and just kind of\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 24:\n",
      "information so you can so we can put these let's extend this a bit further and just kind of\n",
      "clumsy through that all right the idea here is that information gain is basically going to be right the\n",
      "entropy of the parent right which is what we did up here this is the enter fee think of this as the calculations of the\n",
      "parent all right this is the by the parent i mean before the split all right so this is just what our data looks like before we\n",
      "before we use any variable examples to be able to split it right\n",
      "and then it's going to be the minus the average entropy of the children right and the children\n",
      "are going to be what it looks like you know after some type of decision criteria\n",
      "has been made okay so you can think of this as up here what we have this is entropy the\n",
      "parent so we have kind of a perfect division here which is which is which is good maybe that's what we want um\n",
      "you could think of this as you know pluses maybe went on the date right so that's a it\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 25:\n",
      "you could think of this as you know pluses maybe went on the date right so that's a it\n",
      "went on a second date just to continue to extend this example\n",
      "and the dots all right did not\n",
      "okay and then we're gonna then we apply some type of question right so this might be i don't know this\n",
      "might be the marriage question right so\n",
      "all marriage just pretend i know how to spell all right and so and this would be maybe this is yes\n",
      "sorry this is a yes category some people are married all right so as a result um\n",
      "what we see here is that if the answer is yes all right that two of these uh\n",
      "two of these dots stop right there all right so that's an entropy it's a perfect classification relative to all right no date all right so see no\n",
      "date which makes sense all right so they yes all right the people the no date uh stop right there all right but but one\n",
      "person you know this is uh so no all right so despite that\n",
      "there is some uh all right blending of that and it could just be that you know\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 26:\n",
      "there is some uh all right blending of that and it could just be that you know\n",
      "whatever that might be is that there's there's no date here all right so what we see is\n",
      "that there are four out of the all right original six continue on\n",
      "and probably what we see here in this green dot is maybe there's some other reason why that second date didn't occur all right so we'd have to continue\n",
      "making splits but for this there's no there's no additional reason for us to continue here right because we have\n",
      "basically a terminal node okay so we've classified this perfectly\n",
      "associated with this one no date class and that's exactly what we talked about kind of in the beginning\n",
      "about how good a particular question would be but here all right we have some we have\n",
      "some blending right so we don't see a zero so we wouldn't stop we'd continue to go okay all right\n",
      "now in order to be able to calculate this and just erase some of this trying to put this in context all right\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 27:\n",
      "we're gonna take the average of these two numbers right so the average of 0.8\n",
      "0.0 all right and we're going to subtract that from the parent and that will tell us the quality\n",
      "essentially of this marriage question or it would give us the net effect in terms of information gain for this\n",
      "particular question at this particular partition all right so this is a is a yes no so\n",
      "there's just the partition is simple it's just did they get did are they married or not but if it's a continuous variable\n",
      "basically try uh every type of potential partition inside that range of\n",
      "values calculate the information gain to see where it makes the most sense which is the example we saw for um\n",
      "i guess it was pedal width right so at a certain numerical range 1.75\n",
      "or greater or was it less anyway at a certain threshold associated with that that distance for that pedal they\n",
      "had perfect classification for that flower so that's where that um split criteria occurred okay so it\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 28:\n",
      "shows all the different potential ranges inside that basically all the the the potential\n",
      "partitions inside that continuous variable that are represented in the data frame all right they're represented in that vector and chose\n",
      "that one as being the best because it provided almost perfect information for that particular flower all right so what we would do here this\n",
      "is a now it's a weighted average because it takes into consideration what the original\n",
      "split was right okay and so we just subtract uh we add these two things together all\n",
      "right and then we just subtract it from one and so we get 0.54\n",
      "all right okay so that would basically be our our net gain there associated with um\n",
      "associated with our uh information information gain all right so what we would do to finish\n",
      "this is what we see here is this .54 is the result of this weighted average remember that this is\n",
      "our calculation of our enter fee previously we just add these together\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 29:\n",
      "our calculation of our enter fee previously we just add these together\n",
      "0.54 then we would subtract that from one and then here we would say we have a general information gain\n",
      "of 0.46 okay all right so now keep in mind if it was\n",
      "perfect all right and this resulted in a perfect split as in the extreme examples which sometimes we'd like to refer to that\n",
      "this would basically just be zero right and then we'd have you know one minus zero we'd have a perfect information gain\n",
      "of one sorry about that all right and so we would we wouldn't have to move\n",
      "on any further we'd have a variable that perfectly classifies into date and no date as an example\n",
      "right but we don't have that all right it's not a perfect classifier it's somewhere in between it's perfect for this one side but there's still lots of information\n",
      "that could be gained uh and specific to to the um this is the you know when on a date\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 30:\n",
      "that could be gained uh and specific to to the um this is the you know when on a date\n",
      "category so we need to continue to continue to grow this tree out so we could better understand that particular\n",
      "side of the uh side of the equation okay so let's take a look at another\n",
      "another example here all right so what we see in order to start the tree right we need to follow again just these\n",
      "these basically these three steps right i keep giving you steps here but all right we choose a attribute with the\n",
      "highest information gain okay so we can check child nodes and then we repeat repeat steps one and two you know we're\n",
      "cursively into uh no more information can be gained all right\n",
      "so let's take a look at that all right so this is a pretty common example that gets used but what\n",
      "we're trying to do here is to predict you know should we play outside i think the original example here is like should\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 31:\n",
      "we play tennis outside i think are we going to be able to play tennis something like that all right so we have these three variables right this is our\n",
      "predictor right we're trying to predict um whether play happened outside or not and we have\n",
      "these variables to help us like the outlook you know the temperature and then the you know the humidity all in\n",
      "classification all right all in um factors right so let's take a look so this is our our\n",
      "original variable all right so when we look here yes and no all right that's our play so we have a perfect\n",
      "all right we've got a perfect information perfect entropy here here we had two that went and played and\n",
      "two that did not so that that's our parent node at one all right and then we're taking a look at this first\n",
      "variable here sunny so that resulted in one yes all right all right\n",
      "and uh two nodes two nodes all right so we have three sunny variables all\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 32:\n",
      "and uh two nodes two nodes all right so we have three sunny variables all\n",
      "right and then when we look across what we see is we have two nodes and one yes all right so it doesn't give\n",
      "us perfect information all right but it does give us some pretty good information that when it is sunny\n",
      "we have one yes and two nodes all right but then we have you know when it's cloudy which is also\n",
      "here what we see so we have one yes all right so we're trying to figure out\n",
      "play or not play which is at the top and the first variable that we took a look at all right we haven't determined\n",
      "if this is the best variable for us yet but the first one that we took a look at is outlook so we just did this\n",
      "combination again we see a zero here we see a 0.92 here and then we're just\n",
      "going to do this weighted average and then subtract it from our original\n",
      "original entropy of our target variable okay so this is the weighted average\n",
      "we're just going to do this 3x4 because remember we had three\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 33:\n",
      "we're just going to do this 3x4 because remember we had three\n",
      "three sunnies and one cloudy all right i'm just going to multiply that by the by our enthalpy gain here which is just\n",
      "this formula that we talked about earlier and we're going to add that to the other one which is going to be\n",
      "going to be 0 right okay and then we're going to subtract it by 1. so we're going to get all right 0.31\n",
      "okay so let's keep going let's evaluate let's do this same process and we'll look at some of the other variables\n",
      "all right so let's look at hot and cold all right so what we see here all right\n",
      "we see temperature if the temperature is hot right no play no play the temperature is cool\n",
      "it did play and they did play all right we're giving it away here but this is going to result in perfect classification all right so what we're\n",
      "going to see here all right is that the relative to the entropy uh that was seen in the target variable\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 34:\n",
      "when we do this weighted average obvious this is all going to be zero just like i said before it's going to result in a perfect\n",
      "perfect information gain okay so here this perfectly matches our\n",
      "outlier or perfectly matches our target variable whereas the example we had before didn't it\n",
      "wasn't quite a perfect split right we had one yes over here and we have one yes over here so when we calculate\n",
      "information gain it's not quite as good as when we do it here right we have perfect information gain\n",
      "all right so let's we're talking a we've seen an in-between example we've seen a perfect example so now let's look at a\n",
      "perfectly terrible example all right so here what we see is we have perfect disagreement okay\n",
      "regardless of whether it's high or low it looks like there was a decision to play and to not play all right so here we\n",
      "have one yes and one no on both sides of this high\n",
      "low equation so what we're going to see is when we plug this into our uh you know our enthalpy\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 35:\n",
      "low equation so what we're going to see is when we plug this into our uh you know our enthalpy\n",
      "equation and we subtract these from each other uh we're going to get one all right which which is basically means\n",
      "that you know we're not we don't learn anything all right the result of this is one restrict one from one\n",
      "our net information gain here is going to be absolutely nothing okay so if we were to build this\n",
      "classifier uh what we would see here this is this is very uncommon of course\n",
      "but what we see is that our we have this temperature oops\n",
      "temperature would be right at the top all right so that would be the first variable that we would use it provides\n",
      "us the most information and then the next one would be outlook i mean actually technically we wouldn't even move on to a next one because we\n",
      "have perfect classification here so we wouldn't even need another variable so in this in this case when you have perfect information gain\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 36:\n",
      "um you just have a you know a tree with the depth of just one split and that's it okay\n",
      "so that's enthropy all right so information gain is is very similar all right all right so it basically uses\n",
      "the same idea about net gain all right so pi in this equation just represents\n",
      "the probability that a random selection would have state i whether it would have you know you think it was whether\n",
      "someone would be would play or not all right and so the mathematical process this\n",
      "is pretty much the same i don't think i put the actual i thought i had the actual equation in here but\n",
      "uh anyway you kind of get the idea here so it's this is just the same all right this is the example that we saw before\n",
      "all right relative to what the outlook was like right so this\n",
      "is the outlook calculation it's this one right here okay so we use the same\n",
      "same general idea i'm just going to show it using the genie coefficient\n",
      "the difference is that there's not it does not actually include this log conversion okay\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 37:\n",
      "the difference is that there's not it does not actually include this log conversion okay\n",
      "which uh is is most of the reason that uh that most of the packages and uh\n",
      "techniques associated with uh information gain and this uh criteria for selection have a\n",
      "tendency to default to gini to genie index because it's just not as computationally oh yeah genie and purity i'm sorry it's\n",
      "up here right it's not as computationally expensive to not to when you take out that log\n",
      "conversion okay so as a result though it's this the outputs are slightly harder to to\n",
      "understand relative to the zero to one scale but it's really not too bad um so here what\n",
      "we see is you know kind of a a genie index here we're subtracting this from one that's\n",
      "basically point five all right because we have you know just two and two here as compared to one\n",
      "when we do when we did enthalpy here we see it's 0.5 which kind of represents a perfect\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 38:\n",
      "when we do when we did enthalpy here we see it's 0.5 which kind of represents a perfect\n",
      "split okay i would do the same calculations all right generally we're just going to plug this into\n",
      "our equation here and what we're going to see is that from this side we have 0.44 and this actually remains\n",
      "the same as enthalpy you know kind of a one-sided dominated class would be zero\n",
      "and then we're just going to do the same thing we're just going to subtract it from the parent right or just do a weighted average three out of four one out of four\n",
      "all right not that it's going to matter because this is zero and then this is going to be our information gain okay uh\n",
      "so it works very similar it just doesn't have this basically that logarithmic conversion component to it and as a result uh it kind of is the\n",
      "default because it is a little bit more efficient right so we went through each one of these this would result basically in a\n",
      "this is akin to the 0.33 that we saw earlier 0.31\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 39:\n",
      "this is akin to the 0.33 that we saw earlier 0.31\n",
      "0.31 that we saw earlier and if we did the perfect one it\n",
      "would be 0.5 all right okay all right so\n",
      "mean squared error is how it's done if you were doing um if we were doing a continuous\n",
      "reducing basically a regression based tree all right and so\n",
      "this is the equation for that it just tries to reduce the the total error of the predicted values at each node\n",
      "right so the average of each of those groups in terms of the minimizes the mean squared error just\n",
      "uses mean squared error at each point okay i don't have an example for that\n",
      "but it happens very similarly to the way that we see um it occur\n",
      "in with entropy all right it will do the calculations associated with predicting variables inside the tree\n",
      "pick the threshold associated with that calculate the mean squared error associated with using that particular\n",
      "variable at that particular split and then it will include the variable that reduces the mean square error the\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 40:\n",
      "most right at a particular threshold and then move on to the next node and do the same thing right\n",
      "all right so here's another example here just to just ignore this up here oops all right just ignored this\n",
      "we are going to go through this um uh this pregnancy tree example\n",
      "in uh in class i keep skipping through this all right but the idea here is we're trying to figure out uh we're a uh we're a marketing firm\n",
      "we're trying to figure out what our shopping shopper characteristics are trying to figure out whether someone is with child or not and so these are as\n",
      "it turns out the purchasing of folic acid ends up being the best predictor so here we can see this is\n",
      "kind of a pure split between this is this is pregnant and\n",
      "that's the other way around sorry not pregnant and pregnant okay so these\n",
      "are our shoppers that's our variable classification we're trying to predict you know kind of maybe the purchasing habits associated with our\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 41:\n",
      "with our clients and as a result then we can you know email them specials on diapers and vitamins or whatever that might be\n",
      "okay but here what we see is that folic acid does a really good job 120 to 6 in terms of the ratio of pregnancy not\n",
      "pregnant and predicting so that's why it's going to be our top one okay and then eventually we just work\n",
      "through the tree gradually to try and understand\n",
      "all right in particular on kind of the purchasing of non-folic acid to be able to better understand\n",
      "the split between those two there's an arrow pointing here just to note out that this is a you know basically a leaf\n",
      "node all right yeah which is kind of uh\n",
      "high up in the graph okay all right so like i mentioned before decision trees are prone to overfitting\n",
      "and one solution is we can tune those hyper parameters all right so another solution is built to ensemble talked about that\n",
      "all right so these are just uh some examples of hyper parameter tuning that that we can\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 42:\n",
      "all right so these are just uh some examples of hyper parameter tuning that that we can\n",
      "do all right you can set a minimum number of samples to be at a node split all right\n",
      "go back to that 100 100 100 row example that we had maybe you would want to say hey i need to have\n",
      "at least you know 10 data points in every node all right so that would avoid kind of\n",
      "over spitting over splitting right minimum number of samples at a terminal node uh so that would be\n",
      "um you know at what is the if you choose that same example up here we're saying we need at least 10 examples to do a split maybe the\n",
      "terminal node you would say hey i need at least i don't know at least five examples to be in a terminal node or something like\n",
      "that you probably want those numbers to be bigger maybe do 20 as a minimum split and 10 in the terminal something like that\n",
      "and this is typically the way that people do it is they sept a set of maximum to the depth of the tree and depth just means the number of variables\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 43:\n",
      "that you're going to use and the splitting all right so if you have a depth of like five you're not going to have more than five splits\n",
      "okay all right\n",
      "you can also set minimums and maximums on the number of actual terminal nodes\n",
      "okay so that's the number of you know the number of leaves at the very end okay all right and then\n",
      "and the maximum number of features to be considered at each split this is another way that you can handle it all right and so the idea is that you know keeping in\n",
      "mind when we have our we have our top number we're gradually doing all these splits\n",
      "this is not all right have to go this way\n",
      "all right all right\n",
      "all right so as you move down this side of the tree all right the variables that\n",
      "are available all right are ones that have not been used uh anywhere higher all right\n",
      "inside the split criteria and it's the same for this all right so you'll actually you can see that maybe two\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 44:\n",
      "two variables a variable could could get used again all right it could be used twice inside a tree as long as it's not higher\n",
      "inside the split criteria so in our example that we're doing before if oops you know say\n",
      "you know this the first example was folic acid right that sent out this portion of the tree\n",
      "in that portion of the tree folic acid cannot be used anywhere else inside the tree all right but say it was\n",
      "vitamins what's the second criteria here vitamins could also be used\n",
      "like over here all right so we might they might actually show up right next to each other in terms of understanding this\n",
      "population that was in the non-folic acid in this population that was in the folic acid right so what you can do is control the\n",
      "number of features associated to be considered at any one split so if you want to make sure that there are say five features that remain in order to\n",
      "give your decision tree a rich environment with which to make decisions you can control that\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 45:\n",
      "give your decision tree a rich environment with which to make decisions you can control that\n",
      "and if it looks like the tree is complex enough to where maybe there's only you know two variables left to consider you could say hey that's enough let's\n",
      "just stop there because uh you know the decision space is pretty low all right\n",
      "okay so the we talked about trees all right this is\n",
      "you know kind of a clunky transition here but we're moving into a different way to talk about talk about\n",
      "training our machine learning algorithm and this is through cross-validation all right so the package that we're going to use for\n",
      "decision trees actually defaults to this all right so it defaults using ten-fold cross-validation\n",
      "all right so we talked a lot about training test all right the disadvantage between training and\n",
      "test is you know basically they compete against each other all right as the training set gets\n",
      "bigger all right testing goes down all right so there cross validation is a way to get around\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 46:\n",
      "bigger all right testing goes down all right so there cross validation is a way to get around\n",
      "that to use the entire data set all right and the training and the testing process all right so let's\n",
      "let's talk about that all right so we have this tension that's why there's this rope here right all right\n",
      "so uh basically you just select a k and typically it defaults to 10 10k full 10 fold\n",
      "cross validation all right and so at every at every step what happens is is basically um the\n",
      "algorithm will do this 10 times all right basically train say you set this criteria to i don't know what this\n",
      "represents i don't i forgot the number of dots here right one two three four five six\n",
      "anyway that's that that's there's let's assume that there's 24 dots here or something like that and so this represents 25\n",
      "of the data uh and what you're going to do is you're going to train the algorithm with the 75 and then test it with this 25 and then\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 47:\n",
      "do the same thing here all right so train and test do the same thing here train and test\n",
      "train to test you're going to do this 10 times all right so here this is basically 4 k\n",
      "fold cross validation where they divided it into four equally sized parts and then they use\n",
      "those parts in proportion to be able to train to test the data set all right\n",
      "so uh we're going to do that too all right but the idea here is that uh you can use the entire data set in the\n",
      "training process and then do basically internal evaluation with this with this holdout\n",
      "and that allows you to use all the portions of the dataset as compared to just one big chunk for training\n",
      "would be chunk for test all right all right so we'll i'll show that a bit\n",
      "more as once we get into the code\n",
      "okay so um let's take a look at overfitting all\n",
      "right um again uh we've talked about well you guys know what overfitting is let's just\n",
      "jump past that but these are some definitions that we threw in here kind of at the end so ensemble methods\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 48:\n",
      "uh we talked about that a little bit we're gonna talk much more about that next week but that's basically when you take you know many there's many different\n",
      "forms of this but in this example instead of one tree but a whole bunch of trees together and they do majority vote\n",
      "essentially all right all right so you know it is designed to operate\n",
      "efficiently uh you know\n",
      "but it does not guarantee that it can provide you the best the best model because it is slightly different\n",
      "every time okay so let me stop there and then uh this is it's a lot to get through but\n",
      "essentially the key key points here to remember about decision trees is that it's centered\n",
      "on this idea of information gain they do obviously have a tendency to overfit so we want to be able to use those hyper parameters to be able to tune them\n",
      "there's a lot of options to do that we're going to do a bit more technically about what cross validation\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 49:\n",
      "does and how it how we can use it in actual code operations it's a bit it's very handy especially\n",
      "for especially for for decision trees and then also when we think about unbalanced\n",
      "data sets uh that can be helpful for that all right because uh decision trees can't be\n",
      "sensitive to kind of small changes and they're asynch all right so they grow out they're greedy they make these\n",
      "little local optimized decisions but they're also very easy to interpret through\n",
      "visualization and they're intuitive so they can be useful for all those all those types of reasons but\n",
      "we will get much more into it uh in class i'm sorry it's a little bit longer than normal but\n",
      "thanks for that and i will see you soon\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Change the type of text to langchaing document"
   ],
   "metadata": {
    "id": "uDL60Q8dEyjX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.schema import Document\n",
    "def convert_type(chunks):\n",
    "  # Convert chunks into LangChain Document objects\n",
    "  documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "  return documents\n",
    "\n",
    "week7_docs = convert_type(week7_chunks)\n",
    "week9_docs = convert_type(week9_chunks)\n",
    "\n",
    "# Now `documents` is a list of Document objects ready to be used in a vector store\n",
    "for i, doc in enumerate(week9_docs):\n",
    "    print(f\"Document {i+1}:\\n{doc.page_content}\\n{'-'*80}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d97uAbqlNQ1w",
    "outputId": "9eed397a-b10d-4094-dd35-27745e94f5f3",
    "ExecuteTime": {
     "end_time": "2025-01-29T17:19:03.144457Z",
     "start_time": "2025-01-29T17:19:03.029338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "okay all right so let me pop this up here\n",
      "bigger i can do that there we go all right so we've been talking about decision trees today all right we'll build a\n",
      "little bit maybe towards ensemble models towards the end but principally we're just going to talk about decision trees\n",
      "we're going to talk about how they're made or give you some theoretical background related to uh rates of how they are how constructed\n",
      "and how they're used they can be used for both regression and classification we're principally going to talk about classification\n",
      "at this point which is the primary use case for them and then um i'll talk about some of the weaknesses and strengths generally all right so\n",
      "let's just jump right into it here all right okay so this is what a little\n",
      "ahead of myself trying to shrink my there we go okay okay\n",
      "so uh i said all of that already so all right so let's talk right away\n",
      "--------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "so uh i said all of that already so all right so let's talk right away\n",
      "about uh about the way that trees work okay so when we think about them they're kind of this you know this upside down triangle\n",
      "basically they kind of build out this way and they're constructed largely of these uh you know basically these\n",
      "four things right so we have a root node which is basically this top note up here in a decision tree they have these internal\n",
      "nodes which are basically these split nodes they come out like this all right so it has one edge\n",
      "and these edges are just basically the lines that are connecting to connecting the nodes okay and then they\n",
      "have a leaf node or terminal node kind of the same thing it has one node coming in one edge and then no outgoing edges\n",
      "basically you think about them as kind of at the bottom of the tree okay so this is our this is our terminal\n",
      "i'm sorry our root up here this is an internal node and then these are the leaf nodes and\n",
      "--------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "i'm sorry our root up here this is an internal node and then these are the leaf nodes and\n",
      "then all of these things together have these edges that connect them all right hey it's really easy just to look at it this\n",
      "way and so here what we see this is our this is our root note up here all right this is the old classic iris\n",
      "iris data set that everybody just loves seeing over and over again so here we have the root nodes these are\n",
      "internals and then down here on the bottom we see the leaves all right so this is a\n",
      "matplotlib graphic actually coming out of python it\n",
      "does a bit better job and we'll talk about kind of this all the components of this as we move along so what's going on\n",
      "inside the boxes i don't have to totally understand but uh we'll take a look but this is\n",
      "also important note here is that right so a leaf node doesn't have to be all the way down on the bottom all right then we have one right here\n",
      "because we have no edges coming out of this basically what this is saying is that\n",
      "--------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "because we have no edges coming out of this basically what this is saying is that\n",
      "petal width just to give you some context here pedal width does such a good job\n",
      "of classifying setosa right that's remember that's a type of flower\n",
      "that we have there does such a good job of classifying setosa there's no reason to move on as you can see here basically any any\n",
      "petal width that is less than or equal to 1.75\n",
      "all right is going to be fall into the subtosa class right essentially all right so let's\n",
      "keep moving all right so that's that's it at a very high level is what we think about when\n",
      "we we think about decision trees and again it's just like this a cyclical graph that kind of moves down all right but\n",
      "how do they make these choices so let's let's talk about that all right so the most important question so what\n",
      "is the most important question uh to move on to a second date right it seems to me\n",
      "like uh you know are you married is probably a more important criteria than uh hey\n",
      "--------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "like uh you know are you married is probably a more important criteria than uh hey\n",
      "what's your favorite music you know if you don't know this one i guess it depends on your your\n",
      "your ethical standing but yeah in general you have to say to yourself what are these important questions that\n",
      "that you need to ask or need to be informed of in order to move on to a second date all right so\n",
      "when we think about that the question that the most amount of relevant information this is exactly how a decision tree works all\n",
      "right they're going to need they want to be able to figure out which variable gives it the most relevant information\n",
      "to be able to decide uh how to classify our target variable all right all right so\n",
      "you know depending on the condition of the first answer all right then you might select you know the next most important\n",
      "question for in terms of just information gain as maybe the question we're trying to work out here obviously is\n",
      "--------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "you know whether we're going to move to a move to a second date all right so are you married all right if the answer\n",
      "to this question is yes hopefully most people would say whoa okay well check please\n",
      "all right so that that might be it all right if the answer here is yes is yes all right again you would stop\n",
      "all right but then you know the question would be like what is the next series of questions that you might ask\n",
      "all right you know belief in a blue colored sky you know maybe that's good because you could you know\n",
      "get a sense of whether someone is insane or not or you know maybe then you do move on to say you know do we have more things in common you\n",
      "know what are your favorite foods or music or whatever all right so if you if the answer was to\n",
      "no then maybe you move on to the next variable which talks about something that is just kind of more personal to see if\n",
      "you're you know if your likes and dislikes overlap okay\n",
      "--------------------------------------------------------------------------------\n",
      "Document 7:\n",
      "you're you know if your likes and dislikes overlap okay\n",
      "any question is like when should you stop asking questions all right so say you made it you passed the uh past the marriage\n",
      "hurdle and now you're on to what music do you like right and so\n",
      "here you know it might be that you know a certain person you know fifty percent will go on uh on\n",
      "a date if they believe that they like you know acoustic and then maybe this this is disco or something and so you know 50\n",
      "percent of people will go that so then you start getting very specific about likes and dislikes in terms of what type\n",
      "of music alright so you can imagine this would be just a factor level variable that could have all sorts of different types of\n",
      "classical and brock and you know the rap and\n",
      "lots of other types of music which apparently i'm having trouble naming but anyway so you think about that and\n",
      "--------------------------------------------------------------------------------\n",
      "Document 8:\n",
      "depending on the answer to that maybe people would have more information about whether their likes or dislikes overlapped right okay all right\n",
      "so this is basically how this how this works okay so uh it's fairly intuitive that's a big\n",
      "reason that people like decision trees because of this nature you can kind of explain it in simple terms about how they're used and\n",
      "how they make choices all right so we asked the question you know what is\n",
      "what is the variable that has the most important information in terms of the ones and zeros that we're trying to\n",
      "classify and here we're trying to classify whether someone is likely to go on a second date or not all right so we\n",
      "asked this first question about marriage all right and that results similar to what we saw in the flower\n",
      "example to maybe a perfect classification of whether they will go on to the second date or not right the majority of people would not\n",
      "--------------------------------------------------------------------------------\n",
      "Document 9:\n",
      "go on to a second date right knowing that someone was married but if the answer to this is no all right this would be no this would be\n",
      "yes and then up above here we basically would be you know\n",
      "second date is the variable we're trying to classify right so that would just be a list of\n",
      "zeros and ones right zero being no second date and one being yes\n",
      "okay excuse me and then we get more specific right based off the type of\n",
      "music that people are interested in there might be more of a disjointed result\n",
      "right okay so we asked the most important information and then conditioned on the first answer\n",
      "we select the next most important question and when the answer is no longer providing any additional information we\n",
      "stop growing the branch all right it looks like maybe this is a 50 50 split you know 50 50. something like that\n",
      "we kind of stopped growing the blanche all right and we just do this all right and we'll talk about stopping criteria it also has a lot to do with\n",
      "--------------------------------------------------------------------------------\n",
      "Document 10:\n",
      "the complexity perimeter things like that or hyper parameters that you can tune to keep it above a certain level\n",
      "all sorts of different things you just basically keep repeating these two steps over and over again until no more\n",
      "information can be gained with the variables that you have all right so you use all the variables that\n",
      "you have inside your inside your data set to try and gradually get to know better and better\n",
      "whether someone will go on a second date or not all right okay all right\n",
      "so decision trees are hierarchical all right what that means is basically um\n",
      "they're built just in the way that you know we've kind of been viewing them is that they you know they start start small and grow\n",
      "more complex all right decisions decisions are made until a predetermined metric is met and we will\n",
      "talk about what that metric might look like all right it could be you know a certain amount of\n",
      "--------------------------------------------------------------------------------\n",
      "Document 11:\n",
      "talk about what that metric might look like all right it could be you know a certain amount of\n",
      "terminal nodes all right predefined there's lots of different hyper parameters you could actually set a lot of people set the\n",
      "number of layers say if you have you know a huge amount of data at your disposal\n",
      "you know the more layers that are in a tree the more complex the decisions get all right all right and so\n",
      "the model is is built such that a sequence of ordered decisions concerning the value result and you know at the end\n",
      "the model inside the terminal nodes are the leaf nodes all right actually predicts the class label okay i'll give\n",
      "you a percentage likelihood of that as well so it does it has a probabilistic output similar to k n it's non-parametric\n",
      "uh it just means that you know the number of parameters is not determined all right as is the case with linear\n",
      "models you know you kind of know what the variables are going to be with this it's it's not always\n",
      "--------------------------------------------------------------------------------\n",
      "Document 12:\n",
      "models you know you kind of know what the variables are going to be with this it's it's not always\n",
      "predestined right you could have a couple of variables that are incredibly good at predicting these classes you know marriage might be one\n",
      "of them and so you actually wouldn't need that many to have a very high level of certainty or to hit some level of\n",
      "metric that you're targeting or it could be that there are you have a bunch of weak variables so you need to have a very complex tree to\n",
      "understand how people's decisions are are being made right and so we also don't need to worry about\n",
      "any assumptions being met about distributions or perimeters okay\n",
      "so there's no normality assumptions no standardization all right it's just a cyclical graph all\n",
      "right so it's used in model probabilities and causality you can use there are causal trees a bit more\n",
      "complicated to translate but trees consist of these nodes and edges and they're defined by this decision rule\n",
      "--------------------------------------------------------------------------------\n",
      "Document 13:\n",
      "and we're going to we're going to talk about what that rule is all right okay let's talk about the background a\n",
      "little bit all right so it uses recursive binary\n",
      "splitting which basically means that it it considers every possible partition space\n",
      "and then it divides the one based off uh it chooses the partition space basically chooses where to split\n",
      "a variable to locally optimize that information gain right so it does\n",
      "not consider the future of the tree doesn't consider the past it only tries to optimize that\n",
      "individual node those individual internal leaves okay all right\n",
      "and it will consider every possible partition uh available when doing that okay so it\n",
      "this is this is what we call kind of a greedy approach right it's kind of like localized optimality um optimality\n",
      "optimization inside that one internal node okay so it's going to try and make the best choice it can\n",
      "uh inside that internal node right so it's each step of the tree building\n",
      "--------------------------------------------------------------------------------\n",
      "Document 14:\n",
      "uh inside that internal node right so it's each step of the tree building\n",
      "process it picks the best the best split for that particular that particular split it does not consider\n",
      "the future all right all right so trees you know i talked about this they can be they can use be used for um prediction\n",
      "or classification but they use recursive binary splitting in both cases all right um the difference between\n",
      "regressive trees uh is that they're they're going to predict okay so classification trees are going\n",
      "to predict the actual class whereas regression is going to predict a particular number all right\n",
      "right okay so the probability measure um that derives this building for\n",
      "classification comes in two forms all right so we have these these two um measures which are\n",
      "generally used for are very typical it's either it's either a genie index or\n",
      "enthalpy genie index is actually a bit more common you saw that on the on the graph that we showed earlier with the iris data set it actually gave you\n",
      "--------------------------------------------------------------------------------\n",
      "Document 15:\n",
      "the genie index at each internal node but we're going to go through both of them they're very similar they're both\n",
      "related to basically to information gain which is the key component to kind of\n",
      "understand how decision trees work is this idea about that they're they're selecting the variable and how to split that\n",
      "up variable based off the idea that they are maximizing information gained locally at that particular node okay\n",
      "all right all right so the background here this is\n",
      "the cart algorithm all right um it's a classification and regression\n",
      "tree all right it was first uh introduced here by 1984 with these four\n",
      "researchers all right it can be used in numerical categorical data all right so the first or splits the\n",
      "training data into two subsets all right i guess we kind of know that a single feature all right it searches through all the possible features\n",
      "all right to identify the split that produces the purest subsets right so the the purest idea is that purity is\n",
      "--------------------------------------------------------------------------------\n",
      "Document 16:\n",
      "related to maximizing information gain okay all right so it stops once you can't find a pla a split all right that\n",
      "that reduces impurity right or by some other pre-predetermined hyper parameter\n",
      "all right so let me kind of explain a bit more as we go all right it\n",
      "all right so that's a bit of a theoretical background of those two papers it really just takes kind of this idea of information gain and puts it into\n",
      "practice so it's similar to those four steps that we talked about the card algorithm is basically those four steps\n",
      "in practice you know we're repeating steps two and three all right okay it does have uh some pretty\n",
      "some pretty weighty advantages right it is simple to understand and interpret through data visualization\n",
      "this is a single tree remember you just produce a single tree all right it doesn't require almost any data preparation at all all right the\n",
      "other techniques often require normalization or dummy variables need to be created and blank values need\n",
      "--------------------------------------------------------------------------------\n",
      "Document 17:\n",
      "to be removed you know trees have a lot of advantages that they do not have to do a lot of that all right so they're able to handle\n",
      "numerical categorical data all right it uses it what we call a white box bottle all right so you can actually see what's\n",
      "going on all right you can understand why it made the decisions it's made and uh it's very interpretable okay as\n",
      "compared to maybe more black box models some neural network approaches even though we're getting better at explainable ai things like that um\n",
      "and then even when you get up to ensemble models with random force they also become a little a little hard to\n",
      "understand it's pretty straight straightforward to evaluate i mean most of the models nowadays are not too bad you can use\n",
      "these the evaluation metrics that we went over um you know in you know last\n",
      "week in the week before adding you know edit at a pretty high rate uh pretty much for all of these but\n",
      "--------------------------------------------------------------------------------\n",
      "Document 18:\n",
      "they're also very usable here for tree based methods um okay okay it does have some\n",
      "limitations as all algorithms do so this is important to remember what they are otherwise uh\n",
      "we'll be using to use uh use these methods in the wrong circumstances all right they do have\n",
      "like a legitimate uh tendency to overfit you know if you're just growing a tree\n",
      "without any type of restrictions you're almost guaranteeing that it's going to overfit linear models are the same way\n",
      "so we have to control for those a bit all right so you have to think about you know if you have 100 you know if you hunt as an example if\n",
      "you have 100 data points right you have six levels you're gonna have 64 terminal nodes right if you only have 100 data points\n",
      "so you basically you don't have a lot of single a lot of single terminal nodes which means you're basically just memorizing\n",
      "the data they're not going to generalize well and this is again this is our bias variance trade-off we'd want to use hyperparameter two need to\n",
      "--------------------------------------------------------------------------------\n",
      "Document 19:\n",
      "pull that back up or maybe some other uh metrics that we'll talk about in particular this this this idea of this complexity\n",
      "parameter to set that to a certain level that minimizes the error associated with the predictions and the actual values\n",
      "and that would be the metric that we try and target so there's lots of ways to be able to control this so it has this problem\n",
      "we also can control it all right they can also be generally unstable to\n",
      "small variations in the data because of the nature of how they're how they're designed all right so\n",
      "this is why you know a single decision tree depending on how good or bad your data is can be very\n",
      "useful but the truth of the matter is that often people build towards these ensemble models which like\n",
      "range random forest which is just you know thousands hundreds or thousands of trees\n",
      "to be able to compensate for kind of this instability associated with a single tree\n",
      "all right all right so practical distribution tree algorithms are based off of you know\n",
      "--------------------------------------------------------------------------------\n",
      "Document 20:\n",
      "all right all right so practical distribution tree algorithms are based off of you know\n",
      "heuristic which is a greedy we talked about this they make local optimal decisions all right so they can't\n",
      "guarantee that you'll get like the global global optimum all right similar to uh some\n",
      "other methods like this is that it could be possible that you know you don't get the perfect tree which again is a reason to move\n",
      "potentially towards ensembles all right all right so the great uh bias trees if\n",
      "some classes dominate it is there for recommended that you try and balance the data set prior to\n",
      "fitting all right so if you have a heavily unbalanced data set and this is true of many of the classifiers we're going to be talking about it just doesn't perform\n",
      "quite as well all right support vector machines actually have a tendency to work a little bit better as an example for\n",
      "unbalanced data sets a single tree can can be can be can be problematic with data sets that are\n",
      "--------------------------------------------------------------------------------\n",
      "Document 21:\n",
      "unbalanced data sets a single tree can can be can be can be problematic with data sets that are\n",
      "quite unbalanced but again that can be cured by moving to kind of more of an ensemble approach\n",
      "but until we get there we have to understand what one tree is doing before we start building forests all right so let's let's take a look all\n",
      "right these mathematical approaches and i have some examples all right okay so decision trees uh use several\n",
      "types of node splitting criteria all right so these are these are those i've talked about these\n",
      "uh this is what it would be if we had you know kind of a continuous data problem we're trying to actually predict the value all right as\n",
      "compared to predict the class and these are the two common ones for classification they're really very\n",
      "similar all right we're going to take a look at we're actually not going to take a look at this one but we're going to take a look at each one of these\n",
      "--------------------------------------------------------------------------------\n",
      "Document 22:\n",
      "and they both use this idea of information gain to determine variable split criteria all\n",
      "right so let's what is this crazy information gain thing all right so this is enthropy\n",
      "all right all right so that's the that's the um\n",
      "that's the equation there all right so basically it's just this all right so it's the the probability\n",
      "times the log probability of a particular instance you're going to summarize these\n",
      "together all right so just let me let me walk you through this instead of detailing through the equation here\n",
      "all right so what we would see here okay so this is we have a total set of six all\n",
      "right so we're thinking about basically this is our equation in\n",
      "practice here just this portion of it so we're going to sum these two up and then we're going to subtract them from each other and then that's going to give us base\n",
      "the general enthalpy of these six dots\n",
      "all right and you can see here what we have is basically perfect classification all right so or maybe\n",
      "--------------------------------------------------------------------------------\n",
      "Document 23:\n",
      "where we have uh three and three all right so when we subtract these two things from each other\n",
      "right we just get kind of the net entropy associated with this example right we do the same thing here\n",
      "we have a little bit of more discontinuity right so we have four and two okay so we just plug and\n",
      "chug basically into our equation we're just doing the probability here which is two out of six\n",
      "all right and then probability here so four out of six we're just multiplying that by the\n",
      "log two of exactly that same ratio and then subtracting them from each other just to get an idea of the balance right\n",
      "here so what we see here right it's just a total you know kind of one class example obviously we we don't\n",
      "have to do the blue calculation because we know it's zero and so we just do the green side of it which is going to result in basically no\n",
      "information so you can so we can put these let's extend this a bit further and just kind of\n",
      "--------------------------------------------------------------------------------\n",
      "Document 24:\n",
      "information so you can so we can put these let's extend this a bit further and just kind of\n",
      "clumsy through that all right the idea here is that information gain is basically going to be right the\n",
      "entropy of the parent right which is what we did up here this is the enter fee think of this as the calculations of the\n",
      "parent all right this is the by the parent i mean before the split all right so this is just what our data looks like before we\n",
      "before we use any variable examples to be able to split it right\n",
      "and then it's going to be the minus the average entropy of the children right and the children\n",
      "are going to be what it looks like you know after some type of decision criteria\n",
      "has been made okay so you can think of this as up here what we have this is entropy the\n",
      "parent so we have kind of a perfect division here which is which is which is good maybe that's what we want um\n",
      "you could think of this as you know pluses maybe went on the date right so that's a it\n",
      "--------------------------------------------------------------------------------\n",
      "Document 25:\n",
      "you could think of this as you know pluses maybe went on the date right so that's a it\n",
      "went on a second date just to continue to extend this example\n",
      "and the dots all right did not\n",
      "okay and then we're gonna then we apply some type of question right so this might be i don't know this\n",
      "might be the marriage question right so\n",
      "all marriage just pretend i know how to spell all right and so and this would be maybe this is yes\n",
      "sorry this is a yes category some people are married all right so as a result um\n",
      "what we see here is that if the answer is yes all right that two of these uh\n",
      "two of these dots stop right there all right so that's an entropy it's a perfect classification relative to all right no date all right so see no\n",
      "date which makes sense all right so they yes all right the people the no date uh stop right there all right but but one\n",
      "person you know this is uh so no all right so despite that\n",
      "there is some uh all right blending of that and it could just be that you know\n",
      "--------------------------------------------------------------------------------\n",
      "Document 26:\n",
      "there is some uh all right blending of that and it could just be that you know\n",
      "whatever that might be is that there's there's no date here all right so what we see is\n",
      "that there are four out of the all right original six continue on\n",
      "and probably what we see here in this green dot is maybe there's some other reason why that second date didn't occur all right so we'd have to continue\n",
      "making splits but for this there's no there's no additional reason for us to continue here right because we have\n",
      "basically a terminal node okay so we've classified this perfectly\n",
      "associated with this one no date class and that's exactly what we talked about kind of in the beginning\n",
      "about how good a particular question would be but here all right we have some we have\n",
      "some blending right so we don't see a zero so we wouldn't stop we'd continue to go okay all right\n",
      "now in order to be able to calculate this and just erase some of this trying to put this in context all right\n",
      "--------------------------------------------------------------------------------\n",
      "Document 27:\n",
      "we're gonna take the average of these two numbers right so the average of 0.8\n",
      "0.0 all right and we're going to subtract that from the parent and that will tell us the quality\n",
      "essentially of this marriage question or it would give us the net effect in terms of information gain for this\n",
      "particular question at this particular partition all right so this is a is a yes no so\n",
      "there's just the partition is simple it's just did they get did are they married or not but if it's a continuous variable\n",
      "basically try uh every type of potential partition inside that range of\n",
      "values calculate the information gain to see where it makes the most sense which is the example we saw for um\n",
      "i guess it was pedal width right so at a certain numerical range 1.75\n",
      "or greater or was it less anyway at a certain threshold associated with that that distance for that pedal they\n",
      "had perfect classification for that flower so that's where that um split criteria occurred okay so it\n",
      "--------------------------------------------------------------------------------\n",
      "Document 28:\n",
      "shows all the different potential ranges inside that basically all the the the potential\n",
      "partitions inside that continuous variable that are represented in the data frame all right they're represented in that vector and chose\n",
      "that one as being the best because it provided almost perfect information for that particular flower all right so what we would do here this\n",
      "is a now it's a weighted average because it takes into consideration what the original\n",
      "split was right okay and so we just subtract uh we add these two things together all\n",
      "right and then we just subtract it from one and so we get 0.54\n",
      "all right okay so that would basically be our our net gain there associated with um\n",
      "associated with our uh information information gain all right so what we would do to finish\n",
      "this is what we see here is this .54 is the result of this weighted average remember that this is\n",
      "our calculation of our enter fee previously we just add these together\n",
      "--------------------------------------------------------------------------------\n",
      "Document 29:\n",
      "our calculation of our enter fee previously we just add these together\n",
      "0.54 then we would subtract that from one and then here we would say we have a general information gain\n",
      "of 0.46 okay all right so now keep in mind if it was\n",
      "perfect all right and this resulted in a perfect split as in the extreme examples which sometimes we'd like to refer to that\n",
      "this would basically just be zero right and then we'd have you know one minus zero we'd have a perfect information gain\n",
      "of one sorry about that all right and so we would we wouldn't have to move\n",
      "on any further we'd have a variable that perfectly classifies into date and no date as an example\n",
      "right but we don't have that all right it's not a perfect classifier it's somewhere in between it's perfect for this one side but there's still lots of information\n",
      "that could be gained uh and specific to to the um this is the you know when on a date\n",
      "--------------------------------------------------------------------------------\n",
      "Document 30:\n",
      "that could be gained uh and specific to to the um this is the you know when on a date\n",
      "category so we need to continue to continue to grow this tree out so we could better understand that particular\n",
      "side of the uh side of the equation okay so let's take a look at another\n",
      "another example here all right so what we see in order to start the tree right we need to follow again just these\n",
      "these basically these three steps right i keep giving you steps here but all right we choose a attribute with the\n",
      "highest information gain okay so we can check child nodes and then we repeat repeat steps one and two you know we're\n",
      "cursively into uh no more information can be gained all right\n",
      "so let's take a look at that all right so this is a pretty common example that gets used but what\n",
      "we're trying to do here is to predict you know should we play outside i think the original example here is like should\n",
      "--------------------------------------------------------------------------------\n",
      "Document 31:\n",
      "we play tennis outside i think are we going to be able to play tennis something like that all right so we have these three variables right this is our\n",
      "predictor right we're trying to predict um whether play happened outside or not and we have\n",
      "these variables to help us like the outlook you know the temperature and then the you know the humidity all in\n",
      "classification all right all in um factors right so let's take a look so this is our our\n",
      "original variable all right so when we look here yes and no all right that's our play so we have a perfect\n",
      "all right we've got a perfect information perfect entropy here here we had two that went and played and\n",
      "two that did not so that that's our parent node at one all right and then we're taking a look at this first\n",
      "variable here sunny so that resulted in one yes all right all right\n",
      "and uh two nodes two nodes all right so we have three sunny variables all\n",
      "--------------------------------------------------------------------------------\n",
      "Document 32:\n",
      "and uh two nodes two nodes all right so we have three sunny variables all\n",
      "right and then when we look across what we see is we have two nodes and one yes all right so it doesn't give\n",
      "us perfect information all right but it does give us some pretty good information that when it is sunny\n",
      "we have one yes and two nodes all right but then we have you know when it's cloudy which is also\n",
      "here what we see so we have one yes all right so we're trying to figure out\n",
      "play or not play which is at the top and the first variable that we took a look at all right we haven't determined\n",
      "if this is the best variable for us yet but the first one that we took a look at is outlook so we just did this\n",
      "combination again we see a zero here we see a 0.92 here and then we're just\n",
      "going to do this weighted average and then subtract it from our original\n",
      "original entropy of our target variable okay so this is the weighted average\n",
      "we're just going to do this 3x4 because remember we had three\n",
      "--------------------------------------------------------------------------------\n",
      "Document 33:\n",
      "we're just going to do this 3x4 because remember we had three\n",
      "three sunnies and one cloudy all right i'm just going to multiply that by the by our enthalpy gain here which is just\n",
      "this formula that we talked about earlier and we're going to add that to the other one which is going to be\n",
      "going to be 0 right okay and then we're going to subtract it by 1. so we're going to get all right 0.31\n",
      "okay so let's keep going let's evaluate let's do this same process and we'll look at some of the other variables\n",
      "all right so let's look at hot and cold all right so what we see here all right\n",
      "we see temperature if the temperature is hot right no play no play the temperature is cool\n",
      "it did play and they did play all right we're giving it away here but this is going to result in perfect classification all right so what we're\n",
      "going to see here all right is that the relative to the entropy uh that was seen in the target variable\n",
      "--------------------------------------------------------------------------------\n",
      "Document 34:\n",
      "when we do this weighted average obvious this is all going to be zero just like i said before it's going to result in a perfect\n",
      "perfect information gain okay so here this perfectly matches our\n",
      "outlier or perfectly matches our target variable whereas the example we had before didn't it\n",
      "wasn't quite a perfect split right we had one yes over here and we have one yes over here so when we calculate\n",
      "information gain it's not quite as good as when we do it here right we have perfect information gain\n",
      "all right so let's we're talking a we've seen an in-between example we've seen a perfect example so now let's look at a\n",
      "perfectly terrible example all right so here what we see is we have perfect disagreement okay\n",
      "regardless of whether it's high or low it looks like there was a decision to play and to not play all right so here we\n",
      "have one yes and one no on both sides of this high\n",
      "low equation so what we're going to see is when we plug this into our uh you know our enthalpy\n",
      "--------------------------------------------------------------------------------\n",
      "Document 35:\n",
      "low equation so what we're going to see is when we plug this into our uh you know our enthalpy\n",
      "equation and we subtract these from each other uh we're going to get one all right which which is basically means\n",
      "that you know we're not we don't learn anything all right the result of this is one restrict one from one\n",
      "our net information gain here is going to be absolutely nothing okay so if we were to build this\n",
      "classifier uh what we would see here this is this is very uncommon of course\n",
      "but what we see is that our we have this temperature oops\n",
      "temperature would be right at the top all right so that would be the first variable that we would use it provides\n",
      "us the most information and then the next one would be outlook i mean actually technically we wouldn't even move on to a next one because we\n",
      "have perfect classification here so we wouldn't even need another variable so in this in this case when you have perfect information gain\n",
      "--------------------------------------------------------------------------------\n",
      "Document 36:\n",
      "um you just have a you know a tree with the depth of just one split and that's it okay\n",
      "so that's enthropy all right so information gain is is very similar all right all right so it basically uses\n",
      "the same idea about net gain all right so pi in this equation just represents\n",
      "the probability that a random selection would have state i whether it would have you know you think it was whether\n",
      "someone would be would play or not all right and so the mathematical process this\n",
      "is pretty much the same i don't think i put the actual i thought i had the actual equation in here but\n",
      "uh anyway you kind of get the idea here so it's this is just the same all right this is the example that we saw before\n",
      "all right relative to what the outlook was like right so this\n",
      "is the outlook calculation it's this one right here okay so we use the same\n",
      "same general idea i'm just going to show it using the genie coefficient\n",
      "the difference is that there's not it does not actually include this log conversion okay\n",
      "--------------------------------------------------------------------------------\n",
      "Document 37:\n",
      "the difference is that there's not it does not actually include this log conversion okay\n",
      "which uh is is most of the reason that uh that most of the packages and uh\n",
      "techniques associated with uh information gain and this uh criteria for selection have a\n",
      "tendency to default to gini to genie index because it's just not as computationally oh yeah genie and purity i'm sorry it's\n",
      "up here right it's not as computationally expensive to not to when you take out that log\n",
      "conversion okay so as a result though it's this the outputs are slightly harder to to\n",
      "understand relative to the zero to one scale but it's really not too bad um so here what\n",
      "we see is you know kind of a a genie index here we're subtracting this from one that's\n",
      "basically point five all right because we have you know just two and two here as compared to one\n",
      "when we do when we did enthalpy here we see it's 0.5 which kind of represents a perfect\n",
      "--------------------------------------------------------------------------------\n",
      "Document 38:\n",
      "when we do when we did enthalpy here we see it's 0.5 which kind of represents a perfect\n",
      "split okay i would do the same calculations all right generally we're just going to plug this into\n",
      "our equation here and what we're going to see is that from this side we have 0.44 and this actually remains\n",
      "the same as enthalpy you know kind of a one-sided dominated class would be zero\n",
      "and then we're just going to do the same thing we're just going to subtract it from the parent right or just do a weighted average three out of four one out of four\n",
      "all right not that it's going to matter because this is zero and then this is going to be our information gain okay uh\n",
      "so it works very similar it just doesn't have this basically that logarithmic conversion component to it and as a result uh it kind of is the\n",
      "default because it is a little bit more efficient right so we went through each one of these this would result basically in a\n",
      "this is akin to the 0.33 that we saw earlier 0.31\n",
      "--------------------------------------------------------------------------------\n",
      "Document 39:\n",
      "this is akin to the 0.33 that we saw earlier 0.31\n",
      "0.31 that we saw earlier and if we did the perfect one it\n",
      "would be 0.5 all right okay all right so\n",
      "mean squared error is how it's done if you were doing um if we were doing a continuous\n",
      "reducing basically a regression based tree all right and so\n",
      "this is the equation for that it just tries to reduce the the total error of the predicted values at each node\n",
      "right so the average of each of those groups in terms of the minimizes the mean squared error just\n",
      "uses mean squared error at each point okay i don't have an example for that\n",
      "but it happens very similarly to the way that we see um it occur\n",
      "in with entropy all right it will do the calculations associated with predicting variables inside the tree\n",
      "pick the threshold associated with that calculate the mean squared error associated with using that particular\n",
      "variable at that particular split and then it will include the variable that reduces the mean square error the\n",
      "--------------------------------------------------------------------------------\n",
      "Document 40:\n",
      "most right at a particular threshold and then move on to the next node and do the same thing right\n",
      "all right so here's another example here just to just ignore this up here oops all right just ignored this\n",
      "we are going to go through this um uh this pregnancy tree example\n",
      "in uh in class i keep skipping through this all right but the idea here is we're trying to figure out uh we're a uh we're a marketing firm\n",
      "we're trying to figure out what our shopping shopper characteristics are trying to figure out whether someone is with child or not and so these are as\n",
      "it turns out the purchasing of folic acid ends up being the best predictor so here we can see this is\n",
      "kind of a pure split between this is this is pregnant and\n",
      "that's the other way around sorry not pregnant and pregnant okay so these\n",
      "are our shoppers that's our variable classification we're trying to predict you know kind of maybe the purchasing habits associated with our\n",
      "--------------------------------------------------------------------------------\n",
      "Document 41:\n",
      "with our clients and as a result then we can you know email them specials on diapers and vitamins or whatever that might be\n",
      "okay but here what we see is that folic acid does a really good job 120 to 6 in terms of the ratio of pregnancy not\n",
      "pregnant and predicting so that's why it's going to be our top one okay and then eventually we just work\n",
      "through the tree gradually to try and understand\n",
      "all right in particular on kind of the purchasing of non-folic acid to be able to better understand\n",
      "the split between those two there's an arrow pointing here just to note out that this is a you know basically a leaf\n",
      "node all right yeah which is kind of uh\n",
      "high up in the graph okay all right so like i mentioned before decision trees are prone to overfitting\n",
      "and one solution is we can tune those hyper parameters all right so another solution is built to ensemble talked about that\n",
      "all right so these are just uh some examples of hyper parameter tuning that that we can\n",
      "--------------------------------------------------------------------------------\n",
      "Document 42:\n",
      "all right so these are just uh some examples of hyper parameter tuning that that we can\n",
      "do all right you can set a minimum number of samples to be at a node split all right\n",
      "go back to that 100 100 100 row example that we had maybe you would want to say hey i need to have\n",
      "at least you know 10 data points in every node all right so that would avoid kind of\n",
      "over spitting over splitting right minimum number of samples at a terminal node uh so that would be\n",
      "um you know at what is the if you choose that same example up here we're saying we need at least 10 examples to do a split maybe the\n",
      "terminal node you would say hey i need at least i don't know at least five examples to be in a terminal node or something like\n",
      "that you probably want those numbers to be bigger maybe do 20 as a minimum split and 10 in the terminal something like that\n",
      "and this is typically the way that people do it is they sept a set of maximum to the depth of the tree and depth just means the number of variables\n",
      "--------------------------------------------------------------------------------\n",
      "Document 43:\n",
      "that you're going to use and the splitting all right so if you have a depth of like five you're not going to have more than five splits\n",
      "okay all right\n",
      "you can also set minimums and maximums on the number of actual terminal nodes\n",
      "okay so that's the number of you know the number of leaves at the very end okay all right and then\n",
      "and the maximum number of features to be considered at each split this is another way that you can handle it all right and so the idea is that you know keeping in\n",
      "mind when we have our we have our top number we're gradually doing all these splits\n",
      "this is not all right have to go this way\n",
      "all right all right\n",
      "all right so as you move down this side of the tree all right the variables that\n",
      "are available all right are ones that have not been used uh anywhere higher all right\n",
      "inside the split criteria and it's the same for this all right so you'll actually you can see that maybe two\n",
      "--------------------------------------------------------------------------------\n",
      "Document 44:\n",
      "two variables a variable could could get used again all right it could be used twice inside a tree as long as it's not higher\n",
      "inside the split criteria so in our example that we're doing before if oops you know say\n",
      "you know this the first example was folic acid right that sent out this portion of the tree\n",
      "in that portion of the tree folic acid cannot be used anywhere else inside the tree all right but say it was\n",
      "vitamins what's the second criteria here vitamins could also be used\n",
      "like over here all right so we might they might actually show up right next to each other in terms of understanding this\n",
      "population that was in the non-folic acid in this population that was in the folic acid right so what you can do is control the\n",
      "number of features associated to be considered at any one split so if you want to make sure that there are say five features that remain in order to\n",
      "give your decision tree a rich environment with which to make decisions you can control that\n",
      "--------------------------------------------------------------------------------\n",
      "Document 45:\n",
      "give your decision tree a rich environment with which to make decisions you can control that\n",
      "and if it looks like the tree is complex enough to where maybe there's only you know two variables left to consider you could say hey that's enough let's\n",
      "just stop there because uh you know the decision space is pretty low all right\n",
      "okay so the we talked about trees all right this is\n",
      "you know kind of a clunky transition here but we're moving into a different way to talk about talk about\n",
      "training our machine learning algorithm and this is through cross-validation all right so the package that we're going to use for\n",
      "decision trees actually defaults to this all right so it defaults using ten-fold cross-validation\n",
      "all right so we talked a lot about training test all right the disadvantage between training and\n",
      "test is you know basically they compete against each other all right as the training set gets\n",
      "bigger all right testing goes down all right so there cross validation is a way to get around\n",
      "--------------------------------------------------------------------------------\n",
      "Document 46:\n",
      "bigger all right testing goes down all right so there cross validation is a way to get around\n",
      "that to use the entire data set all right and the training and the testing process all right so let's\n",
      "let's talk about that all right so we have this tension that's why there's this rope here right all right\n",
      "so uh basically you just select a k and typically it defaults to 10 10k full 10 fold\n",
      "cross validation all right and so at every at every step what happens is is basically um the\n",
      "algorithm will do this 10 times all right basically train say you set this criteria to i don't know what this\n",
      "represents i don't i forgot the number of dots here right one two three four five six\n",
      "anyway that's that that's there's let's assume that there's 24 dots here or something like that and so this represents 25\n",
      "of the data uh and what you're going to do is you're going to train the algorithm with the 75 and then test it with this 25 and then\n",
      "--------------------------------------------------------------------------------\n",
      "Document 47:\n",
      "do the same thing here all right so train and test do the same thing here train and test\n",
      "train to test you're going to do this 10 times all right so here this is basically 4 k\n",
      "fold cross validation where they divided it into four equally sized parts and then they use\n",
      "those parts in proportion to be able to train to test the data set all right\n",
      "so uh we're going to do that too all right but the idea here is that uh you can use the entire data set in the\n",
      "training process and then do basically internal evaluation with this with this holdout\n",
      "and that allows you to use all the portions of the dataset as compared to just one big chunk for training\n",
      "would be chunk for test all right all right so we'll i'll show that a bit\n",
      "more as once we get into the code\n",
      "okay so um let's take a look at overfitting all\n",
      "right um again uh we've talked about well you guys know what overfitting is let's just\n",
      "jump past that but these are some definitions that we threw in here kind of at the end so ensemble methods\n",
      "--------------------------------------------------------------------------------\n",
      "Document 48:\n",
      "uh we talked about that a little bit we're gonna talk much more about that next week but that's basically when you take you know many there's many different\n",
      "forms of this but in this example instead of one tree but a whole bunch of trees together and they do majority vote\n",
      "essentially all right all right so you know it is designed to operate\n",
      "efficiently uh you know\n",
      "but it does not guarantee that it can provide you the best the best model because it is slightly different\n",
      "every time okay so let me stop there and then uh this is it's a lot to get through but\n",
      "essentially the key key points here to remember about decision trees is that it's centered\n",
      "on this idea of information gain they do obviously have a tendency to overfit so we want to be able to use those hyper parameters to be able to tune them\n",
      "there's a lot of options to do that we're going to do a bit more technically about what cross validation\n",
      "--------------------------------------------------------------------------------\n",
      "Document 49:\n",
      "does and how it how we can use it in actual code operations it's a bit it's very handy especially\n",
      "for especially for for decision trees and then also when we think about unbalanced\n",
      "data sets uh that can be helpful for that all right because uh decision trees can't be\n",
      "sensitive to kind of small changes and they're asynch all right so they grow out they're greedy they make these\n",
      "little local optimized decisions but they're also very easy to interpret through\n",
      "visualization and they're intuitive so they can be useful for all those all those types of reasons but\n",
      "we will get much more into it uh in class i'm sorry it's a little bit longer than normal but\n",
      "thanks for that and i will see you soon\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ],
   "metadata": {
    "id": "oWjwzIVHn_gn",
    "ExecuteTime": {
     "end_time": "2025-01-29T17:19:09.808010Z",
     "start_time": "2025-01-29T17:19:05.673077Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setting up the Data base"
   ],
   "metadata": {
    "id": "-nrwl3YQE6XS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#create index\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "pc = Pinecone(api_key=\"pcsk_3GGcqq_DGzzsTybqrBrVXqf46Uf8Kr1yPHZjsMNA9q7feBVViscuc9XpbPs259m8WtgBRX\")\n",
    "index_name = \"rag-app\"\n",
    "\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name = index_name,\n",
    "        dimension=768,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud = \"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "vector_store = PineconeVectorStore(embedding=embeddings, index=index)"
   ],
   "metadata": {
    "id": "nNzs79xnrPxk",
    "ExecuteTime": {
     "end_time": "2025-01-29T17:19:13.125444Z",
     "start_time": "2025-01-29T17:19:11.188921Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Delete all the unnecessary element in the Database"
   ],
   "metadata": {
    "id": "Xo8KkrqmE96D"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "index.delete(delete_all=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S_Crm9gHCObV",
    "outputId": "c98e231a-dba3-4f9f-f5fe-cbcfa9bc6d5c",
    "ExecuteTime": {
     "end_time": "2025-01-27T18:37:55.917243Z",
     "start_time": "2025-01-27T18:37:55.702953Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Add all the chunks"
   ],
   "metadata": {
    "id": "k5HAcq-bFFG9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "vector_store.add_documents(week7_docs)\n",
    "vector_store.add_documents(week9_docs)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XY08HfeGpIH0",
    "outputId": "4ac4f8b6-8fb0-4ab3-bc48-028401a6ebd6",
    "ExecuteTime": {
     "end_time": "2025-01-27T18:37:59.219837Z",
     "start_time": "2025-01-27T18:37:57.499388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dee51c6e-e9cc-4014-a5e8-2c4736c098c8',\n",
       " '0fa73d2e-787b-46a3-8e00-896ca9da8ac3',\n",
       " '28424573-46f1-469c-a884-7eb12f3e1ecc',\n",
       " '35427ad5-9803-48a6-a38c-c90449b65b4d',\n",
       " '9f079db6-92aa-45ec-9fae-ba7f6ff2cd60',\n",
       " '6b0736a5-ff60-4fae-beda-d00a26dc7afe',\n",
       " '6f87ae92-facb-47a1-9bf0-270188371ddf',\n",
       " '9ba22c26-d778-4f90-9057-b7894ba4115f',\n",
       " 'd47c111f-1262-49b6-9dcc-53053b3a098b',\n",
       " '188925c4-b65e-4044-b871-4ac8bb8f2907',\n",
       " '1a5794fb-acc8-4db6-9282-d6267330d825',\n",
       " 'b9891712-e183-4873-90a0-94cedf4c8431',\n",
       " '96b173f4-7078-4d68-b93e-362749cbbd26',\n",
       " '0eb7047a-02eb-492c-943b-1e48a4233ae4',\n",
       " 'ff2fd068-e755-40e8-8bab-d6b3e9094194',\n",
       " '3695a29d-df90-482e-952e-dfd8cfd569be',\n",
       " 'c25c1770-4b14-4dcd-8057-c435b2869359',\n",
       " '4f35ed08-5a50-4b6f-adfe-278de45a06ba',\n",
       " '33f10cb9-c63c-4e19-af9b-6ad55c005198',\n",
       " 'b2caed61-d393-454d-83aa-fde87ea42535',\n",
       " 'c7990bd1-fab8-4bb1-9bf4-05a50afda5d4',\n",
       " '2d58f743-4b91-4e48-bbd6-4f4420e9c773',\n",
       " 'df618608-24e5-4167-84a5-e76f2f7ec536',\n",
       " 'c25655dd-35cf-4236-84b5-b123cfe88c63',\n",
       " '7668dd01-aa85-48b5-a123-2ee774141e51',\n",
       " 'e553b1da-f525-4084-aef9-1db4be3cc19a',\n",
       " '8510b9f1-c080-409b-9954-31253faa6ecd',\n",
       " '583d281a-dbef-4445-a41b-460571d7e2f1',\n",
       " 'd9421c00-a6a8-4c05-8939-49af5de03fd6',\n",
       " 'f815284a-cc25-459f-ad59-dae228980551',\n",
       " '920fafd7-a70b-4531-b71a-308d472b7d87',\n",
       " '33470eaf-1e24-40c9-82b0-34a301e9d985',\n",
       " '5777ef57-9057-4556-857a-730409d1494e',\n",
       " '5659a491-93a7-4f21-8256-71eec17aefbd',\n",
       " '21624109-8f4b-4e50-b518-85628397d821',\n",
       " '17a6a741-85c0-46ad-8e9f-13ffacfdac7e',\n",
       " '303fc9c7-7fbc-46bc-9c46-caa03eb359f2',\n",
       " '4e957a10-3036-4cbd-8f14-25fc94ce9f5c',\n",
       " '819ed520-8802-43db-ae75-f4609bd52087',\n",
       " '31b8144b-9b48-42f6-bbd9-8cbfa72a1dc4',\n",
       " '4274c0d0-735f-4279-bbd7-6a252a852b1a',\n",
       " 'a925767a-21c5-4aa3-a280-46d3c8f2bfb7',\n",
       " '511d9f56-d1ad-4001-b854-aa7810b3ea66',\n",
       " '4967e945-97b5-48de-a4ff-edde8299ddf2',\n",
       " '29b63412-ed7c-49c5-baa0-d60eef592316',\n",
       " '5425faeb-1bfa-454d-8859-2689f6f1d0ea',\n",
       " '5f39bfac-efca-4d7e-b3f6-991f73837779',\n",
       " '07c95a02-4dae-47ea-bdb1-e8f0c94a9152',\n",
       " '41f5e5c8-541a-428e-a919-c3632ef07083']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PDF Reading"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T17:21:39.976153Z",
     "start_time": "2025-01-29T17:21:38.930442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import fitz  # PymMuPDF\n",
    "import os\n",
    "\n",
    "def extract_pdf_content(file_path):\n",
    "    # Create folders for images and text files\n",
    "    images_folder = \"images\"\n",
    "    texts_folder = \"texts\"\n",
    "    os.makedirs(images_folder, exist_ok=True)\n",
    "    os.makedirs(texts_folder, exist_ok=True)\n",
    "\n",
    "    # Open the PDF file\n",
    "    doc = fitz.open(file_path)\n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "\n",
    "        # Extract text\n",
    "        text = page.get_text()\n",
    "        text_filename = os.path.join(texts_folder, f\"page_{page_num + 1}.txt\")\n",
    "        with open(text_filename, \"w\", encoding=\"utf-8\") as text_file:\n",
    "            text_file.write(text)\n",
    "        print(f\"Saved text for page {page_num + 1} to {text_filename}\")\n",
    "\n",
    "        # Extract images\n",
    "        for img_index, img in enumerate(page.get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_filename = os.path.join(images_folder, f\"page_{page_num + 1}_img_{img_index + 1}.png\")\n",
    "            with open(image_filename, \"wb\") as img_file:\n",
    "                img_file.write(image_bytes)\n",
    "            print(f\"Saved image to {image_filename}\")\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_path = \"Clustering_InClass_9.28.21-1.pdf\"\n",
    "\n",
    "extract_pdf_content(pdf_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved text for page 1 to texts/page_1.txt\n",
      "Saved image to images/page_1_img_1.png\n",
      "Saved image to images/page_1_img_2.png\n",
      "Saved image to images/page_1_img_3.png\n",
      "Saved image to images/page_1_img_4.png\n",
      "Saved text for page 2 to texts/page_2.txt\n",
      "Saved image to images/page_2_img_1.png\n",
      "Saved text for page 3 to texts/page_3.txt\n",
      "Saved text for page 4 to texts/page_4.txt\n",
      "Saved text for page 5 to texts/page_5.txt\n",
      "Saved text for page 6 to texts/page_6.txt\n",
      "Saved image to images/page_6_img_1.png\n",
      "Saved image to images/page_6_img_2.png\n",
      "Saved image to images/page_6_img_3.png\n",
      "Saved text for page 7 to texts/page_7.txt\n",
      "Saved image to images/page_7_img_1.png\n",
      "Saved text for page 8 to texts/page_8.txt\n",
      "Saved image to images/page_8_img_1.png\n",
      "Saved text for page 9 to texts/page_9.txt\n",
      "Saved image to images/page_9_img_1.png\n",
      "Saved text for page 10 to texts/page_10.txt\n",
      "Saved text for page 11 to texts/page_11.txt\n",
      "Saved image to images/page_11_img_1.png\n",
      "Saved image to images/page_11_img_2.png\n",
      "Saved text for page 12 to texts/page_12.txt\n",
      "Saved image to images/page_12_img_1.png\n",
      "Saved text for page 13 to texts/page_13.txt\n",
      "Saved image to images/page_13_img_1.png\n",
      "Saved text for page 14 to texts/page_14.txt\n",
      "Saved image to images/page_14_img_1.png\n",
      "Saved text for page 15 to texts/page_15.txt\n",
      "Saved image to images/page_15_img_1.png\n",
      "Saved text for page 16 to texts/page_16.txt\n",
      "Saved image to images/page_16_img_1.png\n",
      "Saved text for page 17 to texts/page_17.txt\n",
      "Saved image to images/page_17_img_1.png\n",
      "Saved text for page 18 to texts/page_18.txt\n",
      "Saved image to images/page_18_img_1.png\n",
      "Saved text for page 19 to texts/page_19.txt\n",
      "Saved image to images/page_19_img_1.png\n",
      "Saved image to images/page_19_img_2.png\n",
      "Saved text for page 20 to texts/page_20.txt\n",
      "Saved image to images/page_20_img_1.png\n",
      "Saved image to images/page_20_img_2.png\n",
      "Saved text for page 21 to texts/page_21.txt\n",
      "Saved image to images/page_21_img_1.png\n",
      "Saved text for page 22 to texts/page_22.txt\n",
      "Saved image to images/page_22_img_1.png\n",
      "Saved image to images/page_22_img_2.png\n",
      "Saved text for page 23 to texts/page_23.txt\n",
      "Saved text for page 24 to texts/page_24.txt\n",
      "Saved image to images/page_24_img_1.png\n",
      "Saved text for page 25 to texts/page_25.txt\n",
      "Saved image to images/page_25_img_1.png\n",
      "Saved text for page 26 to texts/page_26.txt\n",
      "Saved image to images/page_26_img_1.png\n",
      "Saved text for page 27 to texts/page_27.txt\n",
      "Saved image to images/page_27_img_1.png\n",
      "Saved image to images/page_27_img_2.png\n",
      "Saved text for page 28 to texts/page_28.txt\n",
      "Saved image to images/page_28_img_1.png\n",
      "Saved image to images/page_28_img_2.png\n",
      "Saved text for page 29 to texts/page_29.txt\n",
      "Saved image to images/page_29_img_1.png\n",
      "Saved text for page 30 to texts/page_30.txt\n",
      "Saved image to images/page_30_img_1.png\n",
      "Saved image to images/page_30_img_2.png\n",
      "Saved text for page 31 to texts/page_31.txt\n",
      "Saved image to images/page_31_img_1.png\n",
      "Saved image to images/page_31_img_2.png\n",
      "Saved image to images/page_31_img_3.png\n",
      "Saved image to images/page_31_img_4.png\n",
      "Saved text for page 32 to texts/page_32.txt\n",
      "Saved image to images/page_32_img_1.png\n",
      "Saved text for page 33 to texts/page_33.txt\n",
      "Saved image to images/page_33_img_1.png\n",
      "Saved text for page 34 to texts/page_34.txt\n",
      "Saved image to images/page_34_img_1.png\n",
      "Saved text for page 35 to texts/page_35.txt\n",
      "Saved text for page 36 to texts/page_36.txt\n",
      "Saved image to images/page_36_img_1.png\n",
      "Saved text for page 37 to texts/page_37.txt\n",
      "Saved image to images/page_37_img_1.png\n",
      "Saved image to images/page_37_img_2.png\n",
      "Saved text for page 38 to texts/page_38.txt\n",
      "Saved text for page 39 to texts/page_39.txt\n",
      "Saved image to images/page_39_img_1.png\n",
      "Saved text for page 40 to texts/page_40.txt\n",
      "Saved image to images/page_40_img_1.png\n",
      "Saved image to images/page_40_img_2.png\n",
      "Saved text for page 41 to texts/page_41.txt\n",
      "Saved text for page 42 to texts/page_42.txt\n",
      "Saved image to images/page_42_img_1.png\n",
      "Saved text for page 43 to texts/page_43.txt\n",
      "Saved image to images/page_43_img_1.png\n",
      "Saved text for page 44 to texts/page_44.txt\n",
      "Saved image to images/page_44_img_1.png\n",
      "Saved text for page 45 to texts/page_45.txt\n",
      "Saved image to images/page_45_img_1.png\n",
      "Saved image to images/page_45_img_2.png\n",
      "Saved image to images/page_45_img_3.png\n",
      "Saved text for page 46 to texts/page_46.txt\n",
      "Saved text for page 47 to texts/page_47.txt\n",
      "Saved image to images/page_47_img_1.png\n",
      "Saved text for page 48 to texts/page_48.txt\n",
      "Saved image to images/page_48_img_1.png\n",
      "Saved text for page 49 to texts/page_49.txt\n",
      "Saved image to images/page_49_img_1.png\n",
      "Saved image to images/page_49_img_2.png\n",
      "Saved text for page 50 to texts/page_50.txt\n",
      "Saved image to images/page_50_img_1.png\n",
      "Saved text for page 51 to texts/page_51.txt\n",
      "Saved image to images/page_51_img_1.png\n",
      "Saved text for page 52 to texts/page_52.txt\n",
      "Saved text for page 53 to texts/page_53.txt\n",
      "Saved image to images/page_53_img_1.png\n",
      "Saved text for page 54 to texts/page_54.txt\n",
      "Saved text for page 55 to texts/page_55.txt\n",
      "Saved text for page 56 to texts/page_56.txt\n",
      "Saved image to images/page_56_img_1.png\n",
      "Saved text for page 57 to texts/page_57.txt\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T17:22:05.761024Z",
     "start_time": "2025-01-29T17:22:05.754607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "folder_path = 'texts'  # Replace with your file path\n",
    "elements = os.listdir(folder_path)\n",
    "\n",
    "sorted_filenames = sorted(elements, key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "\n",
    "combined_texts = \"\"\n",
    "print(\"Sorted filenames:\")\n",
    "for filename in sorted_filenames:\n",
    "    # print(\"File Name:\", filename)\n",
    "    with open(folder_path+\"/\"+filename, 'r') as file:\n",
    "        for line in file:\n",
    "            combined_texts += line.strip() + \"\\n\"\n",
    "\n",
    "print(combined_texts)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted filenames:\n",
      "Machine Learning Overview, EDA and Clustering\n",
      "Brian Wright\n",
      "brianwright@virginia.edu\n",
      "2\n",
      "3\n",
      "Given  D :{2,4,10,12,3,11,20,25,30},  and  k=2 clusters\n",
      "Randomly assign the means:  m1=3, m2=4\n",
      "K1={2,3}, K2={4,10,12,11,20,25,30}, m1=2.5, m2=16\n",
      "K1={2,3,4}, K2={10,12,11,20,25,30}, m1=3, m2=18\n",
      "K1={2,3,4,10}, K2={12,11,20,25,30}, m1=4.75, m2=19.6\n",
      "K1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\n",
      "K1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\n",
      "Stop, since the clusters and the means found in\n",
      "all subsequent iterations will be the same.\n",
      "Example of K-Means\n",
      "1. What is Machine Learning?\n",
      "2. What is exploratory data analysis?\n",
      "3. k-means clustering\n",
      "– Does Congress vote in patterns?\n",
      "4. Multi-dimensional k-means clustering\n",
      "– Are NBA players compensated according to performance?\n",
      "Outline: Intro to Unsupervised ML\n",
      "4\n",
      "• Exploratory data analysis or “EDA” is an approach where the intent is to see\n",
      "what the data can tell us beyond modeling or hypothesis testing\n",
      "– Data visualization is one of the most common forms of EDA\n",
      "What is exploratory data analysis?\n",
      "5\n",
      "When data is too big or complex to be analyzed just by\n",
      "visualizing it, these types of analysis can help:\n",
      "1. Clustering: compare pieces of data by measuring\n",
      "similarity among them\n",
      "2. Network analysis: analyze how people, places and\n",
      "entities are connected to evaluate the properties and\n",
      "structure of a network\n",
      "3. Text mining: analyze what large bodies of\n",
      "unstructured or structured text say\n",
      "Types of exploratory data analysis\n",
      "6\n",
      "The data inputs have (x) no target outputs (y)\n",
      "Unsupervised machine learning\n",
      "Input x:\n",
      "Voter\n",
      "Output y:\n",
      "Not given\n",
      "(To be discovered)\n",
      "?\n",
      "We want to impose structure on the inputs (x) to say something\n",
      "meaningful about the data\n",
      "7\n",
      "1. Technique for finding similarity between groups\n",
      "2. Type of unsupervised machine learning\n",
      "• Not the only class of unsupervised learning\n",
      "algorithms\n",
      "3. Similarity needs to be defined\n",
      "• Will depend on attributes of data\n",
      "• Usually a distance metric\n",
      "What is clustering?\n",
      "8\n",
      "Key assumption: data points that are “closer” together are related or similar\n",
      "• Haimowitz and Schwarz 1997 paper on\n",
      "clustering for credit line optimization\n",
      "– http://www.aaai.org/Papers/Workshops/1997/\n",
      "WS-97-07/WS97-07-006.pdf\n",
      "• Cluster existing GE Capital customers based\n",
      "on similarity in use of their credit cards to\n",
      "pay bills and customers’ profitability to GE\n",
      "Capital\n",
      "• Resulted in five clusters of consumer credit\n",
      "behavior\n",
      "• Created classification model to predict\n",
      "customer type and offer tailored products\n",
      "GE Capital case study: grouping clients\n",
      "9\n",
      "Example use case\n",
      "General question\n",
      "Concept\n",
      "Does Congress vote in patterns?\n",
      "Is there a pattern?\n",
      "Is there structure in\n",
      "unstructured data?\n",
      "k-means clustering\n",
      "Are basketball players \"priced\"\n",
      "efficiently (based on performance)?\n",
      "How to uncover trends with\n",
      "many variables that you\n",
      "can't easily visualize?\n",
      "k-means clustering in\n",
      "many dimensions\n",
      "Concept summary\n",
      "12\n",
      "1. Data set consists of 427 members\n",
      "(observations)\n",
      "2. Members served a full year in 2013\n",
      "3. Three vote types:\n",
      "• “Aye”\n",
      "• “Nay”\n",
      "• “Other”\n",
      "Goal: to understand how polarized the\n",
      "US Congress is\n",
      "Political clustering\n",
      "The joint session of Congress on Capitol Hill\n",
      "in Washington\n",
      "13\n",
      "• How do we identify\n",
      "swing votes?\n",
      "– Lobbying\n",
      "– Bridging party lines\n",
      "• Assumption:\n",
      "– Democrats and Republicans\n",
      "vote among partisan lines,\n",
      "which generates clusters\n",
      "Each data point represents a member of Congress\n",
      "Finding voting patterns\n",
      "14\n",
      "Intra vs. inter-cluster distance\n",
      "Intra-Cluster\n",
      "Distance\n",
      "Inter-Cluster\n",
      "Distance\n",
      "Objective: minimize intra-cluster distance, maximize inter-cluster distance\n",
      "15\n",
      "• The centroid is the average location\n",
      "of all points in the cluster\n",
      "• Another definition: the centroid\n",
      "minimizes the distance between a\n",
      "central location and all the data\n",
      "points in the cluster\n",
      "Note: Centroids are generally not\n",
      "existing data points, rather locations in\n",
      "space\n",
      "k-means clustering is based on\n",
      "centroids\n",
      "16\n",
      "1. Randomly choose k data points\n",
      "to be centroids\n",
      "k-means in 4 steps\n",
      "17\n",
      "1. Randomly choose k data points\n",
      "to be centroids\n",
      "2. Assign each point to closest\n",
      "centroid\n",
      "k-means in 4 steps\n",
      "18\n",
      "1. Randomly choose k data points\n",
      "to be centroids\n",
      "2. Assign each point to closest\n",
      "centroid\n",
      "3. Recalculate centroids based on\n",
      "current cluster membership\n",
      "k-means in 4 steps\n",
      "19\n",
      "1. Randomly choose k data\n",
      "points to be centroids\n",
      "2. Assign each point to closest\n",
      "centroid\n",
      "3. Recalculate centroids based\n",
      "on current cluster\n",
      "membership\n",
      "k-means in 4 steps\n",
      "20\n",
      "4. Repeat steps 2-3 with the new centroids until the centroids don’t\n",
      "change anymore\n",
      "Step 1: load packages and data\n",
      "# Install packages\n",
      "install.packages(\"e1071\")\n",
      "install.packages(\"ggplot2\")\n",
      "# Load libraries\n",
      "library(e1071)\n",
      "library(ggplot2)\n",
      "library(help = e1071)\n",
      "Learn about all the functionality of the package, be\n",
      "well informed about what you're doing!\n",
      "21\n",
      "Step 1: load packages and data\n",
      "# Loading house data\n",
      "house_votes_Dem = read_csv(\"house_votes_Dem.csv\")\n",
      "# What does the data look like?\n",
      "View(house_votes_Dem)\n",
      "Script\n",
      "22\n",
      "Step 2: run k-means\n",
      "# Define the columns to be clustered by subsetting the data\n",
      "clust_data_Dem = house_votes_Dem[, c(\"aye\", \"nay\", \"other\")]\n",
      "# Run an algorithm with 2 centers\n",
      "set.seed(1)\n",
      "kmeans_obj_Dem = kmeans(clust_data_Dem, centers = 2,\n",
      "algorithm = \"Lloyd\")\n",
      "# What does the new variable kmeans_obj contain?\n",
      "kmeans_obj_Dem\n",
      "# View the results of each output of the kmeans\n",
      "# function\n",
      "head(kmeans_obj_Dem)\n",
      "Script\n",
      "1. By placing the set of data we want\n",
      "after the comma, we tell R we’re\n",
      "looking for columns\n",
      "2. kmeans uses a different starting\n",
      "data point each time it runs. To\n",
      "make the results reproducible\n",
      "make R start from the same point\n",
      "every time with set.seed()\n",
      "3. We’re not specifying the number\n",
      "of iterations so R defaults to 10\n",
      "4. We’ll see that kmeans produces a\n",
      "list    of vectors of different\n",
      "lengths. As a result, we cannot use\n",
      "the View() function\n",
      "23\n",
      "Step 2: run k-means\n",
      "1. Number of points each cluster contains\n",
      "2. The “location” of each cluster center is specified by 3\n",
      "coordinates, one for each column we’re clustering\n",
      "3. The list assigning either cluster 1 or 2 to each data point\n",
      "1. 79.5% of the variance between the data points is explained\n",
      "by our clustering, we will discuss this in detail later\n",
      "2. List of other types of data included in kmeans_obj\n",
      "24\n",
      "• cluster: a vector indicating the cluster to which each point is allocated\n",
      "• centers: a matrix of cluster centers\n",
      "• totss: the total sum of squares (sum of distances between all points)\n",
      "• withinss: vector of within-cluster sum of distances, one number per cluster\n",
      "• tot.withinss: total within-cluster sum of distances, i.e. sum of withinss\n",
      "• betweenss: the between-cluster sum of squares, i.e. totss - tot.withinss\n",
      "• size: the number of points in each cluster\n",
      "To learn more about the kmeans function run ?kmeans\n",
      "kmeans outputs\n",
      "26\n",
      "Intra vs. inter-cluster distance\n",
      "Intra-Cluster\n",
      "Distance\n",
      "Inter-Cluster\n",
      "Distance\n",
      "withinss\n",
      "betweenss\n",
      "totss = withinss + betweenss\n",
      "27\n",
      "Step 3: visualize plot\n",
      "# Tell R to read the cluster labels as factors so that ggplot2 (the\n",
      "# graphing  package) can read them as category labels instead of\n",
      "# continuous variables (numeric variables).\n",
      "party_clusters_Dem = as.factor(kmeans_obj_Dem$cluster)\n",
      "# What does party_clusters look like?\n",
      "View(party_clusters_Dem)\n",
      "View(as.data.frame(party_clusters_Dem))\n",
      "# Set up labels for our data so that we can compare Democrats and\n",
      "# Republicans.\n",
      "party_labels_Dem = house_votes_Dem$party\n",
      "Script\n",
      "28\n",
      "ggplot(house_votes_Dem, aes(x = aye,\n",
      "y = nay,\n",
      "shape = party_clusters_Dem)) +\n",
      "geom_point(size = 6) +\n",
      "ggtitle(\"Aye vs. Nay votes for Democrat-introduced bills\") +\n",
      "xlab(\"Number of Aye Votes\") +\n",
      "ylab(\"Number of Nay Votes\") +\n",
      "scale_shape_manual(name = \"Cluster\",\n",
      "labels = c(\"Cluster 1\", \"Cluster 2\"),\n",
      "values = c(\"1\", \"2\")) +\n",
      "theme_light()\n",
      "Step 3: visualize plot\n",
      "Cosmetics layer\n",
      "Base layer\n",
      "Geom Layer\n",
      "Titles and axis\n",
      "Shape\n",
      "Theme\n",
      "29\n",
      "Script\n",
      "Step 3: visualize plot\n",
      "30\n",
      "• Two groups exist\n",
      "• Algorithm identifies voting\n",
      "patterns\n",
      "What can we infer about\n",
      "the different clusters?\n",
      "Step 4: analyze results\n",
      "31\n",
      "ggplot(house_votes_Dem, aes(x = yea,\n",
      "y = nay,\n",
      "color = party_labels_Dem,\n",
      "shape = party_clusters_Dem)) +\n",
      "geom_point(size = 6) +\n",
      "ggtitle(\"Aye vs. Nay votes for Democrat-introduced bills\") +\n",
      "xlab(\"Number of Aye Votes\") +\n",
      "ylab(\"Number of Nay Votes\") +\n",
      "scale_shape_manual(name = \"Cluster\",\n",
      "labels = c(\"Cluster 1\", \"Cluster 2\"),\n",
      "values = c(\"1\", \"2\")) +\n",
      "scale_color_manual(name = \"Party\",\n",
      "labels = c(\"Democratic\", \"Republican\"),\n",
      "values = c(\"blue\", \"red\")) +\n",
      "theme_light()\n",
      "Step 5: validate results\n",
      "Cosmetics layer\n",
      "Script\n",
      "Base layer\n",
      "Geom Layer\n",
      "Titles and axis\n",
      "Color and\n",
      "shape\n",
      "Theme\n",
      "32\n",
      "Step 5: validate results\n",
      "33\n",
      "• Diffuse among Democrats\n",
      "• Republicans more dense\n",
      "• Can gauge “outliers”\n",
      "• Can see the polarization\n",
      "between the two political parties\n",
      "Step 6: interpret results\n",
      "34\n",
      "• Clustering is more powerful than the\n",
      "human eye in 3D\n",
      "• Clustering mathematically defines\n",
      "which cluster the peripheral points\n",
      "should be in when it’s not obvious to\n",
      "the human eye\n",
      "• Clustering is helpful when many\n",
      "dimensions / variables exist that you\n",
      "can’t visualize at once\n",
      "– Whiskey similarity example from\n",
      "classification lecture\n",
      "Clustering vs. visualizing\n",
      "Aye, Nay and Other Votes\n",
      "in House of Representatives\n",
      "35\n",
      "• Goals of clustering:\n",
      "– Maximize the separation between clusters\n",
      "• i.e. Maximize inter-cluster distance\n",
      "– Keep similar points in a\n",
      "cluster close together\n",
      "• i.e. Minimize intra-cluster distance\n",
      "How good is the clustering?\n",
      "36\n",
      "• Look at the variance explained by\n",
      "clusters\n",
      "– In particular, the ratio of inter-cluster\n",
      "variance to total variance\n",
      "• How much of the total variance is\n",
      "explained by the clustering?\n",
      "Assessing how well an algorithm performs\n",
      "How good is the clustering?\n",
      "Variation explained by clusters\n",
      "=\n",
      "inter-cluster variance / total variance\n",
      "37\n",
      "• cluster: a vector indicating the cluster to which each point is allocated\n",
      "• centers: a matrix of cluster centers\n",
      "• totss: the total sum of squares (sum of distances between all points)\n",
      "• withinss: vector of within-cluster sum of distances, one number per cluster\n",
      "• tot.withinss: total within-cluster sum of distances, i.e. sum of withinss\n",
      "• betweenss: the between-cluster sum of squares, i.e. totss - tot.withinss\n",
      "• size: the number of points in each cluster\n",
      "To learn more about the kmeans function run ?kmeans\n",
      "kmeans outputs\n",
      "38\n",
      "Intra vs. inter-cluster distance\n",
      "Intra-Cluster\n",
      "Distance\n",
      "Inter-Cluster\n",
      "Distance\n",
      "withinss\n",
      "betweenss\n",
      "totss = withinss + betweenss\n",
      "39\n",
      "How good is the clustering?\n",
      "# Inter-cluster variance,\n",
      "# \"betweenss\" is the sum of the\n",
      "# distances between points from\n",
      "# different clusters\n",
      "num_Dem = kmeans_obj_Dem$betweenss\n",
      "# Total variance\n",
      "# \"totss\" is the sum of the distances\n",
      "# between all the points in\n",
      "# the data set\n",
      "denom_Dem = kmeans_obj_Dem$totss\n",
      "# Variance accounted for by\n",
      "# clusters\n",
      "var_exp_Dem = num_Dem / denom_Dem\n",
      "var_exp_Dem\n",
      "[1] 0.7193405\n",
      "Script\n",
      "40\n",
      "• It’s easier when the number of clusters is known ahead of time, but what if we don't\n",
      "know how many clusters we should have?\n",
      "• Since different starting points may generate different clusters, we need a way to\n",
      "assess cluster quality as well.\n",
      "How do we choose the number of clusters (i.e. k)?\n",
      "How good is the clustering?\n",
      "41\n",
      "1. Elbow method\n",
      "– Computes the percentage of variance explained by clusters for a range of cluster\n",
      "numbers\n",
      "– Plots a graph so results are easier to see\n",
      "– Not guaranteed to work! It depends on the data in question\n",
      "2.\n",
      "NbClust\n",
      "How to select k: two methods\n",
      "– Runs 30 different tests and\n",
      "provides “majority vote” for\n",
      "the best number of clusters\n",
      "(k’s) to use\n",
      "42\n",
      "Elbow method: measure variance\n",
      "# Run algorithm with 3 centers\n",
      "set.seed(1)\n",
      "kmeans_obj_Dem = kmeans(clust_data_Dem,\n",
      "centers = 3,\n",
      "algorithm = \"Lloyd\")\n",
      "# Inter-cluster variance\n",
      "num_Dem = kmeans_obj_Dem$betweenss\n",
      "# Total variance\n",
      "denom_Dem = kmeans_obj_Dem$totss\n",
      "# Variance accounted for by clusters\n",
      "var_exp_Dem = num_Dem / denom_Dem\n",
      "var_exp_Dem\n",
      "[1] 0.7949741\n",
      "Script\n",
      "43\n",
      "• We want to repeat the variance calculation from the previous slide for several\n",
      "numbers of clusters automatically\n",
      "• We can create a function that contains all the steps we want to automate\n",
      "Automating a step we want to repeat\n",
      "function(data, item to iterate through)\n",
      "44\n",
      "# The function explained_variance wraps our code from previous slides.\n",
      "explained_variance = function(data_in, k){\n",
      "# Running k-means algorithm\n",
      "set.seed(1)\n",
      "kmeans_obj = kmeans(data_in, centers = k,\n",
      "algorithm = \"Lloyd\")\n",
      "# Variance accounted for by clusters\n",
      "var_exp = kmeans_obj$betweenss /\n",
      "kmeans_obj$totss\n",
      "var_exp\n",
      "}\n",
      "Automating a step we want to repeat\n",
      "Script\n",
      "1. A new variable is created and set equal\n",
      "to our function()\n",
      "2. The commands inside the function are\n",
      "wrapped in curly braces {}\n",
      "3. Inside the parentheses, we specify the\n",
      "variables that the user will input and\n",
      "that will then be used inside the\n",
      "function where they appear\n",
      "45\n",
      "# Recall the variable we are using for the\n",
      "# data that we're clustering.\n",
      "clust_data_Dem = house_votes_Dem[, c(\"aye\", \"nay\", \"other\")]\n",
      "View(clust_data_Dem)\n",
      "# The sapply() function plugs several values\n",
      "# into explained_variance.\n",
      "explained_var_Dem = sapply(1:10, explained_variance,\n",
      "data_in = clust_data_Dem)\n",
      "View(explained_var_Dem)\n",
      "# Data for ggplot2\n",
      "elbow_data_Dem = data.frame(k = 1:10,\n",
      "explained_var_Dem)\n",
      "View(elbow_data_Dem)\n",
      "Automating a step we want to repeat\n",
      "1.sapply() applies a function to a\n",
      "vector\n",
      "2. We have to tell sapply() that\n",
      "the we want the\n",
      "explained_variance function\n",
      "to use the clust_data data\n",
      "3. Next, we create a data frame that\n",
      "contains both the new variance\n",
      "variable (explained_var_Dem)\n",
      "and the different numbers of k\n",
      "that we used in the previous\n",
      "function (1 through 10)\n",
      "Function we created\n",
      "Script\n",
      "46\n",
      "# Plotting data\n",
      "ggplot(elbow_data_Dem,\n",
      "aes(x = k,\n",
      "y = explained_var_Dem)) +\n",
      "geom_point(size = 4) +\n",
      "geom_line(size = 1) +\n",
      "xlab(\"k\") +\n",
      "ylab(\"Intercluster Variance/Total Variance\") +\n",
      "theme_light()\n",
      "Elbow method: plotting the graph\n",
      "Script\n",
      "1. geom_point() sets the size of the data points\n",
      "2. geom_line() sets the thickness of the line\n",
      "47\n",
      "Looking for the kink in graph of  inter-cluster variance / total\n",
      "variance\n",
      "Elbow method: measure variance\n",
      "Original data\n",
      "Elbow method\n",
      "k = 2\n",
      "48\n",
      "• Library: \"NbClust\"\n",
      "Functions:  \"NbClust\"\n",
      "Inputs:\n",
      "• data – data array or data frame\n",
      "• min.nc / max.nc – minimum/maximum number of clusters\n",
      "• method – \"kmeans\"\n",
      "• There are other, more advanced arguments that can be customized but are outside of\n",
      "the scope of this course and are note necessary to for NbClust to work\n",
      "There are a number of ways to choose the right k.\n",
      "NbClust runs 30 tests and selects k based on majority vote\n",
      "NbClust: k by majority vote\n",
      "NbClust(data, max.nc, method = \"kmeans\")\n",
      "49\n",
      "# Install the package.\n",
      "install.packages(\"NbClust\")\n",
      "library(NbClust)\n",
      "# Run NbClust.\n",
      "nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")\n",
      "# View the output of NbClust.\n",
      "nbclust_obj_Dem\n",
      "# View the output that shows the number of clusters each\n",
      "# method recommends.\n",
      "View(nbclust_obj_Dem$Best.nc)\n",
      "NbClust: k by majority vote\n",
      "Script\n",
      "50\n",
      "NbClust: k by majority vote\n",
      "> nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")\n",
      "...\n",
      "*******************************************************************\n",
      "* Among all indices:\n",
      "* 14 proposed 2 as the best number of clusters\n",
      "* 3 proposed 3 as the best number of clusters\n",
      "* 1 proposed 4 as the best number of clusters\n",
      "* 3 proposed 6 as the best number of clusters\n",
      "* 1 proposed 9 as the best number of clusters\n",
      "* 1 proposed 10 as the best number of clusters\n",
      "* 1 proposed 15 as the best number of clusters\n",
      "***** Conclusion *****\n",
      "* According to the majority rule, the best number of clusters is  2\n",
      "Note: additional information appears; the above information is most relevant to us for now\n",
      "Console\n",
      "51\n",
      "NbClust: k by majority vote\n",
      "> nbclust_obj_Dem\n",
      "Console\n",
      "• nbclust_obj_Dem shows the outputs of NbClust\n",
      "– One of the outputs is Best.nc, which shows the number of clusters\n",
      "recommended by each test\n",
      "52\n",
      "NbClust: k by majority vote\n",
      "• We want to visualize a histogram to make it obvious how many votes there are\n",
      "for each number of clusters\n",
      "53\n",
      "# Subset the 1st row from Best.nc and convert it\n",
      "# to a data frame, so ggplot2 can plot it.\n",
      "freq_k_Dem = nbclust_obj_Dem$Best.nc[1,]\n",
      "freq_k_Dem = data.frame(freq_k_Dem)\n",
      "View(freq_k_Dem)\n",
      "# Check the maximum number of clusters.\n",
      "max(freq_k_Dem)\n",
      "# Plot as a histogram.\n",
      "ggplot(freq_k_Dem,\n",
      "aes(x = freq_k_Dem)) +\n",
      "geom_bar() +\n",
      "scale_x_continuous(breaks = seq(0, 15, by = 1)) +\n",
      "scale_y_continuous(breaks = seq(0, 12, by = 1)) +\n",
      "labs(x = \"Number of Clusters\",\n",
      "y = \"Number of Votes\",\n",
      "title = \"Cluster Analysis\")\n",
      "NbClust: k by majority vote\n",
      "Script\n",
      "2 clusters is the\n",
      "winner with 12 votes\n",
      "54\n",
      "• If you’re a lobbyist, which congressperson can you influence for swing votes?\n",
      "• If you’re managing a campaign and your competitor is always voting along\n",
      "party lines, how can you use that information?\n",
      "• If your congressperson is not an active voter, is she representing your\n",
      "interests?\n",
      "• What do the voting patterns look like for Republican-introduced bills?\n",
      "Application of results\n",
      "55\n",
      "• Could see differences between the\n",
      "patterns of Reb lead bills and Democrat\n",
      "lead bills\n",
      "• Could provide information on\n",
      "congressmen that might be see has\n",
      "swing votes.\n",
      "Implications of results\n",
      "56\n",
      "• We are assuming that the patterns correspond with the same bills being voted\n",
      "on – perhaps some Congressmen have the same number of 'aye' and 'nay'\n",
      "votes, but voted on different bills\n",
      "• Network analysis can help determine additional connections between\n",
      "Congressmen\n",
      "• We haven't taken extenuating factors into account – political initiatives,\n",
      "current events, etc.\n",
      "This is a preliminary analysis that gives us initial\n",
      "insights and can help us direct further research\n",
      "Limitations of results\n",
      "57\n",
      "• The good and bad\n",
      "– + cheap – NO LABELS, labels are expensive to create and maintain\n",
      "– +/- clustering always works\n",
      "– - Many methods to choose from and knowing the right one can be nontrivial and the\n",
      "differences between many are almost zero, so you need to understand what you're\n",
      "doing\n",
      "• The evil\n",
      "– Curse of dimensionality\n",
      "– Clusters may result from poor data quality\n",
      "– Non-deterministic (e.g. k-means) subject to local minimum. Since it works with\n",
      "averages, k-means does not get much better with Big Data (marginal improvements)\n",
      "– Non spherical data may result in poor clustering (depending on method used)\n",
      "– Unequal cluster sizes may result in poor clustering (depending on method used)\n",
      "The good, bad, and evil\n",
      "58\n",
      "59\n",
      "• Analysts need to ask the following questions\n",
      "– Do you want overlapping or non-overlapping clusters?\n",
      "– Does your data satisfy the assumptions of the clustering algorithm?\n",
      "– How was the distance measure identified?\n",
      "– How many clusters and why? Identifying the number of clusters is a difficult task if the\n",
      "number of class labels is not known beforehand\n",
      "– Does your method scale to the size of the data?\n",
      "– Is the compute time congruent with the temporal budget of your business need (i.e.\n",
      "do you get answers back in time to make meaningful decisions)\n",
      "The good, bad, and evil\n",
      "60\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Define the maximum chunk size\n",
    "    chunk_overlap=100  # Define the overlap between chunks\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(combined_texts)\n",
    "\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n{'-'*80}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hgPvjKNwbjBN",
    "outputId": "86b31815-ba3d-4544-8fdd-c679a23c7bd7",
    "ExecuteTime": {
     "end_time": "2025-01-29T17:22:07.562185Z",
     "start_time": "2025-01-29T17:22:07.556635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "Machine Learning Overview, EDA and Clustering\n",
      "Brian Wright\n",
      "brianwright@virginia.edu\n",
      "2\n",
      "3\n",
      "Given  D :{2,4,10,12,3,11,20,25,30},  and  k=2 clusters\n",
      "Randomly assign the means:  m1=3, m2=4\n",
      "K1={2,3}, K2={4,10,12,11,20,25,30}, m1=2.5, m2=16\n",
      "K1={2,3,4}, K2={10,12,11,20,25,30}, m1=3, m2=18\n",
      "K1={2,3,4,10}, K2={12,11,20,25,30}, m1=4.75, m2=19.6\n",
      "K1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\n",
      "K1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\n",
      "Stop, since the clusters and the means found in\n",
      "all subsequent iterations will be the same.\n",
      "Example of K-Means\n",
      "1. What is Machine Learning?\n",
      "2. What is exploratory data analysis?\n",
      "3. k-means clustering\n",
      "– Does Congress vote in patterns?\n",
      "4. Multi-dimensional k-means clustering\n",
      "– Are NBA players compensated according to performance?\n",
      "Outline: Intro to Unsupervised ML\n",
      "4\n",
      "• Exploratory data analysis or “EDA” is an approach where the intent is to see\n",
      "what the data can tell us beyond modeling or hypothesis testing\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 2:\n",
      "what the data can tell us beyond modeling or hypothesis testing\n",
      "– Data visualization is one of the most common forms of EDA\n",
      "What is exploratory data analysis?\n",
      "5\n",
      "When data is too big or complex to be analyzed just by\n",
      "visualizing it, these types of analysis can help:\n",
      "1. Clustering: compare pieces of data by measuring\n",
      "similarity among them\n",
      "2. Network analysis: analyze how people, places and\n",
      "entities are connected to evaluate the properties and\n",
      "structure of a network\n",
      "3. Text mining: analyze what large bodies of\n",
      "unstructured or structured text say\n",
      "Types of exploratory data analysis\n",
      "6\n",
      "The data inputs have (x) no target outputs (y)\n",
      "Unsupervised machine learning\n",
      "Input x:\n",
      "Voter\n",
      "Output y:\n",
      "Not given\n",
      "(To be discovered)\n",
      "?\n",
      "We want to impose structure on the inputs (x) to say something\n",
      "meaningful about the data\n",
      "7\n",
      "1. Technique for finding similarity between groups\n",
      "2. Type of unsupervised machine learning\n",
      "• Not the only class of unsupervised learning\n",
      "algorithms\n",
      "3. Similarity needs to be defined\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 3:\n",
      "• Not the only class of unsupervised learning\n",
      "algorithms\n",
      "3. Similarity needs to be defined\n",
      "• Will depend on attributes of data\n",
      "• Usually a distance metric\n",
      "What is clustering?\n",
      "8\n",
      "Key assumption: data points that are “closer” together are related or similar\n",
      "• Haimowitz and Schwarz 1997 paper on\n",
      "clustering for credit line optimization\n",
      "– http://www.aaai.org/Papers/Workshops/1997/\n",
      "WS-97-07/WS97-07-006.pdf\n",
      "• Cluster existing GE Capital customers based\n",
      "on similarity in use of their credit cards to\n",
      "pay bills and customers’ profitability to GE\n",
      "Capital\n",
      "• Resulted in five clusters of consumer credit\n",
      "behavior\n",
      "• Created classification model to predict\n",
      "customer type and offer tailored products\n",
      "GE Capital case study: grouping clients\n",
      "9\n",
      "Example use case\n",
      "General question\n",
      "Concept\n",
      "Does Congress vote in patterns?\n",
      "Is there a pattern?\n",
      "Is there structure in\n",
      "unstructured data?\n",
      "k-means clustering\n",
      "Are basketball players \"priced\"\n",
      "efficiently (based on performance)?\n",
      "How to uncover trends with\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 4:\n",
      "Are basketball players \"priced\"\n",
      "efficiently (based on performance)?\n",
      "How to uncover trends with\n",
      "many variables that you\n",
      "can't easily visualize?\n",
      "k-means clustering in\n",
      "many dimensions\n",
      "Concept summary\n",
      "12\n",
      "1. Data set consists of 427 members\n",
      "(observations)\n",
      "2. Members served a full year in 2013\n",
      "3. Three vote types:\n",
      "• “Aye”\n",
      "• “Nay”\n",
      "• “Other”\n",
      "Goal: to understand how polarized the\n",
      "US Congress is\n",
      "Political clustering\n",
      "The joint session of Congress on Capitol Hill\n",
      "in Washington\n",
      "13\n",
      "• How do we identify\n",
      "swing votes?\n",
      "– Lobbying\n",
      "– Bridging party lines\n",
      "• Assumption:\n",
      "– Democrats and Republicans\n",
      "vote among partisan lines,\n",
      "which generates clusters\n",
      "Each data point represents a member of Congress\n",
      "Finding voting patterns\n",
      "14\n",
      "Intra vs. inter-cluster distance\n",
      "Intra-Cluster\n",
      "Distance\n",
      "Inter-Cluster\n",
      "Distance\n",
      "Objective: minimize intra-cluster distance, maximize inter-cluster distance\n",
      "15\n",
      "• The centroid is the average location\n",
      "of all points in the cluster\n",
      "• Another definition: the centroid\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 5:\n",
      "of all points in the cluster\n",
      "• Another definition: the centroid\n",
      "minimizes the distance between a\n",
      "central location and all the data\n",
      "points in the cluster\n",
      "Note: Centroids are generally not\n",
      "existing data points, rather locations in\n",
      "space\n",
      "k-means clustering is based on\n",
      "centroids\n",
      "16\n",
      "1. Randomly choose k data points\n",
      "to be centroids\n",
      "k-means in 4 steps\n",
      "17\n",
      "1. Randomly choose k data points\n",
      "to be centroids\n",
      "2. Assign each point to closest\n",
      "centroid\n",
      "k-means in 4 steps\n",
      "18\n",
      "1. Randomly choose k data points\n",
      "to be centroids\n",
      "2. Assign each point to closest\n",
      "centroid\n",
      "3. Recalculate centroids based on\n",
      "current cluster membership\n",
      "k-means in 4 steps\n",
      "19\n",
      "1. Randomly choose k data\n",
      "points to be centroids\n",
      "2. Assign each point to closest\n",
      "centroid\n",
      "3. Recalculate centroids based\n",
      "on current cluster\n",
      "membership\n",
      "k-means in 4 steps\n",
      "20\n",
      "4. Repeat steps 2-3 with the new centroids until the centroids don’t\n",
      "change anymore\n",
      "Step 1: load packages and data\n",
      "# Install packages\n",
      "install.packages(\"e1071\")\n",
      "install.packages(\"ggplot2\")\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 6:\n",
      "# Install packages\n",
      "install.packages(\"e1071\")\n",
      "install.packages(\"ggplot2\")\n",
      "# Load libraries\n",
      "library(e1071)\n",
      "library(ggplot2)\n",
      "library(help = e1071)\n",
      "Learn about all the functionality of the package, be\n",
      "well informed about what you're doing!\n",
      "21\n",
      "Step 1: load packages and data\n",
      "# Loading house data\n",
      "house_votes_Dem = read_csv(\"house_votes_Dem.csv\")\n",
      "# What does the data look like?\n",
      "View(house_votes_Dem)\n",
      "Script\n",
      "22\n",
      "Step 2: run k-means\n",
      "# Define the columns to be clustered by subsetting the data\n",
      "clust_data_Dem = house_votes_Dem[, c(\"aye\", \"nay\", \"other\")]\n",
      "# Run an algorithm with 2 centers\n",
      "set.seed(1)\n",
      "kmeans_obj_Dem = kmeans(clust_data_Dem, centers = 2,\n",
      "algorithm = \"Lloyd\")\n",
      "# What does the new variable kmeans_obj contain?\n",
      "kmeans_obj_Dem\n",
      "# View the results of each output of the kmeans\n",
      "# function\n",
      "head(kmeans_obj_Dem)\n",
      "Script\n",
      "1. By placing the set of data we want\n",
      "after the comma, we tell R we’re\n",
      "looking for columns\n",
      "2. kmeans uses a different starting\n",
      "data point each time it runs. To\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 7:\n",
      "looking for columns\n",
      "2. kmeans uses a different starting\n",
      "data point each time it runs. To\n",
      "make the results reproducible\n",
      "make R start from the same point\n",
      "every time with set.seed()\n",
      "3. We’re not specifying the number\n",
      "of iterations so R defaults to 10\n",
      "4. We’ll see that kmeans produces a\n",
      "list    of vectors of different\n",
      "lengths. As a result, we cannot use\n",
      "the View() function\n",
      "23\n",
      "Step 2: run k-means\n",
      "1. Number of points each cluster contains\n",
      "2. The “location” of each cluster center is specified by 3\n",
      "coordinates, one for each column we’re clustering\n",
      "3. The list assigning either cluster 1 or 2 to each data point\n",
      "1. 79.5% of the variance between the data points is explained\n",
      "by our clustering, we will discuss this in detail later\n",
      "2. List of other types of data included in kmeans_obj\n",
      "24\n",
      "• cluster: a vector indicating the cluster to which each point is allocated\n",
      "• centers: a matrix of cluster centers\n",
      "• totss: the total sum of squares (sum of distances between all points)\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 8:\n",
      "• totss: the total sum of squares (sum of distances between all points)\n",
      "• withinss: vector of within-cluster sum of distances, one number per cluster\n",
      "• tot.withinss: total within-cluster sum of distances, i.e. sum of withinss\n",
      "• betweenss: the between-cluster sum of squares, i.e. totss - tot.withinss\n",
      "• size: the number of points in each cluster\n",
      "To learn more about the kmeans function run ?kmeans\n",
      "kmeans outputs\n",
      "26\n",
      "Intra vs. inter-cluster distance\n",
      "Intra-Cluster\n",
      "Distance\n",
      "Inter-Cluster\n",
      "Distance\n",
      "withinss\n",
      "betweenss\n",
      "totss = withinss + betweenss\n",
      "27\n",
      "Step 3: visualize plot\n",
      "# Tell R to read the cluster labels as factors so that ggplot2 (the\n",
      "# graphing  package) can read them as category labels instead of\n",
      "# continuous variables (numeric variables).\n",
      "party_clusters_Dem = as.factor(kmeans_obj_Dem$cluster)\n",
      "# What does party_clusters look like?\n",
      "View(party_clusters_Dem)\n",
      "View(as.data.frame(party_clusters_Dem))\n",
      "# Set up labels for our data so that we can compare Democrats and\n",
      "# Republicans.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 9:\n",
      "# Set up labels for our data so that we can compare Democrats and\n",
      "# Republicans.\n",
      "party_labels_Dem = house_votes_Dem$party\n",
      "Script\n",
      "28\n",
      "ggplot(house_votes_Dem, aes(x = aye,\n",
      "y = nay,\n",
      "shape = party_clusters_Dem)) +\n",
      "geom_point(size = 6) +\n",
      "ggtitle(\"Aye vs. Nay votes for Democrat-introduced bills\") +\n",
      "xlab(\"Number of Aye Votes\") +\n",
      "ylab(\"Number of Nay Votes\") +\n",
      "scale_shape_manual(name = \"Cluster\",\n",
      "labels = c(\"Cluster 1\", \"Cluster 2\"),\n",
      "values = c(\"1\", \"2\")) +\n",
      "theme_light()\n",
      "Step 3: visualize plot\n",
      "Cosmetics layer\n",
      "Base layer\n",
      "Geom Layer\n",
      "Titles and axis\n",
      "Shape\n",
      "Theme\n",
      "29\n",
      "Script\n",
      "Step 3: visualize plot\n",
      "30\n",
      "• Two groups exist\n",
      "• Algorithm identifies voting\n",
      "patterns\n",
      "What can we infer about\n",
      "the different clusters?\n",
      "Step 4: analyze results\n",
      "31\n",
      "ggplot(house_votes_Dem, aes(x = yea,\n",
      "y = nay,\n",
      "color = party_labels_Dem,\n",
      "shape = party_clusters_Dem)) +\n",
      "geom_point(size = 6) +\n",
      "ggtitle(\"Aye vs. Nay votes for Democrat-introduced bills\") +\n",
      "xlab(\"Number of Aye Votes\") +\n",
      "ylab(\"Number of Nay Votes\") +\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 10:\n",
      "xlab(\"Number of Aye Votes\") +\n",
      "ylab(\"Number of Nay Votes\") +\n",
      "scale_shape_manual(name = \"Cluster\",\n",
      "labels = c(\"Cluster 1\", \"Cluster 2\"),\n",
      "values = c(\"1\", \"2\")) +\n",
      "scale_color_manual(name = \"Party\",\n",
      "labels = c(\"Democratic\", \"Republican\"),\n",
      "values = c(\"blue\", \"red\")) +\n",
      "theme_light()\n",
      "Step 5: validate results\n",
      "Cosmetics layer\n",
      "Script\n",
      "Base layer\n",
      "Geom Layer\n",
      "Titles and axis\n",
      "Color and\n",
      "shape\n",
      "Theme\n",
      "32\n",
      "Step 5: validate results\n",
      "33\n",
      "• Diffuse among Democrats\n",
      "• Republicans more dense\n",
      "• Can gauge “outliers”\n",
      "• Can see the polarization\n",
      "between the two political parties\n",
      "Step 6: interpret results\n",
      "34\n",
      "• Clustering is more powerful than the\n",
      "human eye in 3D\n",
      "• Clustering mathematically defines\n",
      "which cluster the peripheral points\n",
      "should be in when it’s not obvious to\n",
      "the human eye\n",
      "• Clustering is helpful when many\n",
      "dimensions / variables exist that you\n",
      "can’t visualize at once\n",
      "– Whiskey similarity example from\n",
      "classification lecture\n",
      "Clustering vs. visualizing\n",
      "Aye, Nay and Other Votes\n",
      "in House of Representatives\n",
      "35\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 11:\n",
      "Clustering vs. visualizing\n",
      "Aye, Nay and Other Votes\n",
      "in House of Representatives\n",
      "35\n",
      "• Goals of clustering:\n",
      "– Maximize the separation between clusters\n",
      "• i.e. Maximize inter-cluster distance\n",
      "– Keep similar points in a\n",
      "cluster close together\n",
      "• i.e. Minimize intra-cluster distance\n",
      "How good is the clustering?\n",
      "36\n",
      "• Look at the variance explained by\n",
      "clusters\n",
      "– In particular, the ratio of inter-cluster\n",
      "variance to total variance\n",
      "• How much of the total variance is\n",
      "explained by the clustering?\n",
      "Assessing how well an algorithm performs\n",
      "How good is the clustering?\n",
      "Variation explained by clusters\n",
      "=\n",
      "inter-cluster variance / total variance\n",
      "37\n",
      "• cluster: a vector indicating the cluster to which each point is allocated\n",
      "• centers: a matrix of cluster centers\n",
      "• totss: the total sum of squares (sum of distances between all points)\n",
      "• withinss: vector of within-cluster sum of distances, one number per cluster\n",
      "• tot.withinss: total within-cluster sum of distances, i.e. sum of withinss\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 12:\n",
      "• tot.withinss: total within-cluster sum of distances, i.e. sum of withinss\n",
      "• betweenss: the between-cluster sum of squares, i.e. totss - tot.withinss\n",
      "• size: the number of points in each cluster\n",
      "To learn more about the kmeans function run ?kmeans\n",
      "kmeans outputs\n",
      "38\n",
      "Intra vs. inter-cluster distance\n",
      "Intra-Cluster\n",
      "Distance\n",
      "Inter-Cluster\n",
      "Distance\n",
      "withinss\n",
      "betweenss\n",
      "totss = withinss + betweenss\n",
      "39\n",
      "How good is the clustering?\n",
      "# Inter-cluster variance,\n",
      "# \"betweenss\" is the sum of the\n",
      "# distances between points from\n",
      "# different clusters\n",
      "num_Dem = kmeans_obj_Dem$betweenss\n",
      "# Total variance\n",
      "# \"totss\" is the sum of the distances\n",
      "# between all the points in\n",
      "# the data set\n",
      "denom_Dem = kmeans_obj_Dem$totss\n",
      "# Variance accounted for by\n",
      "# clusters\n",
      "var_exp_Dem = num_Dem / denom_Dem\n",
      "var_exp_Dem\n",
      "[1] 0.7193405\n",
      "Script\n",
      "40\n",
      "• It’s easier when the number of clusters is known ahead of time, but what if we don't\n",
      "know how many clusters we should have?\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 13:\n",
      "know how many clusters we should have?\n",
      "• Since different starting points may generate different clusters, we need a way to\n",
      "assess cluster quality as well.\n",
      "How do we choose the number of clusters (i.e. k)?\n",
      "How good is the clustering?\n",
      "41\n",
      "1. Elbow method\n",
      "– Computes the percentage of variance explained by clusters for a range of cluster\n",
      "numbers\n",
      "– Plots a graph so results are easier to see\n",
      "– Not guaranteed to work! It depends on the data in question\n",
      "2.\n",
      "NbClust\n",
      "How to select k: two methods\n",
      "– Runs 30 different tests and\n",
      "provides “majority vote” for\n",
      "the best number of clusters\n",
      "(k’s) to use\n",
      "42\n",
      "Elbow method: measure variance\n",
      "# Run algorithm with 3 centers\n",
      "set.seed(1)\n",
      "kmeans_obj_Dem = kmeans(clust_data_Dem,\n",
      "centers = 3,\n",
      "algorithm = \"Lloyd\")\n",
      "# Inter-cluster variance\n",
      "num_Dem = kmeans_obj_Dem$betweenss\n",
      "# Total variance\n",
      "denom_Dem = kmeans_obj_Dem$totss\n",
      "# Variance accounted for by clusters\n",
      "var_exp_Dem = num_Dem / denom_Dem\n",
      "var_exp_Dem\n",
      "[1] 0.7949741\n",
      "Script\n",
      "43\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 14:\n",
      "var_exp_Dem = num_Dem / denom_Dem\n",
      "var_exp_Dem\n",
      "[1] 0.7949741\n",
      "Script\n",
      "43\n",
      "• We want to repeat the variance calculation from the previous slide for several\n",
      "numbers of clusters automatically\n",
      "• We can create a function that contains all the steps we want to automate\n",
      "Automating a step we want to repeat\n",
      "function(data, item to iterate through)\n",
      "44\n",
      "# The function explained_variance wraps our code from previous slides.\n",
      "explained_variance = function(data_in, k){\n",
      "# Running k-means algorithm\n",
      "set.seed(1)\n",
      "kmeans_obj = kmeans(data_in, centers = k,\n",
      "algorithm = \"Lloyd\")\n",
      "# Variance accounted for by clusters\n",
      "var_exp = kmeans_obj$betweenss /\n",
      "kmeans_obj$totss\n",
      "var_exp\n",
      "}\n",
      "Automating a step we want to repeat\n",
      "Script\n",
      "1. A new variable is created and set equal\n",
      "to our function()\n",
      "2. The commands inside the function are\n",
      "wrapped in curly braces {}\n",
      "3. Inside the parentheses, we specify the\n",
      "variables that the user will input and\n",
      "that will then be used inside the\n",
      "function where they appear\n",
      "45\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 15:\n",
      "that will then be used inside the\n",
      "function where they appear\n",
      "45\n",
      "# Recall the variable we are using for the\n",
      "# data that we're clustering.\n",
      "clust_data_Dem = house_votes_Dem[, c(\"aye\", \"nay\", \"other\")]\n",
      "View(clust_data_Dem)\n",
      "# The sapply() function plugs several values\n",
      "# into explained_variance.\n",
      "explained_var_Dem = sapply(1:10, explained_variance,\n",
      "data_in = clust_data_Dem)\n",
      "View(explained_var_Dem)\n",
      "# Data for ggplot2\n",
      "elbow_data_Dem = data.frame(k = 1:10,\n",
      "explained_var_Dem)\n",
      "View(elbow_data_Dem)\n",
      "Automating a step we want to repeat\n",
      "1.sapply() applies a function to a\n",
      "vector\n",
      "2. We have to tell sapply() that\n",
      "the we want the\n",
      "explained_variance function\n",
      "to use the clust_data data\n",
      "3. Next, we create a data frame that\n",
      "contains both the new variance\n",
      "variable (explained_var_Dem)\n",
      "and the different numbers of k\n",
      "that we used in the previous\n",
      "function (1 through 10)\n",
      "Function we created\n",
      "Script\n",
      "46\n",
      "# Plotting data\n",
      "ggplot(elbow_data_Dem,\n",
      "aes(x = k,\n",
      "y = explained_var_Dem)) +\n",
      "geom_point(size = 4) +\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 16:\n",
      "# Plotting data\n",
      "ggplot(elbow_data_Dem,\n",
      "aes(x = k,\n",
      "y = explained_var_Dem)) +\n",
      "geom_point(size = 4) +\n",
      "geom_line(size = 1) +\n",
      "xlab(\"k\") +\n",
      "ylab(\"Intercluster Variance/Total Variance\") +\n",
      "theme_light()\n",
      "Elbow method: plotting the graph\n",
      "Script\n",
      "1. geom_point() sets the size of the data points\n",
      "2. geom_line() sets the thickness of the line\n",
      "47\n",
      "Looking for the kink in graph of  inter-cluster variance / total\n",
      "variance\n",
      "Elbow method: measure variance\n",
      "Original data\n",
      "Elbow method\n",
      "k = 2\n",
      "48\n",
      "• Library: \"NbClust\"\n",
      "Functions:  \"NbClust\"\n",
      "Inputs:\n",
      "• data – data array or data frame\n",
      "• min.nc / max.nc – minimum/maximum number of clusters\n",
      "• method – \"kmeans\"\n",
      "• There are other, more advanced arguments that can be customized but are outside of\n",
      "the scope of this course and are note necessary to for NbClust to work\n",
      "There are a number of ways to choose the right k.\n",
      "NbClust runs 30 tests and selects k based on majority vote\n",
      "NbClust: k by majority vote\n",
      "NbClust(data, max.nc, method = \"kmeans\")\n",
      "49\n",
      "# Install the package.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 17:\n",
      "NbClust: k by majority vote\n",
      "NbClust(data, max.nc, method = \"kmeans\")\n",
      "49\n",
      "# Install the package.\n",
      "install.packages(\"NbClust\")\n",
      "library(NbClust)\n",
      "# Run NbClust.\n",
      "nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")\n",
      "# View the output of NbClust.\n",
      "nbclust_obj_Dem\n",
      "# View the output that shows the number of clusters each\n",
      "# method recommends.\n",
      "View(nbclust_obj_Dem$Best.nc)\n",
      "NbClust: k by majority vote\n",
      "Script\n",
      "50\n",
      "NbClust: k by majority vote\n",
      "> nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")\n",
      "...\n",
      "*******************************************************************\n",
      "* Among all indices:\n",
      "* 14 proposed 2 as the best number of clusters\n",
      "* 3 proposed 3 as the best number of clusters\n",
      "* 1 proposed 4 as the best number of clusters\n",
      "* 3 proposed 6 as the best number of clusters\n",
      "* 1 proposed 9 as the best number of clusters\n",
      "* 1 proposed 10 as the best number of clusters\n",
      "* 1 proposed 15 as the best number of clusters\n",
      "***** Conclusion *****\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 18:\n",
      "* 1 proposed 15 as the best number of clusters\n",
      "***** Conclusion *****\n",
      "* According to the majority rule, the best number of clusters is  2\n",
      "Note: additional information appears; the above information is most relevant to us for now\n",
      "Console\n",
      "51\n",
      "NbClust: k by majority vote\n",
      "> nbclust_obj_Dem\n",
      "Console\n",
      "• nbclust_obj_Dem shows the outputs of NbClust\n",
      "– One of the outputs is Best.nc, which shows the number of clusters\n",
      "recommended by each test\n",
      "52\n",
      "NbClust: k by majority vote\n",
      "• We want to visualize a histogram to make it obvious how many votes there are\n",
      "for each number of clusters\n",
      "53\n",
      "# Subset the 1st row from Best.nc and convert it\n",
      "# to a data frame, so ggplot2 can plot it.\n",
      "freq_k_Dem = nbclust_obj_Dem$Best.nc[1,]\n",
      "freq_k_Dem = data.frame(freq_k_Dem)\n",
      "View(freq_k_Dem)\n",
      "# Check the maximum number of clusters.\n",
      "max(freq_k_Dem)\n",
      "# Plot as a histogram.\n",
      "ggplot(freq_k_Dem,\n",
      "aes(x = freq_k_Dem)) +\n",
      "geom_bar() +\n",
      "scale_x_continuous(breaks = seq(0, 15, by = 1)) +\n",
      "scale_y_continuous(breaks = seq(0, 12, by = 1)) +\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 19:\n",
      "scale_x_continuous(breaks = seq(0, 15, by = 1)) +\n",
      "scale_y_continuous(breaks = seq(0, 12, by = 1)) +\n",
      "labs(x = \"Number of Clusters\",\n",
      "y = \"Number of Votes\",\n",
      "title = \"Cluster Analysis\")\n",
      "NbClust: k by majority vote\n",
      "Script\n",
      "2 clusters is the\n",
      "winner with 12 votes\n",
      "54\n",
      "• If you’re a lobbyist, which congressperson can you influence for swing votes?\n",
      "• If you’re managing a campaign and your competitor is always voting along\n",
      "party lines, how can you use that information?\n",
      "• If your congressperson is not an active voter, is she representing your\n",
      "interests?\n",
      "• What do the voting patterns look like for Republican-introduced bills?\n",
      "Application of results\n",
      "55\n",
      "• Could see differences between the\n",
      "patterns of Reb lead bills and Democrat\n",
      "lead bills\n",
      "• Could provide information on\n",
      "congressmen that might be see has\n",
      "swing votes.\n",
      "Implications of results\n",
      "56\n",
      "• We are assuming that the patterns correspond with the same bills being voted\n",
      "on – perhaps some Congressmen have the same number of 'aye' and 'nay'\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 20:\n",
      "on – perhaps some Congressmen have the same number of 'aye' and 'nay'\n",
      "votes, but voted on different bills\n",
      "• Network analysis can help determine additional connections between\n",
      "Congressmen\n",
      "• We haven't taken extenuating factors into account – political initiatives,\n",
      "current events, etc.\n",
      "This is a preliminary analysis that gives us initial\n",
      "insights and can help us direct further research\n",
      "Limitations of results\n",
      "57\n",
      "• The good and bad\n",
      "– + cheap – NO LABELS, labels are expensive to create and maintain\n",
      "– +/- clustering always works\n",
      "– - Many methods to choose from and knowing the right one can be nontrivial and the\n",
      "differences between many are almost zero, so you need to understand what you're\n",
      "doing\n",
      "• The evil\n",
      "– Curse of dimensionality\n",
      "– Clusters may result from poor data quality\n",
      "– Non-deterministic (e.g. k-means) subject to local minimum. Since it works with\n",
      "averages, k-means does not get much better with Big Data (marginal improvements)\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 21:\n",
      "averages, k-means does not get much better with Big Data (marginal improvements)\n",
      "– Non spherical data may result in poor clustering (depending on method used)\n",
      "– Unequal cluster sizes may result in poor clustering (depending on method used)\n",
      "The good, bad, and evil\n",
      "58\n",
      "59\n",
      "• Analysts need to ask the following questions\n",
      "– Do you want overlapping or non-overlapping clusters?\n",
      "– Does your data satisfy the assumptions of the clustering algorithm?\n",
      "– How was the distance measure identified?\n",
      "– How many clusters and why? Identifying the number of clusters is a difficult task if the\n",
      "number of class labels is not known beforehand\n",
      "– Does your method scale to the size of the data?\n",
      "– Is the compute time congruent with the temporal budget of your business need (i.e.\n",
      "do you get answers back in time to make meaningful decisions)\n",
      "The good, bad, and evil\n",
      "60\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Convert chunks into LangChain Document objects\n",
    "documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "# Now `documents` is a list of Document objects ready to be used in a vector store\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i+1}:\\n{doc.page_content}\\n{'-'*80}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IkomjBMVc9tK",
    "outputId": "1e5a2da4-88ef-4926-aa41-0b910224abca",
    "ExecuteTime": {
     "end_time": "2025-01-29T17:22:13.170519Z",
     "start_time": "2025-01-29T17:22:13.167288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "Machine Learning Overview, EDA and Clustering\n",
      "Brian Wright\n",
      "brianwright@virginia.edu\n",
      "2\n",
      "3\n",
      "Given  D :{2,4,10,12,3,11,20,25,30},  and  k=2 clusters\n",
      "Randomly assign the means:  m1=3, m2=4\n",
      "K1={2,3}, K2={4,10,12,11,20,25,30}, m1=2.5, m2=16\n",
      "K1={2,3,4}, K2={10,12,11,20,25,30}, m1=3, m2=18\n",
      "K1={2,3,4,10}, K2={12,11,20,25,30}, m1=4.75, m2=19.6\n",
      "K1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\n",
      "K1={2,3,4,10,11,12}, K2={20,25,30}, m1=7, m2=25\n",
      "Stop, since the clusters and the means found in\n",
      "all subsequent iterations will be the same.\n",
      "Example of K-Means\n",
      "1. What is Machine Learning?\n",
      "2. What is exploratory data analysis?\n",
      "3. k-means clustering\n",
      "– Does Congress vote in patterns?\n",
      "4. Multi-dimensional k-means clustering\n",
      "– Are NBA players compensated according to performance?\n",
      "Outline: Intro to Unsupervised ML\n",
      "4\n",
      "• Exploratory data analysis or “EDA” is an approach where the intent is to see\n",
      "what the data can tell us beyond modeling or hypothesis testing\n",
      "--------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "what the data can tell us beyond modeling or hypothesis testing\n",
      "– Data visualization is one of the most common forms of EDA\n",
      "What is exploratory data analysis?\n",
      "5\n",
      "When data is too big or complex to be analyzed just by\n",
      "visualizing it, these types of analysis can help:\n",
      "1. Clustering: compare pieces of data by measuring\n",
      "similarity among them\n",
      "2. Network analysis: analyze how people, places and\n",
      "entities are connected to evaluate the properties and\n",
      "structure of a network\n",
      "3. Text mining: analyze what large bodies of\n",
      "unstructured or structured text say\n",
      "Types of exploratory data analysis\n",
      "6\n",
      "The data inputs have (x) no target outputs (y)\n",
      "Unsupervised machine learning\n",
      "Input x:\n",
      "Voter\n",
      "Output y:\n",
      "Not given\n",
      "(To be discovered)\n",
      "?\n",
      "We want to impose structure on the inputs (x) to say something\n",
      "meaningful about the data\n",
      "7\n",
      "1. Technique for finding similarity between groups\n",
      "2. Type of unsupervised machine learning\n",
      "• Not the only class of unsupervised learning\n",
      "algorithms\n",
      "3. Similarity needs to be defined\n",
      "--------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "• Not the only class of unsupervised learning\n",
      "algorithms\n",
      "3. Similarity needs to be defined\n",
      "• Will depend on attributes of data\n",
      "• Usually a distance metric\n",
      "What is clustering?\n",
      "8\n",
      "Key assumption: data points that are “closer” together are related or similar\n",
      "• Haimowitz and Schwarz 1997 paper on\n",
      "clustering for credit line optimization\n",
      "– http://www.aaai.org/Papers/Workshops/1997/\n",
      "WS-97-07/WS97-07-006.pdf\n",
      "• Cluster existing GE Capital customers based\n",
      "on similarity in use of their credit cards to\n",
      "pay bills and customers’ profitability to GE\n",
      "Capital\n",
      "• Resulted in five clusters of consumer credit\n",
      "behavior\n",
      "• Created classification model to predict\n",
      "customer type and offer tailored products\n",
      "GE Capital case study: grouping clients\n",
      "9\n",
      "Example use case\n",
      "General question\n",
      "Concept\n",
      "Does Congress vote in patterns?\n",
      "Is there a pattern?\n",
      "Is there structure in\n",
      "unstructured data?\n",
      "k-means clustering\n",
      "Are basketball players \"priced\"\n",
      "efficiently (based on performance)?\n",
      "How to uncover trends with\n",
      "--------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "Are basketball players \"priced\"\n",
      "efficiently (based on performance)?\n",
      "How to uncover trends with\n",
      "many variables that you\n",
      "can't easily visualize?\n",
      "k-means clustering in\n",
      "many dimensions\n",
      "Concept summary\n",
      "12\n",
      "1. Data set consists of 427 members\n",
      "(observations)\n",
      "2. Members served a full year in 2013\n",
      "3. Three vote types:\n",
      "• “Aye”\n",
      "• “Nay”\n",
      "• “Other”\n",
      "Goal: to understand how polarized the\n",
      "US Congress is\n",
      "Political clustering\n",
      "The joint session of Congress on Capitol Hill\n",
      "in Washington\n",
      "13\n",
      "• How do we identify\n",
      "swing votes?\n",
      "– Lobbying\n",
      "– Bridging party lines\n",
      "• Assumption:\n",
      "– Democrats and Republicans\n",
      "vote among partisan lines,\n",
      "which generates clusters\n",
      "Each data point represents a member of Congress\n",
      "Finding voting patterns\n",
      "14\n",
      "Intra vs. inter-cluster distance\n",
      "Intra-Cluster\n",
      "Distance\n",
      "Inter-Cluster\n",
      "Distance\n",
      "Objective: minimize intra-cluster distance, maximize inter-cluster distance\n",
      "15\n",
      "• The centroid is the average location\n",
      "of all points in the cluster\n",
      "• Another definition: the centroid\n",
      "--------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "of all points in the cluster\n",
      "• Another definition: the centroid\n",
      "minimizes the distance between a\n",
      "central location and all the data\n",
      "points in the cluster\n",
      "Note: Centroids are generally not\n",
      "existing data points, rather locations in\n",
      "space\n",
      "k-means clustering is based on\n",
      "centroids\n",
      "16\n",
      "1. Randomly choose k data points\n",
      "to be centroids\n",
      "k-means in 4 steps\n",
      "17\n",
      "1. Randomly choose k data points\n",
      "to be centroids\n",
      "2. Assign each point to closest\n",
      "centroid\n",
      "k-means in 4 steps\n",
      "18\n",
      "1. Randomly choose k data points\n",
      "to be centroids\n",
      "2. Assign each point to closest\n",
      "centroid\n",
      "3. Recalculate centroids based on\n",
      "current cluster membership\n",
      "k-means in 4 steps\n",
      "19\n",
      "1. Randomly choose k data\n",
      "points to be centroids\n",
      "2. Assign each point to closest\n",
      "centroid\n",
      "3. Recalculate centroids based\n",
      "on current cluster\n",
      "membership\n",
      "k-means in 4 steps\n",
      "20\n",
      "4. Repeat steps 2-3 with the new centroids until the centroids don’t\n",
      "change anymore\n",
      "Step 1: load packages and data\n",
      "# Install packages\n",
      "install.packages(\"e1071\")\n",
      "install.packages(\"ggplot2\")\n",
      "--------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "# Install packages\n",
      "install.packages(\"e1071\")\n",
      "install.packages(\"ggplot2\")\n",
      "# Load libraries\n",
      "library(e1071)\n",
      "library(ggplot2)\n",
      "library(help = e1071)\n",
      "Learn about all the functionality of the package, be\n",
      "well informed about what you're doing!\n",
      "21\n",
      "Step 1: load packages and data\n",
      "# Loading house data\n",
      "house_votes_Dem = read_csv(\"house_votes_Dem.csv\")\n",
      "# What does the data look like?\n",
      "View(house_votes_Dem)\n",
      "Script\n",
      "22\n",
      "Step 2: run k-means\n",
      "# Define the columns to be clustered by subsetting the data\n",
      "clust_data_Dem = house_votes_Dem[, c(\"aye\", \"nay\", \"other\")]\n",
      "# Run an algorithm with 2 centers\n",
      "set.seed(1)\n",
      "kmeans_obj_Dem = kmeans(clust_data_Dem, centers = 2,\n",
      "algorithm = \"Lloyd\")\n",
      "# What does the new variable kmeans_obj contain?\n",
      "kmeans_obj_Dem\n",
      "# View the results of each output of the kmeans\n",
      "# function\n",
      "head(kmeans_obj_Dem)\n",
      "Script\n",
      "1. By placing the set of data we want\n",
      "after the comma, we tell R we’re\n",
      "looking for columns\n",
      "2. kmeans uses a different starting\n",
      "data point each time it runs. To\n",
      "--------------------------------------------------------------------------------\n",
      "Document 7:\n",
      "looking for columns\n",
      "2. kmeans uses a different starting\n",
      "data point each time it runs. To\n",
      "make the results reproducible\n",
      "make R start from the same point\n",
      "every time with set.seed()\n",
      "3. We’re not specifying the number\n",
      "of iterations so R defaults to 10\n",
      "4. We’ll see that kmeans produces a\n",
      "list    of vectors of different\n",
      "lengths. As a result, we cannot use\n",
      "the View() function\n",
      "23\n",
      "Step 2: run k-means\n",
      "1. Number of points each cluster contains\n",
      "2. The “location” of each cluster center is specified by 3\n",
      "coordinates, one for each column we’re clustering\n",
      "3. The list assigning either cluster 1 or 2 to each data point\n",
      "1. 79.5% of the variance between the data points is explained\n",
      "by our clustering, we will discuss this in detail later\n",
      "2. List of other types of data included in kmeans_obj\n",
      "24\n",
      "• cluster: a vector indicating the cluster to which each point is allocated\n",
      "• centers: a matrix of cluster centers\n",
      "• totss: the total sum of squares (sum of distances between all points)\n",
      "--------------------------------------------------------------------------------\n",
      "Document 8:\n",
      "• totss: the total sum of squares (sum of distances between all points)\n",
      "• withinss: vector of within-cluster sum of distances, one number per cluster\n",
      "• tot.withinss: total within-cluster sum of distances, i.e. sum of withinss\n",
      "• betweenss: the between-cluster sum of squares, i.e. totss - tot.withinss\n",
      "• size: the number of points in each cluster\n",
      "To learn more about the kmeans function run ?kmeans\n",
      "kmeans outputs\n",
      "26\n",
      "Intra vs. inter-cluster distance\n",
      "Intra-Cluster\n",
      "Distance\n",
      "Inter-Cluster\n",
      "Distance\n",
      "withinss\n",
      "betweenss\n",
      "totss = withinss + betweenss\n",
      "27\n",
      "Step 3: visualize plot\n",
      "# Tell R to read the cluster labels as factors so that ggplot2 (the\n",
      "# graphing  package) can read them as category labels instead of\n",
      "# continuous variables (numeric variables).\n",
      "party_clusters_Dem = as.factor(kmeans_obj_Dem$cluster)\n",
      "# What does party_clusters look like?\n",
      "View(party_clusters_Dem)\n",
      "View(as.data.frame(party_clusters_Dem))\n",
      "# Set up labels for our data so that we can compare Democrats and\n",
      "# Republicans.\n",
      "--------------------------------------------------------------------------------\n",
      "Document 9:\n",
      "# Set up labels for our data so that we can compare Democrats and\n",
      "# Republicans.\n",
      "party_labels_Dem = house_votes_Dem$party\n",
      "Script\n",
      "28\n",
      "ggplot(house_votes_Dem, aes(x = aye,\n",
      "y = nay,\n",
      "shape = party_clusters_Dem)) +\n",
      "geom_point(size = 6) +\n",
      "ggtitle(\"Aye vs. Nay votes for Democrat-introduced bills\") +\n",
      "xlab(\"Number of Aye Votes\") +\n",
      "ylab(\"Number of Nay Votes\") +\n",
      "scale_shape_manual(name = \"Cluster\",\n",
      "labels = c(\"Cluster 1\", \"Cluster 2\"),\n",
      "values = c(\"1\", \"2\")) +\n",
      "theme_light()\n",
      "Step 3: visualize plot\n",
      "Cosmetics layer\n",
      "Base layer\n",
      "Geom Layer\n",
      "Titles and axis\n",
      "Shape\n",
      "Theme\n",
      "29\n",
      "Script\n",
      "Step 3: visualize plot\n",
      "30\n",
      "• Two groups exist\n",
      "• Algorithm identifies voting\n",
      "patterns\n",
      "What can we infer about\n",
      "the different clusters?\n",
      "Step 4: analyze results\n",
      "31\n",
      "ggplot(house_votes_Dem, aes(x = yea,\n",
      "y = nay,\n",
      "color = party_labels_Dem,\n",
      "shape = party_clusters_Dem)) +\n",
      "geom_point(size = 6) +\n",
      "ggtitle(\"Aye vs. Nay votes for Democrat-introduced bills\") +\n",
      "xlab(\"Number of Aye Votes\") +\n",
      "ylab(\"Number of Nay Votes\") +\n",
      "--------------------------------------------------------------------------------\n",
      "Document 10:\n",
      "xlab(\"Number of Aye Votes\") +\n",
      "ylab(\"Number of Nay Votes\") +\n",
      "scale_shape_manual(name = \"Cluster\",\n",
      "labels = c(\"Cluster 1\", \"Cluster 2\"),\n",
      "values = c(\"1\", \"2\")) +\n",
      "scale_color_manual(name = \"Party\",\n",
      "labels = c(\"Democratic\", \"Republican\"),\n",
      "values = c(\"blue\", \"red\")) +\n",
      "theme_light()\n",
      "Step 5: validate results\n",
      "Cosmetics layer\n",
      "Script\n",
      "Base layer\n",
      "Geom Layer\n",
      "Titles and axis\n",
      "Color and\n",
      "shape\n",
      "Theme\n",
      "32\n",
      "Step 5: validate results\n",
      "33\n",
      "• Diffuse among Democrats\n",
      "• Republicans more dense\n",
      "• Can gauge “outliers”\n",
      "• Can see the polarization\n",
      "between the two political parties\n",
      "Step 6: interpret results\n",
      "34\n",
      "• Clustering is more powerful than the\n",
      "human eye in 3D\n",
      "• Clustering mathematically defines\n",
      "which cluster the peripheral points\n",
      "should be in when it’s not obvious to\n",
      "the human eye\n",
      "• Clustering is helpful when many\n",
      "dimensions / variables exist that you\n",
      "can’t visualize at once\n",
      "– Whiskey similarity example from\n",
      "classification lecture\n",
      "Clustering vs. visualizing\n",
      "Aye, Nay and Other Votes\n",
      "in House of Representatives\n",
      "35\n",
      "--------------------------------------------------------------------------------\n",
      "Document 11:\n",
      "Clustering vs. visualizing\n",
      "Aye, Nay and Other Votes\n",
      "in House of Representatives\n",
      "35\n",
      "• Goals of clustering:\n",
      "– Maximize the separation between clusters\n",
      "• i.e. Maximize inter-cluster distance\n",
      "– Keep similar points in a\n",
      "cluster close together\n",
      "• i.e. Minimize intra-cluster distance\n",
      "How good is the clustering?\n",
      "36\n",
      "• Look at the variance explained by\n",
      "clusters\n",
      "– In particular, the ratio of inter-cluster\n",
      "variance to total variance\n",
      "• How much of the total variance is\n",
      "explained by the clustering?\n",
      "Assessing how well an algorithm performs\n",
      "How good is the clustering?\n",
      "Variation explained by clusters\n",
      "=\n",
      "inter-cluster variance / total variance\n",
      "37\n",
      "• cluster: a vector indicating the cluster to which each point is allocated\n",
      "• centers: a matrix of cluster centers\n",
      "• totss: the total sum of squares (sum of distances between all points)\n",
      "• withinss: vector of within-cluster sum of distances, one number per cluster\n",
      "• tot.withinss: total within-cluster sum of distances, i.e. sum of withinss\n",
      "--------------------------------------------------------------------------------\n",
      "Document 12:\n",
      "• tot.withinss: total within-cluster sum of distances, i.e. sum of withinss\n",
      "• betweenss: the between-cluster sum of squares, i.e. totss - tot.withinss\n",
      "• size: the number of points in each cluster\n",
      "To learn more about the kmeans function run ?kmeans\n",
      "kmeans outputs\n",
      "38\n",
      "Intra vs. inter-cluster distance\n",
      "Intra-Cluster\n",
      "Distance\n",
      "Inter-Cluster\n",
      "Distance\n",
      "withinss\n",
      "betweenss\n",
      "totss = withinss + betweenss\n",
      "39\n",
      "How good is the clustering?\n",
      "# Inter-cluster variance,\n",
      "# \"betweenss\" is the sum of the\n",
      "# distances between points from\n",
      "# different clusters\n",
      "num_Dem = kmeans_obj_Dem$betweenss\n",
      "# Total variance\n",
      "# \"totss\" is the sum of the distances\n",
      "# between all the points in\n",
      "# the data set\n",
      "denom_Dem = kmeans_obj_Dem$totss\n",
      "# Variance accounted for by\n",
      "# clusters\n",
      "var_exp_Dem = num_Dem / denom_Dem\n",
      "var_exp_Dem\n",
      "[1] 0.7193405\n",
      "Script\n",
      "40\n",
      "• It’s easier when the number of clusters is known ahead of time, but what if we don't\n",
      "know how many clusters we should have?\n",
      "--------------------------------------------------------------------------------\n",
      "Document 13:\n",
      "know how many clusters we should have?\n",
      "• Since different starting points may generate different clusters, we need a way to\n",
      "assess cluster quality as well.\n",
      "How do we choose the number of clusters (i.e. k)?\n",
      "How good is the clustering?\n",
      "41\n",
      "1. Elbow method\n",
      "– Computes the percentage of variance explained by clusters for a range of cluster\n",
      "numbers\n",
      "– Plots a graph so results are easier to see\n",
      "– Not guaranteed to work! It depends on the data in question\n",
      "2.\n",
      "NbClust\n",
      "How to select k: two methods\n",
      "– Runs 30 different tests and\n",
      "provides “majority vote” for\n",
      "the best number of clusters\n",
      "(k’s) to use\n",
      "42\n",
      "Elbow method: measure variance\n",
      "# Run algorithm with 3 centers\n",
      "set.seed(1)\n",
      "kmeans_obj_Dem = kmeans(clust_data_Dem,\n",
      "centers = 3,\n",
      "algorithm = \"Lloyd\")\n",
      "# Inter-cluster variance\n",
      "num_Dem = kmeans_obj_Dem$betweenss\n",
      "# Total variance\n",
      "denom_Dem = kmeans_obj_Dem$totss\n",
      "# Variance accounted for by clusters\n",
      "var_exp_Dem = num_Dem / denom_Dem\n",
      "var_exp_Dem\n",
      "[1] 0.7949741\n",
      "Script\n",
      "43\n",
      "--------------------------------------------------------------------------------\n",
      "Document 14:\n",
      "var_exp_Dem = num_Dem / denom_Dem\n",
      "var_exp_Dem\n",
      "[1] 0.7949741\n",
      "Script\n",
      "43\n",
      "• We want to repeat the variance calculation from the previous slide for several\n",
      "numbers of clusters automatically\n",
      "• We can create a function that contains all the steps we want to automate\n",
      "Automating a step we want to repeat\n",
      "function(data, item to iterate through)\n",
      "44\n",
      "# The function explained_variance wraps our code from previous slides.\n",
      "explained_variance = function(data_in, k){\n",
      "# Running k-means algorithm\n",
      "set.seed(1)\n",
      "kmeans_obj = kmeans(data_in, centers = k,\n",
      "algorithm = \"Lloyd\")\n",
      "# Variance accounted for by clusters\n",
      "var_exp = kmeans_obj$betweenss /\n",
      "kmeans_obj$totss\n",
      "var_exp\n",
      "}\n",
      "Automating a step we want to repeat\n",
      "Script\n",
      "1. A new variable is created and set equal\n",
      "to our function()\n",
      "2. The commands inside the function are\n",
      "wrapped in curly braces {}\n",
      "3. Inside the parentheses, we specify the\n",
      "variables that the user will input and\n",
      "that will then be used inside the\n",
      "function where they appear\n",
      "45\n",
      "--------------------------------------------------------------------------------\n",
      "Document 15:\n",
      "that will then be used inside the\n",
      "function where they appear\n",
      "45\n",
      "# Recall the variable we are using for the\n",
      "# data that we're clustering.\n",
      "clust_data_Dem = house_votes_Dem[, c(\"aye\", \"nay\", \"other\")]\n",
      "View(clust_data_Dem)\n",
      "# The sapply() function plugs several values\n",
      "# into explained_variance.\n",
      "explained_var_Dem = sapply(1:10, explained_variance,\n",
      "data_in = clust_data_Dem)\n",
      "View(explained_var_Dem)\n",
      "# Data for ggplot2\n",
      "elbow_data_Dem = data.frame(k = 1:10,\n",
      "explained_var_Dem)\n",
      "View(elbow_data_Dem)\n",
      "Automating a step we want to repeat\n",
      "1.sapply() applies a function to a\n",
      "vector\n",
      "2. We have to tell sapply() that\n",
      "the we want the\n",
      "explained_variance function\n",
      "to use the clust_data data\n",
      "3. Next, we create a data frame that\n",
      "contains both the new variance\n",
      "variable (explained_var_Dem)\n",
      "and the different numbers of k\n",
      "that we used in the previous\n",
      "function (1 through 10)\n",
      "Function we created\n",
      "Script\n",
      "46\n",
      "# Plotting data\n",
      "ggplot(elbow_data_Dem,\n",
      "aes(x = k,\n",
      "y = explained_var_Dem)) +\n",
      "geom_point(size = 4) +\n",
      "--------------------------------------------------------------------------------\n",
      "Document 16:\n",
      "# Plotting data\n",
      "ggplot(elbow_data_Dem,\n",
      "aes(x = k,\n",
      "y = explained_var_Dem)) +\n",
      "geom_point(size = 4) +\n",
      "geom_line(size = 1) +\n",
      "xlab(\"k\") +\n",
      "ylab(\"Intercluster Variance/Total Variance\") +\n",
      "theme_light()\n",
      "Elbow method: plotting the graph\n",
      "Script\n",
      "1. geom_point() sets the size of the data points\n",
      "2. geom_line() sets the thickness of the line\n",
      "47\n",
      "Looking for the kink in graph of  inter-cluster variance / total\n",
      "variance\n",
      "Elbow method: measure variance\n",
      "Original data\n",
      "Elbow method\n",
      "k = 2\n",
      "48\n",
      "• Library: \"NbClust\"\n",
      "Functions:  \"NbClust\"\n",
      "Inputs:\n",
      "• data – data array or data frame\n",
      "• min.nc / max.nc – minimum/maximum number of clusters\n",
      "• method – \"kmeans\"\n",
      "• There are other, more advanced arguments that can be customized but are outside of\n",
      "the scope of this course and are note necessary to for NbClust to work\n",
      "There are a number of ways to choose the right k.\n",
      "NbClust runs 30 tests and selects k based on majority vote\n",
      "NbClust: k by majority vote\n",
      "NbClust(data, max.nc, method = \"kmeans\")\n",
      "49\n",
      "# Install the package.\n",
      "--------------------------------------------------------------------------------\n",
      "Document 17:\n",
      "NbClust: k by majority vote\n",
      "NbClust(data, max.nc, method = \"kmeans\")\n",
      "49\n",
      "# Install the package.\n",
      "install.packages(\"NbClust\")\n",
      "library(NbClust)\n",
      "# Run NbClust.\n",
      "nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")\n",
      "# View the output of NbClust.\n",
      "nbclust_obj_Dem\n",
      "# View the output that shows the number of clusters each\n",
      "# method recommends.\n",
      "View(nbclust_obj_Dem$Best.nc)\n",
      "NbClust: k by majority vote\n",
      "Script\n",
      "50\n",
      "NbClust: k by majority vote\n",
      "> nbclust_obj_Dem = NbClust(data = clust_data_Dem, method = \"kmeans\")\n",
      "...\n",
      "*******************************************************************\n",
      "* Among all indices:\n",
      "* 14 proposed 2 as the best number of clusters\n",
      "* 3 proposed 3 as the best number of clusters\n",
      "* 1 proposed 4 as the best number of clusters\n",
      "* 3 proposed 6 as the best number of clusters\n",
      "* 1 proposed 9 as the best number of clusters\n",
      "* 1 proposed 10 as the best number of clusters\n",
      "* 1 proposed 15 as the best number of clusters\n",
      "***** Conclusion *****\n",
      "--------------------------------------------------------------------------------\n",
      "Document 18:\n",
      "* 1 proposed 15 as the best number of clusters\n",
      "***** Conclusion *****\n",
      "* According to the majority rule, the best number of clusters is  2\n",
      "Note: additional information appears; the above information is most relevant to us for now\n",
      "Console\n",
      "51\n",
      "NbClust: k by majority vote\n",
      "> nbclust_obj_Dem\n",
      "Console\n",
      "• nbclust_obj_Dem shows the outputs of NbClust\n",
      "– One of the outputs is Best.nc, which shows the number of clusters\n",
      "recommended by each test\n",
      "52\n",
      "NbClust: k by majority vote\n",
      "• We want to visualize a histogram to make it obvious how many votes there are\n",
      "for each number of clusters\n",
      "53\n",
      "# Subset the 1st row from Best.nc and convert it\n",
      "# to a data frame, so ggplot2 can plot it.\n",
      "freq_k_Dem = nbclust_obj_Dem$Best.nc[1,]\n",
      "freq_k_Dem = data.frame(freq_k_Dem)\n",
      "View(freq_k_Dem)\n",
      "# Check the maximum number of clusters.\n",
      "max(freq_k_Dem)\n",
      "# Plot as a histogram.\n",
      "ggplot(freq_k_Dem,\n",
      "aes(x = freq_k_Dem)) +\n",
      "geom_bar() +\n",
      "scale_x_continuous(breaks = seq(0, 15, by = 1)) +\n",
      "scale_y_continuous(breaks = seq(0, 12, by = 1)) +\n",
      "--------------------------------------------------------------------------------\n",
      "Document 19:\n",
      "scale_x_continuous(breaks = seq(0, 15, by = 1)) +\n",
      "scale_y_continuous(breaks = seq(0, 12, by = 1)) +\n",
      "labs(x = \"Number of Clusters\",\n",
      "y = \"Number of Votes\",\n",
      "title = \"Cluster Analysis\")\n",
      "NbClust: k by majority vote\n",
      "Script\n",
      "2 clusters is the\n",
      "winner with 12 votes\n",
      "54\n",
      "• If you’re a lobbyist, which congressperson can you influence for swing votes?\n",
      "• If you’re managing a campaign and your competitor is always voting along\n",
      "party lines, how can you use that information?\n",
      "• If your congressperson is not an active voter, is she representing your\n",
      "interests?\n",
      "• What do the voting patterns look like for Republican-introduced bills?\n",
      "Application of results\n",
      "55\n",
      "• Could see differences between the\n",
      "patterns of Reb lead bills and Democrat\n",
      "lead bills\n",
      "• Could provide information on\n",
      "congressmen that might be see has\n",
      "swing votes.\n",
      "Implications of results\n",
      "56\n",
      "• We are assuming that the patterns correspond with the same bills being voted\n",
      "on – perhaps some Congressmen have the same number of 'aye' and 'nay'\n",
      "--------------------------------------------------------------------------------\n",
      "Document 20:\n",
      "on – perhaps some Congressmen have the same number of 'aye' and 'nay'\n",
      "votes, but voted on different bills\n",
      "• Network analysis can help determine additional connections between\n",
      "Congressmen\n",
      "• We haven't taken extenuating factors into account – political initiatives,\n",
      "current events, etc.\n",
      "This is a preliminary analysis that gives us initial\n",
      "insights and can help us direct further research\n",
      "Limitations of results\n",
      "57\n",
      "• The good and bad\n",
      "– + cheap – NO LABELS, labels are expensive to create and maintain\n",
      "– +/- clustering always works\n",
      "– - Many methods to choose from and knowing the right one can be nontrivial and the\n",
      "differences between many are almost zero, so you need to understand what you're\n",
      "doing\n",
      "• The evil\n",
      "– Curse of dimensionality\n",
      "– Clusters may result from poor data quality\n",
      "– Non-deterministic (e.g. k-means) subject to local minimum. Since it works with\n",
      "averages, k-means does not get much better with Big Data (marginal improvements)\n",
      "--------------------------------------------------------------------------------\n",
      "Document 21:\n",
      "averages, k-means does not get much better with Big Data (marginal improvements)\n",
      "– Non spherical data may result in poor clustering (depending on method used)\n",
      "– Unequal cluster sizes may result in poor clustering (depending on method used)\n",
      "The good, bad, and evil\n",
      "58\n",
      "59\n",
      "• Analysts need to ask the following questions\n",
      "– Do you want overlapping or non-overlapping clusters?\n",
      "– Does your data satisfy the assumptions of the clustering algorithm?\n",
      "– How was the distance measure identified?\n",
      "– How many clusters and why? Identifying the number of clusters is a difficult task if the\n",
      "number of class labels is not known beforehand\n",
      "– Does your method scale to the size of the data?\n",
      "– Is the compute time congruent with the temporal budget of your business need (i.e.\n",
      "do you get answers back in time to make meaningful decisions)\n",
      "The good, bad, and evil\n",
      "60\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "vector_store.add_documents(documents)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KVcULE7gdEqb",
    "outputId": "1a23671b-a125-4a58-bdb5-ef9307897d49",
    "ExecuteTime": {
     "end_time": "2025-01-27T18:38:45.844907Z",
     "start_time": "2025-01-27T18:38:45.390134Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['269383ef-7440-434b-994c-4f4123d42048',\n",
       " '0dacfbf6-9874-498e-a6d5-c89a01fc05c3',\n",
       " '7f16d771-4e7c-4ce9-b6fd-91479a133e49',\n",
       " 'b864a8b9-ec26-4792-a584-1e533c05141a',\n",
       " '47bfa2b0-4ac8-4466-9adb-b6c326e2b979',\n",
       " '8de97e81-28ca-4226-b16c-c1094e856247',\n",
       " 'b716cb7b-f8ab-4aa8-adae-ce8737fa3df5',\n",
       " 'cd3a9670-be6f-4b67-95a3-fe26379d3d07',\n",
       " '31e86125-33e2-49e8-80df-a41349ce00ff',\n",
       " '2977e402-0ba3-4a67-af0e-f195fb2a900b',\n",
       " 'db8cece0-1c6b-4d3e-a471-046bca7f7fbd',\n",
       " '4ef58e04-5875-4da1-9499-3d8344f883c9',\n",
       " 'a3fa2941-9b69-464f-af1d-ff6f50f040ed',\n",
       " '64deae82-c0e1-425a-a912-b354821b77de',\n",
       " '8a6c2040-f6c1-4604-8a97-f6a038621d1f',\n",
       " 'bac165ba-465e-4d3c-851f-7a7413b922e4',\n",
       " '980dc6b9-842e-4b01-8b71-9222d972cf5d',\n",
       " 'bb40e0e0-59e4-426c-8263-9868e7e9c994',\n",
       " '2de0d941-5d3b-4f47-9cb7-b42589eacfbb',\n",
       " 'cde421aa-de2d-4483-9349-209d8c0cc370',\n",
       " '8840a948-d142-4db7-9cc6-f028881efeec']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Retrieving the relevant docs"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T17:22:16.427105Z",
     "start_time": "2025-01-29T17:22:16.050081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"What are the common challenges or limitations of decision trees?\"\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\":3})\n",
    "# retriever.get_relevant_documents(query)\n",
    "retriever.invoke(query)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='c7990bd1-fab8-4bb1-9bf4-05a50afda5d4', metadata={}, page_content=\"unbalanced data sets a single tree can can be can be can be problematic with data sets that are\\nquite unbalanced but again that can be cured by moving to kind of more of an ensemble approach\\nbut until we get there we have to understand what one tree is doing before we start building forests all right so let's let's take a look all\\nright these mathematical approaches and i have some examples all right okay so decision trees uh use several\\ntypes of node splitting criteria all right so these are these are those i've talked about these\\nuh this is what it would be if we had you know kind of a continuous data problem we're trying to actually predict the value all right as\\ncompared to predict the class and these are the two common ones for classification they're really very\\nsimilar all right we're going to take a look at we're actually not going to take a look at this one but we're going to take a look at each one of these\"),\n",
       " Document(id='07c95a02-4dae-47ea-bdb1-e8f0c94a9152', metadata={}, page_content=\"uh we talked about that a little bit we're gonna talk much more about that next week but that's basically when you take you know many there's many different\\nforms of this but in this example instead of one tree but a whole bunch of trees together and they do majority vote\\nessentially all right all right so you know it is designed to operate\\nefficiently uh you know\\nbut it does not guarantee that it can provide you the best the best model because it is slightly different\\nevery time okay so let me stop there and then uh this is it's a lot to get through but\\nessentially the key key points here to remember about decision trees is that it's centered\\non this idea of information gain they do obviously have a tendency to overfit so we want to be able to use those hyper parameters to be able to tune them\\nthere's a lot of options to do that we're going to do a bit more technically about what cross validation\"),\n",
       " Document(id='b2caed61-d393-454d-83aa-fde87ea42535', metadata={}, page_content=\"all right all right so practical distribution tree algorithms are based off of you know\\nheuristic which is a greedy we talked about this they make local optimal decisions all right so they can't\\nguarantee that you'll get like the global global optimum all right similar to uh some\\nother methods like this is that it could be possible that you know you don't get the perfect tree which again is a reason to move\\npotentially towards ensembles all right all right so the great uh bias trees if\\nsome classes dominate it is there for recommended that you try and balance the data set prior to\\nfitting all right so if you have a heavily unbalanced data set and this is true of many of the classifiers we're going to be talking about it just doesn't perform\\nquite as well all right support vector machines actually have a tendency to work a little bit better as an example for\\nunbalanced data sets a single tree can can be can be can be problematic with data sets that are\")]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepare prompts template"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T17:22:18.903254Z",
     "start_time": "2025-01-29T17:22:18.901003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "template = \"\"\"You are an expert LLM assistant specialized in answering questions related to computer science/data science/machine learning/LLM. Use the retrieved information from RAG and your knowledge to respond accurately and clearly to each question.\n",
    "\n",
    "Guidelines:\n",
    "1. Provide concise and informative answers.\n",
    "2. If the question is beyond the scope of your knowledge or the provided information, state, \"I don't know.\"\n",
    "3. If the context section (the information that is returned by RAG pipeline) has no information about a part of the question, please express that \"The retrieved information did not contain answer to this question\" but if you can answer it based on your knowledge please do\n",
    "4. Use examples where applicable to illustrate your answers.\n",
    "5. Maintain a professional and helpful tone.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context:{context}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Format the Documents"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T17:22:20.596296Z",
     "start_time": "2025-01-29T17:22:20.496135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "print(format_docs(retrieved_docs))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unbalanced data sets a single tree can can be can be can be problematic with data sets that are\n",
      "quite unbalanced but again that can be cured by moving to kind of more of an ensemble approach\n",
      "but until we get there we have to understand what one tree is doing before we start building forests all right so let's let's take a look all\n",
      "right these mathematical approaches and i have some examples all right okay so decision trees uh use several\n",
      "types of node splitting criteria all right so these are these are those i've talked about these\n",
      "uh this is what it would be if we had you know kind of a continuous data problem we're trying to actually predict the value all right as\n",
      "compared to predict the class and these are the two common ones for classification they're really very\n",
      "similar all right we're going to take a look at we're actually not going to take a look at this one but we're going to take a look at each one of these\n",
      "\n",
      "uh we talked about that a little bit we're gonna talk much more about that next week but that's basically when you take you know many there's many different\n",
      "forms of this but in this example instead of one tree but a whole bunch of trees together and they do majority vote\n",
      "essentially all right all right so you know it is designed to operate\n",
      "efficiently uh you know\n",
      "but it does not guarantee that it can provide you the best the best model because it is slightly different\n",
      "every time okay so let me stop there and then uh this is it's a lot to get through but\n",
      "essentially the key key points here to remember about decision trees is that it's centered\n",
      "on this idea of information gain they do obviously have a tendency to overfit so we want to be able to use those hyper parameters to be able to tune them\n",
      "there's a lot of options to do that we're going to do a bit more technically about what cross validation\n",
      "\n",
      "all right all right so practical distribution tree algorithms are based off of you know\n",
      "heuristic which is a greedy we talked about this they make local optimal decisions all right so they can't\n",
      "guarantee that you'll get like the global global optimum all right similar to uh some\n",
      "other methods like this is that it could be possible that you know you don't get the perfect tree which again is a reason to move\n",
      "potentially towards ensembles all right all right so the great uh bias trees if\n",
      "some classes dominate it is there for recommended that you try and balance the data set prior to\n",
      "fitting all right so if you have a heavily unbalanced data set and this is true of many of the classifiers we're going to be talking about it just doesn't perform\n",
      "quite as well all right support vector machines actually have a tendency to work a little bit better as an example for\n",
      "unbalanced data sets a single tree can can be can be can be problematic with data sets that are\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T17:22:21.540078Z",
     "start_time": "2025-01-29T17:22:21.537163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = template.format(question = query, context =  format_docs(retrieved_docs))\n",
    "print(prompt)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert LLM assistant specialized in answering questions related to computer science/data science/machine learning/LLM. Use the retrieved information from RAG and your knowledge to respond accurately and clearly to each question.\n",
      "\n",
      "Guidelines:\n",
      "1. Provide concise and informative answers.\n",
      "2. If the question is beyond the scope of your knowledge or the provided information, state, \"I don't know.\"\n",
      "3. If the context section (the information that is returned by RAG pipeline) has no information about a part of the question, please express that \"The retrieved information did not contain answer to this question\" but if you can answer it based on your knowledge please do\n",
      "4. Use examples where applicable to illustrate your answers.\n",
      "5. Maintain a professional and helpful tone.\n",
      "\n",
      "Question: What are the common challenges or limitations of decision trees?\n",
      "\n",
      "Context:unbalanced data sets a single tree can can be can be can be problematic with data sets that are\n",
      "quite unbalanced but again that can be cured by moving to kind of more of an ensemble approach\n",
      "but until we get there we have to understand what one tree is doing before we start building forests all right so let's let's take a look all\n",
      "right these mathematical approaches and i have some examples all right okay so decision trees uh use several\n",
      "types of node splitting criteria all right so these are these are those i've talked about these\n",
      "uh this is what it would be if we had you know kind of a continuous data problem we're trying to actually predict the value all right as\n",
      "compared to predict the class and these are the two common ones for classification they're really very\n",
      "similar all right we're going to take a look at we're actually not going to take a look at this one but we're going to take a look at each one of these\n",
      "\n",
      "uh we talked about that a little bit we're gonna talk much more about that next week but that's basically when you take you know many there's many different\n",
      "forms of this but in this example instead of one tree but a whole bunch of trees together and they do majority vote\n",
      "essentially all right all right so you know it is designed to operate\n",
      "efficiently uh you know\n",
      "but it does not guarantee that it can provide you the best the best model because it is slightly different\n",
      "every time okay so let me stop there and then uh this is it's a lot to get through but\n",
      "essentially the key key points here to remember about decision trees is that it's centered\n",
      "on this idea of information gain they do obviously have a tendency to overfit so we want to be able to use those hyper parameters to be able to tune them\n",
      "there's a lot of options to do that we're going to do a bit more technically about what cross validation\n",
      "\n",
      "all right all right so practical distribution tree algorithms are based off of you know\n",
      "heuristic which is a greedy we talked about this they make local optimal decisions all right so they can't\n",
      "guarantee that you'll get like the global global optimum all right similar to uh some\n",
      "other methods like this is that it could be possible that you know you don't get the perfect tree which again is a reason to move\n",
      "potentially towards ensembles all right all right so the great uh bias trees if\n",
      "some classes dominate it is there for recommended that you try and balance the data set prior to\n",
      "fitting all right so if you have a heavily unbalanced data set and this is true of many of the classifiers we're going to be talking about it just doesn't perform\n",
      "quite as well all right support vector machines actually have a tendency to work a little bit better as an example for\n",
      "unbalanced data sets a single tree can can be can be can be problematic with data sets that are\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Generate an answer\n",
    "### Setting up LLM"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T17:22:38.885271Z",
     "start_time": "2025-01-29T17:22:23.592069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ollama\n",
    "desired_model = \"deepseek-r1:7b\"\n",
    "\n",
    "response = ollama.chat(model=desired_model, messages=[\n",
    "    {\n",
    "        'role':'user',\n",
    "        'content':prompt,\n",
    "    },\n",
    "])\n",
    "\n",
    "\n",
    "ollama_response = response['message']['content']\n",
    "\n",
    "print(ollama_response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out the common challenges or limitations of decision trees based on the context provided. Let me read through the context again and pick out the relevant points.\n",
      "\n",
      "The context mentions unbalanced datasets. It says a single tree might be problematic in such cases because it can't handle the imbalance well. That makes sense because if one class dominates, the tree might not perform well for other classes. The context also talks about using ensemble methods like random forests to overcome this issue once we move beyond single trees.\n",
      "\n",
      "Another point is about node splitting criteria. It mentions different types but doesn't go into detail on their limitations directly. However, decision trees can get complex with too many splits, leading to overfitting. I know that's a common issue because the algorithm makes locally optimal decisions greedily without considering future splits, which might not be globally optimal.\n",
      "\n",
      "The context also notes that decision tree algorithms are based on heuristics and greedy approaches. This means they make the best choice at each step but don't guarantee the global optimum. Additionally, since they build trees from data alone without prior assumptions, their performance can vary with different datasets.\n",
      "\n",
      "Pruning is another thing mentioned implicitly. If a tree isn't pruned, it might become too complex and overfit. The context doesn't specify how to handle this, so I'll have to assume that's a limitation as well.\n",
      "\n",
      "Handling missing values could be tricky because decision trees typically don't handle them well unless specific methods are used for imputation or surrogate splits. But since the context doesn't mention anything about it, maybe that's beyond the current scope.\n",
      "\n",
      "Interpretability is generally good with decision trees because they have clear rules and structures, but in cases where the tree becomes too deep or complex, it might lose some interpretability, which isn't explicitly mentioned here.\n",
      "\n",
      "Putting this all together, I can outline the main limitations: overfitting, sensitivity to unbalanced data without ensemble methods, greedy splits leading to suboptimal trees, lack of handling for missing values (beyond what's provided), and potential complexity that affects interpretability.\n",
      "</think>\n",
      "\n",
      "Decision trees have several notable challenges or limitations:\n",
      "\n",
      "1. **Overfitting**: Decision trees tend to overfit training data due to their tendency to capture noise rather than the underlying pattern. This can be mitigated with pruning techniques, but not always guaranteed.\n",
      "\n",
      "2. **Greedy Splitting**: The algorithm makes locally optimal decisions at each node without considering future splits, which might not lead to globally optimal solutions.\n",
      "\n",
      "3. **Sensitivity to Unbalanced Data**: Single trees perform poorly on unbalanced datasets where one class dominates. Ensemble methods like random forests are often needed for better performance in such cases.\n",
      "\n",
      "4. **Handling Missing Values**: Decision trees generally do not handle missing data efficiently without additional preprocessing or imputation strategies, which can complicate the model.\n",
      "\n",
      "5. **Complexity and Interpretability**: While decision trees offer clear rules, excessive depth or complexity can reduce interpretability, though this is more about how the tree is used rather than a fundamental limitation of the algorithm itself.\n",
      "\n",
      "These limitations highlight why ensemble methods are often preferred over single decision trees for complex datasets.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
