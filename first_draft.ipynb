{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You may need additional installs:\n",
    "# ! pip install langchain langchain-groq langchain-ollama langchain-chroma PyPDF2\n",
    "import os\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# --- LangSmith Settings (optional) ---\n",
    "#os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "#os.environ['LANGSMITH_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "#os.environ['LANGSMITH_PROJECT'] = 'multimodal_RAG'\n",
    "#os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "#os.environ['LANGCHAIN_API_KEY'] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set up Groq API key ---\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass(\"Enter API key for Groq: \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import necessary modules ---\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "# PDF loader from LangChain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "# Groq LLM and Ollama embeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableMap, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1) Load PDF Documents ####\n",
    "# Replace \"path/to/your.pdf\" with the local or remote path to your PDF file.\n",
    "loader = PyPDFLoader(\"data/Lectures/Knn and Prob.pdf\")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2) Split the text into chunks ####\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3) Create Embeddings + Vector Store ####\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Define a function for formatting retrieved docs\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "format_docs_runnable = RunnableLambda(func=format_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Construct the Prompt Template (a Runnable)\n",
    "\n",
    "# Instead of a plain string, we make a ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant.\n",
    "You are given the following context:\n",
    "{context}\n",
    "\n",
    "Please use this context to answer the question below as completely as possible.\n",
    "\n",
    "Question: {question}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 6) Create your LLM ####\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 7) Build the RAG chain ####\n",
    "rag_chain = (\n",
    "    # Create a mapping: \"context\" -> pipeline from retriever to format_docs,\n",
    "    #                   \"question\" -> pass through as-is\n",
    "    RunnableMap({\n",
    "        \"context\": retriever | format_docs_runnable,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    })\n",
    "    # Then pipe the dictionary into the ChatPromptTemplate\n",
    "    | prompt_template\n",
    "    # Then pipe the resulting prompt into the LLM\n",
    "    | llm\n",
    "    # Finally parse the LLM output as a string\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the main takeaway from the PDF is that the k-Nearest Neighbors (kNN) algorithm can be sensitive to the choice of the parameter k, which determines the number of nearest neighbors to consider when making a classification decision.\n",
      "\n",
      "The PDF highlights that:\n",
      "\n",
      "* A small value of k (e.g., k=1) can result in a complex decision boundary that may lead to overfitting.\n",
      "* Increasing k (e.g., k=15) can improve the decision boundary by reducing the impact of noise and outliers, but may also increase the risk of underfitting if k is too large.\n",
      "\n",
      "The PDF also illustrates how the choice of k can affect the classification decision in a specific example, showing how changing k from 3 to 5 can change the classification outcome from Red to Green.\n",
      "\n",
      "Overall, the main takeaway is that the choice of k is crucial in kNN and requires careful consideration to achieve good classification performance.\n"
     ]
    }
   ],
   "source": [
    "#### 8) Ask a question about the PDF ####\n",
    "answer = rag_chain.invoke(\"What is the main takeaway from the PDF?\")\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
