hello okay i'm going to try and make this video a bit shorter than the than the last one so let's jump right into it as a reminder we're going to work through each one of these phases in sequential weeks right we'll have another one just dedicated to evaluations or up here in the idea development right so much of what we're talking about here is kind of these meta concepts associated with defining a problem and the things that we would consider in data science as it relates to defining that problem and getting to know our data okay the first thing to know is that much of data science almost all of data science is not interested in inference but focuses on prediction all right think of inference as kind of traditional statistical measures where we're creating these perimeters i'm sorry if you can hear my dog backing in the background traditional perimeter measures and we're doing p values and we're assessing the model overall model significance and the contribution of various variables in the model all right that's not something that data science focus on data science almost exclusively focuses on prediction and prediction is exactly what you think it is it is how well does this model predict unseen data doesn't have to be like in the future or whatever that is it just has to be on unseen data how well does it predict various classes or point estimates whatever that might be that's mostly the focus almost all the focus of machine learning as compared to inferential the thing about this is is that it changes the types of questions that you can ask all right so typically we're not asking causal questions of machine learning models there are some methods that do that currently but they are not used uh in high volume right now right so most of the goals are centered not necessarily on identifying and understanding which unique features are contributing all right to a model it's more about how well the model is predicting at the aggregate as an example if we're you know getting the most extreme point in a neural network women have millions of weights associated with different classification tax right there's just no way well there is there's maturing ways to unpack that but typically it actually we wouldn't care that much like how the model is being learned we would just care how well it works all right so and we talked about this at some of the criteria like if you have to know you know if if if openness and non-ambiguity is a requirement then certain machine learning models just aren't going to work all right so that's the goal associated with this particular associated with machine learning broadly all right not inference but prediction so it's important to understand the differences between those so typical statistical tests and everything that you've learned kind of in kind of your introduction to statistics would fall into kind of that inferential space all right a key part of building a solution for machine learning all right is to have some type of independent business metric that can be used outside of evaluating the model performance so there's a whole suite of evaluation metrics to assess how well a model is doing that we will talk about a few weeks from now that's one criteria a different criteria is it actually contributing to the problem that we're trying to solve all right so there's examples down here the second one is probably the easiest one to talk through right we could design a machine learning algorithm all right that will identify and email us spam and block it right the idea there is that it would by blocking that particular email right they may have viruses right and so there should be less viruses in our system all right it's not necessarily designed to track the presence of viruses it's designed to track the presence of spam but if it does a good job at identifying spam right then viruses is a nice correlated effect right that's the goals that we got way too many viruses in our network we know they're coming from all the spam let's get a good spam blocker up there so by implementing this machine learning algorithm what we should see is that the number of viruses on the network drops as compared to before it was put into place right and that you can test using traditional you know means measures you know hypothesis testing to say hey was there a significant drop right in the number of viruses post pre and post implementing this machine learning algorithm which is a great way to think about you know deploying machine learning algorithms all right to design kind of experimental studies uh once they're put into place right some other key terms here very high level stuff when we talk about the variable that we're trying to understand the patterns that we understand it's target that's our target variable all right and features are the things that are going to contribute all right to us understanding the patterns of that target variable in the previous example the target variable is spam right whether someone is spam or not right features for that particular data set might be when the email was sent right the the subject of the email the text of the body whether it included the word prince right so it could be all of these features that you build in order to be able to predict whether that is actually spam or not okay so and then i think this is analogous to independent dependent variables if people are familiar with that language so the target would be kind of our dependent variable and then the independent variables would be the features okay all right much of what we think of when you think of classic or machine learning is based off classification okay so classification uh is the process with which we're basically have a categorical dependent variable categorical start using the language categorical target variable our target is a one or a zero or it falls in discrete categories maybe that might be many categories it might be an algorithm that's working to define thousands of categories but typically a common example is a one or a zero right so one being you know that this uh particular person gets access to a loan and 0 means that they do not all right try to classify that or 1 is a spam and 0 is this email is not spam let it go through all right here's some examples here i'll let you guys read through those are pretty obvious but the important thing here to remember is that the models actually don't predict whether something is a one or a zero they predict the percentage likelihood that it that that particular row you think about it in your data set it belongs to the positive class the likelihood that it is a one so the output of these classification models are done in percentage distributions all right so that distribution just like any other percentage this is a one it's a zero to a hundred percent right okay and the threshold all right we'll talk about thresholds more and more the threshold for determining whether someone is a or a particular row is a one or a zero is fifty percent okay let me show you just a little toy example right so say we were trying to decide whether something was spam or not spam right and we had three emails come in right we had three emails come in the data is based off a series of features right all right so these features get passed into our machine learning algorithm we go into machine learning algorithm and out the other side comes percentages right so the output of the first one is 51 the outcome of the next one is 49 the outcome of the last one is 100 okay so take a second and say to yourselves now which class are each one of these going to be determined to belong to okay right the first one is going to be one right because it's above 50 percent all right the default threshold for determining for classifying in most and all machine learning algorithms really is 50 any likelihood that one row belongs to a particular class is above 51 then it gets classified as a one all right the same is true for below 50 if it's 50 by the way classified isn't one all right so this is zero and then you know the model is very certain that this is one okay so that's how that works this part of the process is done automatically but we totally can control where that threshold sits all right talk more about that in just a second the other out the other output just so we're talking about common approaches well it's just regression and this is where we're just trying to predict exact numbers right so the number of points scored by a player the amount of rain that will occur all right you know and maybe a person's weight based off their based off various characteristics right so classification we're thinking about discrete variables all right and regression we're thinking about continuous right machine learning algorithms can do both and here we would measure the quality of the model based off how far off those predictions are in a in a real number sense all right all right so it's negative portion of this class we're going to focus on classification but i am going to show you regression examples as well all right you can think of these 0 to 100 as risk measures right so what's the risk associated with assuming that this model is correct right so if it's 51 all right the model's not too sure about that row right but it's 100 it seems to be pretty confident right so the question you have to ask yourself is like how much risk are you willing to accept right because you can adjust that threshold up and down say i want my model to be above 75 confident that a particular row belongs to a particular class before i assume that it does right and that threshold right what level of risk you're willing to accept could totally vary depending on what type of question you're asking right if you're asking something medical right what is going to be the drug effect or what's the likelihood that this particular treatment it's going to have a positive effect on a on a patient you know it's 75 percent are you comfortable with that like maybe that number needs to be closer to 95 right but we're just talking about whether someone's likely to open up an email maybe it doesn't matter right all right so these are extreme examples but they are very common you know that these types of problems show up especially when we're thinking about you know humans that are involved we have to be very careful about how we do this so the percentages that come out of these probability distributions you can almost think of them as a quantification of certainty or risk associated with each particular item all right well we're going to dive more into that but i just want you to understand that in classification generally the output is not necessarily this belongs to one or zero it's a percentage likelihood and then it's determined typically at the 50 mark and above that's when people belong to various classes all right okay so this is the last slide here for this particular portion of it all right so again we've talked about kind of meta concepts associated with how we're building our approach to constructing a data science problem one of the two important things to consider in this is data to concept and learning difficulty okay so data concept broadly you know is there the data available to be able to support this particular learning goal right does it appear that the data is available all right so you just go through these series of questions is it large enough how hard is it to gather is the data does the data change rapidly all right and do we think this is the right source so you can go through all these questions any one of these is no you might have to just consider what the realm of possibilities are associated with what you can achieve learning difficulty is once you think that you have the data necessary for it like how complex is the actual task right imbalances in the classes mean in our span example say for instance we had 98 of the rows in our data set were all not spam right and only two percent were spam that is super unbalanced all right so it's going to be hard for any algorithm to understand the patterns associated with spam data files if only a very small portion of them are in our data set right so we may have it may go back to kind of data to concept goal we may have enough data but it may just be highly imbalanced all right has this been used in the past you know and does it clearly link to the problem i think these two are highly correlated with each other right do we have examples where this data was used with some success all right do are there canonical examples that we can point to and say hey yes these features clearly help us understand this particular target bear it's been used in the past with success we have our own data now so we we have a certain amount of confidence that it can be done all right and is the you know is the target variable difficult to measure in terms of it being broken down into smaller pieces you know does it need to be broken down into smaller pieces is it just too vague all right and then this final question is the one we got to a little bit earlier you know what risk are we willing to accept like what are the goals of this particular project and you know how can we assess you know kind of the uncertainty that might be provided through our model right like what is that threshold essentially that we're willing willing to put into place all right so this is phase one i know that's a lot that's a real real quick we'll talk more about it on thursday but the idea here is to have you is to give you some tools to think about when you're developing data related questions machine learning questions and give you a foundation and how you would deploy these all right so everything from you know what is this independent metric like what type of question is it classification or regression you know do i understand my data enough working through all these types of scenarios to be able to to be able to measure it all right so these are all important questions that you just need to work through in terms of just brainstorming ahead of actually moving into more formal formal model building processes which we will talk about next week