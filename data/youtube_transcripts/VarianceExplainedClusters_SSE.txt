okay hello uh so i wanna uh just post a little video here on kind of the evaluation metric that we've been using for clustering because i just don't think i've been doing a really great job uh explaining it so uh we're gonna go through it uh i'll try and make it pretty quick but uh all right so let's start so one of the one of the issues here i think kind of on the slides is that we refer to this as inner cluster difference you know the difference between the clusters and then within sum of squares i guess we put between here but then uh when we go down here we actually don't refer to it as within or i'm sorry with we don't refer to it as inner uh we refer to it as between okay so between all right so first thing i'd like to do i probably should just should have had this pulled up but uh uh i want to take a look uh at this i'm going to scroll this up oh my gosh this stuff out of here get out of here uh-oh we have to do a new one all right there we go new whiteboard all right i'm gonna go rainbow inc all right so let's think about the principle behind this uh particular evaluation measure is actually sum of squares error all right that's really what it is we're trying to the entire lloyd's algorithm is actually working just to minimize the amount of error uh represented by the centroids all right so let's think about this from kind of a uh you know the traditional example maybe that you're used to which is you know linear regression right so we have these linear we're trying to fit a line between linear regression right and when we do sum of squares errors basically you know the distance uh sum of the squared distances between these points and the line all right so the summation of all these differences equals the kind of sum of squares error mean square you take the average mean squared error right okay so we got that so i probably should have left that up that's okay so let's compare that to what clustering does so again this is a hyper-dimensional space so keep that in mind but we're showing it too deep we have two clusters one here one here all right and then we have a whole bunch of points all right and then the distance from these points right into the centroids is a version of error right it's the same measure it's just there's not a line right so with regression there's a line all right with the centroids with clustering there's clusters okay so the distance from this all these points to this centroid and the distance from all these points to this centroid is the within square's error you know so i call that within square's error all right within squares there is my lovely rainbow pen all right within squares error all right if we add these two things together assuming we have two centroids one and two if we add those together that equals the total total within the oh you guys know how i am at spelling anyway total within squares error all right so you could think of that as like the sum of squares error that would be analogous to a regression model that's that's explained that's you know explained by the centroids okay all right so and then we have this between all right and so what between is is whatever remaining error is not explained by the uh by the centroids basically the distance between those and the we get to the between by calculating the total sum of squares error right do the totes all right and so the total sum of squares error is the distance from every point from the global middle there's a global middle all right so it's not the mean all right but keep in mind it's you think of this thing as a sphere right is that there's this global space in the middle of that space is the distance from each line uh from that global middle okay so you can think of it as you know uh like the core of the earth right the distance from every point to the core of the earth essentially all right is the total sum of squares error okay and so as we know what that is we can subtract this total within and then that gives us this basically between so we call it between but it's really not between it's kind of all the remaining variants that's not accounted for by the distance from the centroids all right okay so all of these things are a quantification of a reduction of this square's error kind of idea so let me show you i can do this uh everybody learns a different way so we can do it as in a little excel spreadsheet if we want these are our points let's look at it this way 1 through 20. okay these are just uh those have no numerical value they're just points all right so we have 20 points just think of it that way all right uh and then you know they're 1 through 20 all right and then i guess they have some pretending like they have some numerical value but here the global center or the global middle is going to be 10 and a half okay so i'm going to use these as real numbers but in hyperdimensional space it would be the average of the distance uh between all of those features that you're trying to map all right so where that point locates in hyperdimensional space and then the distance would be measured by euclidean alright so we're doing this as if it's just you know addition and subtraction but the the the way it's done in k-means and many other graphical based is the distance in n-dimensional space measured by euclidean distance all right how far away it is okay all right so the global center here is going to be 10 and a half right which is just the middle point between this range of range of numbers 1 to 20. all right so what we do here right is we're just subtracting the distance all right and then i'm squaring it okay so this is again this is just an illustrative example this is not how it would be done mathematically would use lucidity and distance which is a bit different all right but we're just going to subtract the differences here and then we're going to square them we sum that all up right this is the global center so the sum of this is all of the distances the sum of all the distances inside so that's our total variance all right of our data set okay and then say we had two centroids and two centroids one at five and 15. all right you know kind of at the midpoints of both of these potentially all right and then in order to find the distance from centroid 1 assuming that we just you know mapping the first 10 on to that particular point that's what this would be right and that's just squared differences right so here again we're just subtracting 1 from 5 and squaring it so it's all in absolute terms right all right and there's adding that all up and same here it's going to be the same because they're same exact numbers right they're just on different different sides of the scale so the distances are all going to be almost exactly the same so we're going to see these are the sum of a squared error right of both of the centroids so this is within centroid 1 and within is in citrate one within centroid two do the same if we add those together right we sum these together 170 that's right here right so that's our total within sum of squares right then we know our total sum of squares right we did that that's the distance from the global global center of all of these all right and then we just to get the total between we just subtract the sum of the within from the total and that's it we get 495. okay so then if we want to understand the total amount of variance accounted for by two centroids by our two clusters what we do is we take this between number right which again remember is the basically the variance left over after we subtract it from our centroids and we uh divide it in or divide that by the total okay and so in this example that's 0.75 okay all right so what will naturally happen is that while complexity will increase all right the sum of squares error a little pin i can move up here all right so as complexity increases that means as k increases right sum of squares error will go down okay that means the more clusters you put on the graph the distance from those clusters and you think you can think of this as within if you want to do it within sum of squares error will go down all right and variance explained will go up all right so the question you have to ask yourself and this is what we're you know kind of talking about in this class is this we have this elbow right so we know there's going to be a point of diminishing returns okay to where we continue to add additional complexity k goes up variance explained will go up right oops sorry that's not enough variance explained over here we'll go up all right we will have this kind of diminishing returns effect okay so we don't want to try and optimize that we have to use our intuition about what makes sense and that's why we do the elbow chart right and it kind of makes sense if you think about this as like a two-dimensional space right if say in it and it's just terrible example we just had like two points and we're trying to and we just and then you know the clusters were just sitting on top of those points right so this would be where you know the cluster is explained 100 of the of the variants right because the total variance inside right the total variance the totes number uh from the global global mean that would be the distance a to here to here is going to be explained by the between distance right so you'd have 1 divided by 1 which would equal 100 right in this example okay so if the total between equals the total sum you know for some reason it was so clusters are so tight essentially you just have two clusters right this would result in just a one-to-one ratio okay anyway that's an extreme example but i'm just trying to explain that um you know and uh hopefully in kind of just a you know extreme example so you can understand the concepts there okay so essentially what what we're thinking about here you know generally is kind of this reduction of sum of squares error all right but we never want to have we never want to have like a a perfect zero you know the sum of squares error equals zero what does that mean right well it essentially means just like this example it's the same thing right is that there's a cluster for every dot right so the distance from a dot to a cluster is zero right and that would mean that we've overfit so imagine we had in some of the examples we had we had 300 you know 300 rows of data that we have 300 clusters right so you have 300 clusters on here and you've perfectly explained it well you haven't really explained anything you've just memorized the data points so you've over fit right when you get a sum of squares error that equals zero all right so the same thing if you get 100 variance explained you just you just overfit that means you have a one-to-one ratio between the total variance and then kind of the between all right so you're not really learning anything so we think about kind of explaining this we have to look at it not only through the lens of variance explained right or sum of squares error but also um with this idea of what does it look like visually so we put it on the chart okay so that's true of um you know of this week's and all these explanations are here as well so you can work through these now with i think better information right so we have to represent these things visually so that's why we're doing all this work here and it might be i mean clearly we have two clusters here but it might be that you know adding the third one which would increase the various explained and reduce the sum of squares error might be the way to do it all right so it might make more sense because these clear to be outliers but maybe maybe not right so much of what we see in clustering is a bit of human intuition about what makes sense depending on what question you're asking and evaluation of the clusters should be done through that lens as well so you should take into account variance explained but you should also take a look at what it is telling you graphically and see if it makes sense and if you can use it uh you know in a very simple way without without increasing com uh complexity right then that's what you want to do all right um so it's not always necessarily and that's this is the nature of unsupervised machine learning this perfect quantification of variance explained in terms of the things that we're trying to optimize last point hopefully this helps too for those of you that are versed in the just general variance kind of calculation all right this is the equation here right right this is the distance from every point to its mean all right this should be squared all right squared all right and then divided right and this is an unbiased data set so it's so you subtract one divided by this length of the uh length of the um length of the data frame length of the vector right divided by the length okay so that's that your standard squared error right okay so that's in in r that's just a that's just a variance calculation okay all right and so what we have here alright so this this total variance all right is just the variance of the three variables that we're using summed together okay so it's the variance of those three variables already printed out down here on the bottom a so bit of surprise scale there but so and here what we're doing is this is the between this is these are um features of our k-means object right so this is the between sum of squares error all right and then this is the total within sum of squares error all right so the total within the clusters and the total between divided by the length subtracted by one just like the variance calculation okay and so if we compare these two just the variance of the data points we're using and the variance of the objects generated in the k-means algorithm they are the same right so i just i just wanted to show you that maybe to ground that home i mean basically what we're doing here is trying to understand better the variance in terms of clusters as compared to the variance and errors in terms of a line like you might normally see in uh in regression okay that's it i'm going to stop there but i'm happy to ask uh answer uh more questions about this feel free to shoot me an email if need to or of course stop by for for office hours okay