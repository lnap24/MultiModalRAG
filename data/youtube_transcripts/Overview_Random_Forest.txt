okay good morning uh we're gonna talk about uh ensemble methods uh random forest in particular all right these methods uh are pretty much you know in you know the broadest sense of the explanation you just take a bunch of trees uh and you take the average of those all right so with random forest that's what you would do you take a whole bunch of them right and that helps uh in a bunch of different ways right especially for overfitting or for variable selection or model validation so there's a lot of really important parts to ensemble methods ensemble really just means doing a whole bunch of models and kind of doing majority vote or taking the average of those results so let's get let's get a little bit more specific here we'll walk through some examples so that's what i said at the front again it's just basically an aggregation of a bunch of different algorithms right often uh in solving a machine learning problem i hate just to read you know at the end of the process um the approach you take can be combining a whole bunch of different predictors right and even different models all right together and they work better together than they do separately all right i'm gonna focus on random forest it's mostly uh all right uh a bagging model all right i'm gonna get my little pointer to work here right it's mostly bagging all right but we're going to talk about what boosting is two all right there's kind of just a few critical processes i think i spelled this wrong that you have to know associated with um how random forest works right two of the really important ones are about about um bootstrap aggregation which is a bagging method all right and then basically the selection of a smaller amount of features at each leaf node and what that does is it creates a variability associated with how the trees are grown and that allows them to work better together all right so we'll talk we'll talk more about that all right you can also do it where you're building several models that are just different models not the same models like random forces a bunch of decision trees but you can also do it where you're building a k n a logistic regression a support vector machine and a decision tree right so let's just i mean this this is a very simple example right so here what we see is a bunch of different models and we have a prediction say this is just row one right that's row one i don't know why i had to write that out but say it's just row one right and three of the four models are predicting that this no this particular row belongs to class one all right so that's what it would be it would be class one all right so this is the majority right and and all the others are the majority right and the idea is that they work together given the different ways that they train compensate for weaknesses and uh select by majority vote hard voting in this example um the one that works the best all right this is usually a good approach for if you have a whole bunch of weak learners right so if you have a bunch of models that aren't working very well right like their capacity to predict is pretty bad sometimes if you put them together it's pretty good all right so because they learn things very differently the way k n builds a model is very different than the way a support vector machine or decision tree or logistic regression so as a result you know they they have a tendency to learn differently and can compensate for each other's weaknesses all right so how does this work right so essentially scale right scale by scale we need the number of models all right increases the probability of finding like a true majority vote all right we know this because of you know probabilistic theory right associated with uh how we would say determine if a coin is fair right so the number of times you flip a coin right the more times you do it you're going to gravitate towards the mean and you're eventually going to end up at like this 50 50 split right now depending on how hard it is for the particular model to learn that particular features to predict the target variable it might take longer right you might have to do you know ten thousand trees or a thousand trees in this example a thousand coin flips or ten thousand coin flips right if you get to ten thousand you kind of get a ninety percent ninety seven percent chance all right of getting this right in terms of like the actual split which is fifty 50. okay so it works the same way with model building so this is an example here we're actually going to build this chart with some of the code in class and i'll show you you know the variability associated with the error rates and then how they change as the number of trees grow right so on the bottom here instead of coin tosses we would see trees right and then over here instead of the ratio of the number of heads what we would see is just the error rate associated with how the tree is growing right right the misclassification area we could put the accuracy over there we could pick whatever we want okay so here what we see at the beginning right whoa we're first just getting started there's just just a significant amount of variability ah it's all over the place right and then eventually what we see is it starts to stabilize okay across all the examples here we're running many different examples of the coin flip it starts to gravitate towards the mean all right and this is still really stabilized kind of way out here around ten 000. even up here there's still some variability in here a significant amount so that's the idea right it's kind of the more you do a thing the more you get to the truth right and that's the that's a real risk with doing just one decision tree is that it's so non-deterministic in terms of the way that it works it could be subject to error but if you do it many times right we do this over and over again it just compensates for that risk right that's essentially it okay so i said this a bunch of times instead of one tree it's building a whole bunch all right we can limit the growth of the trees with some hyperparameters but we don't have to all right so we can set the number of trees you know in the grown uh and as they grow and then track the error classification then we get a kind of an idea about where our model starts to stabilize right and that's actually we want to pull it back and kind of sit right at that space all right we can use it again for both regression and classification and again it's mostly um kind of boosting and uh bagging okay all right so let's talk about that a little bit tree boosting bagging all right so bagging so the goal here for bagging is akin to controlling overfitting all right so it's designed to reduce variance so overfitting is akin to high variance all right so what we're going to do is we want to create simpler decision boundaries right so we can grow a whole bunch of again i need to work on my spelling we can grow a whole bunch of trees and we can use the distributions of those results to make predictions i'm sorry these are these are fuzzy i pulled these from another slide um okay so what we're doing down here is that what we're trying to do all right uh is compensate for this particular overfitting you see here right if we do just one tree right it'll probably you know it'll notice that there's a point here and it'll learn that and just assume that everything underneath that particular point is blue well well in fact it's not true right see that actually the territory right below here is almost exclusively red right so if we ran this same algorithm here we're just trying to predict like the alcohol and wine all right so here what we're doing um we run this algorithm a whole bunch of times right so instead of doing just one tree we do a bunch of trees what we see is that it learns this pattern better all right and then it can compensate for this and so it actually makes the decision boundary all right less specific all right so it increases kind of the generality of it all right which helps us reduce the variance of the prediction okay so that's the goal there all right to reduce that variance so just let's talk about bagging just generally right this is kind of a klutzy graph but right so as we discussed you know there are several methods um and you can combine them together all right but one was a much more common way to be honest because doing a bunch of methods can be computationally pretty expensive all right it's just a resample okay and that's all this is so resampling is akin to bagging so you go back and you'll resample with replacement what that means is that every row inside the data set is subject to be sampled at every sample all right so we don't we don't take you know the first 10 rows the next 10 rows after that it's just random we say hey just take 10 rows out of the entire data set 10 is way too small you'd do more like 100. so take 100 rows randomly and some of those rows can be sampled over and over and over again that's fine all right but the idea is that each one of the samples will be variable enough will be different enough from each other to create different trees all right so that's one way to control doing to what we want to do is increase the variability the types of trees are growing so we know that even in all these different trees and what are the best features to use what are the best splits to use and how do we classify particular rows given across you know say thousands of trees all right so we can do that and so this is just example say this is our data set here right so here we're sampling different different um rows all right and then we're taking all these results right or here's the sample i'm sorry that we're putting out here and then we're building different trees okay this might be an easier way to look at it all right so this is without replacement all right so we did this way this would be without replacement all right so we would just randomly sample like the rows might be a different order but it would be the same three rows all right these are the same three rows but here with replacement we might actually have duplicates like in this sample right we have two of number one which is fine right so we're sampling with replacement which means that every row is available every time to sample some will be included and some will not all right and what this does if we were to do the same one over and over again maybe this is obvious right we would end up with the same tree we'd have the same we have the same oak tree every time right so we don't want that all right we want some maple trees here we want some pine trees all right along with our along with our big oak okay so they're grown differently because the samples are different okay and what this does is it helps uncorrelate right it's discorrelate the trees from each other all right there are trees that are very different but that's not the only thing that needs to be done right so bagging was introduced kind of in the early 2000s but then in the mid 2000s um bremen and and others introduced another feature associated with decision trees research with random force actually created random force which allowed them to be the really useful algorithm that they are today and that kind of akin to to boosting which means that they would take a subset right of the features that are available and also a subset in combination with a subset of the samples all right so say you have 20 features so what you would say let's say at every decision point in the tree the algorithm only has access to five of those randomly selected features all right so it just randomly selects five at each node and it can use those to build the tree okay so this increases uh just is again is another way all right for us to increase variability in the trees and allows us to better understand which features uh are working and create a more productive ensemble right the more different the trees are the more we learn from them so that's the idea so we sub sampling again the number of features that we have and then we're also sampling all right um the number of uh the data basically right so we're bagging the data okay this helps us reduce bias all right all right and it works against underfitting okay so tendency to underfit the model here all right and so boosting kind of guards against that so here what we're seeing right so this would just be one decision tree all right you're just going to make a broad statement right it's just under fit right say oh everything kind of below this line is going to be red and everything above here is going to be blue well that's not really true right what we see is that there's it's obvious that that red is not beyond this point okay so ada boost adaptive boosting is a method all right that can guard against that so we deploy that right we'll talk a little bit more specifically about that method in particular but what it does is it kind of carves off this point all right and so it makes the model more specific so it increases the decision space and moves this blue over here okay all right so these two things work together right in combination to um to make random forest a really effective method for prediction all right so it's mostly a bagging model all right it actually doesn't do kind of adaptive boosting i'm happy to talk about that but it does select subsequent features which is kind of a boosting methodology all right but these two things work together in combination so bagging kind of reduces the variance associated with the models and then boosting reduces the bias all right so that allows it to be really powerful because it controls for those two things those two things work together in combination two primarily the intention of those all right was against is to create more uh variability associated with the types of trees that have been grown all right as a result what happens is that you need actually a lot more trees right because there's so much variability between them you know kind of the default for a random forest is a thousand tree models all right but depending on the complexity of your uh complexity of your features or the size of the data all right you might have to go way beyond that you know so maybe out to 10 000 or 15 000 trees in order to be able to get some stability and i'll show you how we track that it's not that dissimilar to what we saw in the coin flip right okay so the main tuning parameter all right for random force is going to be my try and that's the number of variables that are available at each decision node for each tree all right so the kind of i think the standard here is to take the square root of the number of features that you have right so if you have you know 20 something features or 16 or something easier on myself then you just you start with four right and see how that works and you can adjust that up and down and the goal here would be to reduce the error quicker all right so we increase the number of my tries it increases the complexity but it also might reduce the error quicker so that's how we tune that up and down just based off the number of trees that it takes based off that my tribe okay all right so uh variable importance we've talked about this before the nice part about random force it's much more robust because it has so many more trees to measure against right so it's ability to select the most important model is really good um it does it similar to what we've seen in the past all right where it will aggregate across all the trees that are built using something called the out of bag error out and so the out of bag out of bag is the sample that was not used to build the tree all right so it'll build the tree then it'll predict with the outer bag air all right and then for every one of those trees it'll aggregate using the outer bag error the total reduction and you know impurity uh at each node associated with each one of the features and then aggregate that together and that will give us the variable importance so it tracks basically the utility of each one of these features for every tree that it builds right based off the i mean anything based off the optimization function and typically that's genie i think we're using genie ratio right in this uh and this default um package that we're going to use and i'll walk through that all right so that's it so again it's it's pretty quick it builds principally off what we've done you know we did clustering last week that was unsupervised but it builds off what we did the previous three weeks in terms of the tree based models right uh and just built a whole bunch of them but we incorporated these new features in terms of you know bagging and subselecting our feature space all right and then we have to think about kind of the complexity and how many trees we want to grow over time until things start to stabilize so uh random forest package in r is basically just a wrapper for the original bremen and cutler they're from berkeley of course and then the original website is really useful too they still have all of the original documentation up there if you want to check it out okay so quicker video this week and i'll see everybody on thursday and we'll jump into some code all right thanks