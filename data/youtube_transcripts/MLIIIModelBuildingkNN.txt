okay so uh we're gonna talk about uh k n and uh we're not gonna talk about probability i do have slides at the end that you can review on your own and there's a few slides here that talk about classification generally that we have kind of already gone through so i'm not going to belabor those some examples the next two slides i've essentially just want to emphasize and i'll let you read through some of these the importance to consider there's a diminishing return on complexity all right so if we consider oh i don't know like this is cost down here and this is accuracy you know on this side what kind of happens is that you'll see that uh accuracy will start to label off as cost continues to grow right so the question is like where do we i guess i could have made this like more obvious that it was a hockey puck you know something like that where do we stop this because if we keep continuing to add more complexity to our models are we really getting any return right so because speed and complexity have cost all right so in this world we have to consider those things so what we want is the most simple accurate model possible all right so we have to consider that comprehensibility is you know we'll get more into this in terms of just interpretability of um of these models as we get into them and then training time is kind of an akin to cost right the more complex we make it the more comp and more costly it will be to learn a model all right so let's get into k-nearest neighbors we've talked about it a little bit in class in particular it's called a memory based learner because it actually doesn't really learn a model right so what it really does is kind of calculate distance of every row remember we kind of did this thing where we had x's around here and a dash and an x and a dash and an x and an x and then this is like uh you know what's this data point all right so what's it going to do is it's going to calculate the distance you know to each one of these data points in this example even though i didn't do a great job of showing it we've got five so we've got two dashes to three x's so guess what question mark equals x all right that's how it works it's just that simple all right but what it does is it calculates the distance you know this will be some type of numeric number and and i and i and so on and so forth for every row uh to the row that we're trying to figure out so it doesn't really learn a model as much as it kind of recalculates it so there's some benefits to that there's some examples this basically just says we have to standardize our data because it uses a distance based measure right so it's calculating distances so we're going to use liquidity and distance to make sure that all our data points are um well anyway we're going to normalize i'm sorry and then we're going to use the ecliption distance to calculate uh the distance from any one point to all the points around it given the k all right so in order to do that again we just have to standardize and you guys know how to do this now so we just need to normalize all right so you can think of training especially creating a big index a database right just like i showed you before of all the distances from any given point and then the test is just just the point right so here's a simple example how to do it this is liquidity and distance up here if you want to calculate it's pretty simple you just subtract the points and square them and then take the square root pretty pretty straightforward just plug and chug there let me get me out of the way here all right so let's look at this example where p and z down here p rate rate p yeah okay i got this backwards before p and z are our existing or current data and q is the new one all right so we think of p and z is training let me think of q as the test all right could i circle more so yeah so what we do is we just calculate the distance between these two so we'll just subtract them here we do three one three one subtracting subtracting subtracting subtracting all right we're squaring this one that's why it's four squaring the distance two and this one's 4 so that's 16. add it all up take the square root and boom there we go 5.39 all right so let's calculate the distance from q to z right this is kind of an extreme example guess what they're exactly the same all right so we put the points around on top of each other the distance is zero right these points are exactly the same so guess what yeah if we were doing it doing a kf2 which by the way you would never do because uh that way you can have ties uh here actually you have a tie but anyway it's probably a bad example but if we were to scale this up this distance to this point would be smaller so guess what this would be a q okay because they're basically right on top of each other anyway that's how the calculation is done in practice the way that the classification is done is we would need another example down here all right so we need another third distance um that was of the same class of the ones we're looking at say we had another z here that was also like super close was like one okay so in this distance we would have two points that were indicating that q would be z uh and zero points that would be indicating and just one point i'm sorry that'd be indicating that that it was q that it was p so we would choose z okay and making things may be more complicated than they have to be but let's take a look at this all right so this is an example from a classification problem where we kind of know we have three divisions of data all right and in this example we're just using one nearest neighbor and again you would never do this all right so what's happening is that any of the points that are closest all the new data whatever point is closest to it it just inherits that form okay so what happens is you've got kind of a really messy what we call this decision boundary right it's oh my gosh look at this thing all right wow yeah so super sloppy in there we've got these little islands right for some reason this is red you know who knows this is green right in the middle of a sea of red all right so we call that little classification islands we've got a little green one right there so it becomes pretty complicated all right so instead of picking just one all right okay nearest neighbor i'm gonna try and expand it all right so if we expand the number of neighbors that we use all right instead of picking one you know when we when we go to try and use um majority vote all right uh it's helpful all right because we have these very complicated boundaries of red green and blue classes we have significant overlap so again what we're doing here with k1 is we're just we're way over fitting all right so this is an example where we're making hyperlocal decisions and that's why you see all these little islands okay it's just way too over fit all right and it's just not going to generalize good to unseen data so we need to back this up all right so what we're going to do is we're going to increase the k and there's a lot of slides here that basically lead to that all right so in this example we've gone all the way up to 15 right that's a non-trivial difference but you can tell here what happens is we've got a much smoother kind of boundary right so some islands here right but generally it you know it's looking pretty good you know we got red here in the middle we've got green down here we've got some error here with some of these red ones right but we've got blue up here all right it's looking pretty good we still have some overlap in air all right we can see that there's some blue in the red and some of that but we've gotten rid of all those islands right and the border has certainly smoothed all right all right so smoother borders and it's easier easier to see the different classes of the variables all right so this is going to generalize much better to unseen data because we backed up we're just giving it more information now we don't know if this is necessarily the right maybe there's a lower number of ks that would work better here but this is just to give you an idea of how the influence of how many of the nearest points you used could take on the classification itself okay all right so let's take a look at just a two-dimensional example all right this is just you know a better example than that than the little terrible one that i drew so here we have green and blue and where this is green and red right we're trying to classify this blue dot all right so there it is i got a lot of points here that say what i just show what i just said all right so here we're using what using a k equals three okay so this is you know this is this is easy stuff here we've got three neighbors two are red one is green boom that mystery spot this guy right here all right that's gonna be red right yes red easy all right but is that the right choice i don't know so let's see the distance here this line that's going to be lucinian distance it's just actually there's several distances that you can use the cleaning is just kind of the default all right manhattan is used sometimes all right but let's expand it let's take a look oh what if we move this out to five and again you wanna use odd numbers so you don't have ties if we move this out to five and we create our little radius circle here around five what we see is a different story all right so now it's three to two green all right for this particular variable all right so we're using the same distance categories but now we would change it now we would change it to green okay so there's gonna be a you know there's going to be a process and we'll talk through it about whether you can you can determine you know what is the correct k right and you can measure that through classification error i keep putting a question mark above this but in the training and testing exercise we actually know what that is right we know what that is all right so we know whether it should be a green or a red okay so in this example whether it's green or red we'll know and if it's right if it's green and it's supposed to be a green then we'll get you know a plus one for this we got it right if it's a red and we label it as a green it gets counted as a miss right a misclassification so we can actually gauge you know which way we want to go okay i'll show you how to do that all right all right so there's a balance between large and small k all right and we'll be able to figure that out all right based off some of these metrics once we kind of get into the r code about how to do it okay here are some benefits of deciding to do large and small generally all right i'll also say this once you get up to a certain level like 25 k's or whatever that every every distance starts to be one all right so the complexity of k n actually taps out pretty quickly this is a theoretical known issue with k is that if you have really no issue with k n if you have really complex data it actually doesn't work all that well at scale so you have to be careful about how about how many neighbors you're using because you're just being incorporating the you know complexity of the data set to where it can't distinguish between the points at all okay all right so distance problem we've covered this it's just the idea like if we don't normalize the data this is nice and normalized this is not right weight is throwing it off okay let's get into some more details all right so k n is non-parametric that means it has no underlying assumptions about the distribution of the model right all right it grows according to the data given all right not some other type of preconceived notion about um you know the linear fit or things like that associated with how the model is learned all right it is a memory based model and what that means is it doesn't really learn a model right it just indexes the rows produces the distance and then tells you how far away all right one or it tells you what one particular class is as compared to the other as a result they call it kind of a lazy learner all right that's another way to say memory based or lazy learner the benefit is that it's it trains really fast right it's you know it spits it out really fast but the prediction can be slow right you can index those rows very quickly but every time you add a new line it's got to index them all over again okay so that can be a slow process right as compared if you had a model where you had all the weights figured out and everything was predetermined and it was ready to go uh you had like a rule based model and you just knew where certain breaks were so it's basically you think of this as just locked and loaded uh when it comes to k n it has to relearn it the benefit of that is that if your data is changing quickly right it has netflix in here somewhere i don't know if netflix would ever use k n just based off the memory problems but if it's if it's if your data is changing very quickly k n is actually not a very bad choice a lot of people use it in text-based models because you can think of text as coming in if you're learning different features the narrative around the text features can change rapidly right it just changed based off you know a single conversation or something so k n actually adjusts pretty well to ebbs and flows of your data so there is a benefit there the the downside is that it is computationally pretty expensive to put in place all right there's lots of ways you can speed it up right uh so let's all right so it's simple and fast to deploy and basically has no no training time because again it just indexes it it's easy to interpret right and it has this kind of doesn't assume any distributions of the data it handles multi-class which most most of the classifications do all right so the the downside is it takes a lot of memory to store all the data and index it every time you want to train imagine if you had you know billions of you know a gigantic data set right and you're trying to use k n to classify it it would have to read through every one of these every one of these elements every time it could be very computationally expensive every one of these you know entries every time so it would just be unfeasible from that perspective it does have the curse of dimensionality um you know there's little difference between the nearest and farthest neighbor and high dimensional data all right everything starts to normalize to one unless you get over you know with like 25 features or more right if you're using 25 essentially you think of 25 variables or more uh it starts to not work that great right can be computationally expensive you got to normalize it but i don't know you got to normalize everything so i'm not sure that's really a downside at this point we're going to i'll switch over to the code and r we'll walk through it in class but um that's it for k n it's pretty simple algorithm all right it has some benefits and downsides but generally you know a really nice place to start a lot of people use k n and i'll just end it right here as kind of an initial model build right they sample the data maybe reduce the space and you can launch a k n model just you know very very quickly right it doesn't take that much computational power if you sample appropriately and then generally has a generally pretty accurate so it's a good way if you're not sure if your data is going to work that well and you don't want to spend a lot of additional effort and resources to build towards more complex models launch it see what happens right good indicator of future results all right that might be its best benefit so i'll stop there and i'll see everyone on thursday