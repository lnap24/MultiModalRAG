okay so uh the camera is not working so we're gonna go we're gonna i'm sorry you won't be able to see me in this one so just just voiceover slides here but this is pretty quick we're going into phase three uh of our kind of development life cycle here's a few things that we want to talk about anyway i just want to bring this back up just like i do kind of every time you know we're in we're in phase 3 but we're still kind of doing initial model building now i want to i should probably do a better job of this in class but initial model building can really be a conceptual model inside your head it doesn't actually have to mean you know that you're developing a machine learning model or it could be a very simple version of your model just a correlation or maybe just exploratory data analytics so we did we did a lot of that kind of already but uh really what we're talking about kind of in this last phase like i said it's kind of in model creation but still could be in this initial model phase all right so uh let's just jump into it just have a few things to cover we've covered a bit of this in class but you know parameters versus hyper parameters all right so remember parameters when we think about you know our you know the every data scientist's goal here just to put things in a this is just an ugly example you know in a data frame right so we have our our variables here so on and so forth yadda yadda right all right so these are our these are our parameters okay these could be the parameters there's another word for this it's just features variables however you want to say it all right the parameters okay are going to be the things inside of the model that we can change all right so already shown you some examples of these but you know k in knn is a hyper parameter right so we can adjust the number of neighbors up and down if you've watched that video already totally understand what i'm talking about so we pass in our parameters and we adjust the hyperparameters there are limitless numbers of these as we move into more machine learning this is a very simple example but essentially it's the things that we tune inside the algorithm all right we're going to talk about thresholding we've talked about this a bit already but um just to be clear right this all of our machine learning outputs give us results in percentages all right and the percentage is just on a distribution from one you know to a hundred okay so we can decide anywhere inside this distribution of 1 200 where we want say we just do it say this is 20 right and this is 80 right so anything on this side of the line is going to be a 1. anything on this side of the line is going to be a zero all right we decide okay the results of most machine learning models is just a distribution of percentages all right and then depending on where your threshold is set it will classify right it'll classify those numbers as either a one or zero it defaults to fifty okay so anything at fifty percent or above is going to be a one all right anything below is going to be a 0. but it does not mean that we can't change it right this is the eraser it appears to be all right it doesn't mean that we could move this way up you know and make it like 75 and that would just depend on the type of question you're asking and how cautious you want to be we'll get more into that all right all right feature engineering is probably a really under discussed topic in data science i would say in almost all cases developing better features will improve your model much more than changing the model let me say that again often the way that we in a different way often the way that people approach machine learning is to try all sorts of different models and just see which one gives you the best results okay that is one way to go a better approach is probably to build better features better parameters right better things to put in the model to make it understand the target better all right this is always gonna be an added benefit at how you do that all right that might mean that you have to go out and just gather more data it might be that you have a bunch of text data and you need to convert that into some type of numeric representation right it might be that you combine features together it might be that you have a whole bunch of states and you collapse those into regions there's lots of things you could do in the feature engineering space by combining or exploring different levels or doing um clustering to create features that will enhance your model's performance okay we'll see different examples of this i'll try and work them in throughout the semester but in a certain way you're already kind of doing it right one hot encoding you're creating different features all right so you're creating features at least to the very least you're giving the machine learning algorithm a better chance to understand the different layers of that particular factor that is a you know an example all right all right last topic here well kind of blast so bias versus variance again uh a very large topic but at its very you know the very highest level here i just want you to make sure to understand that machine learning models require bias and variance to work all right they require a certain level of statistical bias in order for the models to work if they didn't they couldn't generalize all right so here when we look at this all right this example is an example of under fitting all right so here what we're doing is we're just drawing a line and we're not accepting the fact that there's a clear arch here in this data right there's obviously an arch right so this would be an example of creating some type of linear fit right then you can do the typical regression you just stick the line through the middle and hope for the best all right but most approaches in machine learning are not linear right they're usually non-linear approaches non-parametric approaches where they can be flexible in terms of how they draw this dividing line how they connect these uh dots together all right and this one is just under fit here well let's go out to this example right let's go out here all right all the way out here what is this this is way over fit all right this is basically you've just memorized the data right just memorized it all right so imagine if there was just a you know another x that was out here right you know how would this you know what would the prediction be these are pretty far away from that line right so this prediction would be pretty inaccurate right but we did the same thing here right just you know notionally about the same distance because of this arc right it's gonna gonna fit much better it's to be closer it's more accurate okay so this is creating these local optimal lines which always means that when you start to generalize it to larger data it's just not going to work very well right okay so it's just memorizing the patterns that you have in it you're way over fitting okay and of course what we see here is that we have right we have the optimal slope that clearly recognizes this this you know simple kind of diminishing returns line here right with size and price being our two variables who knows what size is anyway so there's some level of diminishing returns right so you can better predict size and price if you just recognize the fact that it has this curve okay all right so the idea is this is where we want to get to i'll say this this is an art okay you know if your model there's been lots of good you know phd level you know uh researchers that have created models that are just way over fit right they just dump everything in there so we have to we have to guard against that and there's you know there's ways to be able to check and usually data partitioning is a good one right and remember if we know that we're memorizing our data um and then we go and use new data on the test table we probably will see is a pretty big drop in the evaluation you can evaluate how the model is trained and i'll get that in a minute you can evaluate how the tuning is affecting that model and then you can finally see the test one way to look at it is that if there is consistency across all three of those layers you might be in that sweet spot there seems to be some inconsistencies to include suddenly it big increases in performance then you might have a problem all right you might be in this over fit category all right all right so typically overfit means you're over performing all right underfit means you're underperforming all right you can kind of look at it that way this little target diagram is kind of helpful i think it's low bias low variance right what this means is is that you know it's very tight in the middle all right so that's the bias part and it's also just in the red so that's the variance all right so we've hit the target in the middle right but we also probably wouldn't generalize very well if we had to predict like other data points kind of somewhere around here you know it might not work very well high variance high bias low variance all right so we still have our tight bundle here right which is our high bias but again we have i'm sorry we have low variance which is the tight bundle right the high bias means that it's off the target all right it's just not quite right so it's estimating error is going to be pretty high all right we have still have a pretty low variance here we have high variance and low bias right so we're kind of in the middle so that's good right we're in the middle but that's good but we've got a good amount of a good amount of variance across the points and then here this is just the worst of both worlds right they still have kind of a tight general you know kind of um you know in one general area but they're way we're way off the target right and it's still spread out a good bit what we probably want to be honest right is something here in the middle all right kind of in this sweet spot right so anyway i'm not gonna be able to draw it with much accuracy here but what we would want if we look out over over in this category here right this might be the closest thing it might be that we would want all our dots to kind of be inside this white area okay so we still have kind of a low amount of you know low amount as to medium bias right so maybe it's just right around this area here but it wouldn't be so tight to where it would be myopic right it would just be too tight we'd still have a good amount of spread so that would be good but generally still on the topic so maybe we you know a best model would be if everything was kind of in those first two circles kind of in here we're generally right on the target but it has enough bias and enough variance in it to be able to accommodate for unseen data right so remember if we have this this here we have inject new data it just won't generalize well right because it just doesn't understand the patterns outside of this tiny little area here okay so generally i don't know if this is more confusing than not but kind of what we're hoping for here is this you know kind of sweet spot between all four of these categories we're not in extremes in any one so here these are supposed to be just four examples of extreme scenarios in combination with low and high bias but really what we want is just we want we want it to be just right right we want it to be medium bias medium variance and that's the sweet spot because that allows us to be able to accommodate for variations in the data right allows us to generalize well and it doesn't allow us to uh you know create you know large errors associated with what we're estimating these new data to be all right i'm going to stop here in just a minute but last point here i just want to make sure to emphasize this is that evaluation is basically all you know it's basically the entire machine learning process all right the more you're evaluating the more you understand how your model is trained the more you understand how it performs on c data the more you understand how it performs on new information the better you're going to be all right so i mean we have lots of situations here where we're doing evaluation right we're doing here we're doing it up here all right we're doing it over here we're doing it here and here all right this is model drift kind of moved off a little bit but all of these and even i mean i mean in large part it's analytics but you know you're kind of doing data drift analysis right you're still kind of evaluating if the data is changing so a lot of this is just about how understanding the weaknesses of your model the better you understand that the better you'll be able to control for them and the better you'll be able to return the value and answer the question that you're hoping to the other thing and we'll talk about this much later in the class is it really guards against any type of potential you know toxic outcomes associated with your models right think about if humans are involved right if you understand the weaknesses and you've really done a robust job of evaluating how the model can work you know what it can do and what it can't do you've gone through the process of using your intuition to see if the data that you have matches the questions that you're trying to do go through all these processes all these gates you're going to do a much better job of potentially avoiding any devastating impacts that kind of machine learning can have right outside of maybe just normal business situations but they've been used in situations that are affecting humans so it's important to keep that in mind the more robust we can be in evaluation and the better we can do a job up front here making sure we got the question and the data the better we'll be all right that's it another quick video i will see everybody in class