foreign exploration all right so we're gonna spend most of our time uh this week just on this one section here and this I mean there's entire classes that can probably be taught just in Eda I mean in fact I'm sure that there there are all right but we're going to hit some of the highlights and especially as it relates to prepping our data in order to get it ready for machine learning applications all right so there's some pretty standardized I think items that we think about when we move into this phase and again this is an alignment with this four plus one model that we've talked about but um and there's there's a lot of depth here but we're going to hit some of the highlights we're going to talk about basic variable types which we've done already but we're going to talk about kind of the functional types associated with you know what are the data frames things we're going to build anyway I don't have to go through all of these um but as you can read through this list you can see that you know this is a pretty big topic here all right that could be covered for you know multiple weeks so we're going to move through some of this pretty quickly um and especially as it relates like I said to kind of getting our data ready for machine learning applications all right all right so I think of these is variable types all right as compared to data types we talked about data types kind of an exhaust in the last couple of weeks but we're going to use all of these moving forward all right and so especially our uh especially data frames all right which gets very familiar with data frames and we're going to do a lot of lists as well all right so lists are compositions of one type of data type data object all right sometimes you can combine them but for the most part they consist of just one type of data type all right uh and then data frames are going to be the you know this is going to be when we load our data in all right it's our two-dimensional data objects and have all sorts of multiple variable types all right so that's strings objects all right complex numbers whatever that might be and that's going to be important because remember as we move through our operations and getting our data prepped we have to make sure we keep a clear eye on what are our data types that we're using and make sure that our variables are in the right one all right we also are going to be using matrices a lot especially for more of our complex machine learning operations and that's going to require us to convert most of our variables all of our variables into some type of numeric form so we're going to cover some of that today as well so it's a lot of it's going to be moving from a data frame to a matrix all right as it relates to kind of using more complex measures we're not even complex really we use anything that has a distance component all right we're trying to measure how far apart two variables are that all has to be in a numeric form so we'll talk about how to do that and we can do that with our categorical variables too by turning them into dummies or one hot encoding making them zeros and ones as compared to just lists of different categories all right nested in one column all right so these are some fun or useful all right functions that you can use right at the get okay so X in this situation all right it's going to represent the data that we have loaded in almost certainly going to be a data frame we're going to use pandas and these are all pan dysfunctions the info one we're going to use a ton all right and that's going to tell us everything from data types to the size of the data set to the number of missing values it's really very helpful all right shape is going to give us the dimensions data types is specifically going to tell us the data types that are in each one of the columns and then describe gives us a whole bunch of summary statistics all right now what I would recommend is oh go ahead and opening up vs code loading in any data set you can even use previous code from some of the other examples that we've used load that in and then give these functions a try all right so just replace the name of the data set that you have loaded in whatever you called it with X here and all of these functions were working you can look at the different information that um they're going to give you so this is important stuff to do right off the right off the gate once we load in our um our data frames to better understand exactly what um what we're working with all right so prevalence we're going to talk about a lot it's kind of like a baseline all right so this is specific to classification problems in particular all right and it's the proportion of a particular population found in the positive class what that means is an example for working on a Target variable we're trying to train an algorithm to predict whether someone will default to a loan or not it's just like a you know pretty pretty typical example used right and this example the prevalence would be the number of people in that particular data set that have defaulted on their loans right so that could be anywhere from 70 30 right so it might be like just 30 people you know out of 100 I'm sorry I'm writing this really clunky 70 people out of 30 people out of 100 have defaulted on their loan and so the prevailence in that particular example would be 30 all right and this is important to know because it kind of tells us how hard that particular Target variable is going to be to learn all right now you can imagine if say this number moved to like 95 to 5 right so that's only five percent of the total people in that data set defaulted on their loan understanding the patterns of those five percent as compared to the 95 is going to be can be very difficult all right and that might mean that we need to have a lot of data or we might have to reshape the data that we have to give the algorithm more information about that five percent right but we have to know that up front so we want to do that we want to understand our Target variable and just get an idea about what that looks like all right so that's an important other thing to understand especially as it relates to classification which is what we're going to spend most of the time on in in class all right all right scaling normalizing and one hot encoding right scaling and normalizing can be kind of seen in the same same vein or they're converting numeric numbers right into some stand format and maybe maybe you've heard of z-scores or something like that and you've used in the past that's an example of standardization all right for what we're going to do in this class we're much more likely to probably use a a min max scalar right where we put all the numeric variables between 0 and 1 and this is really important because especially as an example and we're going to show it in k n this is kind of a distance based measure we want those scales to be the same so that the distances between any two variables are on on uh the same standard scale all right you can imagine if for instance we were comparing weight and height of two individuals a weight can vary significantly you know up over you know 300 pounds right whereas height generally does not have this large numerical variance all right so trying to compare the space between those two variables would be very difficult unless we normalized it right because we put both those variables on a scale between zero and one so I'm going to need to do that okay this one here too for logical right this is the ideas that we would create individual columns for each of the factor variables or categorical variables that we have in our data set all right so if we had uh you know a category just to make it totally generic or it's a color category that had three different variables in it it was either orange three different levels like orange red or green instead of having them nested in one column we would split those out where a zero would be present if that particular row was an orange or uh was or was not an orange would be zero all right if say it was a red then there might be a one underneath red and then there'd be a zero underneath purple I mean that's that's probably a pretty bad example but the idea here is that you know you have these different levels of categorical variables right so wherever they instead of having them nested in one column right so if we had them nested here like one two three all right so instead of having nested under this what we would do is we have individual columns for each one and I'll show you this um in class right so we basically split these out so into individual columns all right and then it's just a zero one underneath each one of these depending on whether they happen to be that fall into that particular that row falls into that particular class all right so this would be like a zero here maybe a zero here this is a kind of a terrible example again and this would be a zero here so this column would be a one this column would be a two and then this column would be a three all right anyway like I said we'll go over that a little bit more in class but again the idea is to turn those categorical variables into numeric all right I talked about the min max scalar all right uh there's entire books have been written on just how to handle missing data all right so we'll confront this a lot there'll be some missing data in your lab as well um I'll have to figure out exactly how to deal with that all right so python comes with a bunch of packages and uh functions that can handle missing data but the key that we want to try and discover all right and we'll talk about this is to understand if there is you know a pattern to that missing data or is it completely at random all right so now if there's a pattern it's important to know that right because it could be information about how we could potentially replace that missing data all right if there appears to be no pattern at all if those variable missiness is not correlated with any other variable Etc that's probably a better scenario all right then we can kind of handle exactly how to how to deal with that random missiness but if there's a pattern that means there is some information inside that data set about why that missing why those missing values are there and we got to figure that out all right otherwise we're kind of going into building our algorithms a bit blind all right a good way to start out just to get just to get up and running to see how much missing data is it's just again to use this info function all right and it'll tell us in you know tell us in a print out there exactly how many um how many values are missing in each variable so it kind of gives you a chance to discover patterns as well right so we see two variables or three variables or four variables that all have the exact same number of missing values usually that's an indicator that there's some pattern and we've got to better understand that all right as a general metric you know if we have more than you know five or ten percent of missing values you really got to take a look at that and see what's going on in any one column and develop a plan and the plan might be to delete that variable um or it might be to reduce the entire size of the data set you know uh to limit it to the total non-missing values in that particular category those are two you know blunt approaches and then there also could be some choices about how we do imputation which is how we replace those missing values with with other values like the mean all right okay uh we're also going to have to do partitioning and sampling all right so a foundation uh foundational concept of machine learning is how we do this training tuning and test data sets and what that involves is dividing our data set into three partitions one that's fairly large that's probably the training set that might be eighty percent of the data and then two smaller data sets all right used for tuning and test all right so tuning is a way that we can modify our algorithm whether that is the hyper parameters or remember the hyper parameters are this particular features of an algorithm that can be adjusted or we do data engineering right which are the actual variables that are going into the model we might change those and then seeing how that affects the performance of the model all right so we do all sorts of things in the training and tuning phase to try and optimize this performance and when we think it's totally done we think we've got a really good model going based off the back and forth between training and tuning data set and I'll show you how to do this and then the very last step is to push in that test data set to get a final validation on how things worked all right yeah the concept behind this is that um what we want to do is have an independent sample right that's un uh that wasn't used in the training process at all all right to test and see how the model's working right it's kind of like an independent Test example there's lots of different ways to do that one of the main ways and this is the sklearn package or scikit learn let's do this function called train test split all right and I'll show you how to do that and we're also going to need to do stratified sampling all right stratified sampling means that we're gonna split the data set based off the essentially the prevalence in the Target variable all right so we want the same amount percentage-wise of information to go into the tune the test and the training set as it relates to that Target variable again just to go back to the example we had if we had a 70 30 split right we want 70 30 split of that training uh to go into the training set all right to go into the tunes set and to go in the test set and what the 70 30 that ratio to show up in each one of these partitions would be a problem as an example if we I'll had you know say that shifted to you know 90 to 10 right in the test set all right then when we went to go and and test to see if how well we've trained it it might not work that well right because we don't have a representative sample of what we actually trade the train the data on so that's an important thing to keep in mind that's why we do stratified sampling all right we'll talk talk a little bit more about that all right so cross validation is a concept I'm just going to introduce all right that we're going to show up a little bit later it's kind of a stretch concept the idea here all right is that what you do is you take that training set all right and you divide it further into multiple partitions and train multiple models and then average those together all right to create a more robust model as compared to creating just one test set right and just one training set all right so what this does is it allows us to use the power of sampling we're going to go back and Sample that training data set multiple times build multiple models all right and then average those out um to help us better understand how the model is working in a more robust way on that training set all right so this is an example here all right so where we might take the train set and we divide it into essentially four parts right so here this would be the first part right and so what we might do here is that the model would get trained on this partition of the data and then just test it on this fourth of it and then here it would get tested on this one trained I'm sorry on these three partitions and then tested on this kind of I don't know it was that third quartile and then here you can see it's just what we do is essentially remove a fourth of the data every time right that does not overlap with the fourth of the data that got removed before and then you just average all of these models together all right to create a more robust output all right as compared to just doing one large training and then testing data set all right because no matter what you do if you add more data to the training you're going to take it away from the test that's why this kind of you know tug of war rope is represented here because it they steal from each other but here's a little bit more of a robust way to handle that all right and we'll talk more about that um you know as we move forward with the class we're going to be our default way for training some of these methods um turns on these models but I just want to introduce that right at the beginning okay so that's phase two we're going to move into phase three all right uh next week and the week after we're gonna re we're gonna we're gonna keep touching on these topics every week so don't think this is the only time all this is going to show up this this phase is a is a is a x expansive area all right of the machine learning life cycle we're going to go into it uh in great detail throughout the semester and more things we're going to add more stuff to this this is just like a quick overview of some of the topics that we're going to be going to be covering in this week but again um there'll be more things that we add into there and you could practice using those over and over again and then next week we're going to go into things that are much more specific to kind of model building all right in particular all right we introduce this a little bit about the difference between hyper parameters and parameters but all these topics here um really have to do with specifically the training process in particular whereas what we're looking at here is kind of more like prepping the data in order to get it ready for training all right so that's where we're at and I will see everybody in class