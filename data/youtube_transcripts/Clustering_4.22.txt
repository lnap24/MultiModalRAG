okay so uh this morning we're going to talk about what this morning during this video we're going to talk about unsupervised machine learning that has us over here you guys have seen this slide before we've been mostly doing supervised already k n and we did decision trees but now we're going to move into unsupervised which as a reminder means you know we just don't have any labels all right no labels on the data that means we have to identify these patterns without necessarily a target to match them to okay so the one approach that we're going to use in this and there are many is is clustering and that's a pretty large space as well but we're going to do k-means which is actually something we've seen before if you remember we walked through this example right where we have a random string of numbers and the ideas that we need to maximize the distance between these two clusters right we're saying we have two clusters and so we keep trading the numbers back and forth right between the clusters until right the space between these averages of the clusters doesn't change and then we stop okay so the starting criteria for this is just a random selection of uh centroid all right and then and then uh they allocate out the numbers to those centroids based off distance thing and then the machine calculates the average all right and then it takes the average of these new clusters right the new centroids based off the allocation takes the average again just like it's doing here right and then it sees if it's getting better and it keeps going and going until it doesn't that's basically it we're going to get into more details about it it can be a form of exploratory data analysis we're going to use it as a model as well but you can think of it as kind of exploratory and the way that we think about the way that we say move me around here the way that we reason we say is exploratory because much of clustering especially k-means is done through visualization all right so this can think of this week as an exercise and data visualization right through through the modeling i'm disappearing here but you won't be able to really understand the clusters or at least conceptualize them right uh until you can see them all right so much of the analysis is done through the actual development of graphics associated with how the clusters are being represented in real space right okay so that's why you know we kind of consider this an endeavor in a date of viz but uh it's also just a you know very useful tool for predictive analytics as well okay so there's lots of ways to do exploratory data analysis okay i mean like i said most of these are done through visualization this is just a couple of examples here i actually have a bonus lecture for you guys if you want to work through it in colab on using text and we do word clouds and all sorts of things um you know the sentiment analysis but basically the idea behind the text mining portion was much broader discussion but is to take those take those words and turn them into numbers all right that's one way to do it network analysis is another this is where the distance between these nodes is is highly meaningful and then we're going to do or do clustering all these are you know analytical procedures but they can be used in a very visual way so we can think of them as kind of advanced exploratory data analysis techniques or maybe in kind of the traditional camp of of data mining you know people might think of it as that okay so again we got no labels right no labels we don't know what the target is so that's the keys we got to figure that out all right so what is clustering okay so maybe it's a pretty simple right so it's a type of unsupervised machine learning all right all right there's many classes of machine learning algorithms unsupervised machine learning algorithms all right we always use a distance metric here all right so we're going to have to go back to just like we did with k n and k n is kind of a type of clustering to be honest you can think of it that way is that we're going to have to use a distance metric we're going to use liquidity and distance just like we did with k n all right and it's going to be in a hyperdimensional space but we're going to represent the clusters in two dimensions all right so i'm going to get more into that but essentially the idea is that the closer the data points are to each other in this hyperdimensional space the more they potentially are related and that's it maybe it's intuitive but the idea is can we find these points that are highly highly related to each other that are close together and create boundaries around them and say hey anyone inside this boundary we believe that this type of uh this type of class right this type of variable all right and then we have to define exactly what that means okay this is an example here all right uh you know it's kind of a classic one you know 1997 or less this is very old at this point but it was one of the first papers that used clustering to do kind of customer segmentation all right so ge capital it cluster their customers based on similarity all right so i'd use this to promote credit card bills or to get you know get bills paid better and increase profitability in terms of the way that it was targeting customers all right so it was able to to bucket these in areas where some were delinquent or people had lots of revolving credit which is just credit cards all right or this was maybe and again without exactly getting through that these are these are segments here of customers that were highly valuable in terms of market targeting okay they weren't in delinquency they didn't necessarily have a lot of revolving credit yet so maybe they were good in terms of targeting for additional credit cards or other types of financial instruments right the important part to remember here is they didn't necessarily have these labels ahead of time all right so they just clustered these groups together and then said hey what do these patterns represent all right well majority of the people over here are in delinquency it seems like that seems to be a pretty clear pattern and they don't have a lot of revolving credit and then they were able to use this framework all right to develop marketing materials on top of it but they didn't realize it was going to look like this ahead of time that's the idea behind unsupervised machine learning you don't really know what you don't know right you don't know what it's going to look like up front and then when you get it it requires a bit of a qualitative interpretation like what does this necessarily mean like okay so we see these groups great but what's going on there all right and that's where kind of uh your subject matter expertise starts to come in or you have to dig deeper into what exactly is you know what do these clusters mean you know how can we look at the different variables that are composing them and then uh and start to develop a strategy around them all right so in a lot of ways i think this is fair unsupervised machine learning is much more complicated than supervised machine learning because it requires kind of you know multiple steps along the way right we do all the same data preparation you know we build the models and a lot of ways we can validate them the same way by building many at the same time doing kind of a cross-validation process and what we did with the trees but it's not necessarily a canonical output right and it could be that even though we're going to look at variance explained even even if it's not a great model it still might be really really useful in terms of what it's telling us you know in terms of where outliers lie or how that there's really seems to be really one tight cluster then maybe totally none other clusters that seem to emerge but that's information you know it's really good so there's lots of stuff that can be pulled out of uh clustering but it does require you know a good amount of getting out of the hood and taking a look and seeing what's going on and like i said interpreting in a kind of a qualitative way as compared to just a pure kind of analytical evaluation approach it's a little bit more of an art all right so we're going to take a look at um voting patterns among congress this is an easy one because you know the voting patterns are highly polarized and this data is old should really update it but it still works you know it's still the same deal i think it's from the you know early 20 teens but uh we're going to use okay means we're going to see is there a pattern you know is there some structure in this unstructured data all right so let's take a look okay so the data set consists of 427 members it's from i'm sorry 2013. but trust me it doesn't really change and uh there's three types of votes we got the yay the nay and the other okay so as you can imagine um there's going to be voting heavily on party lines okay all right so the data points that we're going to have for this example are going to be democratic bills all right they're going to be introduced so i'll get to that in a minute so some of the things that we might want to identify with this say that you're a you know a republican or a democrat or a lobbying firm right you want to say to yourself do do people historically vote on party lines i mean the answer to that is probably yes all right but are there some examples of representatives that maybe don't right or is there certain types of bills right that may not fall into the same party diamond ism right so in order to do that we'd want to create these clusters right and take a look and see maybe that there are some outliers i mean you can see even here before we get into it maybe there are right if this is a democratic and this is a republican cluster so let's take a look all right all right before we get into it i want to talk about the nature of clustering the idea here and this is going to be the key point that was going to help you understand how uh clustering works okay there's going to be these kind of these measures of cluster quality all right so here we have inter cluster difference all right this is the difference between the clusters all right from the very outside of each ring right and then there's inter cluster difference and that's the average from the centroid to all the points inside that particular cluster so it's this concept is pretty pretty easy all right so the idea is that what you want to do is you want to minimize that inter intra cluster distance very easy for me to say you want to make that as small as possible but the space between the clusters right i'm going to make that as large as possible but they play off each other right if one's tighter the other one's going to be bigger all right so there's an effect there's a push and pull there all right but that's the idea like if we had perfect clustering you can imagine you know you'd have like one really tight cluster down here and another one like way over here right like oh my gosh these these two groups are totally different from each other they're tight and they're super far apart right so that's intuitive right how we measure it is a little less intuitive but we're going to walk through it all right so the centroid and again we talked about this before but the centroid as a reminder is the average all right distance from all the points to that centroid okay all right so it's the average it's the middle of all these points that have been assigned to that particular cluster right there could be eight of these right and so all the points that are assigned to that cluster it would be the average distance using euclidean distance to that centroid all right and you know so you can think of it another way the centroid minimizes the distance between all the points in that particular cluster all right it's the middle point it's not necessarily a point itself all right it could be it's just a middle point and that particular clusters okay so uh four steps in k means we've gone over this but we'll do it again all right so say this is our data right these are the number of nay votes here and these are the number of yay votes here right so we assume this would be democrats and this would be republicans if these are you know democratic industry introduced bills democrats are going to vote for it and republicans are going to vote against it okay so the first thing we do is we just randomly select two points right okay just randomly select just like we did in our example that we walked through those are just thrown on there they could be anywhere okay that's important thing to remember i mean that's why we need to set our seed for this in particular when we go to do it because these points reset every time all right and then what happens is is that the uh points get allocated to each cluster right so this isn't a perfect example right in terms of where these clusters are sitting so just you know work with me through this kind of theoretically but the idea here is now that you know these random points uh random centroids have been put on all of these points are assigned to the centroid that is closest right so each point gets assigned to whichever centroid they are closest to and that's just liquidity and distance all right all right so this point will go here right you know this point i'll go here you guys get it okay so in theory this is what it would look like afterwards all right then what would happen right is that those centers would be re-calculated based off the new cluster membership okay so as our centroids remember they were like over here and over here before but now once we allocate all these points it just takes the average right so it tries to find the middle point of all of these clusters right finds the middle point and then it just it just recalculates the centroids all right and that's it and so this process just keeps going and then again you know based off this new right based off this new reallocation and as you can imagine the most gains are going to happen right early on right in this process right after that it's going to be kind of small movements of the centroids inside the established clusters unless you have many clusters then then they can move move around a lot but if you have you know less than less than five you know something like that it's not gonna be a whole bunch of change you know in terms of the number of steps that it takes to for this to for this to fit all right so uh right and so it just keeps going through this process again and again until no uh until no additional right until no additional change can occur until the space between those means has been maximized just like we saw in the numerical example we worked through it right there would make no sense to reallocate right the numbers in between the groups anymore because they're as far apart as they could get any reallocation would actually result in depreciation of the model right it would make the model worse so that's how it stops right so when it knows that it's maximized that inner and intra all right cluster difference okay so that's all this says it's just repeats those steps over and over again as we can see here right what's going on this might be really might be really useful right is that you know um in this example there appears to be right some people that are classified in cluster one that seems to be much more dominated by democratic votes it looks like i think i may have misspoke earlier i have two data sets i have one for republicans that are introduced the bills and won that for democrats it looks like this is the republican one um because this is the number of yea votes here and as you can imagine republicans will vote for republican bills all right and democrats will vote against them all right so these are likely republican introduced bills but it seems here that there are a few i'm sorry that there's some noise in the background it seems like there's a few um democrats right or i'm sorry republicans that are voting kind of more uh in aligned with democratic patterns generally they're not you know really far up into the cluster maybe like this person is here right right but they are simply not in the center here so we can leave this center point as kind of you know those voting exclusively on party lines so there's some variation in terms of the patterns with which they're voting okay so that might be useful for us to to explore and get a better understanding of like who are these particular uh representatives right and they might be the ones that if we were introducing a bill saying we're democrat we're introducing abilities they may represent swingboss right because it appears that at least in comparison to their peers and then in comparison to this you know kind of the tightness associated with this cluster is that they do have a tendency whoever they are to vote kind of along not necessarily along party lines and that's good information for us to have we also notice generally you know i think it's fair if you look at the um patterns associated with democrats or republicans is that the diffusion of the democratic voting pattern is much not i don't say much but it's certainly wider right than the republican okay so we can see here that just in generalities right i mean this is a much tighter cluster and we could actually measure these individual clusters and we'll see this as comparison to the uh democratic one right and that's good news for republicans all right so republicans are kind of staying on party lines depending on your view of course of political diamondism uh are standing on party lines but is that democrat it's not as much right they may be more subject to um you know vote in favor of some of these republican bills so that's interesting inside itself there's a lot of code in here i'm not going to spend a ton of time on that because we're going to go through it in class uh it's all in the slide so if you want to start working through it then i uploaded the arm cards in our script and our script up to um up to the codelab so you can take a look at look at all that okay so i'm just going to keep going all right so here's here's an important slide right so these are all the measures essentially of the variance or of the patterns associated with the clusters themselves all right so the total sum of squares right this is the sum of squares the distance between all the points right and this is measured from the global mean of oh god i'm gonna have to like write out global this is this is just a mistake it's from the global mean of the entire spread of the data so we're going to normalize the data right then we're going to project it onto a hyperdimensional space and you can think of this as like you know the core right this is the core of the earth assuming there's like a perfect sphere whatever it is the middle point of the entire um the entire spread of the data all right so we measure every point every distance linear distance every point from this global center and that total right that total spread is what we call the total sum of squares okay distance from all the points all right the between is just the thing that we saw before and i just highlight these two because it's easier to talk through two of them because the rest of them are basically just um derivations of these two all right so between the clusters is the distance right between those boundaries of the clusters right and that's a calculation you could look at the total sum of squares error minus the total within this within sum of squares error it's easy for me to say all right but the total within cluster sum of distances is the distance from every point right to the centroid that is assigned to that particular cluster and then it's summed up right so you have two of those all right so you have one for cluster two and one for cluster one in the example we saw up here right the total variance in this would probably be probably larger right than the total variance here okay so what happened is that because of that right the between distance between these clusters would be smaller right so more variants accounted for in terms of the cluster not necessarily great right we want that to be tighter all right we want those clusters to be tight we want the space between them to be wide all right so when we do this ratio essentially all right which is just a total sum of squares minus the total within this right which is just the variance accounted for by each of the particular clusters we get this between measure all right and we'll talk a little bit more about the um in just a second so here this is better i can show a little visual example all right so and then another way to look at total sum of squares error right is just the it's going to be the inter cluster difference which is the between sum of squares error and then within summer squares area add those two things together it should be the total sum of squares error okay that's all the error on the on the plot again there's a bunch of code here okay and the you know the quality of this graphic is terrible but this is for democratic induced bills right here all right so we can think about you know the two clusters here i think this label might be wrong all right so again these are just we just i'm just projecting the perimeter essentially of these two clusters just to show you it with real data all right so here we can see it's clear that two groups exist all right group one and group two right and then you know how do we infer the differences between those will use those calculations that we just went through all right okay so i think this bill this this pattern is a little bit more accurate here in terms of the way that we're projecting it with the republican but it's sorry but the democrats down here and they're republicans up here okay all right so just showing that same thing again and then here again what we have is kind of outliers these could be gauged as uh and then maybe some over here too right engaged as uh targets for a potential um lobbying activity right or you're going to reach out to those and say hey it looks like you're not voting traditionally along these pattern party lines you know you maybe want to go in a little bit more history about what are they voting for and what they aren't and see if they can bring them over to your side when you introduce a new bill it would certainly be interesting to see how this occurs over time to see if polarization is increasing my suspicion is that it is all right we could also look at it this way i'm going to show you guys encode how to do this all right kind of do a three-dimensional plot all right where you can flip it around and here we have all three pieces of data aside from just nay and the a we also have the other votes it ends up being a pretty small percentage others are just like abstained or not present in absence something like that which is the biggest they're pretty minority in this particular group but this is a nice way to look at it and i'll show you guys how to do some of these 3d graphing so you can kind of flip things around and that allows you to see um more of the influences on the data points as compared to just just two okay so again here what we're trying to do how good is the clustering we're trying to maximize the separation and keep those clusters together all right so one of the ways that we're going to look at this right is we're going to explore a variable called variation explained all right so the total variation explained it's just going to be the inner cluster variance which is the between sum of squares divided by the total variance all right which is just the total sum of squares okay so we're just going to divide that what is the ratio of all the possible variants how much of it is in this between space all right so that what leaves us with right is basically how much of the variance is explained by the clusters right so it just says you know given the differences between them given the space which is good we want as much space how much is left over just in the clusters themselves and that'll tell us the total variance explained by the particular clusters all right and we can use this calculation it's the same slide again here we can use this calculation all right let's show you down here right in order to optimize our clustering algorithms all right we can use it uh just in the elbow method right we could select a bunch of different there's such a bunch of different numbers of clusters right for here we kind of knew that there was going to be two because because of republicans and democrats so we had kind of a lead in there which makes it easy to show but it might be that we don't know that right in most cases you don't all right so we can use this measure and we want to optimize this right we want our clusters to explain given what's left in terms of the total variance how much is explained by the clusters themselves so that's what we want to try and figure out and so we can we can do that it's pretty easy when we do what k means we'll have all of these measures and we just create this little calculation basically just divide uh you know the total uh the total sum of squares error here right denominator right by the between variance and we just get a ratio all right higher the better all right so um that's how we can optimize okay there's other ways to do it too this is an example you guys remember this we use this elbow method just like we did for k n nb klust is basically a package inside r that we'll talk through which runs a whole bunch of different models and then uses majority vote as you can imagine there's many ways to do clustering basically the variance between the methods is dictated by the measure that they use right it's just how they measure distance that's pretty much it there's there's other things to get into there but if you think of it just that way there's lots of ways to measure the distance between points and so many of these clustering methods use different approaches to measure it euclidean is the almost kind of universal approach but lots of other lots of other ways to do it i'm just going to walk through these real quick all right let's go walk through these slides real quick and try and get to the point where we see kind of this graphical output all right we're going to go through all this and here we go so it'll look just like this and here we see kind of our you know this is our uh optimization function here you can think of it that way we're trying to optimize this variance explained and then we just see this elbow method right just like i said before you know typically much of the benefits of this clustering approach are going to be realized really early on and then aft after here this is all you know this is in diminishing returns right so maybe we could go up to three you know that might be interesting right like we showed sorry not to make you see sick here but we showed here all right and maybe three is is better in terms of the way that we're thinking about this data maybe three represents a segment of representatives that are simply not voting very tightly along party lines right so even if the variance explains just goes up a little and went up a non-trivial amount maybe eight percent maybe three is the better approach just because it's easier to visualize it right so we tagging these people as threes makes it easier all right we still have this guy over here right extreme outlier but you know uh maybe that's a better approach just because it's easier to visualize what the goals are for this particular project that we're trying to capture right beyond that i can't see any advantages which is the nice part about this elbow method makes it really simple and be clustered two things by majority vote we'll get into more of that see if i get down to where we have the graph yeah essentially these are all the number of clusters the number of clusters recommended by each of the methods okay and then the number of votes the number of methods that voted for that number of clusters all right so here we have for the number two all right we have you know over 12 you know more than half assuming there's 30 of the methods essentially voted that two was the best number of clusters to optimize the algorithm so this is how this works it just writes a whole bunch of different methods and then sums up um which ones voted for which number of clusters and then you can see it kind of nationally um where with 12 those looks like it's more um you can see it right here graphically okay all right so i'm running along here so i'm going to start um wrapping up all right uh certainly we could see pattern differences and republican light bills as compared to democratic all right who provide information on congressman for swing votes right um so there are some limitations that we need to be aware of you know assuming the patterns correspond with bills being voted perhaps congressmen have the same number of yay or neighbors but they voted on different bills you know so we wouldn't get that you know we wouldn't get that nuance right exactly about how the patterns are associated with particular bills so it's really kind of an aggregate measure all right network analysis might actually be better right in terms of to think about the relationships between particular congressmen maybe it's that there's some geographical kind of voting patterns that need to be sparsed down we don't necessarily have that here all right and so we can't we can't can't take into consideration all these you know extenuating circumstances right in terms of political initiatives things like that it's very it's very high that's why we think of it as kind of an exploratory measure but in many cases it works really well right okay so the good is that it's cheap there's no labels labels are expensive and take a heck of a lot of time all right i think of armies of interns labeling dogs versus hot dogs right all right i know this is going to sound counterintuitive but clustering always works right if you have no clusters what it tells you is that you have no clusters right and that's information that you can use all right might be that you need to go back and rethink about the questions you're asking of your customers or the data you're gathering right if it's not providing you information then that's information in itself so it always works and typically there are at least one or two clusters that work really well and then there could be some diffusion amongst amongst the others all right there are a ton of different ways to do clustering i mean i just just i mean here just an mb cluster 30 different methods and that's just for k-means there are a lot all right they fall into kind of you know broad categories you can think of them as kind of unsupervised and hierarchical i'm not going to talk about hierarchical um but you know anyway that there are many many many methods that's the point and it's sometimes it's hard to choose the right one all right we have a dimensionality problem if a space starts to get too complex i mean clusters are probably just not going to work you have to go to some type of network a more sophisticated kind of sophisticated model uh and just like any other bad data in a bad model out all right phoenix does not get much better with large large amounts of data you know you can kind of segment off and then it doesn't look like there's some type of patterns emerge sticking more and more information in there usually doesn't make it significantly better as compared to other machine learning methods that it might okay it can handle spherical data i'll explain that in a second and unequal cluster sizes may uh do weird things decay means all right so let's just take a look at this real quick i think this is the last slide here i'm running a little long this is an example of spherical data you see it's in the circle patterns all right and this isn't all that uncommon okay and so what k means does right this is a centroid and this is a centroid so what it does is it divides this data in half instead of clustering it in the proper pattern like we see here okay which db scan which is just another approach can do all right so there's another example of it here where we see that just the nature of the algorithm in terms of how it is optimized doesn't handle well with these types of waveforms all right so here it has the dividing line again kind of in this horizontal pattern and really what it should be doing is these is identifying these circular patterns which is properly identified here okay the same the same as here if you have two clusters that are really closely related to each other it may sub optimize it right say hey these are the same thing when really it appears that they are that they are different so that happens okay and you can see here spectral clustering does a pretty good job but it might not do a two good job with this right this is just an example of many different models being used on the same the same data all right mean shift that's the same problem here anyway i'll let you continue to explore in and ward is just awful across the board okay so uh with that all right k-means is it's computationally pretty cheap in terms of just putting things together so that's good these other methods maybe not so much all right so there's some advantages there all right we'll go through some of these when we uh when we get into class and i'll see you thursday we'll walk through all the code all right thanks a bunch bye