okay all right so now we're in uh phase two of our kind of machine learning pipeline i'm just gonna get this pulled all the way up here all right so just as a reminder let's move me kind of out of the way here uh as a reminder this is uh where we're at kind of here so we talked about that first we guys went through that case study where we talked about you know kind of data acquisition and questions about whether the data would necessarily fit our questions and so now we're moving over here we've decided that you know it looks like we've got a good question going uh we believe that we have the data necessary to potentially answer it two big things that we need to cover we've got a metric that will kind of independently track and now we're moving into the next phase which is kind of getting a better understanding of the data that we have all right that we've gathered through however that we've acquired so we have to make sure that it's clean and ready to go and then we have to kind of just do some initial framing of the data in order to make sure that it can uh can answer our questions so we'll get into more of this kind of down the road this second phase is really going to focus on this all right we're just going to get the data cleaned up prove a better understanding of what it can and can't do all right it's really kind of interesting prep for model creation all right there's a little thing great all right so here are the things that we are going to explore in this i'm trying to do it as quickly as possible i just want to know that each one of these and we're going to talk more about them throughout the semester each one of these um well i mean several of these are quite expansive topics all right so missing data you know people have written entire books on just how to handle that particular problem which gives you some quick examples of how to deal with that cross-validation is another one that's a pretty deep topic there's lots of ways to think about doing that and we could spend quite a lot of time on data types and variable types but we're just going to skim over kind of the high level stuff give you the necessary tools that you need to get started and then as we go along in the semester we're going to add a bit more complexity so don't feel like we have to understand all of this right away we're just going to get through um kind of an initial overview of these topics and then we're going to keep using them throughout the semester should become much more familiar and i will expand the kind of scope of them all right so uh variable types let's get right into this all right so many of you should probably already kind of be familiar with this but there are essentially five variable types here all right i added this one all right so even though it says five there are six i need to check my slides before i start writing all right these by all intensive purposes are the same thing it just has to do with memory space essentially like this is bounded i think to maybe a billion psi something like that and this is just limitless all right complex numbers all of these things you should probably understand we're going to get into factors a good bit because that's an important kind of thing to understand it's really kind of a unique class of character so that's why some people don't list it as like a canonical type by itself but its properties are so distinctive from string and character values i think it's important to understand it by itself okay all right so that's we have the variable types right these are variable types we also have data types okay so we have lists all right and our object contain all sorts of different elements including vectors and functions and you can have lists inside of lists nest list you can have data frames inside a list right it's a very expansive type of data type all right you also have a vector and a series or just one one data type of these okay all right so it's just one of those you're going to take a single column in a data frame as a single vector a single list of character words whatever that might be all right that can be a vector we already looked at an array remember we had a multi-dimensional array think of that as kind of basically a you know a list or vector with more with two or more dimensions right so you know two by two by three all right has this third dimension right that's we think of as a two by two by three array all right a matrix is something that we're probably all a bit more familiar with just a two dimensional data structure it's typically numeric doesn't have to be it can be uh one of these all right but you think of a matrix as basically a collection of vectors all right they have to be the same data type right our same variable type it's kind of a collection and a data frame is the one that we're going to use the most it's the most flexible object inside in terms of i'm sorry flexible data type inside most object-oriented programming languages all right so we can combine many of these variable types all right into a two-dimensional rectangle table all right so it's has the same kind of characteristics as a matrix in terms of its dimensions all right but key characteristics separating from a matrix to a data frame is that we can combine variable types right so we can have factors and numeric integers and characters right we're all being one data type but it only has two dimensions that's another important characteristic we have multiple data frames together and multiple dimensions and you start to move into an array all right okay these are some usable um usable variable and data type functions okay so we covered this one right gives you the structure of the uh particular data frame or list or whatever that might be all right class will tell you what type of data type it is all right so you do that or it will tell you what type of variable type it is right so that's a polymorphic function for those of you that are familiar with that is that it can take many forms depending on what is passed into it okay all right names will give you all the column names all right just kind of give you an idea there length will tell you the length and dimensions will give you the dimensions of the object so if you're wondering if it's an array like what dimensions are you could pass it in it will tell you it has three dimensions right if it's a data frame i'll tell you what the two dimensions are in length and width all right so this is going to be one of those things where i want you guys to try this out a little bit all right so open up rstudio and try these functions out on the mt cars data set and then let me know if you agree with the output okay in particular with str i think we've looked at this a little bit before but take a look at what's there and see if you agree with the output all right just pause right here go ahead and get that started we're going to use the mt cards data set kind of throughout this lecture you're going to be jumping back and forth so make sense just to go ahead and pause and get that pulled up okay welcome back so um prevalence right so it's pretty easy but it can be kind of tricky in um in verbalization right in order just to be able to get it quite right so it's the proportion of the particular population found in the positive class all right so now positive class doesn't have to be like some type of positive attribute actually it could be you know the presence of spam and one would say that as positive but that's the target variables one classified presence the presence of spam equals one all right that's an example so prevalence is basically just a balance of our target variable all right so we think of ones and zeros one zero one zero one one i don't know i'm trying to balance this out all right let's say this is a zero all right so we've got three zeros and three ones all right our prevalence here would be fifty percent all right so fifty percent of the data has ones fifty percent of it has zeros okay so that would be the prevalence so we just think about this in terms of that target variable all right so go back to the mt cars data set let's let's do a little bit of work there too all right see if you can figure out what the prevalence is of the vs variable all right it's a little hint here all right you might be able to use the table function all right there's easier ways to do this once we get in there's lots of functions that give us the prevalence but table is going to be a nice way for you to think about being able to do that it'll just show you the different classes and then you can just do some simple math all right see if you can figure out the prevalence for the vs variable it's just ones and zeros in that particular variable all right let's jump back over there come on back when you're ready okay all right so scaling normalizing and one hot encoding all right so we've already covered a lot of ground here um most not most but many uh data science algorithms machine learning algorithms are going to require you to normalize the data in order for it to be passed and be learned appropriately all right so the main kind of scaling normalization technique that we're going to use it's going to be a min max scaler all right i'll get into one i'm coding just a second but a mid max scaler takes something and it just places it inside a range of 0 to 100 okay so what it does is it will maintain the variance of that particular data of that particular object or the particular variable so just bear with me here just imagine if you had a variable that was associated with weight right so say it was down here to the very extreme you know 400 or something right so it probably looks something like this right right so we did a min max scalar on this we're basically just normalizing it instead of from zero to one we're just changing the x values from zero to a hundred all right and the distribution is going to look exactly the same all right pardon my terrible drawing but when you look at the calculations of how this would look on just a simple kind of density plot here it'll look the same these will match right the only thing that'll be different all right is the range of these values right and what this does is it allows us to put all the variables independent of their range inside the same independent other variants inside the same range okay so if we had another variable say we're trying to measure let's just see how many times cal people went to california in one year right so what's the average of that i don't know maybe it's between one and five all right so we're trying to compare something from zero to four hundred to one and five you know that would be pretty hard to compare okay but we just take this one instead of contracting it we do exactly the same thing all right we just expanded out between zero and 100 and then wait and the number of times people happen to travel to the west coast would have the same basis for comparison all right this is important because often remember if i remember if you remember back to k n right we do k n how does k n work right remember our in class example right we're trying to figure out what this particular data point is based off all the data points around it all right and so maybe we'd pick three or they did a pretty bad job and anyway so this one would be plus right because it's got two of the three two of the three all right it's at a particular range or our pluses we'll get more into that just for this example but see this wouldn't work right because this is a graphical based measure if these ranges weren't even all right what we want is kind of this non-biased spherical multi-dimensional space right if we have variables that are contributing to this that are all sorts of different sizes we would have some weird shape anyway i'm just making this up and so the balance of this wouldn't work very well all right so what would happen is that something between 0 and 5 will probably get used to be more way more heavily on the classification than something was from 0 to 400 alright so there would be an unbalanced nature to the how that my hyper-dimensional kind of high dimensional space so that region was being generated inside the algorithm all right so we want those to be normalized it also just makes things that to be honest quite easier to interpret it makes things faster because everything's kind of on one scale makes calculations move quicker all right so there's lots of good reasons to do that you don't always have to do it decision trees for an example don't necessarily depend on the scales being said being the same but it still can kind of be best practice to do that all right so that's a min max scalar all right we've got a function that does it all right i'll show you guys how to use that all right okay all right so the other thing we want to talk about where it didn't index scalar what's the dearest factor is vector variables all right this is for one hot encoding okay so when people are familiar with kind of the inferential language around this if you're familiar with dummy variables right one hung coding is the same thing as a dummy variable i just said in computer science language all right so say we had a you know we had a factor that had four levels right one two three four all right so this is how it would normally be represented inside our data frame just in this little list like this all right but uh in order for us to be able to use this in uh certain machine learning algorithms and uh graphical based ones to include all right in order for it to be efficient all right we want to um we want to get rid of these nested levels and we want to actually just spread them out and have them be their own levels so instead of a instead of four levels going down vertically what we would see is we would have four variables we have x1 x2 x3 and x oh and x4 okay so the way this would translate that would be a one zero and a zero to zero all right this one would be a zero and a one and a zero and zero all right you guys get it okay all right and this is corresponding to the different r oh sorry this would be a four it'll be a one all right so just pretend these are different rows right one two it's probably a bacteria you should use a b c and d might be easier if you see this corresponds here one goes there two goes there three goes there four goes there so instead of the numbers just the presence of that particular factor level in that row is represented by a one right this is in computer language which is just ones and zeros it makes everything much more computationally faster and allows you to use certain factor levels inside certain machine learning algorithms where you couldn't do it before okay so it's easy so we got two of those min max scalar right we're going to use that a lot and then this one hot encoding all right both very traditional kind of machine learning prep all right so what i'd like you to do this might take you a little bit longer all right all right is to use the min max scaling function in the grad descent package okay so you have to load this package in all right and you're going to have to find the midmax scalar function and then just scale empty cars all right put it into a new object what i mean by that is you know something that looks like this all right and then whatever the function is it's probably just something along these lines i don't actually have it totally memorized min max and then just pass in empty cars and then just uh take a look and x see what you said figure out what type of object is use a class function all right all right and see if you can view it as a data frame all right so try and give that a run okay all right it might be kind of a special class of object it's a little bit tricky but you'll figure it out okay all right welcome back again all right okay so missing data a huge topic here all right so we could spend many many weeks on this all right but what i want you to make sure to understand is that this is something you have to check for when you start working on your data all right you can't just go in blindly and assume that every that every cell is populated all right it has to be a pretty big problem relative to um it's a common mistake i'll just say that way students go in and they'll start working and all of a sudden they can't get things to work they don't understand why well it turns out they have some n a missing one you know cell is empty and many of the machine learning algorithms are evaluation techniques that we're going to use just won't won't run if there are n a's inside the data set or any type of missing data all right so r comes a whole bunch of functions and packages to handle missing data the most common one that people use is the mice package all right stands for mean imputation something like that anyway so it stands for missing imputation i probably shouldn't have started because i'm not totally sure anyway there's some acronym there to include missing an imputation all right should i look that up right the idea is to give you tools to not only identify patterns of missing data but to also replace them imputation is another word for putting in for replacing missing data all right sometimes you can just use the most common one that that that the mice package uses something called predictive mean modeling all right predicted mean modeling and what that does is it basically creates a predictive model for what that particular cell would be based off the cells around it right and then inputs that output so it kind of predicts something about just developing like a regression output predicting what you know any one missing data point might be and then inputs it as a result okay so the things that we need to make sure to understand here i think this is not always emphasized as message it should be is that what we want is our missing data to be a completely random all right it'd be great you don't ever want to have missing data but when you pull it up and you find that there seems to be large amounts of data missing in one column or there appears to be equal amounts of missing data in two columns or whatever that pattern might be right whatever your human intelligence intuition looks at and says oh boy there looks to be patterns here it's non-trivial that there are um you know the same large amounts of missing data in one row as in another then you need to dig under the hood and try and figure out why all right so that could be a systemic problem with how the data is gathered right if you have that when you go to delete it what you're going to be doing is deleting some systemic error or issue with the data which means it would be a biased model because of that all right so that's a problem so what we wanted to do is i guess i gotta hand myself what we want it to be is totally at random and not missing not at random i don't know if you like do we really need acronyms for this but i mean we have them but i don't know if we need them but we want it to be missing uh at random all right so a nice way to just to get a good look at this all right is just use a summary function right on a data frame so you can do that with the beaches data set it has a little bit of missing data i would have stuck with empty cars but there's no missing data there so you can do that and it'll just show it should show you at the very bottom after it gives you kind of neat little summaries of you know the mean the mode the range all sorts of stuff like that for each of the each of the variables it should show at the bottom where missing data is so try and do that all right policy pause it here data frame you have to load beaches and the mice package well i know you have to load into my package yet i'm going to have you something to do with this summary is a base r function that'll be in base r and then check and see if you can find the missing data in the beaches data frame all right and then just come back okay now that you're back all right uh generally we wanted to avoid if there are missing values better than five percent i don't know i sometimes break this rule but if there's if this if more than five to ten percent something like that of the one particular variable is missing that's a big chunk all right so you have to figure out exactly why that is the case all right so you could delete those values all right that's one way to handle missing data there's not that many of them you just go ahead and delete them or you can try and attempt imputation right and so again the field of imputation is is just fast one of the the most simplest way to do it is mean imputation and that would just be to take the average of that row and put it in for the missing data all right uh i would encourage i would encourage you not to do that if you get anywhere close to this uh if there doesn't appear to be a pattern right in the data and you're under five percent maybe mean imputation is fine if there's there's not a pattern but you're above five percent that's a lot of mean average values to put in to replace you're probably better off deleting those um if there's no pattern if there's a pattern and it's above five percent you've got to figure out where the data is coming from and why this is occurring or due to a much more robust process for imputation otherwise you will almost certainly have biases built into your data okay so the mice package you'll have to load that in there is a function there called missing data pattern md dot pattern you can pass that in and it will show you a little bit more detail similar to what summary does about where that missing data is all right we can also use something called complete cases all right to remove the nas i'll show you how to do that in this slide all right so once we've identified where are we taking a look either through md pattern you can use that on beaches if you want to it should be just about the same information um in the mice package or just using summary and we've used our intuition about whether we think there's a pattern or not or how much of the data there is missing you know you can just do simple math there once you know the number of missing cases then complete cases is a way to remove there's a lot there's lots of ways to do this by the way is n a you can use like an exclamation point to get rid of the nas i mean there's there's lots of ways to do it this is just one okay the nice part about complete cases is it kind of works just as an index all right so you think of it index it just searches and it'll use boolean um indexing right to show whether something is missing or not just as true false true false true false throughout the entire data set all right so you have this data you know kind of this data frame full of trues and falses all right and here if you just have a single vector all right you know what a vector is it's just any one list of things all right not a list but anything but just one column of a particular variable type all right this is the variable here right you pass in complete cases it will create an index of the missing ones all right when you pass that into it all right and basically you're just here you're using the same variable here three times all right so this is creating the index inside and then it's reducing the space of that particular vector by just the true cases that are in here and then passing it into a new it's just replacing the old one all right so these are depending on what the number of missing values are it just replaces over there so it will complete cases as a function to identify those missing location of those missing values and delete them all right this is the same thing you can do it on a data frame here we're just in a much more kind of traditional row column format right identifies all the locations of where there are missing values and keep in mind it will delete the whole row all right so that's why this is in the row place remember when we do indexing it's row column okay so identify where all the rows are that have missing data and it'll delete them okay and then pass it back into your original data frame you also change the name of this data frame if you want just to be cautious say we'll call it data frame one or data frame clean or something and then here you can just get a little bit more specific right if you just want to be very discreet about where the rows are coming from right which columns it should search for all right then you could do it that way all right okay all right so try these uh on the uh anyway we did this on the on the we did this on the beaches dataset all right but then try and use this technique right here right and just remove the missing values all right use complete cases just in the same syntax that's located here right and see if you can get rid of those missing values all right all right okay partitioning and sampling all right so a big portion of what we do and after we've cleaned our data set got our variable classes in the right right um right class okay we've done 100 coding we've normalized we're doing all this data prep right we're getting trying to get ready for before machine learning right and the next step is to do data partitioning and sampling right it's kind of the same thing right so we need to split our data into three sections okay remember we have this is our large data set all right we need to separate it into training into tuning into testing right so typically the way you do this is something about one of my these are uneven 20 20 all right and this would be like 60 all right okay and so this 60 this big number up here we're going to reserve for training and the other two are going to be split equally between tune and test the reason we do this is that we want the data that we build the algorithm with the one that we learn it on to not be the one that we have evaluated on we're going to try and do some level of separation to try and give it new data you know we talked about this over and over again that you won't really know how it works until you put it out into the real world but this is one way to do it all right just to get started give yourself a chance at evaluating just how well the algorithm working and it's it's the way that machine learning is done all right so this is how we define these things this is how we build the original model all right some people get tuning can be a little confusing okay so tuning is the data that we pass in to the trained model by train meaning that we trained a model using sixty percent of it and the first thing we do is we evaluate it using that tune data all right and we do that principally because we're going to make changes to that algorithm all right we're going to change the hyper parameters right we're going to build more features okay we're going to maybe over sample we're going to under sample right there's all sorts of things that we can do to modify the changing process training process that will affect how that machine learning algorithm is learned right and so we're going to use the tuning set that every time we make a little tweak all right we're going to use the tuning set to see how well the model improved right and if it improves then we're going to keep those changes right if it doesn't improve we might go back and change something else and then we'll evaluate it again with the tuning set it's not until we're done until we think we're gonna make any more tweaks to the hyper parameters or to the data features that we're passing in or you know any of the other number of changes that you could make are we going to use that test set all right let me put that test set take it out put it on the shelf just forget about it all right we do that the very end all right until we've punished this data so we can't think of any more creative ways to to put it into our into our algorithm and make it perform better then we'll kick out test and we'll do like one final evaluation say all right all right we need all the changes now let's see how it did okay that's the idea behind these three different partitions right one that's a bulk for training another one is used to evaluate after work doing feature engineering and manipulating the algorithm to see how see how they would work and then very finally at the end we pass in this test data set all right the function we're going to use for this is create data partition as part of a carrot package and you can load this in if you want to the major setback with this is that it it's really designed just to do one partition all right so to solve this problem we run the function twice all right so we'll do one partition where we do 60 40. we'll get our 60 percent up here and then we'll just split this one again to get our two twenty sorry kind of annoying but you know oh well uh if we're in we're in python the data partition function they're actually much better you can set it out to what what partitions that you want works a little bit better but in this particular one in r it's a little bit more complicated to be able to do three partitions so we just run it twice as the easiest way to do it all right the important part and the reason we use the nice part about great data partition now that i've said lots of bad things about it is that it does allow us to do stratified sampling okay if you see code online if you're doing machine learning and people are not doing stratified stratified sampling their models cannot be trusted right stratified sampling allows you to maintain the distribution or the prevalence all right we know prevalence is the prevalence of the target variable in each one of our partitions if we don't do that you can imagine a scenario where we might create this final partition say and if we just had ones and zeros there would be no ones in it right it would just be all zeros or just be all zeros up here right and that would be very biased we would have no idea how well the algorithm was actually working or if our changes were effective okay so we need to make sure that we do stratified sampling to maintain the distribution of the class levels or if it's a continuous variable of the actual variance right so the variances are all the same you think about the samples are all the same like this if we were to run little density plots on each one of these partitions in order to be able to get a balanced evaluation right so that's the critical part about this sampling all right we need to make sure to do that all right okay we're going to talk more about this in class i just want to give you a sense about this on now cross validation is a way to train our model okay so we do cross-validation when we're doing this particular using the training data all right so we're doing can i circle that enough all right we're doing the training data okay the problem with doing just the kind of canonical splits that we've talked about is that the bigger we make the training set the smaller we make the test set so there's this constant tug of war between the two of them like what's the right balance do we need more training data but then we might not be able to evaluate it enough all right cross validation helps us solve this problem all right what it does all right we'll talk more about this this k is just a number all right typically 10 or 5 right in this example i think it's 4 i guess instead of dividing it instead of dividing our data into these big three parts what we'll do so we'll take this one big part here right we'll still do these two still get rid of those two parts but then we're going to train our model differently on this 60 instead of passing all of that 60 in what we're going to do is we're going to divide that up into little individual training and test sections okay all right so instead of learning the model one time all right we're going to learn the model in this example four times right so we're going to train it four times with say eighty percent of the data dedicated to train and twenty percent of it dedicated to test but that test set right it's gonna be hold out gonna be different than the next hold out and different than the next hold out and different in the next and in theory the way that this works right is that we get to use the entire data set right multiple times instead of just passing it all in at once and getting one big model all right so we create some variation this is akin if you're if you're familiar with the term it's essentially bootstrapping all right so we're taking many different samples all right of the same training data and learning many models so it's a way for us to get a better sense of what the best model is as compared to just running it one time the way this works is just runs it a bunch of times and then it just takes an average of the results just combines all the models together takes an average of the results and that's what you use as your model okay we'll talk more about this cross-validation does take some time to wrap your head around um so again i'm just introducing it here you know we'll talk a little bit and i'll show you how to do it once we get into some of the modeling processes but hopefully over time it'll come kind of it'll become intuitive to you okay all right so that's it this went a little longer than i i had hoped it was covered a lot of ground here and we're going to review a lot of these topics in in code all right on thursday i'll go through some examples and then we'll have a lab for you that has you practicing these techniques all right so the next phase of this all right after we've got through kind of the data preparatory phase you know gone through clean anyway done all the things is to move into model building itself right into solution development all right so here we're still kind of doing data prep you can think of this as kind of a combining maybe establish this combining these two steps all right well look i got a different color combining these two steps together once we move into initial model building all right thanks and i'll see everybody on thursday